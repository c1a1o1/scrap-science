[
	{
		"Title": "Deep Learning Computed Tomography",
		"Description": "Tobias Wrfl, Florin C. Ghesu, Vincent Christlein, Andreas Maier.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "In this paper, we demonstrate that image reconstruction can be expressed in terms of neural networks. We show that filtered back-projection can be mapped identically onto a deep neural network architecture. As for the case of iterative reconstruction, the straight forward realization as matrix multiplication is not feasible. Thus, we propose to compute the back-projection layer efficiently as fixed function and its gradient as projection operation. This allows a data-driven approach for joint optimization of correction steps in projection domain and image domain. As a proof of concept, we demonstrate that we are able to learn weightings and additional filter layers that consistently reduce the reconstruction error of a limited angle reconstruction by a factor of two while keeping the same computational complexity as filtered back-projection. We believe that this kind of learning approach can be extended to any common CT artifact compensation heuristic and will outperform hand-crafted artifact correction methods in the future.",
		"email": [
			"wuerflts@outlook.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_50",
		"source": "miccai",
		"year": 2016,
		"key": "9e55d494-b7ac-41a5-be04-dc0ce103287f",
		"use": 1
	},
	{
		"Title": "Compressed Sensing Dynamic MRI Reconstruction Using GPU-accelerated 3D Convolutional Sparse Coding",
		"Description": "Tran Minh Quan, Won-Ki Jeong.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "In this paper, we introduce a fast alternating method for reconstructing highly undersampled dynamic MRI data using 3D convolutional sparse coding. The proposed solution leverages Fourier Convolution Theorem to accelerate the process of learning a set of 3D filters and iteratively refine the MRI reconstruction based on the sparse codes found subsequently. In contrast to conventional CS methods which exploit the sparsity by applying universal transforms such as wavelet and total variation, our approach extracts and adapts the temporal information directly from the MRI data using compact shift-invariant 3D filters. We provide a highly parallel algorithm with GPU support for efficient computation, and therefore, the reconstruction outperforms CPU implementation of the state-of-the art dictionary learning-based approaches by up to two orders of magnitude.",
		"email": [
			"wkjeong@unist.ac.kr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_56",
		"source": "miccai",
		"year": 2016,
		"key": "6886df87-3ed1-40d7-a6e4-07c4dd1847b1",
		"use": 1
	},
	{
		"Title": "Learning-Based Topological Correction for Infant Cortical Surfaces",
		"Description": "Shijie Hao, Gang Li, Li Wang, Yu Meng, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Reconstruction of topologically correct and accurate cortical surfaces from infant MR images is of great importance in neuroimaging mapping of early brain development. However, due to rapid growth and ongoing myelination, infant MR images exhibit extremely low tissue contrast and dynamic appearance patterns, thus leading to much more topological errors (holes and handles) in the cortical surfaces derived from tissue segmentation results, in comparison to adult MR images which typically have good tissue contrast. Existing methods for topological correction either rely on the minimal correction criteria, or ad hoc rules based on image intensity priori, thus often resulting in erroneous correction and large anatomical errors in reconstructed infant cortical surfaces. To address these issues, we propose to correct topological errors by learning information from the anatomical references, i.e., manually corrected images. Specifically, in our method, we first locate candidate voxels of topologically defected regions by using a topology-preserving level set method. Then, by leveraging rich information of the corresponding patches from reference images, we build region-specific dictionaries from the anatomical references and infer the correct labels of candidate voxels using sparse representation. Notably, we further integrate these two steps into an iterative framework to enable gradual correction of large topological errors, which are frequently occurred in infant images and cannot be completely corrected using one-shot sparse representation. Extensive experiments on infant cortical surfaces demonstrate that our method not only effectively corrects the topological defects, but also leads to better anatomical consistency, compared to the state-of-the-art methods.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_26",
		"source": "miccai",
		"year": 2016,
		"key": "a14e3e55-bba2-43e8-9fda-1a197c013b1b",
		"use": 1
	},
	{
		"Title": "A Hybrid Multishape Learning Framework for Longitudinal Prediction of Cortical Surfaces and Fiber Tracts Using Neonatal Data",
		"Description": "Islem Rekik, Gang Li, Pew-Thian Yap, Geng Chen, Weili Lin, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Dramatic changes of the human brain during the first year of postnatal development are poorly understood due to their multifold complexity. In this paper, we present the first attempt to jointly predict, using neonatal data, the dynamic growth pattern of brain cortical surfaces (collection of 3D triangular faces) and fiber tracts (collection of 3D lines). These two entities are modeled jointly as a multishape (a set of interlinked shapes). We propose a hybrid learning-based multishape prediction framework that captures both the diffeomorphic evolution of the cortical surfaces and the non-diffeomorphic growth of fiber tracts. In particular, we learn a set of geometric and dynamic cortical features and fiber connectivity features that characterize the relationships between cortical surfaces and fibers at different timepoints (0, 3, 6, and 9 months of age). Given a new neonatal multishape at 0 month of age, we hierarchically predict, at 3, 6 and 9 months, the postnatal cortical surfaces vertex-by-vertex along with fibers connected to adjacent faces to these vertices. This is achieved using a new fiber-to-face metric that quantifies the similarity between multishapes. For validation, we propose several evaluation metrics to thoroughly assess the performance of our framework. The results confirm that our framework yields good prediction accuracy of complex neonatal multishape development within a few seconds.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_25",
		"source": "miccai",
		"year": 2016,
		"key": "c1700478-be59-420d-88fb-b5dcbd8875ea",
		"use": 1
	},
	{
		"Title": "7T-Guided Learning Framework for Improving the Segmentation of 3T MR Images",
		"Description": "Khosro Bahrami, Islem Rekik, Feng Shi, Yaozong Gao, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "The emerging era of ultra-high-field MRI using 7T MRI scanners dramatically improved sensitivity, image resolution, and tissue contrast when compared to 3T MRI scanners in examining various anatomical structures. The advantages of these high-resolution MR images include higher segmentation accuracy of MRI brain tissues. However, currently, accessibility to 7T MRI scanners remains much more limited than 3T MRI scanners due to technological and economical constraints. Hence, we propose in this work the first learning-based model that improves the segmentation of an input 3T MR image with any conventional segmentation method, through the reconstruction of a higher-quality 7T-like MR image, without actually acquiring an ultra-high-field 7T MRI. Our proposed framework comprises two main steps. First, we estimate a non-linear mapping from 3T MRI to 7T MRI space, using random forest regression model with novel weighting and ensembling schemes, to reconstruct initial 7T-like MR images. Second, we use a group sparse representation with a new pre-selection approach to further refine the 7T-like MR image reconstruction. We evaluated our 7T MRI reconstruction results along with their segmentation results using 13 subjects acquired with both 3T and 7T MR images. For tissue segmentation, we applied two widely used segmentation methods (FAST and SPM) to perform the experiments. Our results showed (1) the improvement of WM, GM and CSF brain tissues segmentation results when guided by reconstructed 7T-like images compared to 3T MR images, and (2) the outperformance of the proposed 7T MRI reconstruction method when compared to other state-of-the-art methods.",
		"email": [
			"dinggang_shen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_66",
		"source": "miccai",
		"year": 2016,
		"key": "f348f9dd-d412-491c-ab55-530a27ff0cf0",
		"use": 1
	},
	{
		"Title": "Real-Time Standard Scan Plane Detection and Localisation in Fetal Ultrasound Using Fully Convolutional Neural Networks",
		"Description": "Christian F. Baumgartner, Konstantinos Kamnitsas, Jacqueline Matthew, Sandra Smith, Bernhard Kainz, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Fetal mid-pregnancy scans are typically carried out according to fixed protocols. Accurate detection of abnormalities and correct biometric measurements hinge on the correct acquisition of clearly defined standard scan planes. Locating these standard planes requires a high level of expertise. However, there is a worldwide shortage of expert sonographers. In this paper, we consider a fully automated system based on convolutional neural networks which can detect twelve standard scan planes as defined by the UK fetal abnormality screening programme. The network design allows real-time inference and can be naturally extended to provide an approximate localisation of the fetal anatomy in the image. Such a framework can be used to automate or assist with scan plane selection, or for the retrospective retrieval of scan planes from recorded videos. The method is evaluated on a large database of 1003 volunteer mid-pregnancy scans. We show that standard planes acquired in a clinical scenario are robustly detected with a precision and recall of 69 % and 80 %, which is superior to the current state-of-the-art. Furthermore, we show that it can retrospectively retrieve correct scan planes with an accuracy of 71 % for cardiac views and 81 % for non-cardiac views.",
		"email": [
			"c.baumgartner@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_24",
		"source": "miccai",
		"year": 2016,
		"key": "d887ae66-0e4d-4940-85c0-536d1d6db1be",
		"use": 1
	},
	{
		"Title": "A Hierarchical Convolutional Neural Network for Mitosis Detection in Phase-Contrast Microscopy Images",
		"Description": "Yunxiang Mao, Zhaozheng Yin.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "We propose a Hierarchical Convolution Neural Network (HCNN) for mitosis event detection in time-lapse phase contrast microscopy. Our method contains two stages: first, we extract candidate spatial-temporal patch sequences in the input image sequences which potentially contain mitosis events. Then, we identify if each patch sequence contains mitosis event or not using a hieratical convolutional neural network. In the experiments, we validate the design of our proposed architecture and evaluate the mitosis event detection performance. Our method achieves 99.1 % precision and 97.2 % recall in very challenging image sequences of multipolar-shaped C3H10T1/2 mesenchymal stem cells and outperforms other state-of-the-art methods. Furthermore, the proposed method does not depend on hand-crafted feature design or cell tracking. It can be straightforwardly adapted to event detection of other different cell types.",
		"email": [
			"yinz@mst.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_79",
		"source": "miccai",
		"year": 2016,
		"key": "dc0a4f99-72b0-43c3-8768-1edde309d927",
		"use": 1
	},
	{
		"Title": "Identifying Patients at Risk for Aortic Stenosis Through Learning from Multimodal Data",
		"Description": "Tanveer Syeda-Mahmood, Yanrong Guo, Mehdi Moradi, D. Beymer, D. Rajan, Yu Cao, Yaniv Gur, Mohammadreza Negahdar.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "In this paper we present a new method of uncovering patients with aortic valve diseases in large electronic health record systems through learning with multimodal data. The method automatically extracts clinically-relevant valvular disease features from five multimodal sources of information including structured diagnosis, echocardiogram reports, and echocardiogram imaging studies. It combines these partial evidence features in a random forests learning framework to predict patients likely to have the disease. Results of a retrospective clinical study from a 1000 patient dataset are presented that indicate that over 25 % new patients with moderate to severe aortic stenosis can be automatically discovered by our method that were previously missed from the records.",
		"email": [
			"stf@us.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_28",
		"source": "miccai",
		"year": 2016,
		"key": "800ec847-c45e-4f3e-ad19-888b27cb4db7",
		"use": 1
	},
	{
		"Title": "Multi-input Cardiac Image Super-Resolution Using Convolutional Neural Networks",
		"Description": "Ozan Oktay, Wenjia Bai, Matthew Lee, Ricardo Guerrero, Konstantinos Kamnitsas, Jose Caballero, Antonio de Marvao, Stuart Cook, Declan ORegan, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "3D cardiac MR imaging enables accurate analysis of cardiac morphology and physiology. However, due to the requirements for long acquisition and breath-hold, the clinical routine is still dominated by multi-slice 2D imaging, which hamper the visualization of anatomy and quantitative measurements as relatively thick slices are acquired. As a solution, we propose a novel image super-resolution (SR) approach that is based on a residual convolutional neural network (CNN) model. It reconstructs high resolution 3D volumes from 2D image stacks for more accurate image analysis. The proposed model allows the use of multiple input data acquired from different viewing planes for improved performance. Experimental results on 1233 cardiac short and long-axis MR image stacks show that the CNN model outperforms state-of-the-art SR methods in terms of image quality while being computationally efficient. Also, we show that image segmentation and motion tracking benefits more from SR-CNN when it is used as an initial upscaling method than conventional interpolation methods for the subsequent analysis.",
		"email": [
			"o.oktay13@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_29",
		"source": "miccai",
		"year": 2016,
		"key": "adc9c811-a3c2-454d-9c30-87ced207e823",
		"use": 1
	},
	{
		"Title": "3D Deep Learning for Multi-modal Imaging-Guided Survival Time Prediction of Brain Tumor Patients",
		"Description": "Dong Nie, Han Zhang, Ehsan Adeli, Luyan Liu, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "High-grade glioma is the most aggressive and severe brain tumor that leads to death of almost 50 % patients in 12 years. Thus, accurate prognosis for glioma patients would provide essential guidelines for their treatment planning. Conventional survival prediction generally utilizes clinical information and limited handcrafted features from magnetic resonance images (MRI), which is often time consuming, laborious and subjective. In this paper, we propose using deep learning frameworks to automatically extract features from multi-modal preoperative brain images (i.e., T1 MRI, fMRI and DTI) of high-grade glioma patients. Specifically, we adopt 3D convolutional neural networks (CNNs) and also propose a new network architecture for using multi-channel data and learning supervised features. Along with the pivotal clinical features, we finally train a support vector machine to predict if the patient has a long or short overall survival (OS) time. Experimental results demonstrate that our methods can achieve an accuracy as high as 89.9 % We also find that the learned features from fMRI and DTI play more important roles in accurately predicting the OS time, which provides valuable insights into functional neuro-oncological applications.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_25",
		"source": "miccai",
		"year": 2016,
		"key": "6db9349a-0156-4513-bdf0-473317bed8d1",
		"use": 1
	},
	{
		"Title": "Progressive Graph-Based Transductive Learning for Multi-modal Classification of Brain Disorder Disease",
		"Description": "Zhengxia Wang, Xiaofeng Zhu, Ehsan Adeli, Yingying Zhu, Chen Zu, Feiping Nie, Dinggang Shen, Guorong Wu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Graph-based Transductive Learning (GTL) is a powerful tool in computer-assisted diagnosis, especially when the training data is not sufficient to build reliable classifiers. Conventional GTL approaches first construct a fixed subject-wise graph based on the similarities of observed features (i.e., extracted from imaging data) in the feature domain, and then follow the established graph to propagate the existing labels from training to testing data in the label domain. However, such a graph is exclusively learned in the feature domain and may not be necessarily optimal in the label domain. This may eventually undermine the classification accuracy. To address this issue, we propose a progressive GTL (pGTL) method to progressively find an intrinsic data representation. To achieve this, our pGTL method iteratively (1) refines the subject-wise relationships observed in the feature domain using the learned intrinsic data representation in the label domain, (2) updates the intrinsic data representation from the refined subject-wise relationships, and (3) verifies the intrinsic data representation on the training data, in order to guarantee an optimal classification on the new testing data. Furthermore, we extend our pGTL to incorporate multi-modal imaging data, to improve the classification accuracy and robustness as multi-modal imaging data can provide complementary information. Promising classification results in identifying Alzheimers disease (AD), Mild Cognitive Impairment (MCI), and Normal Control (NC) subjects are achieved using MRI and PET data.",
		"email": [
			"grwu@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_34",
		"source": "miccai",
		"year": 2016,
		"key": "4bf6add7-7739-4172-932b-b6b0192c7669",
		"use": 1
	},
	{
		"Title": "New Multi-task Learning Model to Predict Alzheimers Disease Cognitive Assessment",
		"Description": "Zhouyuan Huo, Dinggang Shen, Heng Huang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "As a neurodegenerative disorder, the Alzheimers disease (AD) status can be characterized by the progressive impairment of memory and other cognitive functions. Thus, it is an important topic to use neuroimaging measures to predict cognitive performance and track the progression of AD. Many existing cognitive performance prediction methods employ the regression models to associate cognitive scores to neuroimaging measures, but these methods do not take into account the interconnected structures within imaging data and those among cognitive scores. To address this problem, we propose a novel multi-task learning model for minimizing the k smallest singular values to uncover the underlying low-rank common subspace and jointly analyze all the imaging and clinical data. The effectiveness of our method is demonstrated by the clearly improved prediction performances in all empirical AD cognitive scores prediction cases.",
		"email": [
			"heng@uta.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_37",
		"source": "miccai",
		"year": 2016,
		"key": "2386a8ac-afa8-4f6e-b896-611d4c465c73",
		"use": 1
	},
	{
		"Title": "Diagnosis of Alzheimers Disease Using View-Aligned Hypergraph Learning with Incomplete Multi-modality Data",
		"Description": "Mingxia Liu, Jun Zhang, Pew-Thian Yap, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Effectively utilizing incomplete multi-modality data for diagnosis of Alzheimers disease (AD) is still an area of active research. Several multi-view learning methods have recently been developed to deal with missing data, with each view corresponding to a specific modality or a combination of several modalities. However, existing methods usually ignore the underlying coherence among views, which may lead to sub-optimal learning performance. In this paper, we propose a view-aligned hypergraph learning (VAHL) method to explicitly model the coherence among the views. Specifically, we first divide the original data into several views based on possible combinations of modalities, followed by a sparse representation based hypergraph construction process in each view. A view-aligned hypergraph classification (VAHC) model is then proposed, by using a view-aligned regularizer to model the view coherence. We further assemble the class probability scores generated from VAHC via a multi-view label fusion method to make a final classification decision. We evaluate our method on the baseline ADNI-1 database having 807 subjects and three modalities (i.e., MRI, PET, and CSF). Our method achieves at least a \\(4.6\\,\\%\\) improvement in classification accuracy compared with state-of-the-art methods for AD/MCI diagnosis.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_36",
		"source": "miccai",
		"year": 2016,
		"key": "b6a2fc3c-4831-43da-a074-ba57acabf333",
		"use": 1
	},
	{
		"Title": "Subtype Cell Detection with an Accelerated Deep Convolution Neural Network",
		"Description": "Sheng Wang, Jiawen Yao, Zheng Xu, Junzhou Huang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Robust cell detection in histopathological images is a crucial step in the computer-assisted diagnosis methods. In addition, recent studies show that subtypes play an significant role in better characterization of tumor growth and outcome prediction. In this paper, we propose a novel subtype cell detection method with an accelerated deep convolution neural network. The proposed method not only detects cells but also gives subtype cell classification for the detected cells. Based on the subtype cell detection results, we extract subtype cell related features and use them in survival prediction. We demonstrate that our proposed method has excellent subtype cell detection performance and our proposed subtype cell features can achieve more accurate survival prediction.",
		"email": [
			"jzhuang@uta.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_74",
		"source": "miccai",
		"year": 2016,
		"key": "3f3f637f-3add-4124-9970-1822ad2edf60",
		"use": 1
	},
	{
		"Title": "3D Segmentation of Glial Cells Using Fully Convolutional Networks and k-Terminal Cut",
		"Description": "Lin Yang, Yizhe Zhang, Ian H. Guldner, Siyuan Zhang, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Glial cells play an important role in regulating synaptogenesis, development of blood-brain barrier, and brain tumor metastasis. Quantitative analysis of glial cells can offer new insights to many studies. However, the complicated morphology of the protrusions of glial cells and the entangled cell-to-cell network cause significant difficulties to extracting quantitative information in images. In this paper, we present a new method for instance-level segmentation of glial cells in 3D images. First, we obtain accurate voxel-level segmentation by leveraging the recent advances of fully convolutional networks (FCN). Then we develop a k-terminal cut algorithm to disentangle the complex cell-to-cell connections. During the cell cutting process, to better capture the nature of glial cells, a shape prior computed based on a multiplicative Voronoi diagram is exploited. Extensive experiments using real 3D images show that our method has superior performance over the state-of-the-art methods.",
		"email": [
			"lyang5@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_76",
		"source": "miccai",
		"year": 2016,
		"key": "2d881c46-b6a3-44ba-9077-95770e51511e",
		"use": 1
	},
	{
		"Title": "Coronary Centerline Extraction via Optimal Flow Paths and CNN Path Pruning",
		"Description": "Mehmet A. Glsn, Gareth Funka-Lea, Puneet Sharma, Saikiran Rapaka, Yefeng Zheng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "We present a novel method for the automated extraction of blood vessel centerlines. There are two major contributions. First, in order to avoid the shortcuts to which minimal path methods are prone, we find optimal paths in a computed flow field. We solve for a steady state porous media flow inside a region of interest and trace centerlines as maximum flow paths. We explain how to estimate anisotropic orientation tensors which are used as permeability tensors in our flow field computation. Second, we introduce a convolutional neural network (CNN) classifier for removing extraneous paths in the detected centerlines. We apply our method to the extraction of coronary artery centerlines found in Computed Tomography Angiography (CTA). The robustness and stability of our method are enhanced by using a model-based detection of coronary specific territories and main branches to constrain the search space [15]. Validation against 20 comprehensively annotated datasets had a sensitivity and specificity at or above 90 %. Validation against 106 clinically annotated coronary arteries showed a sensitivity above 97 %.",
		"email": [
			"akif.gulsun@siemens.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_37",
		"source": "miccai",
		"year": 2016,
		"key": "33002db3-c936-45fe-b865-dabe075e258a",
		"use": 1
	},
	{
		"Title": "Recognizing End-Diastole and End-Systole Frames via Deep Temporal Regression Network",
		"Description": "Bin Kong, Yiqiang Zhan, Min Shin, Thomas Denny, Shaoting Zhang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "Accurate measurement of left ventricular volumes and Ejection Fraction from cine MRI is of paramount importance to the evaluation of cardiovascular functions, yet it usually requires laborious and tedious work of trained experts to interpret them. To facilitate this procedure, numerous computer aided diagnosis (CAD) methods and tools have been proposed, most of which focus on the left or right ventricle segmentation. However, the identification of ES and ED frames from cardiac sequences is largely ignored, which is a key procedure in the automated workflow. This seemingly easy task is quite challenging, due to the requirement of high accuracy (i.e., precisely identifying specific frames from a sequence) and subtle differences among consecutive frames. Recently, with the rapid growth of annotated data and the increasing computational power, deep learning methods have been widely exploited in medical image analysis. In this paper, we propose a novel deep learning architecture, named as temporal regression network (TempReg-Net), to accurately identify specific frames from MRI sequences, by integrating the Convolutional Neural Network (CNN) with the Recurrent Neural Network (RNN). Specifically, a CNN encodes the spatial information of a cardiac sequence, and a RNN decodes the temporal information. In addition, we design a new loss function in our network to constrain the structure of predicted labels, which further improves the performance. Our approach is extensively validated on thousands of cardiac sequences and the average difference is merely 0.4 frames, comparing favorably with previous systems.",
		"email": [
			"szhang16@uncc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_31",
		"source": "miccai",
		"year": 2016,
		"key": "727fff8e-0874-46e5-8244-24e1b1fdd6c4",
		"use": 1
	},
	{
		"Title": "Automatic Lymph Node Cluster Segmentation Using Holistically-Nested Neural Networks and Structured Optimization in CT Images",
		"Description": "Isabella Nogues, Le Lu, Xiaosong Wang, Holger Roth, Gedas Bertasius, Nathan Lay, Jianbo Shi, Yohannes Tsehay, Ronald M. Summers.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Lymph node segmentation is an important yet challenging problem in medical image analysis. The presence of enlarged lymph nodes (LNs) signals the onset or progression of a malignant disease or infection. In the thoracoabdominal (TA) body region, neighboring enlarged LNs often spatially collapse into swollen lymph node clusters (LNCs) (up to 9 LNs in our dataset). Accurate segmentation of TA LNCs is complexified by the noticeably poor intensity and texture contrast among neighboring LNs and surrounding tissues, and has not been addressed in previous work. This paper presents a novel approach to TA LNC segmentation that combines holistically-nested neural networks (HNNs) and structured optimization (SO). Two HNNs, built upon recent fully convolutional networks (FCNs) and deeply supervised networks (DSNs), are trained to learn the LNC appearance (HNN-A) or contour (HNN-C) probabilistic output maps, respectively. HNN first produces the class label maps with the same resolution as the input image, like FCN. Afterwards, HNN predictions for LNC appearance and contour cues are formulated into the unary and pairwise terms of conditional random fields (CRFs), which are subsequently solved using one of three different SO methods: dense CRF, graph cuts, and boundary neural fields (BNF). BNF yields the highest quantitative results. Its mean Dice coefficient between segmented and ground truth LN volumes is 82.1 %  9.6 %, compared to 73.0 %  17.6 % for HNN-A alone. The LNC relative volume (\\(cm^3\\)) difference is 13.7 %  13.1 %, a promising result for the development of LN imaging biomarkers based on volumetric measurements.",
		"email": [
			"isabella.nogues@nih.gov"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_45",
		"source": "miccai",
		"year": 2016,
		"key": "a7381612-159e-4da4-9d3a-9e0f853b61bd",
		"use": 1
	},
	{
		"Title": "Automated Segmentation of Knee MRI Using Hierarchical Classifiers and Just Enough Interaction Based Learning: Data from Osteoarthritis Initiative",
		"Description": "Satyananda Kashyap, Ipek Oguz, Honghai Zhang, Milan Sonka.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "We present a fully automated learning-based approach for segmenting knee cartilage in presence of osteoarthritis (OA). The algorithm employs a hierarchical set of two random forest classifiers. The first is a neighborhood approximation forest, the output probability map of which is utilized as a feature set for the second random forest (RF) classifier. The output probabilities of the hierarchical approach are used as cost functions in a Layered Optimal Graph Segmentation of Multiple Objects and Surfaces (LOGISMOS). In this work, we highlight a novel post-processing interaction called just-enough interaction (JEI) which enables quick and accurate generation of a large set of training examples. Disjoint sets of 15 and 13 subjects were used for training and tested on another disjoint set of 53 knee datasets. All images were acquired using double echo steady state (DESS) MRI sequence and are from the osteoarthritis initiative (OAI) database. Segmentation performance using the learning-based cost function showed significant reduction in segmentation errors (\\(p< 0.05\\)) in comparison with conventional gradient-based cost functions.",
		"email": [
			"satyananda-kashyap@uiowa.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_40",
		"source": "miccai",
		"year": 2016,
		"key": "36d4b2a4-c78b-4fd4-a56d-6279eca93e33",
		"use": 1
	},
	{
		"Title": "Automatic Liver and Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D Conditional Random Fields",
		"Description": "Patrick Ferdinand Christ, Mohamed Ezzeldin A. Elshaer, Florian Ettlinger, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Marco Armbruster, Felix Hofmann, Melvin DAnastasi, Wieland H. Sommer, Seyed-Ahmad Ahmadi, Bjoern H. Menze.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Automatic segmentation of the liver and its lesion is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT abdomen images using cascaded fully convolutional neural networks (CFCNs) and dense 3D conditional random fields (CRFs). We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions from the predicted liver ROIs of step 1. We refine the segmentations of the CFCN using a dense 3D CRF that accounts for both spatial coherence and appearance. CFCN models were trained in a 2-fold cross-validation on the abdominal CT dataset 3DIRCAD comprising 15 hepatic tumor volumes. Our results show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over \\(94\\,\\%\\) for liver with computation times below 100 s per volume. We experimentally demonstrate the robustness of the proposed method as a decision support system with a high accuracy and speed for usage in daily clinical routine.",
		"email": [
			"Patrick.Christ@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_48",
		"source": "miccai",
		"year": 2016,
		"key": "7494d0c9-6cbb-406f-b6ed-684285f00504",
		"use": 1
	},
	{
		"Title": "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation",
		"Description": "zgn iek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox, Olaf Ronneberger.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.",
		"email": [
			"cicek@cs.uni-freiburg.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_49",
		"source": "miccai",
		"year": 2016,
		"key": "a39bb370-35b1-4b54-bb60-299b16506148",
		"use": 1
	},
	{
		"Title": "Topology Aware Fully Convolutional Networks for Histology Gland Segmentation",
		"Description": "Acha BenTaieb, Ghassan Hamarneh.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "The recent success of deep learning techniques in classification and object detection tasks has been leveraged for segmentation tasks. However, a weakness of these deep segmentation models is their limited ability to encode high level shape priors, such as smoothness and preservation of complex interactions between object regions, which can result in implausible segmentations. In this work, by formulating and optimizing a new loss, we introduce the first deep network trained to encode geometric and topological priors of containment and detachment. Our results on the segmentation of histology glands from a dataset of 165 images demonstrate the advantage of our novel loss terms and show how our topology aware architecture outperforms competing methods by up to 10 % in both pixel-level accuracy and object-level Dice.",
		"email": [
			"abentaie@sfu.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_53",
		"source": "miccai",
		"year": 2016,
		"key": "74904754-69a8-474f-a7bf-c036f0ac13d4",
		"use": 1
	},
	{
		"Title": "Pancreas Segmentation in MRI Using Graph-Based Decision Fusion on Convolutional Neural Networks",
		"Description": "Jinzheng Cai, Le Lu, Zizhao Zhang, Fuyong Xing, Lin Yang, Qian Yin.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Automated pancreas segmentation in medical images is a prerequisite for many clinical applications, such as diabetes inspection, pancreatic cancer diagnosis, and surgical planing. In this paper, we formulate pancreas segmentation in magnetic resonance imaging (MRI) scans as a graph based decision fusion process combined with deep convolutional neural networks (CNN). Our approach conducts pancreatic detection and boundary segmentation with two types of CNN models respectively: (1) the tissue detection step to differentiate pancreas and non-pancreas tissue with spatial intensity context; (2) the boundary detection step to allocate the semantic boundaries of pancreas. Both detection results of the two networks are fused together as the initialization of a conditional random field (CRF) framework to obtain the final segmentation output. Our approach achieves the mean dice similarity coefficient (DSC) \\(76.1\\,\\%\\) with the standard deviation of \\(8.7\\,\\%\\) in a dataset containing 78 abdominal MRI scans. The proposed algorithm achieves the best results compared with other state of the arts.",
		"email": [
			"lin.yang@bme.ufl.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_51",
		"source": "miccai",
		"year": 2016,
		"key": "c7074c99-d48a-49b7-831e-efaca031d63e",
		"use": 1
	},
	{
		"Title": "Model-Based Segmentation of Vertebral Bodies from MR Images with 3D CNNs",
		"Description": "Robert Korez, Botjan Likar, Franjo Pernu, Toma Vrtovec.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "We propose an automated method for supervised segmentation of vertebral bodies (VBs) from three-dimensional (3D) magnetic resonance (MR) spine images that is based on coupling deformable models with convolutional neural networks (CNNs). We designed a 3D CNN architecture that learns the appearance from a training set of VBs to generate 3D spatial VB probability maps, which guide deformable models towards VB boundaries. The proposed method was applied to segment 161 VBs from 3D MR spine images of 23 subjects, and the results were compared to reference segmentations. By yielding an overall Dice similarity coefficient of \\(93.4\\,{\\pm }\\,1.7\\,\\%\\), mean symmetric surface distance of \\(0.54\\,{\\pm }\\,0.14\\,\\text {mm}\\) and Hausdorff distance of \\(3.83\\,{\\pm }\\,1.04\\,\\text {mm}\\), the proposed method proved superior to existing VB segmentation methods.",
		"email": [
			"tomaz.vrtovec@fe.uni-lj.si"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_50",
		"source": "miccai",
		"year": 2016,
		"key": "6b6e0bef-95bb-488b-a014-78b7fc7c6b68",
		"use": 1
	},
	{
		"Title": "Gland Instance Segmentation by Deep Multichannel Side Supervision",
		"Description": "Yan Xu, Yang Li, Mingyuan Liu, Yipei Wang, Maode Lai, Eric I-Chao Chang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "In this paper, we propose a new image instance segmentation method that segments individual glands (instances) in colon histology images. This is a task called instance segmentation that has recently become increasingly important. The problem is challenging since not only do the glands need to be segmented from the complex background, they are also required to be individually identified. Here we leverage the idea of image-to-image prediction in recent deep learning by building a framework that automatically exploits and fuses complex multichannel information, regional and boundary patterns, with side supervision (deep supervision on side responses) in gland histology images. Our proposed system, deep multichannel side supervision (DMCS), alleviates heavy feature design due to the use of convolutional neural networks guided by side supervision. Compared to methods reported in the 2015 MICCAI Gland Segmentation Challenge, we observe state-of-the-art results based on a number of evaluation metrics.",
		"email": [
			"echang@microsoft.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_57",
		"source": "miccai",
		"year": 2016,
		"key": "4233649d-605f-46c2-8ddd-03aef95e8f7b",
		"use": 1
	},
	{
		"Title": "Iterative Multi-domain Regularized Deep Learning for Anatomical Structure Detection and Segmentation from Ultrasound Images",
		"Description": "Hao Chen, Yefeng Zheng, Jin-Hyeong Park, Pheng-Ann Heng, S. Kevin Zhou.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Accurate detection and segmentation of anatomical structures from ultrasound images are crucial for clinical diagnosis and biometric measurements. Although ultrasound imaging has been widely used with superiorities such as low cost and portability, the fuzzy border definition and existence of abounding artifacts pose great challenges for automatically detecting and segmenting the complex anatomical structures. In this paper, we propose a multi-domain regularized deep learning method to address this challenging problem. By leveraging the transfer learning from cross domains, the feature representations are effectively enhanced. The results are further improved by the iterative refinement. Moreover, our method is quite efficient by taking advantage of a fully convolutional network, which is formulated as an end-to-end learning framework of detection and segmentation. Extensive experimental results on a large-scale database corroborated that our method achieved a superior detection and segmentation accuracy, outperforming other methods by a significant margin and demonstrating competitive capability even compared to human performance.",
		"email": [
			"hchen@cse.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_56",
		"source": "miccai",
		"year": 2016,
		"key": "9ebd23fb-3726-4725-867d-2d4262b29437",
		"use": 1
	},
	{
		"Title": "Deep Learning for Multi-task Medical Image Segmentation in Multiple Modalities",
		"Description": "Pim Moeskops, Jelmer M. Wolterink, Bas H. M. van der Velden, Kenneth G. A. Gilhuijs, Tim Leiner, Max A. Viergever, Ivana Igum.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Automatic segmentation of medical images is an important task for many clinical applications. In practice, a wide range of anatomical structures are visualised using different imaging modalities. In this paper, we investigate whether a single convolutional neural network (CNN) can be trained to perform different segmentation tasks.",
		"email": [
			"pim@isi.uu.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_55",
		"source": "miccai",
		"year": 2016,
		"key": "bd27eabe-d266-4d83-a11e-d2032d612f88",
		"use": 1
	},
	{
		"Title": "Enhanced Probabilistic Label Fusion by Estimating Label Confidences Through Discriminative Learning",
		"Description": "Oualid M. Benkarim, Gemma Piella, Miguel Angel Gonzlez Ballester, Gerard Sanroma.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Multiple-atlas segmentation has recently shown success in automatic segmentation of brain images. It consists in registering the labelmaps from a set of atlases to the anatomy of a target image, and then fusing the multiple labelmaps into a consensus segmentation on the target image. Accurately estimating the confidence of each atlas decision is key for the success of label fusion. Common approaches either rely on local patch similarity, probabilistic statistical frameworks or a combination of both. We present a probabilistic label fusion framework that takes into account label confidence at each point. Maximum likelihood atlas confidences are estimated by explicitly modelling the relationship between image appearance and segmentation errors. We also propose a novel type of label-dependent appearance features based on atlas labelmaps. Our results indicate that the proposed label fusion framework achieves state-of-the-art performance in the segmentation of subcortical structures.",
		"email": [
			"oualid.benkarim@upf.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_58",
		"source": "miccai",
		"year": 2016,
		"key": "2584d931-a9ed-46df-9b0e-a19b77ef4ea8",
		"use": 1
	},
	{
		"Title": "Learning Optimization Updates for Multimodal Registration",
		"Description": "Benjamn Gutirrez-Becker, Diana Mateus, Loc Peter, Nassir Navab.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "We address the problem of multimodal image registration using a supervised learning approach. We pose the problem as a regression task, whose goal is to estimate the unknown geometric transformation from the joint appearance of the fixed and moving images. Our method is based on (i) context-aware features, which allow us to guide the registration using not only local, but also global structural information, and (ii) regression forests to map the very large contextual feature space to transformation parameters. Our approach improves the capture range, as we demonstrate on the publicly available IXI dataset. Furthermore, it can also handle difficult settings where other similarity metrics tend to fail; for instance, we show results on the deformable registration of Intravascular Ultrasound (IVUS) and Histology images.",
		"email": [
			"gutierrez.becker@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_3",
		"source": "miccai",
		"year": 2016,
		"key": "cda0b951-8b02-46f9-be0d-cce064c55f14",
		"use": 1
	},
	{
		"Title": "A Deep Metric for Multimodal Registration",
		"Description": "Martin Simonovsky, Benjamn Gutirrez-Becker, Diana Mateus, Nassir Navab, Nikos Komodakis.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "Multimodal registration is a challenging problem due the high variability of tissue appearance under different imaging modalities. The crucial component here is the choice of the right similarity measure. We make a step towards a general learning-based solution that can be adapted to specific situations and present a metric based on a convolutional neural network. Our network can be trained from scratch even from a few aligned image pairs. The metric is validated on intersubject deformable registration on a dataset different from the one used for training, demonstrating good generalization. In this task, we outperform mutual information by a significant margin.",
		"email": [
			"martin.simonovsky@enpc.fr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_2",
		"source": "miccai",
		"year": 2016,
		"key": "d74f2664-440e-4019-914c-93fc62ad5ccc",
		"use": 1
	},
	{
		"Title": "Learning-Based Multimodal Image Registration for Prostate Cancer Radiation Therapy",
		"Description": "Xiaohuan Cao, Yaozong Gao, Jianhua Yang, Guorong Wu, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "Computed tomography (CT) is widely used for dose planning in the radiotherapy of prostate cancer. However, CT has low tissue contrast, thus making manual contouring difficult. In contrast, magnetic resonance (MR) image provides high tissue contrast and is thus ideal for manual contouring. If MR image can be registered to CT image of the same patient, the contouring accuracy of CT could be substantially improved, which could eventually lead to high treatment efficacy. In this paper, we propose a learning-based approach for multimodal image registration. First, to fill the appearance gap between modalities, a structured random forest with auto-context model is learnt to synthesize MRI from CT and vice versa. Then, MRI-to-CT registration is steered in a dual manner of registering images with same appearances, i.e., (1) registering the synthesized CT with CT, and (2) also registering MRI with the synthesized MRI. Next, a dual-core deformation fusion framework is developed to iteratively and effectively combine these two registration results. Experiments on pelvic CT and MR images have shown the improved registration performance by our proposed method, compared with the existing non-learning based registration methods.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_1",
		"source": "miccai",
		"year": 2016,
		"key": "31fcd870-a6d4-4ba4-8426-d7ba194efb0b",
		"use": 1
	},
	{
		"Title": "Deep Fusion Net for Multi-atlas Segmentation: Application to Cardiac MR Images",
		"Description": "Heran Yang, Jian Sun, Huibin Li, Lisheng Wang, Zongben Xu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Atlas selection and label fusion are two major challenges in multi-atlas segmentation. In this paper, we propose a novel deep fusion net for better solving these challenges. Deep fusion net is a deep architecture by concatenating a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. This network is trained end-to-end for automatically learning deep features achieving optimal performance in a NL-PLF framework. The learned deep features are further utilized in defining a similarity measure for atlas selection. Experimental results on Cardiac MR images for left ventricular segmentation demonstrate that our approach is effective both in atlas selection and multi-atlas label fusion, and achieves state of the art in performance.",
		"email": [
			"jiansun@mail.xjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_60",
		"source": "miccai",
		"year": 2016,
		"key": "b1656d93-32e6-4d8a-8df4-b2a5d68ce42b",
		"use": 1
	},
	{
		"Title": "Boundary Mapping Through Manifold Learning for Connectivity-Based Cortical Parcellation",
		"Description": "Salim Arslan, Sarah Parisot, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "The study of the human connectome is becoming more popular due to its potential to reveal the brain function and structure. A critical step in connectome analysis is to parcellate the cortex into coherent regions that can be used to build graphical models of connectivity. Computing an optimal parcellation is of great importance, as this stage can affect the performance of the subsequent analysis. To this end, we propose a new parcellation method driven by structural connectivity estimated from diffusion MRI. We learn a manifold from the local connectivity properties of an individual subject and identify parcellation boundaries as points in this low-dimensional embedding where the connectivity patterns change. We compute spatially contiguous and non-overlapping parcels from these boundaries after projecting them back to the native cortical surface. Our experiments with a set of 100 subjects show that the proposed method can produce parcels with distinct patterns of connectivity and a higher degree of homogeneity at varying resolutions compared to the state-of-the-art methods, hence can potentially provide a more reliable set of network nodes for connectome analysis.",
		"email": [
			"s.arslan13@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_14",
		"source": "miccai",
		"year": 2016,
		"key": "d4ee0eab-8ec3-4db5-97c9-a11fa6a9e86e",
		"use": 1
	},
	{
		"Title": "Recognizing Surgical Activities with Recurrent Neural Networks",
		"Description": "Robert DiPietro, Colin Lea, Anand Malpani, Narges Ahmidi, S. Swaroop Vedula, Gyusung I. Lee, Mija R. Lee, Gregory D. Hager.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "We apply recurrent neural networks to the task of recognizing surgical activities from robot kinematics. Prior work in this area focuses on recognizing short, low-level activities, or gestures, and has been based on variants of hidden Markov models and conditional random fields. In contrast, we work on recognizing both gestures and longer, higher-level activites, or maneuvers, and we model the mapping from kinematics to gestures/maneuvers with recurrent neural networks. To our knowledge, we are the first to apply recurrent neural networks to this task. Using a single model and a single set of hyperparameters, we match state-of-the-art performance for gesture recognition and advance state-of-the-art performance for maneuver recognition, in terms of both accuracy and edit distance. Code is available at https://github.com/rdipietro/miccai-2016-surgical-activity-rec.",
		"email": [
			"rdipietro@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_64",
		"source": "miccai",
		"year": 2016,
		"key": "0e5e1d37-3bbe-4ecc-b4bc-98e2f6fd94f3",
		"use": 1
	},
	{
		"Title": "Pareto Front vs. Weighted Sum for Automatic Trajectory Planning of Deep Brain Stimulation",
		"Description": "Noura Hamz, Jimmy Voirin, Pierre Collet, Pierre Jannin, Claire Haegelen, Caroline Essert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Preoperative path planning for Deep Brain Stimulation (DBS) is a multi-objective optimization problem consisting in searching the best compromise between multiple placement constraints. Its automation is usually addressed by turning the problem into mono-objective thanks to an aggregative approach. However, despite its intuitiveness, this approach is known for its incapacity to find all optimal solutions. In this work, we introduce an approach based on multi-objective dominance to DBS path planning. We compare it to a classical aggregative weighted sum of the multiple constraints and to a manual planning thanks to a retrospective study performed by a neurosurgeon on 14 DBS cases. The results show that the dominance-based method is preferred over manual planning, and covers a larger choice of relevant optimal entry points than the traditional weighted sum approach which discards interesting solutions that could be preferred by surgeons.",
		"email": [
			"n.hamze@unistra.fr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_62",
		"source": "miccai",
		"year": 2016,
		"key": "1418b1c4-064f-4f15-9206-659724df4060",
		"use": 1
	},
	{
		"Title": "Regressing Heatmaps for Multiple Landmark Localization Using CNNs",
		"Description": "Christian Payer, Darko tern, Horst Bischof, Martin Urschler.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "We explore the applicability of deep convolutional neural networks (CNNs) for multiple landmark localization in medical image data. Exploiting the idea of regressing heatmaps for individual landmark locations, we investigate several fully convolutional 2D and 3D CNN architectures by training them in an end-to-end manner. We further propose a novel SpatialConfiguration-Net architecture that effectively combines accurate local appearance responses with spatial landmark configurations that model anatomical variation. Evaluation of our different architectures on 2D and 3D hand image datasets show that heatmap regression based on CNNs achieves state-of-the-art landmark localization performance, with SpatialConfiguration-Net being robust even in case of limited amounts of training data.",
		"email": [
			"christian.payer@icg.tugraz.at"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_27",
		"source": "miccai",
		"year": 2016,
		"key": "183f72d5-8860-43cb-a808-515244feb10b",
		"use": 1
	},
	{
		"Title": "Discover Mouse Gene Coexpression Landscape Using Dictionary Learning and Sparse Coding",
		"Description": "Yujie Li, Hanbo Chen, Xi Jiang, Xiang Li, Jinglei Lv, Hanchuan Peng, Joe Z. Tsien, Tianming Liu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Gene coexpression patterns carry rich information of complex brain structures and functions. Characterization of these patterns in an unbiased and integrated manner will illuminate the higher order transcriptome organization and offer molecular foundations of functional circuitry. Here we demonstrate a data-driven method that can effectively extract coexpression networks from transcriptome profiles using the Allen Mouse Brain Atlas dataset. For each of the obtained networks, both genetic compositions and spatial distributions in brain volume are learned. A simultaneous knowledge of precise spatial distributions of specific gene as well as the networks the gene plays in and the weights it carries can bring insights into the molecular mechanism of brain formation and functions. Gene ontologies and the comparisons with published data reveal interesting functions of the identified coexpression networks, including major cell types, biological functions, brain regions, and/or brain diseases.",
		"email": [
			"YL31679@uga.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_8",
		"source": "miccai",
		"year": 2016,
		"key": "ff6dc0c3-aa9c-4b4d-8fdb-2211ae20a3d4",
		"use": 1
	},
	{
		"Title": "Spatial Clockwork Recurrent Neural Network for Muscle Perimysium Segmentation",
		"Description": "Yuanpu Xie, Zizhao Zhang, Manish Sapkota, Lin Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Accurate segmentation of perimysium plays an important role in early diagnosis of many muscle diseases because many diseases contain different perimysium inflammation. However, it remains as a challenging task due to the complex appearance of the perymisum morphology and its ambiguity to the background area. The muscle perimysium also exhibits strong structure spanned in the entire tissue, which makes it difficult for current local patch-based methods to capture this long-range context information. In this paper, we propose a novel spatial clockwork recurrent neural network (spatial CW-RNN) to address those issues. Specifically, we split the entire image into a set of non-overlapping image patches, and the semantic dependencies among them are modeled by the proposed spatial CW-RNN. Our method directly takes the 2D structure of the image into consideration and is capable of encoding the context information of the entire image into the local representation of each patch. Meanwhile, we leverage on the structured regression to assign one prediction mask rather than a single class label to each local patch, which enables both efficient training and testing. We extensively test our method for perimysium segmentation using digitized muscle microscopy images. Experimental results demonstrate the superiority of the novel spatial CW-RNN over other existing state of the arts.",
		"email": [
			"lin.yang@bme.ufl.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_22",
		"source": "miccai",
		"year": 2016,
		"key": "c9eefe5d-221b-42d9-a072-5de558d64c8d",
		"use": 1
	},
	{
		"Title": "Automated Age Estimation from Hand MRI Volumes Using Deep Learning",
		"Description": "Darko tern, Christian Payer, Vincent Lepetit, Martin Urschler.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Biological age (BA) estimation from radiologic data is an important topic in clinical medicine, e.g. in determining endocrinological diseases or planning paediatric orthopaedic surgeries, while in legal medicine it is employed to approximate chronological age. In this work, we propose the use of deep convolutional neural networks (DCNN) for automatic BA estimation from hand MRI volumes, inspired by the way radiologists visually perform age estimation using established staging schemes that follow physical maturation. In our results we outperform the state of the art automatic BA estimation method, achieving a mean error between estimated and ground truth BA of \\(0.36\\,\\pm \\,0.30\\) years, which is in line with radiologists doing visual BA estimation.",
		"email": [
			"stern@icg.tugraz.at"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_23",
		"source": "miccai",
		"year": 2016,
		"key": "9920dae0-6b38-4fbc-a5b5-b5080e2fd048",
		"use": 1
	},
	{
		"Title": "A Deep Learning Approach for Semantic Segmentation in Histology Tissue Images",
		"Description": "Jiazhuo Wang, John D. MacKenzie, Rageshree Ramachandran, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "To make reliable diagnosis, pathologists often need to identify certain special regions in medical images. In inflammatory bowel disease (IBD) diagnosis via histology tissue image examination, muscle regions are known to have no immune cell infiltration, and thus are ignored by pathologists. Also, messy regions (e.g., due to distortion and poor staining) are low in diagnostic yield. Hence, excluding muscle and messy regions to focus on vital regions is crucial for accurate diagnosis of IBD. In this paper, we propose a novel deep neural network based on fully convolutional networks (FCN) to identify muscle and messy regions, in an end-to-end fashion. First, we address the challenge of having limited medical training data, for training our deep neural network (a common problem for medical image processing, which may impede the application of the powerful deep learning method). Second, to deal with target regions of largely different sizes and arbitrary shapes, our deep neural network explores multi-scale information and structural information. Experimental results on clinical images show that our approach outperforms the state-of-the-art FCN for semantic segmentation of muscle and messy regions. Our approach may be readily extended to identify other types of regions in a variety of medical imaging applications.",
		"email": [
			"jwang12@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_21",
		"source": "miccai",
		"year": 2016,
		"key": "1f2a6c4b-9fba-4b9f-b7cb-c194e08fb9c7",
		"use": 1
	},
	{
		"Title": "Deep Neural Networks for Fast Segmentation of 3D Medical Images",
		"Description": "Karl Fritscher, Patrik Raudaschl, Paolo Zaffino, Maria Francesca Spadea, Gregory C. Sharp, Rainer Schubert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "\nDuring the last years Deep Learning and especially Convolutional Neural Networks (CNN) have set new standards for different computer vision tasks like image classification and semantic segmentation. In this paper, a CNN for 3D volume segmentation based on recently introduced deep learning components will be presented. In addition to using image patches as input for a CNN, the usage of orthogonal patches, which combine shape and locality information with intensity information for CNN training will be evaluated. For this purpose a publically available CT dataset of the head-neck region has been used and the results have been compared with other state-of-the art atlas- and model-based segmentation approaches.",
		"email": [
			"karl.fritscher@umit.at"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_19",
		"source": "miccai",
		"year": 2016,
		"key": "504ca368-f37c-4e5b-b476-e59a7373f937",
		"use": 1
	},
	{
		"Title": "A Cross-Modality Neural Network Transform for Semi-automatic Medical Image Annotation",
		"Description": "Mehdi Moradi, Yufan Guo, Yaniv Gur, Mohammadreza Negahdar, Tanveer Syeda-Mahmood.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "There is a pressing need in the medical imaging community to build large scale datasets that are annotated with semantic descriptors. Given the cost of expert produced annotations, we propose an automatic methodology to produce semantic descriptors for images. These can then be used as weakly labeled instances or reviewed and corrected by clinicians. Our solution is in the form of a neural network that maps a given image to a new space formed by a large number of text paragraphs written about similar, but different images, by a human expert. We then extract semantic descriptors from the text paragraphs closest to the output of the transform network to describe the input image. We used deep learning to learn mappings between images/texts and their corresponding fixed size spaces, but a shallow network as the transform between the image and text spaces. This limits the complexity of the transform model and reduces the amount of data, in the form of image and text pairs, needed for training it. We report promising results for the proposed model in automatic descriptor generation in the case of Doppler images of cardiac valves and show that the system catches up to 91 % of the disease instances and 77 % of disease severity modifiers.",
		"email": [
			"mmoradi@us.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_35",
		"source": "miccai",
		"year": 2016,
		"key": "2790b468-ebc6-46f9-aa23-455052c316e8",
		"use": 1
	},
	{
		"Title": "Sub-category Classifiers for Multiple-instance Learning and Its Application to Retinal Nerve Fiber Layer Visibility Classification",
		"Description": "Siyamalan Manivannan, Caroline Cobb, Stephen Burgess, Emanuele Trucco.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "We propose a novel multiple instance learning method to assess the visibility (visible/not visible) of the retinal nerve fiber layer (RNFL) in fundus camera images. Using only image-level labels, our approach learns to classify the images as well as to localize the RNFL visible regions. We transform the original feature space to a discriminative subspace, and learn a region-level classifier in that subspace. We propose a margin-based loss function to jointly learn this subspace and the region-level classifier. Experiments with a RNFL dataset containing 576 images annotated by two experienced ophthalmologists give an agreement (kappa values) of 0.65 and 0.58 respectively, with an inter-annotator agreement of 0.62. Note that our system gives higher agreements with the more experienced annotator. Comparative tests with three public datasets (MESSIDOR and DR for diabetic retinopathy, UCSB for breast cancer) show improved performance over the state-of-the-art.",
		"email": [
			"smanivannan@dundee.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_36",
		"source": "miccai",
		"year": 2016,
		"key": "f2aa0b14-f27b-4a14-8355-78361f5f0a30",
		"use": 1
	},
	{
		"Title": "Self-Transfer Learning for Weakly Supervised Lesion Localization",
		"Description": "Sangheum Hwang, Hyo-Eun Kim.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Recent advances of deep learning have achieved remarkable performances in various computer vision tasks including weakly supervised object localization. Weakly supervised object localization is practically useful since it does not require fine-grained annotations. Current approaches overcome the difficulties of weak supervision via transfer learning from pre-trained models on large-scale general images such as ImageNet. However, they cannot be utilized for medical image domain in which do not exist such priors. In this work, we present a novel weakly supervised learning framework for lesion localization named as self-transfer learning (STL). STL jointly optimizes both classification and localization networks to help the localization network focus on correct lesions without any types of priors. We evaluate STL framework over chest X-rays and mammograms, and achieve significantly better localization performance compared to previous weakly supervised localization approaches.",
		"email": [
			"shwang@lunit.io"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_28",
		"source": "miccai",
		"year": 2016,
		"key": "d569a69c-00ee-46ce-b818-d8c182b1c6c9",
		"use": 1
	},
	{
		"Title": "A Learning-Free Approach to Whole Spine Vertebra Localization in MRI",
		"Description": "Marko Rak, Klaus-Dietz Tnnies.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "In recent years, analysis of magnetic resonance images of the spine gained considerable interest with vertebra localization being a key step for higher level analysis. Approaches based on trained appearance - which are de facto standard - may be inappropriate for certain tasks, because processing usually takes several minutes or training data is unavailable. Learning-free approaches have yet to show there competitiveness for whole-spine localization. Our work fills this gap. We combine a fast engineered detector with a novel vertebrae appearance similarity concept. The latter can compete with trained appearance, which we show on a data set of 64 \\(T_1\\)- and 64 \\(T_2\\)-weighted images. Our detection took \\(27.7 \\pm 3.78\\) s with a detection rate of 96.0 % and a distance to ground truth of \\(3.45 \\pm 2.2\\) mm, which is well below the slice thickness.",
		"email": [
			"rak@isg.cs.ovgu.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_33",
		"source": "miccai",
		"year": 2016,
		"key": "116adfbe-143c-4d51-bd01-48155fc2443d",
		"use": 1
	},
	{
		"Title": "Automated Diagnosis of Neural Foraminal Stenosis Using Synchronized Superpixels Representation",
		"Description": "Xiaoxu He, Yilong Yin, Manas Sharma, Gary Brahm, Ashley Mercado, Shuo Li.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Neural foramina stenosis (NFS), as a common spine disease, affects \\(80\\,\\%\\) of people. Clinical diagnosis by physicians manual segmentation is inefficient and laborious. Automated diagnosis is highly desirable but faces the class overlapping problem derived from the diverse shape and size. In this paper, a fully automated diagnosis approach is proposed for NFS. It is based on a newly proposed synchronized superpixels representation (SSR) model where a highly discriminative feature space is obtained for accurately and easily classifying neural foramina into normal and stenosed classes. To achieve it, class labels (0:normal,1:stenosed) are integrated to guide manifold alignment which correlates images from the same class, so that intra-class difference is reduced and the inter-class margin are maximized. The overall result reaches a high accuracy (\\(98.52\\,\\%\\)) in 110 mid-sagittal MR spine images collected from 110 subjects. Hence, with our approach, an efficient and accurate clinical tool is provided to greatly reduce the burden of physicians and ensure the timely treatment of NFS.",
		"email": [
			"xhe244@uwo.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_39",
		"source": "miccai",
		"year": 2016,
		"key": "a0bf5518-ec30-429c-87a2-27d4ee21bd16",
		"use": 1
	},
	{
		"Title": "Identifying Relationships in Functional and Structural Connectome Data Using a Hypergraph Learning Method",
		"Description": "Brent C. Munsell, Guorong Wu, Yue Gao, Nicholas Desisto, Martin Styner.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "The brain connectome provides an unprecedented degree of information about the organization of neuronal network architecture, both at a regional level, as well as regarding the entire brain network. Over the last several years the neuroimaging community has made tremendous advancements in the analysis of structural connectomes derived from white matter fiber tractography or functional connectomes derived from time-series blood oxygen level signals. However, computational techniques that combine structural and functional connectome data to discover complex relationships between fiber density and signal synchronization, including the relationship with health and disease, has not been consistently performed. To overcome this shortcoming, a novel connectome feature selection technique is proposed that uses hypergraphs to identify connectivity relationships when structural and functional connectome data is combined. Using publicly available connectome data from the UMCD database, experiments are provided that show SVM classifiers trained with structural and functional connectome features selected by our method are able to correctly identify autism subjects with 88 % accuracy. These results suggest our combined connectome feature selection approach may improve outcome forecasting in the context of autism.",
		"email": [
			"munsellb@cofc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_2",
		"source": "miccai",
		"year": 2016,
		"key": "c7411717-4854-4fd2-860f-54bed286ee82",
		"use": 1
	},
	{
		"Title": "Structured Sparse Kernel Learning for Imaging Genetics Based Alzheimers Disease Diagnosis",
		"Description": "Jailin Peng, Le An, Xiaofeng Zhu, Yan Jin, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "A kernel-learning based method is proposed to integrate multimodal imaging and genetic data for Alzheimers disease (AD) diagnosis. To facilitate structured feature learning in kernel space, we represent each feature with a kernel and then group kernels according to modalities. In view of the highly redundant features within each modality and also the complementary information across modalities, we introduce a novel structured sparsity regularizer for feature selection and fusion, which is different from conventional lasso and group lasso based methods. Specifically, we enforce a penalty on kernel weights to simultaneously select features sparsely within each modality and densely combine different modalities. We have evaluated the proposed method using magnetic resonance imaging (MRI) and positron emission tomography (PET), and single-nucleotide polymorphism (SNP) data of subjects from Alzheimers Disease Neuroimaging Initiative (ADNI) database. The effectiveness of our method is demonstrated by both the clearly improved prediction accuracy and the discovered brain regions and SNPs relevant to AD.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_9",
		"source": "miccai",
		"year": 2016,
		"key": "bb36c84b-d229-44a4-871b-b3f2f1844c68",
		"use": 1
	},
	{
		"Title": "3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes",
		"Description": "Qi Dou, Hao Chen, Yueming Jin, Lequan Yu, Jing Qin, Pheng-Ann Heng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper, we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly, we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties, and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN, a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.",
		"email": [
			"qdou@cse.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_18",
		"source": "miccai",
		"year": 2016,
		"key": "73f3b6bd-b094-43df-9998-f93ed8687a5e",
		"use": 1
	},
	{
		"Title": "A Surgical Guidance System for Big-Bubble Deep Anterior Lamellar Keratoplasty",
		"Description": "Hessam Roodaki, Chiara Amat di San Filippo, Daniel Zapp, Nassir Navab, Abouzar Eslami.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention   MICCAI 2016",
		"abstract": "Deep Anterior Lamellar Keratoplasty using Big-Bubble technique (BB-DALK) is a delicate and complex surgical procedure with a multitude of benefits over Penetrating Keratoplasty (PKP). Yet the steep learning curve and challenges associated with BB-DALK prevents it from becoming the standard procedure for keratoplasty. Optical Coherence Tomography (OCT) aids surgeons to carry out BB-DALK in a shorter time with a higher success rate but also brings complications of its own such as image occlusion by the instrument, the constant need to reposition and added distraction. This work presents a novel real-time guidance system for BB-DALK which is practically a complete tool for smooth execution of the procedure. The guidance system comprises of modified 3D+t OCT acquisitions, advanced visualization, tracking of corneal layers and providing depth information using Augmented Reality. The system is tested by an ophthalmic surgeon performing BB-DALK on several ex vivo pig eyes. Results from multiple evaluations show a maximum tracking error of 8.8 micrometers.",
		"email": [
			"he.roodaki@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46720-7_44",
		"source": "miccai",
		"year": 2016,
		"key": "25b12693-f865-4c8d-8c14-a5f646b59ee8",
		"use": 1
	},
	{
		"Title": "Probe-Based Rapid Hybrid Hyperspectral and Tissue Surface Imaging Aided by Fully Convolutional Networks",
		"Description": "Jianyu Lin, Neil T. Clancy, Xueqing Sun, Ji Qi, Mirek Janatka, Danail Stoyanov, Daniel S. Elson.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
		"abstract": "\nTissue surface shape and reflectance spectra provide rich intra-operative information useful in surgical guidance. We propose a hybrid system which displays an endoscopic image with a fast joint inspection of tissue surface shape using structured light (SL) and hyperspectral imaging (HSI). For SL a miniature fibre probe is used to project a coloured spot pattern onto the tissue surface. In HSI mode standard endoscopic illumination is used, with the fibre probe collecting reflected light and encoding the spatial information into a linear format that can be imaged onto the slit of a spectrograph. Correspondence between the arrangement of fibres at the distal and proximal ends of the bundle was found using spectral encoding. Then during pattern decoding, a fully convolutional network (FCN) was used for spot detection, followed by a matching propagation algorithm for spot identification. This method enabled fast reconstruction (12 frames per second) using a GPU. The hyperspectral image was combined with the white light image and the reconstructed surface, showing the spectral information of different areas. Validation of this system using phantom and ex vivo experiments has been demonstrated.",
		"email": [
			"j.lin12@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46726-9_48",
		"source": "miccai",
		"year": 2016,
		"key": "15dd6c09-150a-42e1-a873-eb442511b71c",
		"use": 1
	},
	{
		"Title": "Deep Retinal Image Understanding",
		"Description": "Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbelez, Luc Van Gool.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "This paper presents Deep Retinal Image Understanding (DRIU), a unified framework of retinal image analysis that provides both retinal vessel and optic disc segmentation. We make use of deep Convolutional Neural Networks (CNNs), which have proven revolutionary in other fields of computer vision such as object detection and image classification, and we bring their power to the study of eye fundus images. DRIU uses a base network architecture on which two set of specialized layers are trained to solve both the retinal vessel and optic disc segmentation. We present experimental validation, both qualitative and quantitative, in four public datasets for these tasks. In all of them, DRIU presents super-human performance, that is, it shows results more consistent with a gold standard than a second human annotator used as control.",
		"email": [
			"kmaninis@vision.ee.ethz.ch"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_17",
		"source": "miccai",
		"year": 2016,
		"key": "032fe1dd-5b12-4cdd-b02a-226cc3f659f0",
		"use": 1
	},
	{
		"Title": "DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field",
		"Description": "Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, Jiang Liu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Retinal vessel segmentation is a fundamental step for various ocular imaging applications. In this paper, we formulate the retinal vessel segmentation problem as a boundary detection task and solve it using a novel deep learning architecture. Our method is based on two key ideas: (1) applying a multi-scale and multi-level Convolutional Neural Network (CNN) with a side-output layer to learn a rich hierarchical representation, and (2) utilizing a Conditional Random Field (CRF) to model the long-range interactions between pixels. We combine the CNN and CRF layers into an integrated deep network called DeepVessel. Our experiments show that the DeepVessel system achieves state-of-the-art retinal vessel segmentation performance on the DRIVE, STARE, and CHASE_DB1 datasets with an efficient running time.",
		"email": [
			"fuhz@i2r.a-star.edu.sg"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_16",
		"source": "miccai",
		"year": 2016,
		"key": "152b2c25-315d-47c7-82a1-792cee5fd019",
		"use": 1
	},
	{
		"Title": "Learning from Experts: Developing Transferable Deep Features for Patient-Level Lung Cancer Prediction",
		"Description": "Wei Shen, Mu Zhou, Feng Yang, Di Dong, Caiyun Yang, Yali Zang, Jie Tian.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "Due to recent progress in Convolutional Neural Networks (CNNs), developing image-based CNN models for predictive diagnosis is gaining enormous interest. However, to date, insufficient imaging samples with truly pathological-proven labels impede the evaluation of CNN models at scale. In this paper, we formulate a domain-adaptation framework that learns transferable deep features for patient-level lung cancer malignancy prediction. The presented work learns CNN-based features from a large discovery set (2272 lung nodules) with malignancy likelihood labels involving multiple radiologists assessments, and then tests the transferable predictability of these CNN-based features on a diagnosis-definite set (115 cases) with true pathologically-proven lung cancer labels. We evaluate our approach on the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset, where both human expert labeling information on cancer malignancy likelihood and a set of pathologically-proven malignancy labels were provided. Experimental results demonstrate the superior predictive performance of the transferable deep features on predicting true patient-level lung cancer malignancy (Acc = 70.69 %, AUC = 0.66), which outperforms a nodule-level CNN model (Acc = 65.38 %, AUC = 0.63) and is even comparable to that of using the radiologists knowledge (Acc = 72.41 %, AUC = 0.76). The proposed model can largely reduce the demand for pathologically-proven data, holding promise to empower cancer diagnosis by leveraging multi-source CT imaging datasets.",
		"email": [
			"fengyang@bjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_15",
		"source": "miccai",
		"year": 2016,
		"key": "c3e314ec-31c1-4626-884d-503448d5c820",
		"use": 1
	},
	{
		"Title": "Multimodal Deep Learning for Cervical Dysplasia Diagnosis",
		"Description": "Tao Xu, Han Zhang, Xiaolei Huang, Shaoting Zhang, Dimitris N. Metaxas.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "To improve the diagnostic accuracy of cervical dysplasia, it is important to fuse multimodal information collected during a patients screening visit. However, current multimodal frameworks suffer from low sensitivity at high specificity levels, due to their limitations in learning correlations among highly heterogeneous modalities. In this paper, we design a deep learning framework for cervical dysplasia diagnosis by leveraging multimodal information. We first employ the convolutional neural network (CNN) to convert the low-level image data into a feature vector fusible with other non-image modalities. We then jointly learn the non-linear correlations among all modalities in a deep neural network. Our multimodal framework is an end-to-end deep network which can learn better complementary features from the image and non-image modalities. It automatically gives the final diagnosis for cervical dysplasia with 87.83 % sensitivity at 90 % specificity on a large dataset, which significantly outperforms methods using any single source of information alone and previous multimodal frameworks.",
		"email": [
			"xih206@lehigh.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_14",
		"source": "miccai",
		"year": 2016,
		"key": "c98dab29-09cf-471e-a594-145f83a71d94",
		"use": 1
	},
	{
		"Title": "The Automated Learning of Deep Features for Breast Mass Classification from Mammograms",
		"Description": "Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2016",
		"abstract": "The classification of breast masses from mammograms into benign or malignant has been commonly addressed with machine learning classifiers that use as input a large set of hand-crafted features, usually based on general geometrical and texture information. In this paper, we propose a novel deep learning method that automatically learns features based directly on the optmisation of breast mass classification from mammograms, where we target an improved classification performance compared to the approach described above. The novelty of our approach lies in the two-step training process that involves a pre-training based on the learning of a regressor that estimates the values of a large set of hand-crafted features, followed by a fine-tuning stage that learns the breast mass classifier. Using the publicly available INbreast dataset, we show that the proposed method produces better classification results, compared with the machine learning model using hand-crafted features and with deep learning method trained directly for the classification stage without the pre-training stage. We also show that the proposed method produces the current state-of-the-art breast mass classification results for the INbreast dataset. Finally, we integrate the proposed classifier into a fully automated breast mass detection and segmentation, which shows promising results.",
		"email": [
			"neeraj.dhungel@adelaide.edu.au"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-46723-8_13",
		"source": "miccai",
		"year": 2016,
		"key": "69eea495-c8ca-4538-bb20-d9f07e003a68",
		"use": 1
	},
	{
		"Title": "BRIEFnet: Deep Pancreas Segmentation Using Binary Sparse Convolutions",
		"Description": "Mattias P. Heinrich, Ozan Oktay.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Dense prediction using deep convolutional neural networks (CNNs) has recently advanced the field of segmentation in computer vision and medical imaging. In contrast to patch-based classification, it requires only a single path through a deep network to segment every voxel in an image. However, it is difficult to incorporate contextual information without using contracting (pooling) layers, which would reduce the spatial accuracy for thinner structures. Consequently, huge receptive fields are required which might lead to disproportionate computational demand. Here, we propose to use binary sparse convolutions in the first layer as a particularly effective approach to reduce complexity while achieving high accuracy. The concept is inspired by the successful BRIEF descriptors and complemented with \\(1\\times 1\\) convolutions (cf. network in network) to further reduce the number of trainable parameters. Sparsity is in particular important for small datasets often found in medical imaging. Our experimental validation demonstrates accuracies for pancreas segmentation in CT that are comparable with state-of-the-art deep learning approaches and registration based multi-atlas segmentation with label fusion. The whole network, which also includes a classic CNN path to improve local details, can be trained in 10 min. Segmenting a new scan takes 3 s even without using a GPU.",
		"email": [
			"heinrich@imi.uni-luebeck.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_38",
		"source": "miccai",
		"year": 2017,
		"key": "faea9274-73ea-4dc5-83e8-71f23e19bf6c",
		"use": 1
	},
	{
		"Title": "Semi-supervised Deep Learning for Fully Convolutional Networks",
		"Description": "Christoph Baur, Shadi Albarqouni, Nassir Navab.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.",
		"email": [
			"c.baur@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_36",
		"source": "miccai",
		"year": 2017,
		"key": "e2faa4c5-196b-450d-96e6-d40bcecebfde",
		"use": 1
	},
	{
		"Title": "Unsupervised Feature Learning for Endomicroscopy Image Retrieval",
		"Description": "Yun Gu, Khushi Vyas, Jie Yang, Guang-Zhong Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Learning the visual representation for medical images is a critical task in computer-aided diagnosis. In this paper, we propose Unsupervised Multimodal Graph Mining (UMGM) to learn the discriminative features for probe-based confocal laser endomicroscopy (pCLE) mosaics of breast tissue. We build a multiscale multimodal graph based on both pCLE mosaics and histology images. The positive pairs are mined via cycle consistency and the negative pairs are extracted based on geodetic distance. Given the positive and negative pairs, the latent feature space is discovered by reconstructing the similarity between pCLE and histology images. Experiments on a database with 700 pCLE mosaics demonstrate that the proposed method outperforms previous works on pCLE feature learning. Specially, the top-1 accuracy in an eight-class retrieval task is 0.659 which leads to 10% improvement compared with the state-of-the-art method.",
		"email": [
			"jieyang@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_8",
		"source": "miccai",
		"year": 2017,
		"key": "ed79404b-8f53-4063-be1f-0a2ecc70d166",
		"use": 1
	},
	{
		"Title": "Deep Learning for Sensorless 3D Freehand Ultrasound Imaging",
		"Description": "Raphael Prevost, Mehrdad Salehi, Julian Sprung, Robert Bauer, Wolfgang Wein.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "3D freehand ultrasound imaging is a very promising imaging modality but its acquisition is often neither portable nor practical because of the required external tracking hardware. Building a sensorless solution that is fully based on image analysis would thus have many potential applications. However, previously proposed approaches rely on physical models whose assumptions only hold on synthetic or phantom datasets, failing to translate to actual clinical acquisitions with sufficient accuracy. In this paper, we investigate the alternative approach of using statistical learning to circumvent this problem. To that end, we are leveraging the unique modeling capabilities of convolutional neural networks in order to build an end-to-end system where we directly predict the ultrasound probe motion from the images themselves. Based on thorough experiments using both phantom acquisitions and a set of 100 in-vivo long ultrasound sweeps for vein mapping, we show that our novel approach significantly outperforms the standard method and has direct clinical applicability, with an average drift error of merely 7\\(\\%\\) over the whole length of each ultrasound clip.",
		"email": [
			"prevost@imfusion.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_71",
		"source": "miccai",
		"year": 2017,
		"key": "4701ba68-c759-40fd-abfa-781cde280c63",
		"use": 1
	},
	{
		"Title": "Learning-Based Ensemble Average Propagator Estimation",
		"Description": "Chuyang Ye.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "By capturing the anisotropic water diffusion in tissue, diffusion magnetic resonance imaging (dMRI) provides a unique tool for noninvasively probing the tissue microstructure and orientation in the human brain. The diffusion profile can be described by the ensemble average propagator (EAP), which is inferred from observed diffusion signals. However, accurate EAP estimation using the number of diffusion gradients that is clinically practical can be challenging. In this work, we propose a deep learning algorithm for EAP estimation, which is named learning-based ensemble average propagator estimation (LEAPE). The EAP is commonly represented by a basis and its associated coefficients, and here we choose the SHORE basis and design a deep network to estimate the coefficients. The network comprises two cascaded components. The first component is a multiple layer perceptron (MLP) that simultaneously predicts the unknown coefficients. However, typical training loss functions, such as mean squared errors, may not properly represent the geometry of the possibly non-Euclidean space of the coefficients, which in particular causes problems for the extraction of directional information from the EAP. Therefore, to regularize the training, in the second component we compute an auxiliary output of approximated fiber orientation (FO) errors with the aid of a second MLP that is trained separately. We performed experiments using dMRI data that resemble clinically achievable q-space sampling, and observed promising results compared with the conventional EAP estimation method.",
		"email": [
			"chuyang.ye@nlpr.ia.ac.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_68",
		"source": "miccai",
		"year": 2017,
		"key": "b4c1b385-60be-43f5-9369-e19720474399",
		"use": 1
	},
	{
		"Title": "A Sparse Bayesian Learning Algorithm for White Matter Parameter Estimation from Compressed Multi-shell Diffusion MRI",
		"Description": "Pramod Kumar Pisharady, Stamatios N. Sotiropoulos, Guillermo Sapiro, Christophe Lenglet.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "We propose a sparse Bayesian learning algorithm for improved estimation of white matter fiber parameters from compressed (under-sampled q-space) multi-shell diffusion MRI data. The multi-shell data is represented in a dictionary form using a non-monoexponential decay model of diffusion, based on continuous gamma distribution of diffusivities. The fiber volume fractions with predefined orientations, which are the unknown parameters, form the dictionary weights. These unknown parameters are estimated with a linear un-mixing framework, using a sparse Bayesian learning algorithm. A localized learning of hyperparameters at each voxel and for each possible fiber orientations improves the parameter estimation. Our experiments using synthetic data from the ISBI 2012 HARDI reconstruction challenge and in-vivo data from the Human Connectome Project demonstrate the improvements.",
		"email": [
			"pramodkp@umn.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_69",
		"source": "miccai",
		"year": 2017,
		"key": "3678d0b7-bf9a-49ed-a6fe-03afd4aae6db",
		"use": 1
	},
	{
		"Title": "Learn to Track: Deep Learning for Tractography",
		"Description": "Philippe Poulin, Marc-Alexandre Ct, Jean-Christophe Houde, Laurent Petit, Peter F. Neher, Klaus H. Maier-Hein, Hugo Larochelle, Maxime Descoteaux.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "We show that deep learning techniques can be applied successfully to fiber tractography. Specifically, we use feed-forward and recurrent neural networks to learn the generation process of streamlines directly from diffusion-weighted imaging (DWI) data. Furthermore, we empirically study the behavior of the proposed models on a realistic white matter phantom with known ground truth. We show that their performance is competitive to that of commonly used techniques, even when the models are used on DWI data unseen at training time. We also show that our models are able to recover high spatial coverage of the ground truth white matter pathways while better controlling the number of false connections. In fact, our experiments suggest that exploiting past information within a streamlines trajectory during tracking helps predict the following direction.",
		"email": [
			"philippe.poulin2@usherbrooke.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_62",
		"source": "miccai",
		"year": 2017,
		"key": "1402d796-5830-4647-bd48-c0c684ee197f",
		"use": 1
	},
	{
		"Title": "FiberNET: An Ensemble Deep Learning Framework for Clustering White Matter Fibers",
		"Description": "Vikash Gupta, Sophia I. Thomopoulos, Faisal M. Rashid, Paul M. Thompson.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "White matter tracts are commonly analyzed in studies of micro-structural integrity and anatomical connectivity in the brain. Over the last decade, it has been an open problem as to how best to cluster white matter fibers, extracted from whole-brain tractography, into anatomically meaningful groups. Some existing techniques use region of interest (ROI) based clustering, atlas-based labeling, or unsupervised spectral clustering. ROI-based clustering is popular for analyzing anatomical connectivity among a set of ROIs, but it does not always partition the brain into recognizable fiber bundles. Here we propose an approach using convolutional neural networks (CNNs) to learn shape features of the fiber bundles, which are then exploited to cluster white matter fibers. To achieve such clustering, we first need to re-parameterize the fibers in an intrinsic space. The clustering is performed in induced parameterized coordinates. To our knowledge, this is one of the first approaches for fiber clustering using deep learning techniques. The results show strong accuracy - on a par with or better than other state-of-the-art methods.",
		"email": [
			"vikash.gupta@loni.usc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_63",
		"source": "miccai",
		"year": 2017,
		"key": "a4f56ac0-a9fc-45ba-8c1a-db13e73414f4",
		"use": 1
	},
	{
		"Title": "Fiber Orientation Estimation Guided by a Deep Network",
		"Description": "Chuyang Ye, Jerry L. Prince.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Diffusion magnetic resonance imaging (dMRI) is currently the only tool for noninvasively imaging the brains white matter tracts. The fiber orientation (FO) is a key feature computed from dMRI for tract reconstruction. Because the number of FOs in a voxel is usually small, dictionary-based sparse reconstruction has been used to estimate FOs. However, accurate estimation of complex FO configurations in the presence of noise can still be challenging. In this work we explore the use of a deep network for FO estimation in a dictionary-based framework and propose an algorithm named Fiber Orientation Reconstruction guided by a Deep Network (FORDN). FORDN consists of two steps. First, we use a smaller dictionary encoding coarse basis FOs to represent diffusion signals. To estimate the mixture fractions of the dictionary atoms, a deep network is designed to solve the sparse reconstruction problem. Second, the coarse FOs inform the final FO estimation, where a larger dictionary encoding a dense basis of FOs is used and a weighted \\(\\ell _{1}\\)-norm regularized least squares problem is solved to encourage FOs that are consistent with the network output. FORDN was evaluated and compared with state-of-the-art algorithms that estimate FOs using sparse reconstruction on simulated and typical clinical dMRI data. The results demonstrate the benefit of using a deep network for FO estimation.",
		"email": [
			"chuyang.ye@nlpr.ia.ac.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_66",
		"source": "miccai",
		"year": 2017,
		"key": "e020100e-6426-4cf0-8a57-3550729ddd71",
		"use": 1
	},
	{
		"Title": "White Matter Fiber Representation Using Continuous Dictionary Learning",
		"Description": "Guy Alexandroni, Yana Podolsky, Hayit Greenspan, Tal Remez, Or Litany, Alexander Bronstein, Raja Giryes.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "With increasingly sophisticated Diffusion Weighted MRI acquisition methods and modeling techniques, very large sets of streamlines (fibers) are presently generated per imaged brain. These reconstructions of white matter architecture, which are important for human brain research and pre-surgical planning, require a large amount of storage and are often unwieldy and difficult to manipulate and analyze. This work proposes a novel continuous parsimonious framework in which signals are sparsely represented in a dictionary with continuous atoms. The significant innovation in our new methodology is the ability to train such continuous dictionaries, unlike previous approaches that either used pre-fixed continuous transforms or training with finite atoms. This leads to an innovative fiber representation method, which uses Continuous Dictionary Learning to sparsely code each fiber with high accuracy. This method is tested on numerous tractograms produced from the Human Connectome Project data and achieves state-of-the-art performances in compression ratio and reconstruction error.",
		"email": [
			"PodolskyYana@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_65",
		"source": "miccai",
		"year": 2017,
		"key": "bfff91e1-c274-4b4b-93d3-60278492523c",
		"use": 1
	},
	{
		"Title": "Sparse Multi-kernel Based Multi-task Learning for Joint Prediction of Clinical Scores and Biomarker Identification in Alzheimers Disease",
		"Description": "Peng Cao, Xiaoli Liu, Jinzhu Yang, Dazhe Zhao, Osmar Zaiane.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Machine learning methods have been used to predict the clinical scores and identify the image biomarkers from individual MRI scans. Recently, the multi-task learning (MTL) with sparsity-inducing norm have been widely studied to investigate the prediction power of neuroimaging measures by incorporating inherent correlations among multiple clinical cognitive measures. However, most of the existing MTL algorithms are formulated linear sparse models, in which the response (e.g., cognitive score) is a linear function of predictors (e.g., neuroimaging measures). To exploit the nonlinear relationship between the neuroimaging measures and cognitive measures, we consider that tasks to be learned share a common subset of features in the kernel space as well as the kernel functions. Specifically, we propose a multi-kernel based multi-task learning with a mixed sparsity-inducing norm to better capture the complex relationship between the cognitive scores and the neuroimaging measures. The formation can be efficiently solved by mirror-descent optimization. Experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) database showed that the proposed algorithm achieved better prediction performance than state-of-the-art linear based methods both on single MRI and multiple modalities.",
		"email": [
			"caopeng@cse.neu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_23",
		"source": "miccai",
		"year": 2017,
		"key": "0674efcf-4018-4816-b32d-54668ced2100",
		"use": 1
	},
	{
		"Title": "Pancreas Segmentation in MRI Using Graph-Based Decision Fusion on Convolutional Neural Networks",
		"Description": "Jinzheng Cai, Le Lu, Yuanpu Xie, Fuyong Xing, Lin Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.",
		"email": [
			"lin.yang@bme.ufl.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_77",
		"source": "miccai",
		"year": 2017,
		"key": "0dcbe112-5363-4da2-8b0c-0f96564f7815",
		"use": 1
	},
	{
		"Title": "Deep Reinforcement Learning for Active Breast Lesion Detection from DCE-MRI",
		"Description": "Gabriel Maicas, Gustavo Carneiro, Andrew P. Bradley, Jacinto C. Nascimento, Ian Reid.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We present a novel methodology for the automated detection of breast lesions from dynamic contrast-enhanced magnetic resonance volumes (DCE-MRI). Our method, based on deep reinforcement learning, significantly reduces the inference time for lesion detection compared to an exhaustive search, while retaining state-of-art accuracy.",
		"email": [
			"gabriel.maicas@adelaide.edu.au"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_76",
		"source": "miccai",
		"year": 2017,
		"key": "6e29e1d4-56bc-4662-a342-51aea4dd51b5",
		"use": 1
	},
	{
		"Title": "Joint Detection and Diagnosis of Prostate Cancer in Multi-parametric MRI Based on Multimodal Convolutional Neural Networks",
		"Description": "Xin Yang, Zhiwei Wang, Chaoyue Liu, Hung Minh Le, Jingyu Chen, Kwang-Ting (Tim) Cheng, Liang Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "This paper presents an automated method for jointly localizing prostate cancer (PCa) in multi-parametric MRI (mp-MRI) images and assessing the aggressiveness of detected lesions. Our method employs multimodal multi-label convolutional neural networks (CNNs), which are trained in a weakly-supervised manner by providing a set of prostate images with image-level labels without priors of lesions locations. By distinguishing images with different labels, discriminative visual patterns related to indolent PCa and clinically significant (CS) PCa are automatically learned from clutters of prostate tissues. Cancer response maps (CRMs) with each pixel indicating the likelihood of being part of indolent/CS are explicitly generated at the last convolutional layer. We define new back-propagate error of CNN to enforce both optimized classification results and consistent CRMs for different modalities. Our method enables the feature learning processes of different modalities to mutually influence each other and, in turn yield more representative features. Comprehensive evaluation based on 402 lesions demonstrates superior performance of our method to the state-of-the-art method [13].",
		"email": [
			"xinyang2014@hust.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_49",
		"source": "miccai",
		"year": 2017,
		"key": "36bd496a-5833-4907-85b7-063d908da51c",
		"use": 1
	},
	{
		"Title": "Precise Ultrasound Bone Registration with Learning-Based Segmentation and Speed of Sound Calibration",
		"Description": "Mehrdad Salehi, Raphael Prevost, Jos-Luis Moctezuma, Nassir Navab, Wolfgang Wein.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Ultrasound imaging is increasingly used in navigated surgery and registration-based applications. However, spatial information quality in ultrasound is relatively inferior to other modalities. Main limiting factors for an accurate registration between ultrasound and other modalities are tissue deformation and speed of sound variation throughout the body. The bone surface in ultrasound is a landmark which is less affected by such geometric distortions. In this paper, we present a workflow to accurately register intra-operative ultrasound images to a reference pre-operative CT volume based on an automatic and real-time image processing pipeline. We show that a convolutional neural network is able to produce robust, accurate and fast bone segmentation of such ultrasound images. We also develop a dedicated method to perform online speed of sound calibration by focusing on the bone area and optimizing the appearance of steered compounded images. We provide extensive validation on both phantom and real cadaver data obtaining overall errors under one millimeter.",
		"email": [
			"salehi@imfusion.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_77",
		"source": "miccai",
		"year": 2017,
		"key": "a7a3c3da-4751-4433-a822-e2f6a5b7b588",
		"use": 1
	},
	{
		"Title": "Progressive and Multi-path Holistically Nested Neural Networks for Pathological Lung Segmentation from CT Images",
		"Description": "Adam P. Harrison, Ziyue Xu, Kevin George, Le Lu, Ronald M. Summers, Daniel J. Mollura.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Pathological lung segmentation (PLS) is an important, yet challenging, medical image application due to the wide variability of pathological lung appearance and shape. Because PLS is often a pre-requisite for other imaging analytics, methodological simplicity and generality are key factors in usability. Along those lines, we present a bottom-up deep-learning based approach that is expressive enough to handle variations in appearance, while remaining unaffected by any variations in shape. We incorporate the deeply supervised learning framework, but enhance it with a simple, yet effective, progressive multi-path scheme, which more reliably merges outputs from different network stages. The result is a deep model able to produce finer detailed masks, which we call progressive holistically-nested networks (P-HNNs). Using extensive cross-validation, our method is tested on a multi-institutional dataset comprising 929 CT scans (848 publicly available), of pathological lungs, reporting mean dice scores of 0.985 and demonstrating significant qualitative and quantitative improvements over state-of-the art approaches.",
		"email": [
			"ziyue.xu@nih.gov"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_71",
		"source": "miccai",
		"year": 2017,
		"key": "1a00aa50-ae34-410a-8eb4-b59d044db759",
		"use": 1
	},
	{
		"Title": "Error Corrective Boosting for Learning Fully Convolutional Networks with Limited Data",
		"Description": "Abhijit Guha Roy, Sailesh Conjeti, Debdoot Sheet, Amin Katouzian, Nassir Navab, Christian Wachinger.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Training deep fully convolutional neural networks (F-CNNs) for semantic image segmentation requires access to abundant labeled data. While large datasets of unlabeled image data are available in medical applications, access to manually labeled data is very limited. We propose to automatically create auxiliary labels on initially unlabeled data with existing tools and to use them for pre-training. For the subsequent fine-tuning of the network with manually labeled data, we introduce error corrective boosting (ECB), which emphasizes parameter updates on classes with lower accuracy. Furthermore, we introduce SkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that combines skip connections with the unpooling strategy for upsampling. The SD-Net addresses challenges of severe class imbalance and errors along boundaries. With application to whole-brain MRI T1 scan segmentation, we generate auxiliary labels on a large dataset with FreeSurfer and fine-tune on two datasets with manual annotations. Our results show that the inclusion of auxiliary labels and ECB yields significant improvements. SD-Net segments a 3D scan in 7 s in comparison to 30 h for the closest multi-atlas segmentation method, while reaching similar performance. It also outperforms the latest state-of-the-art F-CNN models.",
		"email": [
			"abhijit.guha-roy@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_27",
		"source": "miccai",
		"year": 2017,
		"key": "1cd05f1b-8cb1-4e58-9df7-9a8ab152734b",
		"use": 1
	},
	{
		"Title": "Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution",
		"Description": "Ryutaro Tanno, Daniel E. Worrall, Aurobrata Ghosh, Enrico Kaden, Stamatios N. Sotiropoulos, Antonio Criminisi, Daniel C. Alexander.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "In this work, we investigate the value of uncertainty modelling in 3D super-resolution with convolutional neural networks (CNNs). Deep learning has shown success in a plethora of medical image transformation problems, such as super-resolution (SR) and image synthesis. However, the highly ill-posed nature of such problems results in inevitable ambiguity in the learning of networks. We propose to account for intrinsic uncertainty through a per-patch heteroscedastic noise model and for parameter uncertainty through approximate Bayesian inference in the form of variational dropout. We show that the combined benefits of both lead to the state-of-the-art performance SR of diffusion MR brain images in terms of errors compared to ground truth. We further show that the reduced error scores produce tangible benefits in downstream tractography. In addition, the probabilistic nature of the methods naturally confers a mechanism to quantify uncertainty over the super-resolved output. We demonstrate through experiments on both healthy and pathological brains the potential utility of such an uncertainty measure in the risk assessment of the super-resolved images for subsequent clinical use.",
		"email": [
			"ryutaro.tanno.15@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_70",
		"source": "miccai",
		"year": 2017,
		"key": "9d02821e-972a-488c-8560-b381183c6b06",
		"use": 1
	},
	{
		"Title": "Deep Adversarial Networks for Biomedical Image Segmentation Utilizing Unannotated Images",
		"Description": "Yizhe Zhang, Lin Yang, Jianxu Chen, Maridel Fredericksen, David P. Hughes, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Semantic segmentation is a fundamental problem in biomedical image analysis. In biomedical practice, it is often the case that only limited annotated data are available for model training. Unannotated images, on the other hand, are easier to acquire. How to utilize unannotated images for training effective segmentation models is an important issue. In this paper, we propose a new deep adversarial network (DAN) model for biomedical image segmentation, aiming to attain consistently good segmentation results on both annotated and unannotated images. Our model consists of two networks: (1) a segmentation network (SN) to conduct segmentation; (2) an evaluation network (EN) to assess segmentation quality. During training, EN is encouraged to distinguish between segmentation results of unannotated images and annotated ones (by giving them different scores), while SN is encouraged to produce segmentation results of unannotated images such that EN cannot distinguish these from the annotated ones. Through an iterative adversarial training process, because EN is constantly criticizing the segmentation results of unannotated images, SN can be trained to produce more and more accurate segmentation for unannotated and unseen samples. Experiments show that our proposed DAN model is effective in utilizing unannotated image data to obtain considerably better segmentation.",
		"email": [
			"yzhang29@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_47",
		"source": "miccai",
		"year": 2017,
		"key": "ab526ed4-7f2b-4707-9733-6348431683ed",
		"use": 1
	},
	{
		"Title": "Multi-level Multi-task Structured Sparse Learning for Diagnosis of Schizophrenia Disease",
		"Description": "Mingliang Wang, Xiaoke Hao, Jiashuang Huang, Kangcheng Wang, Xijia Xu, Daoqiang Zhang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In recent studies, it has attracted increasing attention in multi-frequency bands analysis for diagnosis of schizophrenia (SZ). However, most existing feature selection methods designed for multi-frequency bands analysis do not take into account the inherent structures (i.e., both frequency specificity and complementary information) from multi-frequency bands in the model, which are limited to identify the discriminative feature subset in a single step. To address this problem, we propose a multi-level multi-task structured sparse learning (MLMT-TS) framework to explicitly consider the common features with a hierarchical structure. Specifically, we introduce two regularization terms in the hierarchical framework to impose the common features across different bands and the specificity from individuals. Then, the selected features are used to construct multiple support vector machine (SVM) classifiers. Finally, we adopt an ensemble strategy to combine outputs of all SVM classifiers to achieve the final decision. Our method has been evaluated on 46 subjects, and the superior classification results demonstrate the effectiveness of our proposed method as compared to other methods.",
		"email": [
			"dqzhang@nuaa.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_6",
		"source": "miccai",
		"year": 2017,
		"key": "8c1be504-64c6-40c1-b859-019349aa5f06",
		"use": 1
	},
	{
		"Title": "Improving Needle Detection in 3D Ultrasound Using Orthogonal-Plane Convolutional Networks",
		"Description": "Arash Pourtaherian, Farhad Ghazvinian Zanjani, Svitlana Zinger, Nenad Mihajlovic, Gary Ng, Hendrikus Korsten, Peter de With.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Successful automated detection of short needles during an intervention is necessary to allow the physician identify and correct any misalignment of the needle and the target at early stages, which reduces needle passes and improves health outcomes. In this paper, we present a novel approach to detect needle voxels in 3D ultrasound volume with high precision using convolutional neural networks. Each voxel is classified from locally-extracted raw data of three orthogonal planes centered on it. We propose a bootstrap re-sampling approach to enhance the training in our highly imbalanced data. The proposed method successfully detects 17G and 22G needles with a single trained network, showing a robust generalized approach. Extensive ex-vivo evaluations on 3D ultrasound datasets of chicken breast show 25% increase in F1-score over the state-of-the-art feature-based method. Furthermore, very short needles inserted for only 5 mm in the volume are detected with tip localization errors of \\({<}\\)0.5 mm, indicating that the tip is always visible in the detected plane.",
		"email": [
			"a.pourtaherian@tue.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_69",
		"source": "miccai",
		"year": 2017,
		"key": "76636907-6f4a-4ec4-bb4b-f18d04a62c4d",
		"use": 1
	},
	{
		"Title": "Image Super Resolution Using Generative Adversarial Networks and Local Saliency Maps for Retinal Image Analysis",
		"Description": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Sajini Hewavitharanage, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose an image super resolution (ISR) method using generative adversarial networks (GANs) that takes a low resolution input fundus image and generates a high resolution super resolved (SR) image upto scaling factor of 16. This facilitates more accurate automated image analysis, especially for small or blurred landmarks and pathologies. Local saliency maps, which define each pixels importance, are used to define a novel saliency loss in the GAN cost function. Experimental results show the resulting SR images have perceptual quality very close to the original images and perform better than competing methods that do not weigh pixels according to their importance. When used for retinal vasculature segmentation, our SR images result in accuracy levels close to those obtained when using the original images.",
		"email": [
			"dwarim@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_44",
		"source": "miccai",
		"year": 2017,
		"key": "4f9b4e28-1dcf-4d76-ae43-fdf66bb9ded4",
		"use": 1
	},
	{
		"Title": "Connectome-Based Pattern Learning Predicts Histology and Surgical Outcome of Epileptogenic Malformations of Cortical Development",
		"Description": "Seok-Jun Hong, Boris Bernhardt, Ravnoor Gill, Neda Bernasconi, Andrea Bernasconi.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Focal cortical dysplasia (FCD) type II, a surgically amenable epileptogenic malformation, is characterized by intracortical dyslamination and dysmorphic neurons, either in isolation (IIA) or together with balloon cells (IIB). While evidence suggests diverging local function between these two histological grades, patterns of connectivity to the rest of the brain remain unknown. We present a novel MRI framework that subdivides a given FCD lesion into a set of smaller cortical patches using hierarchical clustering of resting-state functional connectivity profiles. We assessed the yield of this connectome-based subtyping to predict histological grade and response to surgery in individual patients. As the human functional connectome consists of multiple large-scale communities (e.g., the default mode and fronto-parietal networks), we dichotomized connectivity profiles of lesional patches into connectivity to the cortices belonging to the same functional community (intra-community) and to other communities (inter-community). Clustering these community-based patch profiles in 27 patients with histologically-proven FCD objectively identified three distinct lesional classes with (1) decreased intra- and inter-community connectivity, (2) decreased intra-community but normal inter-community connectivity, and (3) increased intra- as well as inter-community connectivity, relative to 34 healthy controls. Ensemble classifiers informed by these classes predicted histological grading (i.e., IIA vs. IIB) and post-surgical outcome (i.e., seizure-free vs. non-free) with high accuracy (84%, above-chance significance based on permutation tests, p < 0.01), suggesting benefits of MRI-based connectome stratification for individualized presurgical prognostics.",
		"email": [
			"sjhong@bic.mni.mcgill.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_45",
		"source": "miccai",
		"year": 2017,
		"key": "54ccae5b-629a-425a-b1b5-8dd18f045181",
		"use": 1
	},
	{
		"Title": "Robust Non-rigid Registration Through Agent-Based Action Learning",
		"Description": "Julian Krebs, Tommaso Mansi, Herv Delingette, Li Zhang, Florin C. Ghesu, Shun Miao, Andreas K. Maier, Nicholas Ayache, Rui Liao, Ali Kamen.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Robust image registration in medical imaging is essential for comparison or fusion of images, acquired from various perspectives, modalities or at different times. Typically, an objective function needs to be minimized assuming specific a priori deformation models and predefined or learned similarity measures. However, these approaches have difficulties to cope with large deformations or a large variability in appearance. Using modern deep learning (DL) methods with automated feature design, these limitations could be resolved by learning the intrinsic mapping solely from experience. We investigate in this paper how DL could help organ-specific (ROI-specific) deformable registration, to solve motion compensation or atlas-based segmentation problems for instance in prostate diagnosis. An artificial agent is trained to solve the task of non-rigid registration by exploring the parametric space of a statistical deformation model built from training data. Since it is difficult to extract trustworthy ground-truth deformation fields, we present a training scheme with a large number of synthetically deformed image pairs requiring only a small number of real inter-subject pairs. Our approach was tested on inter-subject registration of prostate MR data and reached a median DICE score of .88 in 2-D and .76 in 3-D, therefore showing improved results compared to state-of-the-art registration algorithms.",
		"email": [
			"julian.krebs@inria.fr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_40",
		"source": "miccai",
		"year": 2017,
		"key": "01216481-2b9e-4f77-99d9-0850ec13abe8",
		"use": 1
	},
	{
		"Title": "Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation",
		"Description": "Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Image segmentation is a fundamental problem in biomedical image analysis. Recent advances in deep learning have achieved promising results on many biomedical image segmentation benchmarks. However, due to large variations in biomedical images (different modalities, image settings, objects, noise, etc.), to utilize deep learning on a new application, it usually needs a new set of training data. This can incur a great deal of annotation effort and cost, because only biomedical experts can annotate effectively, and often there are too many instances in images (e.g., cells) to annotate. In this paper, we aim to address the following question: With limited effort (e.g., time) for annotation, what instances should be annotated in order to attain the best performance? We present a deep active learning framework that combines fully convolutional network (FCN) and active learning to significantly reduce annotation effort by making judicious suggestions on the most effective annotation areas. We utilize uncertainty and similarity information provided by FCN and formulate a generalized version of the maximum set cover problem to determine the most representative and uncertain areas for annotation. Extensive experiments using the 2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image segmentation dataset show that, using annotation suggestions by our method, state-of-the-art segmentation performance can be achieved by using only 50% of training data.",
		"email": [
			"lyang5@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_46",
		"source": "miccai",
		"year": 2017,
		"key": "0bf2f7a9-0374-425e-8c57-322f378a11ce",
		"use": 1
	},
	{
		"Title": "Deep Neural Networks Predict Remaining Surgery Duration from Cholecystectomy Videos",
		"Description": "Ivan Aksamentov, Andru Putra Twinanda, Didier Mutter, Jacques Marescaux, Nicolas Padoy.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "For every hospital, it is desirable to fully utilize its operating room (OR) capacity. Inaccurate planning of OR occupancy impacts patient comfort, safety and financial turnover of the hospital. A source of suboptimal scheduling often lies in the incorrect estimation of the surgery duration, which may vary significantly due to the diversity of patient conditions, surgeon skills and intraoperative situations. We propose automatic methods to estimate the remaining surgery duration in real-time by using only the image feed from the endoscopic camera and no other sensor. These approaches are based on neural networks designed to learn the workflow of an endoscopic procedure. We train and evaluate our models on a large dataset of 120 endoscopic cholecystectomies. Results show the strong benefits of these approaches when surgeries last longer than usual and promise practical improvements in OR management.",
		"email": [
			"ivan.aksamentov@etu.unistra.fr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_66",
		"source": "miccai",
		"year": 2017,
		"key": "1b58ba2b-12d6-4e45-9afd-f16c8cbeaf39",
		"use": 1
	},
	{
		"Title": "DOTE: Dual cOnvolutional filTer lEarning for Super-Resolution and Cross-Modality Synthesis in MRI",
		"Description": "Yawen Huang, Ling Shao, Alejandro F. Frangi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Cross-modal image synthesis is a topical problem in medical image computing. Existing methods for image synthesis are either tailored to a specific application, require large scale training sets, or are based on partitioning images into overlapping patches. In this paper, we propose a novel Dual cOnvolutional filTer lEarning (DOTE) approach to overcome the drawbacks of these approaches. We construct a closed loop joint filter learning strategy that generates informative feedback for model self-optimization. Our method can leverage data more efficiently thus reducing the size of the required training set. We extensively evaluate DOTE in two challenging tasks: image super-resolution and cross-modality synthesis. The experimental results demonstrate superior performance of our method over other state-of-the-art methods.",
		"email": [
			"yhuang36@sheffield.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_11",
		"source": "miccai",
		"year": 2017,
		"key": "39c0c3c7-d3ce-4aa7-8238-0ddc2cbc1d86",
		"use": 1
	},
	{
		"Title": "Classification of Pancreatic Cysts in Computed Tomography Images Using a Random Forest and Convolutional Neural Network Ensemble",
		"Description": "Konstantin Dmitriev, Arie E. Kaufman, Ammar A. Javed, Ralph H. Hruban, Elliot K. Fishman, Anne Marie Lennon, Joel H. Saltz.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "There are many different types of pancreatic cysts. These range from completely benign to malignant, and identifying the exact cyst type can be challenging in clinical practice. This work describes an automatic classification algorithm that classifies the four most common types of pancreatic cysts using computed tomography images. The proposed approach utilizes the general demographic information about a patient as well as the imaging appearance of the cyst. It is based on a Bayesian combination of the random forest classifier, which learns subclass-specific demographic, intensity, and shape features, and a new convolutional neural network that relies on the fine texture information. Quantitative assessment of the proposed method was performed using a 10-fold cross validation on 134 patients and reported a classification accuracy of 83.6%.",
		"email": [
			"kdmitriev@cs.stonybrook.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_18",
		"source": "miccai",
		"year": 2017,
		"key": "96b4dc16-adb4-42f1-ab0b-7ddb6091f794",
		"use": 1
	},
	{
		"Title": "Medical Image Synthesis with Context-Aware Generative Adversarial Networks",
		"Description": "Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su Ruan, Qian Wang, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Computed tomography (CT) is critical for various clinical applications, e.g., radiation treatment planning and also PET attenuation correction in MRI/PET scanner. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve radiations. Therefore, recently researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiation planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network (FCN) to generate CT given the MR image. To better model the nonlinear mapping from MRI to CT and produce more realistic images, we propose to use the adversarial training strategy to train the FCN. Moreover, we propose an image-gradient-difference based loss function to alleviate the blurriness of the generated CT. We further apply Auto-Context Model (ACM) to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MR images, and also outperforms three state-of-the-art methods under comparison.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_48",
		"source": "miccai",
		"year": 2017,
		"key": "9e6d81e8-1a2e-4ce0-9bec-c1f543d09173",
		"use": 1
	},
	{
		"Title": "Clinical Target-Volume Delineation in Prostate Brachytherapy Using Residual Neural Networks",
		"Description": "Emran Mohammad Abu Anas, Saman Nouranian, S. Sara Mahdavi, Ingrid Spadinger, William J. Morris, Septimu E. Salcudean, Parvin Mousavi, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Low dose-rate prostate brachytherapy is commonly used to treat early stage prostate cancer. This intervention involves implanting radioactive seeds inside a volume containing the prostate. Planning the intervention requires obtaining a series of ultrasound images from the prostate. This is followed by delineation of a clinical target volume, which mostly traces the prostate boundary in the ultrasound data, but can be modified based on institution-specific clinical guidelines. Here, we aim to automate the delineation of clinical target volume by using a new deep learning network based on residual neural nets and dilated convolution at deeper layers. In addition, we propose to include an exponential weight map in the optimization to improve local prediction. We train the network on 4,284 expert-labeled transrectal ultrasound images and test it on an independent set of 1,081 ultrasound images. With respect to the gold-standard delineation, we achieve a mean Dice similarity coefficient of 94%, a mean surface distance error of 1.05 mm and a mean Hausdorff distance error of 3.0 mm. The obtained results are statistically significantly better than two previous state-of-the-art techniques.",
		"email": [
			"emrana@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_42",
		"source": "miccai",
		"year": 2017,
		"key": "e29fbb26-9cbd-44e1-985b-d3edc4651795",
		"use": 1
	},
	{
		"Title": "Learning-Based Spatiotemporal Regularization and Integration of Tracking Methods for Regional 4D Cardiac Deformation Analysis",
		"Description": "Allen Lu, Maria Zontak, Nripesh Parajuli, John C. Stendahl, Nabil Boutagy, Melissa Eberle, Imran Alkhalil, Matthew ODonnell, Albert J. Sinusas, James S. Duncan.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Dense cardiac motion tracking and deformation analysis from echocardiography is important for detection and localization of myocardial dysfunction. However, tracking methods are often unreliable due to inherent ultrasound imaging properties. In this work, we propose a new data-driven spatiotemporal regularization strategy. We generate 4D Lagrangian displacement patches from different input sources as training data and learn the regularization procedure via a multi-layered perceptron (MLP) network. The learned regularization procedure is applied to initial noisy tracking results. We further propose a framework for integrating tracking methods to produce better overall estimations. We demonstrate the utility of this approach on block-matching, surface tracking, and free-form deformation-based methods. Finally, we quantitatively and qualitatively evaluate our performance on both tracking and strain accuracy using both synthetic and in vivo data.",
		"email": [
			"allen.lu@yale.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_37",
		"source": "miccai",
		"year": 2017,
		"key": "104c30ba-f019-4a53-9d15-209e2e9d3e06",
		"use": 1
	},
	{
		"Title": "Joint Craniomaxillofacial Bone Segmentation and Landmark Digitization by Context-Guided Fully Convolutional Networks",
		"Description": "Jun Zhang, Mingxia Liu, Li Wang, Si Chen, Peng Yuan, Jianfu Li, Steve Guo-Fang Shen, Zhen Tang, Ken-Chung Chen, James J. Xia, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Generating accurate 3D models from cone-beam computed tomography (CBCT) images is an important step in developing treatment plans for patients with craniomaxillofacial (CMF) deformities. This process often involves bone segmentation and landmark digitization. Since anatomical landmarks generally lie on the boundaries of segmented bone regions, the tasks of bone segmentation and landmark digitization could be highly correlated. However, most existing methods simply treat them as two standalone tasks, without considering their inherent association. In addition, these methods usually ignore the spatial context information (i.e., displacements from voxels to landmarks) in CBCT images. To this end, we propose a context-guided fully convolutional network (FCN) for joint bone segmentation and landmark digitization. Specifically, we first train an FCN to learn the displacement maps to capture the spatial context information in CBCT images. Using the learned displacement maps as guidance information, we further develop a multi-task FCN to jointly perform bone segmentation and landmark digitization. Our method has been evaluated on 107 subjects from two centers, and the experimental results show that our method is superior to the state-of-the-art methods in both bone segmentation and landmark digitization.",
		"email": [
			"jxia@houstonmethodist.org"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_81",
		"source": "miccai",
		"year": 2017,
		"key": "b0f422a3-ce15-4326-b28b-ded1e906fffd",
		"use": 1
	},
	{
		"Title": "Statistical Learning of Spatiotemporal Patterns from Longitudinal Manifold-Valued Networks",
		"Description": "I. Koval, J.-B. Schiratti, A. Routier, M. Bacci, O. Colliot, S. Allassonnire, S. Durrleman, The Alzheimers Disease Neuroimaging Initiative.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "We introduce a mixed-effects model to learn spatiotemporal patterns on a network by considering longitudinal measures distributed on a fixed graph. The data come from repeated observations of subjects at different time points which take the form of measurement maps distributed on a graph such as an image or a mesh. The model learns a typical group-average trajectory characterizing the propagation of measurement changes across the graph nodes. The subject-specific trajectories are defined via spatial and temporal transformations of the group-average scenario, thus estimating the variability of spatiotemporal patterns within the group. To estimate population and individual model parameters, we adapted a stochastic version of the Expectation-Maximization algorithm, the MCMC-SAEM. The model is used to describe the propagation of cortical atrophy during the course of Alzheimers Disease. Model parameters show the variability of this average pattern of atrophy in terms of trajectories across brain regions, age at disease onset and pace of propagation. We show that the personalization of this model yields accurate prediction of maps of cortical thickness in patients.",
		"email": [
			"igor.koval@icm-institute.org"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_52",
		"source": "miccai",
		"year": 2017,
		"key": "fbfd5f08-c8ce-48b6-a21e-18570dc81351",
		"use": 1
	},
	{
		"Title": "Distance Metric Learning Using Graph Convolutional Networks: Application to Functional Brain Networks",
		"Description": "Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Evaluating similarity between graphs is of major importance in several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. The choice of a distance or similarity metric is, however, not trivial and can be highly dependent on the application at hand. In this work, we propose a novel metric learning method to evaluate distance between graphs that leverages the power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. We demonstrate the potential of our method in the field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. In this problem, the definition of an appropriate graph similarity function is critical to unveil patterns of disruptions associated with certain brain disorders. Experimental results on the ABIDE dataset show that our method can learn a graph similarity metric tailored for a clinical application, improving the performance of a simple k-nn classifier by 11.9% compared to a traditional distance metric.",
		"email": [
			"ira.ktena@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_54",
		"source": "miccai",
		"year": 2017,
		"key": "73394c0a-2816-447e-b423-2e26f9b5a6b9",
		"use": 1
	},
	{
		"Title": "Training CNNs for Image Registration from Few Samples with Model-based Data Augmentation",
		"Description": "Hristina Uzunova, Matthias Wilms, Heinz Handels, Jan Ehrhardt.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Convolutional neural networks (CNNs) have been successfully used for fast and accurate estimation of dense correspondences between images in computer vision applications. However, much of their success is based on the availability of large training datasets with dense ground truth correspondences, which are only rarely available in medical applications. In this paper, we, therefore, address the problem of CNNs learning from few training data for medical image registration. Our contributions are threefold: (1) We present a novel approach for learning highly expressive appearance models from few training samples, (2) we show that this approach can be used to synthesize huge amounts of realistic ground truth training data for CNN-based medical image registration, and (3) we adapt the FlowNet architecture for CNN-based optical flow estimation to the medical image registration problem. This pipeline is applied to two medical data sets with less than 40 training images. We show that CNNs learned from the proposed generative model outperform those trained on random deformations or displacement fields estimated via classical image registration.",
		"email": [
			"ehrhardt@imi.uni-luebeck.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_26",
		"source": "miccai",
		"year": 2017,
		"key": "30dc6851-fba4-4315-8457-78079a3eea8f",
		"use": 1
	},
	{
		"Title": "Nonrigid Image Registration Using Multi-scale 3D Convolutional Neural Networks",
		"Description": "Hessam Sokooti, Bob de Vos, Floris Berendsen, Boudewijn P. F. Lelieveldt, Ivana Igum, Marius Staring.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper we propose a method to solve nonrigid image registration through a learning approach, instead of via iterative optimization of a predefined dissimilarity metric. We design a Convolutional Neural Network (CNN) architecture that, in contrast to all other work, directly estimates the displacement vector field (DVF) from a pair of input images. The proposed RegNet is trained using a large set of artificially generated DVFs, does not explicitly define a dissimilarity metric, and integrates image content at multiple scales to equip the network with contextual information. At testing time nonrigid registration is performed in a single shot, in contrast to current iterative methods. We tested RegNet on 3D chest CT follow-up data. The results show that the accuracy of RegNet is on par with a conventional B-spline registration, for anatomy within the capture range. Training RegNet with artificially generated DVFs is therefore a promising approach for obtaining good results on real clinical data, thereby greatly simplifying the training problem. Deformable image registration can therefore be successfully casted as a learning problem.",
		"email": [
			"h.sokooti_oskooyi@lumc.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_27",
		"source": "miccai",
		"year": 2017,
		"key": "4d2a4df9-ef0f-46a7-afec-ac044a704772",
		"use": 1
	},
	{
		"Title": "Learning and Incorporating Shape Models for Semantic Segmentation",
		"Description": "H. Ravishankar, R. Venkataramani, S. Thiruvenkadam, P. Sudhakar, V. Vaidya.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Semantic segmentation has been popularly addressed using Fully convolutional networks (FCN) (e.g. U-Net) with impressive results and has been the forerunner in recent segmentation challenges. However, FCN approaches do not necessarily incorporate local geometry such as smoothness and shape, whereas traditional image analysis techniques have benefitted greatly by them in solving segmentation and tracking problems. In this work, we address the problem of incorporating shape priors within the FCN segmentation framework. We demonstrate the utility of such a shape prior in robust handling of scenarios such as loss of contrast and artifacts. Our experiments show \\(\\approx 5\\%\\) improvement over U-Net for the challenging problem of ultrasound kidney segmentation.",
		"email": [
			"rahul.Venkataramani@ge.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_24",
		"source": "miccai",
		"year": 2017,
		"key": "5579ac85-a36b-465f-aa4c-34e71f1120a7",
		"use": 1
	},
	{
		"Title": "Learning Deep Features for Automated Placement of Correspondence Points on Ensembles of Complex Shapes",
		"Description": "Praful Agrawal, Ross T. Whitaker, Shireen Y. Elhabian.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Correspondence-based shape models are an enabling technology for various medical imaging applications that rely on statistical analysis of populations of anatomical shape. One strategy for automatic correspondence placement is to simultaneously learn a compact representation of the underlying anatomical variation in the shape space while capturing the geometric characteristics of individual shapes. The inherent geometric complexity and population-level shape variation in anatomical structures introduce significant challenges in finding optimal shape correspondence models. Existing approaches adopt iterative optimization schemes with objective functions derived from probabilistic modeling of shape space, e.g. entropy of Gaussian-distributed shape space, to find useful sets of dense correspondence on shape ensembles. Nonetheless, anatomical shape distributions can be far more complex than this Gaussian assumption, which entails linear shape variation. Recent works address this limitation by adopting an application-specific notion of correspondence through lifting positional data to a higher-dimensional feature space (e.g. sulcal depth, brain connectivity, and geodesic distance to anatomical landmarks), with the goal of simplifying the optimization problem. However, this typically requires a careful selection of hand-crafted features and their success heavily rely on expertise in finding such features consistently. This paper proposes an automated feature learning approach using deep convolutional neural networks for optimization of dense point correspondence on shape ensembles. The proposed method endows anatomical shapes with learned features that enhance the shape correspondence objective function to deal with complex objects and populations. Results demonstrate that deep learning based features perform better than methods that rely on position and compete favorably with hand-crafted features.",
		"email": [
			"prafulag@cs.utah.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_22",
		"source": "miccai",
		"year": 2017,
		"key": "4b8bce6f-46c8-4917-a911-7e6559f990af",
		"use": 1
	},
	{
		"Title": "Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans",
		"Description": "Yuyin Zhou, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic segmentation of an organ and its cystic region is a prerequisite of computer-aided diagnosis. In this paper, we focus on pancreatic cyst segmentation in abdominal CT scan. This task is important and very useful in clinical practice yet challenging due to the low contrast in boundary, the variability in location, shape and the different stages of the pancreatic cancer. Inspired by the high relevance between the location of a pancreas and its cystic region, we introduce extra deep supervision into the segmentation network, so that cyst segmentation can be improved with the help of relatively easier pancreas segmentation. Under a reasonable transformation function, our approach can be factorized into two stages, and each stage can be efficiently optimized via gradient back-propagation throughout the deep networks. We collect a new dataset with 131 pathological samples, which, to the best of our knowledge, is the largest set for pancreatic cyst segmentation. Without human assistance, our approach reports a \\(63.44\\%\\) average accuracy, measured by the Dice-Srensen coefficient (DSC), which is higher than the number (\\(60.46\\%\\)) without deep supervision.",
		"email": [
			"198808xc@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_26",
		"source": "miccai",
		"year": 2017,
		"key": "494d7b3d-3983-4071-a025-95c4d36cbe51",
		"use": 1
	},
	{
		"Title": "Multimodal Image Registration with Deep Context Reinforcement Learning",
		"Description": "Kai Ma, Jiangping Wang, Vivek Singh, Birgi Tamersoy, Yao-Jen Chang, Andreas Wimmer, Terrence Chen.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic and robust registration between real-time patient imaging and pre-operative data (e.g. CT and MRI) is crucial for computer-aided interventions and AR-based navigation guidance. In this paper, we present a novel approach to automatically align range image of the patient with pre-operative CT images. Unlike existing approaches based on the surface similarity optimization process, our algorithm leverages the contextual information of medical images to resolve data ambiguities and improve robustness. The proposed algorithm is derived from deep reinforcement learning algorithm that automatically learns to extract optimal feature representation to reduce the appearance discrepancy between these two modalities. Quantitative evaluations on 1788 pairs of CT and depth images from real clinical setting demonstrate that the proposed method achieves the state-of-the-art performance.",
		"email": [
			"kai.ma@siemens.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_28",
		"source": "miccai",
		"year": 2017,
		"key": "a1d021b0-7a84-4f18-833f-0789cea02b15",
		"use": 1
	},
	{
		"Title": "Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness",
		"Description": "Wufeng Xue, Andrea Lum, Ashley Mercado, Mark Landis, James Warrington, Shuo Li.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: (1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; (2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190 mm\\(^2\\), 1.41 mm, 2.68 mm for average areas, RWT, dimensions and error rate of 10.4% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function.",
		"email": [
			"slishuo@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_32",
		"source": "miccai",
		"year": 2017,
		"key": "624a1790-4af2-4f6e-8ee3-f080fd301d48",
		"use": 1
	},
	{
		"Title": "Detection and Characterization of the Fetal Heartbeat in Free-hand Ultrasound Sweeps with Weakly-supervised Two-streams Convolutional Networks",
		"Description": "Yuan Gao, J. Alison Noble.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Assessment of fetal cardiac activity is essential to confirm pregnancy viability in obstetric ultrasound. However, automated detection and localization of a beating fetal heart, in free-hand ultrasound sweeps, is a very challenging task, due to high variation in heart appearance, scale and position (because of heart deformation, scanning orientations and artefacts). In this paper, we present a two-stream Convolutional Network (ConvNet) -a temporal sequence learning model- that recognizes heart frames and localizes the heart using only weak supervision. Our contribution is three-fold: (i) to the best of our knowledge, this is the first work to use two-stream spatio-temporal ConvNets in analysis of free-hand fetal ultrasound videos. The model is compact, and can be trained end-to-end with only image level labels, (ii) the model enforces rotation invariance, which does not require additional augmentation in the training data, and (iii) the model is particularly robust for heart detection, which is important in our application where there can be additional distracting textures, such as acoustic shadows. Our results demonstrate that the proposed two-stream ConvNet architecture significantly outperforms single stream spatial ConvNets (90.3% versus 74.9%), in terms of heart identification.",
		"email": [
			"Yuan.Gao2@eng.ox.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_35",
		"source": "miccai",
		"year": 2017,
		"key": "fcbc1221-908c-477f-a196-67e9fd7f314e",
		"use": 1
	},
	{
		"Title": "Direct Detection of Pixel-Level Myocardial Infarction Areas via a Deep-Learning Algorithm",
		"Description": "Chenchu Xu, Lei Xu, Zhifan Gao, Shen Zhao, Heye Zhang, Yanping Zhang, Xiuquan Du, Shu Zhao, Dhanjoo Ghista, Shuo Li.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.",
		"email": [
			"hy.zhang@siat.ac.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_28",
		"source": "miccai",
		"year": 2017,
		"key": "b7397772-45d0-4a41-9295-6017d317f30f",
		"use": 1
	},
	{
		"Title": "DARWIN: Deformable Patient Avatar Representation With Deep Image Network",
		"Description": "Vivek Singh, Kai Ma, Birgi Tamersoy, Yao-Jen Chang, Andreas Wimmer, Thomas ODonnell, Terrence Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we present a technical approach to robustly estimate the detailed patient body surface mesh under clothing cover from a single snapshot of a range sensor. Existing methods either lack level of detail of the estimated patient body model, fail to estimate the body model robustly under clothing cover, or lack sufficient evaluation over real patient datasets. In this work, we overcome these limitations by learning deep convolutional networks over real clinical dataset with large variation and augmentation. Our approach is validated with experiments conducted over 1063 human subjects from 3 different hospitals and surface errors are measured against groundtruth from CT data.",
		"email": [
			"vivek-singh@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_56",
		"source": "miccai",
		"year": 2017,
		"key": "79c30bcc-52db-44c3-ae95-ecdb20321da4",
		"use": 1
	},
	{
		"Title": "Maximum Mean Discrepancy Based Multiple Kernel Learning for Incomplete Multimodality Neuroimaging Data",
		"Description": "Xiaofeng Zhu, Kim-Han Thung, Ehsan Adeli, Yu Zhang, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "It is challenging to use incomplete multimodality data for Alzheimers Disease (AD) diagnosis. The current methods to address this challenge, such as low-rank matrix completion (i.e., imputing the missing values and unknown labels simultaneously) and multi-task learning (i.e., defining one regression task for each combination of modalities and then learning them jointly), are unable to model the complex data-to-label relationship in AD diagnosis and also ignore the heterogeneity among the modalities. In light of this, we propose a new Maximum Mean Discrepancy (MMD) based Multiple Kernel Learning (MKL) method for AD diagnosis using incomplete multimodality data. Specifically, we map all the samples from different modalities into a Reproducing Kernel Hilbert Space (RKHS), by devising a new MMD algorithm. The proposed MMD method incorporates data distribution matching, pair-wise sample matching and feature selection in an unified formulation, thus alleviating the modality heterogeneity issue and making all the samples comparable to share a common classifier in the RKHS. The resulting classifier obviously captures the nonlinear data-to-label relationship. We have tested our method using MRI and PET data from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset for AD diagnosis. The experimental results show that our method outperforms other methods.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_9",
		"source": "miccai",
		"year": 2017,
		"key": "cbb5e3c1-59f9-4cdc-8604-f2d0f9eba989",
		"use": 1
	},
	{
		"Title": "Active Learning and Proofreading for Delineation of Curvilinear Structures",
		"Description": "Agata Mosinska, Jakub Tarnawski, Pascal Fua.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Many state-of-the-art delineation methods rely on supervised machine learning algorithms. As a result, they require manually annotated training data, which is tedious to obtain. Furthermore, even minor classification errors may significantly affect the topology of the final result. In this paper we propose a generic approach to addressing both of these problems by taking into account the influence of a potential misclassification on the resulting delineation. In an Active Learning context, we identify parts of linear structures that should be annotated first in order to train a classifier effectively. In a proofreading context, we similarly find regions of the resulting reconstruction that should be verified in priority to obtain a nearly-perfect result. In both cases, by focusing the attention of the human expert on potential classification mistakes which are the most critical parts of the delineation, we reduce the amount of required supervision. We demonstrate the effectiveness of our approach on microscopy images depicting blood vessels and neurons.",
		"email": [
			"agata.mosinska@epfl.ch"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_19",
		"source": "miccai",
		"year": 2017,
		"key": "6cdc1a0f-0fa7-4daa-98b0-eea90ee65a5b",
		"use": 1
	},
	{
		"Title": "Deep Multi-task Multi-channel Learning for Joint Classification and Regression of Brain Status",
		"Description": "Mingxia Liu, Jun Zhang, Ehsan Adeli, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Jointly identifying brain diseases and predicting clinical scores have attracted increasing attention in the domain of computer-aided diagnosis using magnetic resonance imaging (MRI) data, since these two tasks are highly correlated. Although several joint learning models have been developed, most existing methods focus on using human-engineered features extracted from MRI data. Due to the possible heterogeneous property between human-engineered features and subsequent classification/regression models, those methods may lead to sub-optimal learning performance. In this paper, we propose a deep multi-task multi-channel learning (DM\\(^2\\)L) framework for simultaneous classification and regression for brain disease diagnosis, using MRI data and personal information (i.e., age, gender, and education level) of subjects. Specifically, we first identify discriminative anatomical landmarks from MR images in a data-driven manner, and then extract multiple image patches around these detected landmarks. A deep multi-task multi-channel convolutional neural network is then developed for joint disease classification and clinical score regression. We train our model on a large multi-center cohort (i.e., ADNI-1) and test it on an independent cohort (i.e., ADNI-2). Experimental results demonstrate that DM\\(^2\\)L is superior to the state-of-the-art approaches in brain diasease diagnosis.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_1",
		"source": "miccai",
		"year": 2017,
		"key": "bec31f41-d0ad-44e4-92ba-c01483809fa7",
		"use": 1
	},
	{
		"Title": "Intraoperative Organ Motion Models with an Ensemble of Conditional Generative Adversarial Networks",
		"Description": "Yipeng Hu, Eli Gibson, Tom Vercauteren, Hashim U. Ahmed, Mark Emberton, Caroline M. Moore, J. Alison Noble, Dean C. Barratt.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we describe how a patient-specific, ultrasound-probe-induced prostate motion model can be directly generated from a single preoperative MR image. Our motion model allows for sampling from the conditional distribution of dense displacement fields, is encoded by a generative neural network conditioned on a medical image, and accepts random noise as additional input. The generative network is trained by a minimax optimisation with a second discriminative neural network, tasked to distinguish generated samples from training motion data. In this work, we propose that (1) jointly optimising a third conditioning neural network that pre-processes the input image, can effectively extract patient-specific features for conditioning; and (2) combining multiple generative models trained separately with heuristically pre-disjointed training data sets can adequately mitigate the problem of mode collapse. Trained with diagnostic T2-weighted MR images from 143 real patients and 73,216 3D dense displacement fields from finite element simulations of intraoperative prostate motion due to transrectal ultrasound probe pressure, the proposed models produced physically-plausible patient-specific motion of prostate glands. The ability to capture biomechanically simulated motion was evaluated using two errors representing generalisability and specificity of the model. The median values, calculated from a 10-fold cross-validation, were 2.8  0.3 mm and 1.7  0.1 mm, respectively. We conclude that the introduced approach demonstrates the feasibility of applying state-of-the-art machine learning algorithms to generate organ motion models from patient images, and shows significant promise for future research.",
		"email": [
			"yipeng.hu@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_42",
		"source": "miccai",
		"year": 2017,
		"key": "7070a246-17c9-4714-9a10-9673cec66b5f",
		"use": 1
	},
	{
		"Title": "Deep Learning for Isotropic Super-Resolution from Non-isotropic 3D Electron Microscopy",
		"Description": "Larissa Heinrich, John A. Bogovic, Stephan Saalfeld.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "The most sophisticated existing methods to generate 3D isotropic super-resolution (SR) from non-isotropic electron microscopy (EM) are based on learned dictionaries. Unfortunately, none of the existing methods generate practically satisfying results. For 2D natural images, recently developed super-resolution methods that use deep learning have been shown to significantly outperform the previous state of the art. We have adapted one of the most successful architectures (FSRCNN) for 3D super-resolution, and compared its performance to a 3D U-Net architecture that has not been used previously to generate super-resolution. We trained both architectures on artificially downscaled isotropic ground truth from focused ion beam milling scanning EM (FIB-SEM) and tested the performance for various hyperparameter settings.",
		"email": [
			"saalfelds@janelia.hhmi.org"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_16",
		"source": "miccai",
		"year": 2017,
		"key": "21c603b3-aaa9-4952-94fa-53c592349d27",
		"use": 1
	},
	{
		"Title": "Neuron Segmentation Using Deep Complete Bipartite Networks",
		"Description": "Jianxu Chen, Sreya Banerjee, Abhinav Grama, Walter J. Scheirer, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we consider the problem of automatically segmenting neuronal cells in dual-color confocal microscopy images. This problem is a key task in various quantitative analysis applications in neuroscience, such as tracing cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using fully convolutional networks (FCN), has profoundly changed segmentation research in biomedical imaging. We face two major challenges in this problem. First, neuronal cells may form dense clusters, making it difficult to correctly identify all individual cells (even to human experts). Consequently, segmentation results of the known FCN-type models are not accurate enough. Second, pixel-wise ground truth is difficult to obtain. Only a limited amount of approximate instance-wise annotation can be collected, which makes the training of FCN models quite cumbersome. We propose a new FCN-type deep learning model, called deep complete bipartite networks (CB-Net), and a new scheme for leveraging approximate instance-wise annotation to train our pixel-wise prediction model. Evaluated using seven real datasets, our proposed new CB-Net model outperforms the state-of-the-art FCN models and produces neuron segmentation results of remarkable quality.",
		"email": [
			"jchen16@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_3",
		"source": "miccai",
		"year": 2017,
		"key": "f27c57aa-d939-4f8b-ad4d-4267a3863f32",
		"use": 1
	},
	{
		"Title": "Isotropic Reconstruction of 3D Fluorescence Microscopy Images Using Convolutional Neural Networks",
		"Description": "Martin Weigert, Loic Royer, Florian Jug, Gene Myers.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to 3 synthetic and 3 real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.",
		"email": [
			"mweigert@mpi-cbg.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_15",
		"source": "miccai",
		"year": 2017,
		"key": "0f778905-5108-4b43-b90d-d35faf5b09d4",
		"use": 1
	},
	{
		"Title": "Deformable Image Registration Based on Similarity-Steered CNN Regression",
		"Description": "Xiaohuan Cao, Jianhua Yang, Jun Zhang, Dong Nie, Minjeong Kim, Qian Wang, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "\nExisting deformable registration methods require exhaustively iterative optimization, along with careful parameter tuning, to estimate the deformation field between images. Although some learning-based methods have been proposed for initiating deformation estimation, they are often template-specific and not flexible in practical use. In this paper, we propose a convolutional neural network (CNN) based regression model to directly learn the complex mapping from the input image pair (i.e., a pair of template and subject) to their corresponding deformation field. Specifically, our CNN architecture is designed in a patch-based manner to learn the complex mapping from the input patch pairs to their respective deformation field. First, the equalized active-points guided sampling strategy is introduced to facilitate accurate CNN model learning upon a limited image dataset. Then, the similarity-steered CNN architecture is designed, where we propose to add the auxiliary contextual cue, i.e., the similarity between input patches, to more directly guide the learning process. Experiments on different brain image datasets demonstrate promising registration performance based on our CNN model. Furthermore, it is found that the trained CNN model from one dataset can be successfully transferred to another dataset, although brain appearances across datasets are quite variable.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_35",
		"source": "miccai",
		"year": 2017,
		"key": "d5cde4b2-e1fe-43e3-9c16-20a0dcd67ceb",
		"use": 1
	},
	{
		"Title": "SVF-Net: Learning Deformable Image Registration Using Shape Matching",
		"Description": "Marc-Michel Roh, Manasi Datar, Tobias Heimann, Maxime Sermesant, Xavier Pennec.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we propose an innovative approach for registration based on the deterministic prediction of the parameters from both images instead of the optimization of a energy criteria. The method relies on a fully convolutional network whose architecture consists of contracting layers to detect relevant features and a symmetric expanding path that matches them together and outputs the transformation parametrization. Whereas convolutional networks have seen a widespread expansion and have been already applied to many medical imaging problems such as segmentation and classification, its application to registration has so far faced the challenge of defining ground truth data on which to train the algorithm. Here, we present a novel training strategy to build reference deformations which relies on the registration of segmented regions of interest. We apply this methodology to the problem of inter-patient heart registration and show an important improvement over a state of the art optimization based algorithm. Not only our method is more accurate but it is also faster - registration of two 3D-images taking less than 30 ms second on a GPU - and more robust to outliers.",
		"email": [],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_31",
		"source": "miccai",
		"year": 2017,
		"key": "cb4de793-676b-48ca-9f31-ab84784e6675",
		"use": 1
	},
	{
		"Title": "Scalable Multimodal Convolutional Networks for Brain Tumour Segmentation",
		"Description": "Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Brain tumour segmentation plays a key role in computer-assisted surgery. Deep neural networks have increased the accuracy of automatic segmentation significantly, however these models tend to generalise poorly to different imaging modalities than those for which they have been designed, thereby limiting their applications. For example, a network architecture initially designed for brain parcellation of monomodal T1 MRI can not be easily translated into an efficient tumour segmentation network that jointly utilises T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable multimodal deep learning architecture using new nested structures that explicitly leverage deep features within or across modalities. This aims at making the early layers of the architecture structured and sparse so that the final architecture becomes scalable to the number of modalities. We evaluate the scalable architecture for brain tumour segmentation and give evidence of its regularisation effect compared to the conventional concatenation approach.",
		"email": [
			"l.fidon@cs.ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_33",
		"source": "miccai",
		"year": 2017,
		"key": "e130851a-fd82-432b-aa52-74ebe10cd737",
		"use": 1
	},
	{
		"Title": "Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection",
		"Description": "Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li, Xiaogang Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose a convolution neural network based algorithm for simultaneously diagnosing diabetic retinopathy and highlighting suspicious regions. Our contributions are two folds: (1) a network termed Zoom-in-Net which mimics the zoom-in process of a clinician to examine the retinal images. Trained with only image-level supervisions, Zoom-in-Net can generate attention maps which highlight suspicious regions, and predicts the disease level accurately based on both the whole image and its high resolution suspicious patches. (2) Only four bounding boxes generated from the automatically learned attention maps are enough to cover 80% of the lesions labeled by an experienced ophthalmologist, which shows good localization ability of the attention maps. By clustering features at high response locations on the attention maps, we discover meaningful clusters which contain potential lesions in diabetic retinopathy. Experiments show that our algorithm outperform the state-of-the-art methods on two datasets, EyePACS and Messidor.",
		"email": [
			"zwang@ee.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_31",
		"source": "miccai",
		"year": 2017,
		"key": "f9d5f750-e718-4859-9de2-813093cbab2b",
		"use": 1
	},
	{
		"Title": "Boundary Regularized Convolutional Neural Network for Layer Parsing of Breast Anatomy in Automated Whole Breast Ultrasound",
		"Description": "Cheng Bian, Ran Lee, Yi-Hong Chou, Jie-Zhi Cheng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "A boundary regularized deep convolutional encoder-decoder network (ConvEDNet) is developed in this study to address the difficult anatomical layer parsing problem in the noisy Automated Whole Breast Ultrasound (AWBUS) images. To achieve better network initialization, a two-stage adaptive domain transfer (2DT) is employed to land the VGG-16 encoder on the AWBUS domain with the bridge of network training for AWBUS edge detector. The knowledge transferred encoder is denoted as VGG-USEdge. To further augment the training of ConvEDNet, a deep boundary supervision (DBS) strategy is introduced to regularize the feature learning for better robustness to speckle noise and shadowing effect. We argue that simply counting on the image context cue, which can be learnt with the guidance of label maps, may not be sufficient to deal with the intrinsic noisy property of ultrasound images. With the regularization of boundary cue, the segmentation learning can be boosted. The efficacy of the proposed 2DT-DBS ConvEDNet is corroborated with the extensive comparison to the state-of-the-art deep learning segmentation methods. The segmentation results may assist the clinical image reading, particularly for junior medical doctors and residents and help to reduce false-positive findings from a computer-aided detection scheme.",
		"email": [
			"jzcheng@ntu.edu.tw"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_30",
		"source": "miccai",
		"year": 2017,
		"key": "709ba2a6-026c-4b35-a07c-1c4db52b4496",
		"use": 1
	},
	{
		"Title": "Quality Assessment of Echocardiographic Cine Using Recurrent Neural Networks: Feasibility on Five Standard View Planes",
		"Description": "Amir H. Abdi, Christina Luong, Teresa Tsang, John Jue, Ken Gin, Darwin Yeung, Dale Hawley, Robert Rohling, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Echocardiography (echo) is a clinical imaging technique which is highly dependent on operator experience. We aim to reduce operator variability in data acquisition by automatically computing an echo quality score for real-time feedback. We achieve this with a deep neural network model, with convolutional layers to extract hierarchical features from the input echo cine and recurrent layers to leverage the sequential information in the echo cine loop. Using data from 509 separate patient studies, containing 2,450 echo cines across five standard echo imaging planes, we achieved a mean quality score accuracy of 85\\(\\%\\) compared to the gold-standard score assigned by experienced echosonographers. The proposed approach calculates the quality of a given 20 frame echo sequence within 10 ms, sufficient for real-time deployment.",
		"email": [
			"amirabdi@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_35",
		"source": "miccai",
		"year": 2017,
		"key": "db513b4e-5613-4db1-88e0-cd22add0babc",
		"use": 1
	},
	{
		"Title": "Fast Background Removal Method for 3D Multi-channel Deep Tissue Fluorescence Imaging",
		"Description": "Chenchen Li, Xiaowei Li, Hongji Cao, He Jiang, Xiaotie Deng, Danny Z. Chen, Lin Yang, Zhifeng Shao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "The recent advances in tissue clearing and optical imaging have enabled us to obtain three-dimensional high-resolution images of various tissues. However, the severe background noise remains a major obstacle. In addition, there is an urgent need for fast background ground correction methods. In this paper, we present a fast background removal method for 3D multi-channel deep tissue fluorescence images, in which the objectives of different channels are well separated. We first conduct a window-based normalization to distinguish foreground signals from background noises in all channels. Then, we identify the pure background regions by conducting subtraction of images in different channels, which allow us to estimate the background noises of the whole images by interpolation. Experiments on real 3D datasets of mouse stomach show our method has superior performance and efficiency comparing with the current state-of-the-art background correction methods.",
		"email": [
			"lcc1992@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_11",
		"source": "miccai",
		"year": 2017,
		"key": "7202eeb1-7f23-4095-b622-19589d86464a",
		"use": 1
	},
	{
		"Title": "Retrospective Head Motion Estimation in Structural Brain MRI with 3D CNNs",
		"Description": "Juan Eugenio Iglesias, Garikoitz Lerma-Usabiaga, Luis C. Garcia-Peraza-Herrera, Sara Martinez, Pedro M. Paz-Alonso.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Head motion is one of the most important nuisance variables in neuroimaging, particularly in studies of clinical or special populations, such as children. However, the possibility of estimating motion in structural MRI is limited to a few specialized sites using advanced MRI acquisition techniques. Here we propose a supervised learning method to retrospectively estimate motion from plain MRI. Using sparsely labeled training data, we trained a 3D convolutional neural network to assess if voxels are corrupted by motion or not. The output of the network is a motion probability map, which we integrate across a region of interest (ROI) to obtain a scalar motion score. Using cross-validation on a dataset of \\(n=48\\) healthy children scanned at our center, and the cerebral cortex as ROI, we show that the proposed measure of motion explains away 37% of the variation in cortical thickness. We also show that the motion score is highly correlated with the results from human quality control of the scans. The proposed technique can not only be applied to current studies, but also opens up the possibility of reanalyzing large amounts of legacy datasets with motion into consideration: we applied the classifier trained on data from our center to the ABIDE dataset (autism), and managed to recover group differences that were confounded by motion.",
		"email": [
			"e.iglesias@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_36",
		"source": "miccai",
		"year": 2017,
		"key": "8ca84de9-d2e7-42e8-8621-bae249273b43",
		"use": 1
	},
	{
		"Title": "Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample Filtering and Hybrid-Loss Residual Learning",
		"Description": "Qi Dou, Hao Chen, Yueming Jin, Huangjing Lin, Jing Qin, Pheng-Ann Heng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we propose a novel framework with 3D convolutional networks (ConvNets) for automated detection of pulmonary nodules from low-dose CT scans, which is a challenging yet crucial task for lung cancer early diagnosis and treatment. Different from previous standard ConvNets, we try to tackle the severe hard/easy sample imbalance problem in medical datasets and explore the benefits of localized annotations to regularize the learning, and hence boost the performance of ConvNets to achieve more accurate detections. Our proposed framework consists of two stages: (1) candidate screening, and (2) false positive reduction. In the first stage, we establish a 3D fully convolutional network, effectively trained with an online sample filtering scheme, to sensitively and rapidly screen the nodule candidates. In the second stage, we design a hybrid-loss residual network which harnesses the location and size information as important cues to guide the nodule recognition procedure. Experimental results on the public large-scale LUNA16 dataset demonstrate superior performance of our proposed method compared with state-of-the-art approaches for the pulmonary nodule detection task.",
		"email": [
			"qdou@cse.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_72",
		"source": "miccai",
		"year": 2017,
		"key": "a105f539-734b-4936-8646-4f1e7c27b077",
		"use": 1
	},
	{
		"Title": "Segmentation-Free Kidney Localization and Volume Estimation Using Aggregated Orthogonal Decision CNNs",
		"Description": "Mohammad Arafat Hussain, Alborz Amir-Khalili, Ghassan Hamarneh, Rafeef Abugharbieh.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Kidney volume is an important bio-marker in the clinical diagnosis of various renal diseases. For example, it plays an essential role in follow-up evaluation of kidney transplants. Most existing methods for volume estimation rely on kidney segmentation as a prerequisite step, which has various limitations such as initialization-sensitivity and computationally-expensive optimization. In this paper, we propose a hybrid localization-volume estimation deep learning approach capable of (i) localizing kidneys in abdominal CT images, and (ii) estimating renal volume without requiring segmentation. Our approach involves multiple levels of self-learning of image representation using convolutional neural layers, which we show better capture the rich and complex variability in kidney data, demonstrably outperforming hand-crafted feature representations. We validate our method on clinical data of 100 patients with a total of 200 kidney samples (left and right). Our results demonstrate a 55% increase in kidney boundary localization accuracy, and a 30% increase in volume estimation accuracy compared to recent state-of-the-art methods deploying regression-forest-based learning for the same tasks.",
		"email": [
			"arafat@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_70",
		"source": "miccai",
		"year": 2017,
		"key": "bc79bc9c-9e65-46b4-bb3d-00c2c15f744f",
		"use": 1
	},
	{
		"Title": "Semi-supervised Segmentation of Optic Cup in Retinal Fundus Images Using Variational Autoencoder",
		"Description": "Suman Sedai, Dwarikanath Mahapatra, Sajini Hewavitharanage, Stefan Maetschke, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Accurate segmentation of optic cup and disc in retinal fundus images is essential to compute the cup to disc ratio parameter, which is important for glaucoma assessment. The ill-defined boundaries of optic cup makes the segmentation a lot more challenging compared to optic disc. Existing approaches have mainly used fully supervised learning that requires many labeled samples to build a robust segmentation framework. In this paper, we propose a novel semi-supervised method to segment the optic cup, which can accurately localize the anatomy using limited number of labeled samples. The proposed method leverages the inherent feature similarity from a large number of unlabeled images to train the segmentation model from a smaller number of labeled images. It first learns the parameters of a generative model from unlabeled images using variational autoencoder. The trained generative model provides the feature embedding of the images which allows the clustering of the related observation in the latent feature space. We combine the feature embedding with the segmentation autoencoder which is trained on the labeled images for pixel-wise segmentation of the cup region. The main novelty of the proposed approach is in the utilization of generative models for semi-supervised segmentation. Experimental results show that the proposed method successfully segments optic cup with small number of labeled images, and unsupervised feature embedding learned from unlabeled data improves the segmentation accuracy. Given the challenge of access to annotated medical images in every clinical application, the proposed framework is a key contribution and applicable for segmentation of different anatomies across various medical imaging modalities.",
		"email": [
			"ssedai@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_9",
		"source": "miccai",
		"year": 2017,
		"key": "4a8b78d1-241b-4aee-a3fe-919f671db336",
		"use": 1
	},
	{
		"Title": "Segmentation of Intracranial Arterial Calcification with Deeply Supervised Residual Dropout Networks",
		"Description": "Gerda Bortsova, Gijs van Tulder, Florian Dubost, Tingying Peng, Nassir Navab, Aad van der Lugt, Daniel Bos, Marleen De Bruijne.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Intracranial carotid artery calcification (ICAC) is a major risk factor for stroke, and might contribute to dementia and cognitive decline. Reliance on time-consuming manual annotation of ICAC hampers much demanded further research into the relationship between ICAC and neurological diseases. Automation of ICAC segmentation is therefore highly desirable, but difficult due to the proximity of the lesions to bony structures with a similar attenuation coefficient. In this paper, we propose a method for automatic segmentation of ICAC; the first to our knowledge. Our method is based on a 3D fully convolutional neural network that we extend with two regularization techniques. Firstly, we use deep supervision to encourage discriminative features in the hidden layers. Secondly, we augment the network with skip connections, as in the recently developed ResNet, and dropout layers, inserted in a way that skip connections circumvent them. We investigate the effect of skip connections and dropout. In addition, we propose a simple problem-specific modification of the network objective function that restricts the focus to the most important image regions and simplifies the optimization. We train and validate our model using 882 CT scans and test on 1,000. Our regularization techniques and objective improve the average Dice score by 7.1%, yielding an average Dice of 76.2% and 97.7% correlation between predicted ICAC volumes and manual annotations.",
		"email": [
			"gerdabortsova@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_41",
		"source": "miccai",
		"year": 2017,
		"key": "3347f4b6-76b5-455d-8815-9779e4a99438",
		"use": 1
	},
	{
		"Title": "Tracking and Segmentation of the Airways in Chest CT Using a Fully Convolutional Network",
		"Description": "Qier Meng, Holger R. Roth, Takayuki Kitasaka, Masahiro Oda, Junji Ueno, Kensaku Mori.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Airway segmentation plays an important role in analyzing chest computed tomography (CT) volumes such as lung cancer detection, chronic obstructive pulmonary disease (COPD), and surgical navigation. However, due to the complex tree-like structure of the airways, obtaining segmentation results with high accuracy for a complete 3D airway extraction remains a challenging task. In recent years, deep learning based methods, especially fully convolutional networks (FCN), have improved the state-of-the-art in many segmentation tasks. 3D U-Net is an example that optimized for 3D biomedical imaging. It consists of a contracting encoder part to analyze the input volume and a successive decoder part to generate integrated 3D segmentation results. While 3D U-Net can be trained for any 3D segmentation task, its direct application to airway segmentation is challenging due to differently sized airway branches. In this work, we combine 3D deep learning with image-based tracking in order to automatically extract the airways. Our method is driven by adaptive cuboidal volume of interest (VOI) analysis using a 3D U-Net model. We track the airways along their centerlines and set VOIs according to the diameter and running direction of each airway. After setting a VOI, the 3D U-Net is utilized to extract the airway region inside the VOI. All extracted candidate airway regions are unified to form an integrated airway tree. We trained on 30 cases and tested our method on an additional 20 cases. Compared with other state-of-the-art airway tracking and segmentation methods, our method can increase the detection rate by 5.6 while decreasing the false positives (FP) by 0.7 percentage points.",
		"email": [
			"kensaku@is.nagoya-u.ac.jp"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_23",
		"source": "miccai",
		"year": 2017,
		"key": "ce3af24e-55c7-4d5d-a42a-f5d65bf6b0d5",
		"use": 1
	},
	{
		"Title": "Deep Correlational Learning for Survival Prediction from Multi-modality Data",
		"Description": "Jiawen Yao, Xinliang Zhu, Feiyun Zhu, Junzhou Huang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Technological advances have created a great opportunity to provide multi-view data for patients. However, due to the large discrepancy between different heterogeneous views, traditional survival models are unable to efficiently handle multiple modalities data as well as learn very complex interactions that can affect survival outcomes in various ways. In this paper, we develop a Deep Correlational Survival Model (DeepCorrSurv) for the integration of multi-view data. The proposed network consists of two sub-networks, view-specific and common sub-network. To remove the view discrepancy, the proposed DeepCorrSurv first explicitly maximizes the correlation among the views. Then it transfers feature hierarchies from view commonality and specifically fine-tunes on the survival regression task. Extensive experiments on real lung and brain tumor data sets demonstrated the effectiveness of the proposed DeepCorrSurv model using multiple modalities data across different tumor types.",
		"email": [
			"jzhuang@uta.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_46",
		"source": "miccai",
		"year": 2017,
		"key": "b4800daa-cdde-4988-9531-76dc5772500c",
		"use": 1
	},
	{
		"Title": "Semi-supervised Learning for Biomedical Image Segmentation via Forest Oriented Super Pixels(Voxels)",
		"Description": "Lin Gu, Yinqiang Zheng, Ryoma Bise, Imari Sato, Nobuaki Imanishi, Sadakazu Aiso.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we focus on semi-supervised learning for biomedical image segmentation, so as to take advantage of huge unlabelled data. We observe that there usually exist some homogeneous connected areas of low confidence in biomedical images, which tend to confuse the classifier trained with limited labelled samples. To cope with this difficulty, we propose to construct forest oriented super pixels(voxels) to augment the standard random forest classifier, in which super pixels(voxels) are built upon the forest based code. Compared to the state-of-the-art, our proposed method shows superior segmentation performance on challenging 2D/3D biomedical images. The full implementation (based on Matlab) is available at https://github.com/lingucv/ssl_superpixels.",
		"email": [
			"ling@nii.ac.jp"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_80",
		"source": "miccai",
		"year": 2017,
		"key": "5800ba0b-fa3e-4ca1-92b8-5fbf3f2d6051",
		"use": 1
	},
	{
		"Title": "Joint Reconstruction and Segmentation of 7T-like MR Images from 3T MRI Based on Cascaded Convolutional Neural Networks",
		"Description": "Khosro Bahrami, Islem Rekik, Feng Shi, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "7T MRI scanner provides MR images with higher resolution and better contrast than 3T MR scanners. This helps many medical analysis tasks, including tissue segmentation. However, currently there is a very limited number of 7T MRI scanners worldwide. This motivates us to propose a novel image post-processing framework that can jointly generate high-resolution 7T-like images and their corresponding high-quality 7T-like tissue segmentation maps, solely from the routine 3T MR images. Our proposed framework comprises two parallel components, namely (1) reconstruction and (2) segmentation. The reconstruction component includes the multi-step cascaded convolutional neural networks (CNNs) that map the input 3T MR image to a 7T-like MR image, in terms of both resolution and contrast. Similarly, the segmentation component involves another paralleled cascaded CNNs, with a different architecture, to generate high-quality segmentation maps. These cascaded feedbacks between the two designed paralleled CNNs allow both tasks to mutually benefit from each another when learning the respective reconstruction and segmentation mappings. For evaluation, we have tested our framework on 15 subjects (with paired 3T and 7T images) using a leave-one-out cross-validation. The experimental results show that our estimated 7T-like images have richer anatomical details and better segmentation results, compared to the 3T MRI. Furthermore, our method also achieved better results in both reconstruction and segmentation tasks, compared to the state-of-the-art methods.",
		"email": [
			"dinggang_shen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_87",
		"source": "miccai",
		"year": 2017,
		"key": "31bb8bfb-9ed3-48d7-b52f-ab99bb7b5746",
		"use": 1
	},
	{
		"Title": "CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins from MRI Using Multi-view CNN",
		"Description": "Aliasghar Mortazi, Rashed Karim, Kawal Rhode, Jeremy Burt, Ulas Bagci.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Anatomical and biophysical modeling of left atrium (LA) and proximal pulmonary veins (PPVs) is important for clinical management of several cardiac diseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA and PPVs through visualization. However, there is a strong need for an advanced image segmentation method to be applied to cardiac MRI for quantitative analysis of LA and PPVs. In this study, we address this unmet clinical need by exploring a new deep learning-based segmentation strategy for quantification of LA and PPVs with high accuracy and heightened efficiency. Our approach is based on a multi-view convolutional neural network (CNN) with an adaptive fusion strategy and a new loss function that allows fast and more accurate convergence of the backpropagation based optimization. After training our network from scratch by using more than 60K 2D MRI images (slices), we have evaluated our segmentation strategy to the STACOM 2013 cardiac segmentation challenge benchmark. Qualitative and quantitative evaluations, obtained from the segmentation challenge, indicate that the proposed method achieved the state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10s in GPU, and 7.5 min in CPU).",
		"email": [
			"a.mortazi@knights.ucf.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_43",
		"source": "miccai",
		"year": 2017,
		"key": "bcb7cf05-eb92-40c0-aef3-5fffe9e9d11c",
		"use": 1
	},
	{
		"Title": "Semi-supervised Learning for Network-Based Cardiac MR Image Segmentation",
		"Description": "Wenjia Bai, Ozan Oktay, Matthew Sinclair, Hideaki Suzuki, Martin Rajchl, Giacomo Tarroni, Ben Glocker, Andrew King, Paul M. Matthews, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Training a fully convolutional network for pixel-wise (or voxel-wise) image segmentation normally requires a large number of training images with corresponding ground truth label maps. However, it is a challenge to obtain such a large training set in the medical imaging domain, where expert annotations are time-consuming and difficult to obtain. In this paper, we propose a semi-supervised learning approach, in which a segmentation network is trained from both labelled and unlabelled data. The network parameters and the segmentations for the unlabelled data are alternately updated. We evaluate the method for short-axis cardiac MR image segmentation and it has demonstrated a high performance, outperforming a baseline supervised method. The mean Dice overlap metric is 0.92 for the left ventricular cavity, 0.85 for the myocardium and 0.89 for the right ventricular cavity. It also outperforms a state-of-the-art multi-atlas segmentation method by a large margin and the speed is substantially faster.",
		"email": [
			"w.bai@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_29",
		"source": "miccai",
		"year": 2017,
		"key": "2235cad8-ce23-49e4-a34e-9e76bfdfef49",
		"use": 1
	},
	{
		"Title": "Using Convolutional Neural Networks to Automatically Detect Eye-Blink Artifacts in Magnetoencephalography Without Resorting to Electrooculography",
		"Description": "Prabhat Garg, Elizabeth Davenport, Gowtham Murugesan, Ben Wagner, Christopher Whitlow, Joseph Maldjian, Albert Montillo.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "\nMagnetoencephelography (MEG) is a functional neuroimaging tool that records the magnetic fields induced by neuronal activity; however, signal from muscle activity often corrupts the data. Eye-blinks are one of the most common types of muscle artifact. They can be recorded by affixing eye proximal electrodes, as in electrooculography (EOG), however this complicates patient preparation and decreases comfort. Moreover, it can induce further muscular artifacts from facial twitching. We propose an EOG free, data driven approach. We begin with Independent Component Analysis (ICA), a well-known preprocessing approach that factors observed signal into statistically independent components. When applied to MEG, ICA can help separate neuronal components from non-neuronal ones, however, the components are randomly ordered. Thus, we develop a method to assign one of two labels, non-eye-blink or eye-blink, to each component.",
		"email": [
			"Albert.Montillo@UTSouthwestern.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_43",
		"source": "miccai",
		"year": 2017,
		"key": "5ac8a0da-583c-4b03-b728-2cc3cc242be8",
		"use": 1
	},
	{
		"Title": "Personalized Pancreatic Tumor Growth Prediction via Group Learning",
		"Description": "Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Tumor growth prediction, a highly challenging task, has long been viewed as a mathematical modeling problem, where the tumor growth pattern is personalized based on imaging and clinical data of a target patient. Though mathematical models yield promising results, their prediction accuracy may be limited by the absence of population trend data and personalized clinical characteristics. In this paper, we propose a statistical group learning approach to predict the tumor growth pattern that incorporates both the population trend and personalized data. In order to discover high-level features from multimodal imaging data, a deep convolutional neural network approach is developed to model the voxel-wise spatio-temporal tumor progression. The deep features are combined with the time intervals and the clinical factors to feed a process of feature selection. Our predictive model is pretrained on a group data set and personalized on the target patient data to estimate the future spatio-temporal progression of the patients tumor. Multimodal imaging data at multiple time points are used in the learning, personalization and inference stages. Our method achieves a Dice coefficient of \\(86.8\\%\\,\\pm \\,3.6\\%\\) and RVD of \\(7.9\\%\\,\\pm \\,5.4\\%\\) on a pancreatic tumor data set, outperforming the DSC of \\(84.4\\%\\,\\pm \\,4.0\\%\\) and RVD \\(13.9\\%\\,\\pm \\,9.8\\%\\) obtained by a previous state-of-the-art model-based method.",
		"email": [
			"jyao@cc.nih.gov"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_48",
		"source": "miccai",
		"year": 2017,
		"key": "038f089e-4e21-42fc-952c-aa9641efbc91",
		"use": 1
	},
	{
		"Title": "Boundary-Aware Fully Convolutional Network for Brain Tumor Segmentation",
		"Description": "Haocheng Shen, Ruixuan Wang, Jianguo Zhang, Stephen J. McKenna.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose a novel, multi-task, fully convolutional network (FCN) architecture for automatic segmentation of brain tumor. This network extracts multi-level contextual information by concatenating hierarchical feature representations extracted from multimodal MR images along with their symmetric-difference images. It achieves improved segmentation performance by incorporating boundary information directly into the loss function. The proposed method was evaluated on the BRATS13 and BRATS15 datasets and compared with competing methods on the BRATS13 testing set. Segmented tumor boundaries obtained were better than those obtained by single-task FCN and by FCN with CRF. The method is among the most accurate available and has relatively low computational cost at test time.",
		"email": [
			"h.y.shen@dundee.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_49",
		"source": "miccai",
		"year": 2017,
		"key": "3c58351c-af26-4a52-b9ea-4fc4b8762b22",
		"use": 1
	},
	{
		"Title": "Transfer Learning for Domain Adaptation in MRI: Application in Brain Lesion Segmentation",
		"Description": "Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer, Elena Marchiori, Mehran Pesteie, Charles R. G. Guttmann, Frank-Erik de Leeuw, Clare M. Tempany, Bram van Ginneken, Andriy Fedorov, Purang Abolmaesumi, Bram Platel, William M. WellsIII.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis and treatment. However, variations in MRI acquisition protocols result in different appearances of normal and diseased tissue in the images. Convolutional neural networks (CNNs), which have shown to be successful in many medical image analysis tasks, are typically sensitive to the variations in imaging protocols. Therefore, in many cases, networks trained on data acquired with one MRI protocol, do not perform satisfactorily on data acquired with different protocols. This limits the use of models trained with large annotated legacy datasets on a new dataset with a different domain which is often a recurring situation in clinical settings. In this study, we aim to answer the following central questions regarding domain adaptation in medical image analysis: Given a fitted legacy model, (1) How much data from the new domain is required for a decent adaptation of the original network?; and, (2) What portion of the pre-trained model parameters should be retrained given a certain number of the new domain training samples? To address these questions, we conducted extensive experiments in white matter hyperintensity segmentation task. We trained a CNN on legacy MR images of brain and evaluated the performance of the domain-adapted network on the same task with images from a different domain. We then compared the performance of the model to the surrogate scenarios where either the same trained network is used or a new network is trained from scratch on the new dataset. The domain-adapted network tuned only by two training examples achieved a Dice score of 0.63 substantially outperforming a similar network trained on the same set of examples from scratch.",
		"email": [
			"mehrtash@bwh.harvard.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_59",
		"source": "miccai",
		"year": 2017,
		"key": "5fb6157d-69e9-40d6-8542-5c9ef50c68c7",
		"use": 1
	},
	{
		"Title": "Automatic Liver Segmentation Using an Adversarial Image-to-Image Network",
		"Description": "Dong Yang, Daguang Xu, S. Kevin Zhou, Bogdan Georgescu, Mingqing Chen, Sasa Grbic, Dimitris Metaxas, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.",
		"email": [
			"daguang.xu@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_58",
		"source": "miccai",
		"year": 2017,
		"key": "d6616a70-363c-4363-af7a-f0a6a4ffc402",
		"use": 1
	},
	{
		"Title": "Combining Spatial and Non-spatial Dictionary Learning for Automated Labeling of Intra-ventricular Hemorrhage in Neonatal Brain MRI",
		"Description": "Mengyuan Liu, Steven P. Miller, Vann Chau, Colin Studholme.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "A specific challenge to accurate tissue quantification in premature neonatal MRI data is posed by Intra-Ventricular Hemorrhage (IVH), where severe cases can be accompanied by extreme and complex Ventriculomegaly (VM). IVH is apparent on MRI as bright signal pooling within the ventricular space in locations related to the original bleed and how the blood pools and clots due to gravity. High variability in the location and extent of IVH and in the shape and size of the ventricles due to ventriculomegaly (VM), combined with a lack of large sets of training images covering all possible configurations, mean it is not feasible to approach the problem using whole brain dictionary learning. Here, we propose a novel sparse dictionary approach that utilizes a spatial dictionary for normal tissues structures, and a non-spatial component to delineate IVH and VM structure. We examine the behavior of this approach using a dataset of premature neonatal MRI scans with severe IVH and VM, finding improvements in the segmentation accuracy compared to the conventional segmentation. This approach provides the first automatic whole-brain segmentation framework for severe IVH and VM in premature neonatal brain MRIs.",
		"email": [
			"liumyzoe@uw.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_90",
		"source": "miccai",
		"year": 2017,
		"key": "3cad7f20-c2ed-4b68-9a11-8729a96ee08b",
		"use": 1
	},
	{
		"Title": "Hierarchical Multimodal Fusion of Deep-Learned Lesion and Tissue Integrity Features in Brain MRIs for Distinguishing Neuromyelitis Optica from Multiple Sclerosis",
		"Description": "Youngjin Yoo, Lisa Y. W. Tang, Su-Hyun Kim, Ho Jin Kim, Lisa Eunyoung Lee, David K. B. Li, Shannon Kolind, Anthony Traboulsee, Roger Tam.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Neuromyelitis optica spectrum disorder (NMOSD) is a disease of the central nervous system that is often misdiagnosed as multiple sclerosis (MS) because they share similar clinical and radiological characteristics. Two key pathological signs of NMOSD and MS that are detectable on magnetic resonance imaging (MRI) are white matter lesions and alterations in tissue integrity as measured by fractional anisotropy (FA) values on diffusion tensor images (DTIs). This paper proposes a multimodal deep learning model that discovers latent features in brain lesion masks and DTIs for distinguishing NMOSD from MS. The main technical challenge is to optimally extract and integrate features from two very heterogeneous image types (lesion masks and FA maps). Our solution is to first build two modality-specific pathways, each designed to accommodate the expected feature density and scale, then integrate them into a hierarchical multimodal fusion (HMF) model. The HMF model contains two multimodal fusion layers operating at two different scales, which in turn are joined by a multi-scale fusion layer. We hypothesize that the HMF approach would allow the automatic extraction of joint-features of heterogeneous image types to be optimized with greater efficiency and accuracy than the traditional multimodal approach of combining only the top-layer modality-specific features with a single fusion layer. The proposed model gives an average diagnostic accuracy of 81.3% (85.3% sensitivity and 75.0% specificity) on 82 NMOSD patients and 52 MS patients in a seven-fold cross-validation, which significantly outperforms the user-defined MRI features previously used in clinical studies, as well as deep-learned features using the conventional fusion approach.",
		"email": [
			"youngjin.yoo@alumni.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_55",
		"source": "miccai",
		"year": 2017,
		"key": "681497a6-3ffd-43e6-8dd4-350cf0860647",
		"use": 1
	},
	{
		"Title": "Deep Image-to-Image Recurrent Network with Shape Basis Learning for Automatic Vertebra Labeling in Large-Scale 3D CT Volumes",
		"Description": "Dong Yang, Tao Xiong, Daguang Xu, S. Kevin Zhou, Zhoubing Xu, Mingqing Chen, JinHyeong Park, Sasa Grbic, Trac D. Tran, Sang Peter Chin, Dimitris Metaxas, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic vertebra localization and identification in 3D medical images plays an important role in many clinical tasks, including pathological diagnosis, surgical planning and postoperative assessment. In this paper, we propose an automatic and efficient algorithm to localize and label the vertebra centroids in 3D CT volumes. First, a deep image-to-image network (DI2IN) is deployed to initialize vertebra locations, employing the convolutional encoder-decoder architecture. Next, the centroid probability maps from DI2IN are modeled as a sequence according to the spatial relationship of vertebrae, and evolved with the convolutional long short-term memory (ConvLSTM) model. Finally, the landmark positions are further refined and regularized by another neural network with a learned shape basis. The whole pipeline can be conducted in the end-to-end manner. The proposed method outperforms other state-of-the-art methods on a public database of 302 spine CT volumes with various pathologies. To further boost the performance and validate that large labeled training data can benefit the deep learning algorithms, we leverage the knowledge of additional 1000 3D CT volumes from different patients. Our experimental results show that training with a large database improves the performance of proposed framework by a large margin and achieves an identification rate of 89%.",
		"email": [
			"daguang.xu@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_57",
		"source": "miccai",
		"year": 2017,
		"key": "3fd3d653-5fa9-445d-9f1a-b48dad50cb2b",
		"use": 1
	},
	{
		"Title": "Deep Convolutional Encoder-Decoders for Prostate Cancer Detection and Classification",
		"Description": "Atilla P. Kiraly, Clement Abi Nader, Ahmet Tuysuzoglu, Robert Grimm, Berthold Kiefer, Noha El-Zehiry, Ali Kamen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Prostate cancer accounts for approximately 11% of all cancer cases. Definitive diagnosis is made by histopathological examination of tissue biopsies. Recently, there have been strong correlations established between pre-biopsy multi-parametric MR image findings and the histopathology results. We investigate novel deep learning networks that provide tumor localization and classification solely based on prostate multi-parametric MR images using images with biopsy confirmed lesions. We propose to use a multi-channel image-to-image convolutional encoder-decoders where responses signify localized lesions and output channels represent different tumor classes. We take simple point locations in the labeled ground truth data and train networks to output Gaussian kernels around those points across multiple channels. This approach allows for both localization and classification within a single run. The input data consists of axial T2-weighted images, apparent diffusion coefficient maps, high b-value diffusion-weighted images, and K-trans parameter maps from 202 patients. The images were co-registered on a per patient basis and exhaustive comparisons were performed with 5-fold cross-validation across three different models with increasing complexity. The highest average classification area-under-the-curve (AUC) achieved was 83.4% using a medium complexity model, in which no skip-connection were used across layers. In individual k-folds, AUCs above 90% were achieved. The results demonstrate promise for directly determining tumor malignancy without performing an invasive biopsy procedure.",
		"email": [
			"atilla.kiraly@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_56",
		"source": "miccai",
		"year": 2017,
		"key": "05169dac-9ffa-4258-a2d5-70abd057bc99",
		"use": 1
	},
	{
		"Title": "SD-Layer: Stain Deconvolutional Layer for CNNs in Medical Microscopic Imaging",
		"Description": "Rahul Duggal, Anubha Gupta, Ritu Gupta, Pramit Mallick.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Convolutional Neural Networks (CNNs) are typically trained in the RGB color space. However, in medical imaging, we believe that pixel stain quantities offer a fundamental view of the interaction between tissues and stain chemicals. Since the optical density (OD) colorspace allows to compute pixel stain quantities from pixel RGB intensities using the Beer-Lamberts law, we propose a stain deconvolutional layer, hereby named as SD-Layer, affixed at the front of CNN that performs two functions: (1) it transforms the input RGB microscopic images to Optical Density (OD) space and (2) this layer deconvolves OD image with the stain basis learned through backpropagation and provides tissue-specific stain absorption quantities as input to the following CNN layers. With the introduction of only nine additional learnable parameters in the proposed SD-Layer, we obtain a considerably improved performance on two standard CNN architectures: AlexNet and T-CNN. Using the T-CNN architecture prefixed with the proposed SD-Layer, we obtain 5-fold cross-validation accuracy of 93.2% in the problem of differentiating malignant immature White Blood Cells (WBCs) from normal immature WBCs for cancer detection.",
		"email": [
			"anubha@iiitd.ac.in"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_50",
		"source": "miccai",
		"year": 2017,
		"key": "d2d9b10e-3e41-4ed6-8bf2-838294a1d885",
		"use": 1
	},
	{
		"Title": "Quantification of Metabolites in Magnetic Resonance Spectroscopic Imaging Using Machine Learning",
		"Description": "Dhritiman Das, Eduardo Coello, Rolf F. Schulte, Bjoern H. Menze.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a clinical imaging modality for measuring tissue metabolite levels in-vivo. An accurate estimation of spectral parameters allows for better assessment of spectral quality and metabolite concentration levels. The current gold standard quantification method is the LCModel - a commercial fitting tool. However, this fails for spectra having poor signal-to-noise ratio (SNR) or a large number of artifacts. This paper introduces a framework based on random forest regression for accurate estimation of the output parameters of a model based analysis of MR spectroscopy data. The goal of our proposed framework is to learn the spectral features from a training set comprising of different variations of both simulated and in-vivo brain spectra and then use this learning for the subsequent metabolite quantification. Experiments involve training and testing on simulated and in-vivo human brain spectra. We estimate parameters such as concentration of metabolites and compare our results with that from the LCModel.",
		"email": [
			"dhritiman.das@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_53",
		"source": "miccai",
		"year": 2017,
		"key": "6f73a9e2-c11e-4956-8bad-bc94dc92ef86",
		"use": 1
	},
	{
		"Title": "Fast Prospective Detection of Contrast Inflow in X-ray Angiograms with Convolutional Neural Network and Recurrent Neural Network",
		"Description": "Hua Ma, Pierre Ambrosini, Theo van Walsum.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic detection of contrast inflow in X-ray angiographic sequences can facilitate image guidance in computer-assisted cardiac interventions. In this paper, we propose two different approaches for prospective contrast inflow detection. The methods were developed and evaluated to detect contrast frames from X-ray sequences. The first approach trains a convolutional neural network (CNN) to distinguish whether a frame has contrast agent or not. The second method extracts contrast features from images with enhanced vessel structures; the contrast frames are then detected based on changes in the feature curve using long short-term memory (LSTM), a recurrent neural network architecture. Our experiments show that both approaches achieve good performance on detection of the beginning contrast frame from X-ray sequences and are more robust than a state-of-the-art method. As the proposed methods work in prospective settings and run fast, they have the potential of being used in clinical practice.",
		"email": [
			"h.ma@erasmusmc.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_52",
		"source": "miccai",
		"year": 2017,
		"key": "4a6c110a-9d9a-4d77-919c-cf113072b108",
		"use": 1
	},
	{
		"Title": "Integrating Statistical Prior Knowledge into Convolutional Neural Networks",
		"Description": "Fausto Milletari, Alex Rothberg, Jimmy Jia, Michal Sofka.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "In this work we show how to integrate prior statistical knowledge, obtained through principal components analysis (PCA), into a convolutional neural network in order to obtain robust predictions even when dealing with corrupted or noisy data. Our network architecture is trained end-to-end and includes a specifically designed layer which incorporates the dataset modes of variation discovered via PCA and produces predictions by linearly combining them. We also propose a mechanism to focus the attention of the CNN on specific regions of interest of the image in order to obtain refined predictions. We show that our method is effective in challenging segmentation and landmark localization tasks.",
		"email": [
			"fmilletari@4catalyzer.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_19",
		"source": "miccai",
		"year": 2017,
		"key": "437f8056-40cd-4d9a-8dd2-64d207fd7d98",
		"use": 1
	},
	{
		"Title": "Adaptable Landmark Localisation: Applying Model Transfer Learning to a Shape Model Matching System",
		"Description": "C. Lindner, D. Waring, B. Thiruvenkatachari, K. OBrien, T. F. Cootes.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "We address the challenge of model transfer learning for a shape model matching (SMM) system. The goal is to adapt an existing SMM system to work effectively with new data without rebuilding the system from scratch.",
		"email": [
			"t.cootes@manchester.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_17",
		"source": "miccai",
		"year": 2017,
		"key": "8338bcaf-7084-4182-a011-7d681378abe2",
		"use": 1
	},
	{
		"Title": "Learning-Based Multi-atlas Segmentation of the Lungs and Lobes in Proton MR Images",
		"Description": "Hoileong Lee, Tahreema Matin, Fergus Gleeson, Vicente Grau.",
		"ShortDetails": "Medical Image Computing and Computer Assisted Intervention  MICCAI 2017",
		"abstract": "Delineation of the lung and lobar anatomy in MR images is challenging due to the limited image contrast and the absence of visible interlobar fissures. Here we propose a novel automated lung and lobe segmentation method for pulmonary MR images. This segmentation method employs prior information of the lungs and lobes extracted from CT in the form of multiple MRI atlases, and adopts a learning-based atlas-encoding scheme, based on random forests, to improve the performance of multi-atlas segmentation. In particular, we encode each CT-derived MRI atlas by training an atlas-specific random forest for each structure of interest. In addition to appearance features, we also extract label context features from the registered atlases to introduce additional information to the non-linear mapping process. We evaluated our proposed framework on 10 clinical MR images acquired from COPD patients. It outperformed state-of-the-art approaches in segmenting the lungs and lobes, yielding a mean Dice score of 95.7%.",
		"email": [
			"hoileong.lee@eng.ox.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66182-7_13",
		"source": "miccai",
		"year": 2017,
		"key": "c0ea3326-6ee1-44b0-81c8-08f89822bacb",
		"use": 1
	},
	{
		"Title": "Convolutional Neural Network and In-Painting Techniques for the Automatic Assessment of Scoliotic Spine Surgery from Biplanar Radiographs",
		"Description": "B. Aubert, P. A. Vidal, S. Parent, T. Cresson, C. Vazquez, J. De Guise.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Assessing the effectiveness of scoliosis surgery requires the quantification of 3D spinal deformities from pre- and post-operative radiographs. This can be achieved from 3D reconstructed models of the spine but a fast-automatic method to recover this model from pre- and post-operative radiographs remains a challenge. For example, the vertebraes visibility varies considerably and large metallic objects occlude important landmarks in postoperative radiographs. This paper presents a method for automatic 3D spine reconstruction from pre- and post-operative calibrated biplanar radiographs. We fitted a statistical shape model of the spine to images by using a 3D/2D registration based on convolutional neural networks. The metallic structures in postoperative radiographs were detected and removed using an image in-painting method to improve the performance of vertebrae registration. We applied the method to a set of 38 operated patients and clinical parameters were computed (such as the Cobb and kyphosis/lordosis angles, and vertebral axial rotations) from the pre- and post-operative 3D reconstructions. Compared to manual annotations, the proposed automatic method provided values with a mean absolute error <5.6 and <6.8 for clinical angles; <1.5 mm and <2.3 mm for vertebra locations; and <4.5 and <3.7 for vertebra orientations, respectively for pre- and post-operative times. The fast-automatic 3D reconstruction from pre- and post in-painted images provided a relevant set of parameters to assess the spine surgery without any human intervention.",
		"email": [
			"benjamin.aubert@etsmtl.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_78",
		"source": "miccai",
		"year": 2017,
		"key": "1fa87958-5819-4327-95a9-80ad524a22f8",
		"use": 1
	},
	{
		"Title": "Skin Disease Recognition Using Deep Saliency Features and Multimodal Learning of Dermoscopy and Clinical Images",
		"Description": "Zongyuan Ge, Sergey Demyanov, Rajib Chakravorty, Adrian Bowling, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Skin cancer is the most common cancer world-wide, among which Melanoma the most fatal cancer, accounts for more than 10,000 deaths annually in Australia and United States. The 5-year survival rate for Melanoma can be increased over 90% if detected in its early stage. However, intrinsic visual similarity across various skin conditions makes the diagnosis challenging both for clinicians and automated classification methods. Many automated skin cancer diagnostic systems have been proposed in literature, all of which consider solely dermoscopy images in their analysis. In reality, however, clinicians consider two modalities of imaging; an initial screening using clinical photography images to capture a macro view of the mole, followed by dermoscopy imaging which visualizes morphological structures within the skin lesion. Evidences show that these two modalities provide complementary visual features that can empower the decision making process. In this work, we propose a novel deep convolutional neural network (DCNN) architecture along with a saliency feature descriptor to capture discriminative features of the two modalities for skin lesions classification. The proposed DCNN accepts a pair images of clinical and dermoscopic view of a single lesion and is capable of learning single-modality and cross-modality representations, simultaneously. Using one of the largest collected skin lesion datasets, we demonstrate that the proposed multi-modality method significantly outperforms single-modality methods on three tasks; differentiation between 15 various skin diseases, distinguishing cancerous (3 cancer types including melanoma) from non-cancerous moles, and detecting melanoma from benign cases.",
		"email": [
			"zongyuan@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_29",
		"source": "miccai",
		"year": 2017,
		"key": "6e0bf652-8bf3-44ff-9dd7-d0ffd7c921a1",
		"use": 1
	},
	{
		"Title": "Hybrid Mass Detection in Breast MRI Combining Unsupervised Saliency Analysis and Deep Learning",
		"Description": "Guy Amit, Omer Hadad, Sharon Alpert, Tal Tlusty, Yaniv Gur, Rami Ben-Ari, Sharbell Hashoul.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "\nTo interpret a breast MRI study, a radiologist has to examine over 1000 images, and integrate spatial and temporal information from multiple sequences. The automated detection and classification of suspicious lesions can help reduce the workload and improve accuracy. We describe a hybrid mass-detection algorithm that combines unsupervised candidate detection with deep learning-based classification. The detection algorithm first identifies image-salient regions, as well as regions that are cross-salient with respect to the contralateral breast image. We then use a convolutional neural network (CNN) to classify the detected candidates into true-positive and false-positive masses. The network uses a novel multi-channel image representation; this representation encompasses information from the anatomical and kinetic image features, as well as saliency maps. We evaluated our algorithm on a dataset of MRI studies from 171 patients, with 1957 annotated slices of malignant (59%) and benign (41%) masses. Unsupervised saliency-based detection provided a sensitivity of 0.96 with 9.7 false-positive detections per slice. Combined with CNN classification, the number of false positive detections dropped to 0.7 per slice, with 0.85 sensitivity. The multi-channel representation achieved higher classification performance compared to single-channel images. The combination of domain-specific unsupervised methods and general-purpose supervised learning offers advantages for medical imaging applications, and may improve the ability of automated algorithms to assist radiologists.",
		"email": [
			"guyam@il.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_68",
		"source": "miccai",
		"year": 2017,
		"key": "1ba645e5-19c3-4ccb-a880-68116e3c7e81",
		"use": 1
	},
	{
		"Title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification",
		"Description": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations. (Code: https://github.com/wentaozhu/deep-mil-for-whole-mammogram-classification.git).",
		"email": [
			"wentaoz1@ics.uci.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_69",
		"source": "miccai",
		"year": 2017,
		"key": "2dd240a9-7ffa-40e0-9c5e-000e3f4f4c13",
		"use": 1
	},
	{
		"Title": "Learning CNNs with Pairwise Domain Adaption for Real-Time 6DoF Ultrasound Transducer Detection and Tracking from X-Ray Images",
		"Description": "Jiannan Zheng, Shun Miao, Rui Liao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we present a fully automatic and real-time CNN-based system that achieves highly accurate and robust 6DoF pose estimation and tracking of Transesophageal Echocardiography (TEE) transducer from 2D X-ray images, a key enabler for integrating ultrasound and fluoroscopic image guidance in hybrid operating rooms for catheter-based procedures. Lightweight hierarchical CNNs are first pre-trained purely on a large number of synthetically-generated X-ray images with known ground truth poses. The pre-trained CNNs are then refined for generalization using only a small number of real X-ray images with annotated poses via our proposed pairwise domain adaptation scheme. To resolve the pose ambiguity caused by the self-symmetry of the TEE transducer and the translucent nature of X-ray imaging, a CNN classifier is trained to classify a correct pose from its flipped counterpart by seeing a large number of synthetically-generated pairs. The proposed system is validated on 1,663 fluoroscopic images from clinical studies, and achieves an error rate of 6.53% with a clinically relevant criteria (i.e., Projected Target Registration Error larger than 2.5 mm) and a frame rate of 83.3 frames per second in tracking mode, outperforming the state-of-the-art methods in terms of both accuracy and speed.",
		"email": [
			"jiannan.zheng@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_73",
		"source": "miccai",
		"year": 2017,
		"key": "1f99b446-1c84-444e-801e-17794c63bc52",
		"use": 1
	},
	{
		"Title": "Retinal Microaneurysm Detection Using Clinical Report Guided Multi-sieving CNN",
		"Description": "Ling Dai, Bin Sheng, Qiang Wu, Huating Li, Xuhong Hou, Weiping Jia, Ruogu Fang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Timely detection and treatment of microaneurysms (MA) is a critical step to prevent the development of vision-threatening eye diseases such as diabetic retinopathy. However, detecting MAs in fundus images is a highly challenging task due to the large variation of imaging conditions. In this paper, we focus on developing an interleaved deep mining technique to cope intelligently with the unbalanced MA detection problem. Specifically, we present a clinical report guided multi-sieving convolutional neural network (MS-CNN) which leverages a small amount of supervised information in clinical reports to identify the potential MA regions via a text-to-image mapping in the feature space. These potential MA regions are then interleaved with the fundus image information for multi-sieving deep mining in a highly unbalanced classification problem. Critically, the clinical reports are employed to bridge the semantic gap between low-level image features and high-level diagnostic information. Extensive evaluations show our framework achieves 99.7% precision and 87.8% recall, comparing favorably with the state-of-the-art algorithms. Integration of expert domain knowledge and image information demonstrates the feasibility to reduce the training difficulty of the classifiers under extremely unbalanced data distribution.",
		"email": [
			"shengbin@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_60",
		"source": "miccai",
		"year": 2017,
		"key": "e016800b-8c64-4362-8887-4820f32eb5bc",
		"use": 1
	},
	{
		"Title": "Lesion Detection and Grading of Diabetic Retinopathy via Two-Stages Deep Convolutional Neural Networks",
		"Description": "Yehui Yang, Tao Li, Wensi Li, Haishan Wu, Wei Fan, Wensheng Zhang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose an automatic diabetic retinopathy (DR) analysis algorithm based on two-stages deep convolutional neural networks (DCNN). Compared to existing DCNN-based DR detection methods, the proposed algorithm has the following advantages: (1) Our algorithm can not only point out the lesions in fundus color images, but also give the severity grades of DR. (2) By introducing an imbalanced weighting scheme, more attentions will be payed on lesion patches for DR grading, which significantly improves the performance of DR grading under the same implementation setup. In this study, we label 12, 206 lesion patches and re-annotate the DR grades of 23, 595 fundus images from Kaggle competition dataset. Under the guidance of clinical ophthalmologists, the experimental results show that our lesion detection net achieves comparable performance with trained human observers, and the proposed imbalanced weighted scheme also be proved to significantly enhance the capability of our DCNN-based DR grading algorithm.",
		"email": [
			"yangyehuisw@126.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_61",
		"source": "miccai",
		"year": 2017,
		"key": "23fd214a-6e13-4f1b-988c-dba594a6e0cc",
		"use": 1
	},
	{
		"Title": "Deep Multiple Instance Hashing for Scalable Medical Image Retrieval",
		"Description": "Sailesh Conjeti, Magdalini Paschali, Amin Katouzian, Nassir Navab.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, for the first time, we introduce a multiple instance (MI) deep hashing technique for learning discriminative hash codes with weak bag-level supervision suited for large-scale retrieval. We learn such hash codes by aggregating deeply learnt hierarchical representations across bag members through an MI pool layer. For better trainability and retrieval quality, we propose a two-pronged approach that includes robust optimization and training with an auxiliary single instance hashing arm which is down-regulated gradually. We pose retrieval for tumor assessment as an MI problem because tumors often coexist with benign masses and could exhibit complementary signatures when scanned from different anatomical views. Experimental validations demonstrate improved retrieval performance over the state-of-the-art methods.",
		"email": [
			"sailesh.conjeti@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_63",
		"source": "miccai",
		"year": 2017,
		"key": "db36571c-48b4-41ce-b16d-05c37480ece8",
		"use": 1
	},
	{
		"Title": "Accurate Pulmonary Nodule Detection in Computed Tomography Images Using Deep Convolutional Neural Networks",
		"Description": "Jia Ding, Aoxue Li, Zhiqiang Hu, Liwei Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Early detection of pulmonary cancer is the most promising way to enhance a patients chance for survival. Accurate pulmonary nodule detection in computed tomography (CT) images is a crucial step in diagnosing pulmonary cancer. In this paper, inspired by the successful use of deep convolutional neural networks (DCNNs) in natural image recognition, we propose a novel pulmonary nodule detection approach based on DCNNs. We first introduce a deconvolutional structure to Faster Region-based Convolutional Neural Network (Faster R-CNN) for candidate detection on axial slices. Then, a three-dimensional DCNN is presented for the subsequent false positive reduction. Experimental results of the LUng Nodule Analysis 2016 (LUNA16) Challenge demonstrate the superior detection performance of the proposed approach on nodule detection (average FROC-score of 0.893, ranking the 1st place over all submitted results), which outperforms the best result on the leaderboard of the LUNA16 Challenge (average FROC-score of 0.864).",
		"email": [
			"wanglw@cis.pku.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_64",
		"source": "miccai",
		"year": 2017,
		"key": "4685d32f-aaf5-4a84-b06c-7de8b22795bc",
		"use": 1
	},
	{
		"Title": "Discriminative Localization in CNNs for Weakly-Supervised Segmentation of Pulmonary Nodules",
		"Description": "Xinyang Feng, Jie Yang, Andrew F. Laine, Elsa D. Angelini.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automated detection and segmentation of pulmonary nodules on lung computed tomography (CT) scans can facilitate early lung cancer diagnosis. Existing supervised approaches for automated nodule segmentation on CT scans require voxel-based annotations for training, which are labor- and time-consuming to obtain. In this work, we propose a weakly-supervised method that generates accurate voxel-level nodule segmentation trained with image-level labels only. By adapting a convolutional neural network (CNN) trained for image classification, our proposed method learns discriminative regions from the activation maps of convolution units at different scales, and identifies the true nodule location with a novel candidate-screening framework. Experimental results on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised nodule segmentation framework achieves competitive performance compared to a fully-supervised CNN-based segmentation method.",
		"email": [
			"e.angelini@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_65",
		"source": "miccai",
		"year": 2017,
		"key": "a7dc0acc-2126-46a0-816d-942ad4c726f3",
		"use": 1
	},
	{
		"Title": "Liver Lesion Detection Based on Two-Stage Saliency Model with Modified Sparse Autoencoder",
		"Description": "Yixuan Yuan, Max Q.-H. Meng, Wenjian Qin, Lei Xing.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Liver lesion detection is an important task for diagnosis and surgical planning of focal liver disease. The large numbers of images in routine liver CT studies, in addition to their high diversity in appearance, have been hurdles for detecting all lesions by visual inspection. Automated methods for lesion identification are desirable, but the results of current approaches are limited due to the diversity of the training sets and the extensive tuning of parameters. In this paper, we propose a novel saliency model for lesion detection in CT images. First, we segment the image into multi-scale patch sizes. Then, a two-stage saliency model is proposed to detect liver lesions. In the first stage, we calculate the gray level contrast saliency map based on a prior knowledge to reduce the influence of blood vessels in CT images. In the second stage, we propose a modified sparse autoencoder (SAE) with neighbourhood information to learn discriminative features directly from raw patch features and adopt Locality-constrained Linear Coding (LLC) method to encode the obtained discriminative features of each patch. Then the second saliency map is calculated based on feature uniqueness and spatial distribution of patches. Followed by an appropriate mapping fusion, the liver lesions can be detected well. With \\(7\\times 7\\) sized patches, a 120 visual word dictionary, and 14 feature dimension, our model achieved 90.81% accuracy for lesion detection.",
		"email": [
			"yxyuan@stanford.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_66",
		"source": "miccai",
		"year": 2017,
		"key": "f2c0ecd5-7b5a-4be7-be1e-92d5132da206",
		"use": 1
	},
	{
		"Title": "Manifold Learning of COPD",
		"Description": "Felix J. S. Bragman, Jamie R. McClelland, Joseph Jacob, John R. Hurst, David J. Hawkes.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Analysis of CT scans for studying Chronic Obstructive Pulmonary Disease (COPD) is generally limited to mean scores of disease extent. However, the evolution of local pulmonary damage may vary between patients with discordant effects on lung physiology. This limits the explanatory power of mean values in clinical studies. We present local disease and deformation distributions to address this limitation. The disease distribution aims to quantify two aspects of parenchymal damage: locally diffuse/dense disease and global homogeneity/heterogeneity. The deformation distribution links parenchymal damage to local volume change. These distributions are exploited to quantify inter-patient differences. We used manifold learning to model variations of these distributions in 743 patients from the COPDGene study. We applied manifold fusion to combine distinct aspects of COPD into a single model. We demonstrated the utility of the distributions by comparing associations between learned embeddings and measures of severity. We also illustrated the potential to identify trajectories of disease progression in a manifold space of COPD.",
		"email": [
			"f.bragman@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_67",
		"source": "miccai",
		"year": 2017,
		"key": "3e755add-ad9c-44f1-8e0f-bbde25a9aaac",
		"use": 1
	},
	{
		"Title": "Improving Needle Detection in 3D Ultrasound Using Orthogonal-Plane Convolutional Networks",
		"Description": "Arash Pourtaherian, Farhad Ghazvinian Zanjani, Svitlana Zinger, Nenad Mihajlovic, Gary Ng, Hendrikus Korsten, Peter de With.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Successful automated detection of short needles during an intervention is necessary to allow the physician identify and correct any misalignment of the needle and the target at early stages, which reduces needle passes and improves health outcomes. In this paper, we present a novel approach to detect needle voxels in 3D ultrasound volume with high precision using convolutional neural networks. Each voxel is classified from locally-extracted raw data of three orthogonal planes centered on it. We propose a bootstrap re-sampling approach to enhance the training in our highly imbalanced data. The proposed method successfully detects 17G and 22G needles with a single trained network, showing a robust generalized approach. Extensive ex-vivo evaluations on 3D ultrasound datasets of chicken breast show 25% increase in F1-score over the state-of-the-art feature-based method. Furthermore, very short needles inserted for only 5 mm in the volume are detected with tip localization errors of \\({<}\\)0.5 mm, indicating that the tip is always visible in the detected plane.",
		"email": [
			"a.pourtaherian@tue.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_69",
		"source": "miccai",
		"year": 2017,
		"key": "10032814-e550-4068-8984-a92dc63a5ee2",
		"use": 1
	},
	{
		"Title": "Deep Neural Networks Predict Remaining Surgery Duration from Cholecystectomy Videos",
		"Description": "Ivan Aksamentov, Andru Putra Twinanda, Didier Mutter, Jacques Marescaux, Nicolas Padoy.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "For every hospital, it is desirable to fully utilize its operating room (OR) capacity. Inaccurate planning of OR occupancy impacts patient comfort, safety and financial turnover of the hospital. A source of suboptimal scheduling often lies in the incorrect estimation of the surgery duration, which may vary significantly due to the diversity of patient conditions, surgeon skills and intraoperative situations. We propose automatic methods to estimate the remaining surgery duration in real-time by using only the image feed from the endoscopic camera and no other sensor. These approaches are based on neural networks designed to learn the workflow of an endoscopic procedure. We train and evaluate our models on a large dataset of 120 endoscopic cholecystectomies. Results show the strong benefits of these approaches when surgeries last longer than usual and promise practical improvements in OR management.",
		"email": [
			"ivan.aksamentov@etu.unistra.fr"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_66",
		"source": "miccai",
		"year": 2017,
		"key": "e43314d8-312b-4107-b2ea-78aba33842fd",
		"use": 1
	},
	{
		"Title": "Convolutional Neural Network and In-Painting Techniques for the Automatic Assessment of Scoliotic Spine Surgery from Biplanar Radiographs",
		"Description": "B. Aubert, P. A. Vidal, S. Parent, T. Cresson, C. Vazquez, J. De Guise.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Assessing the effectiveness of scoliosis surgery requires the quantification of 3D spinal deformities from pre- and post-operative radiographs. This can be achieved from 3D reconstructed models of the spine but a fast-automatic method to recover this model from pre- and post-operative radiographs remains a challenge. For example, the vertebraes visibility varies considerably and large metallic objects occlude important landmarks in postoperative radiographs. This paper presents a method for automatic 3D spine reconstruction from pre- and post-operative calibrated biplanar radiographs. We fitted a statistical shape model of the spine to images by using a 3D/2D registration based on convolutional neural networks. The metallic structures in postoperative radiographs were detected and removed using an image in-painting method to improve the performance of vertebrae registration. We applied the method to a set of 38 operated patients and clinical parameters were computed (such as the Cobb and kyphosis/lordosis angles, and vertebral axial rotations) from the pre- and post-operative 3D reconstructions. Compared to manual annotations, the proposed automatic method provided values with a mean absolute error <5.6 and <6.8 for clinical angles; <1.5 mm and <2.3 mm for vertebra locations; and <4.5 and <3.7 for vertebra orientations, respectively for pre- and post-operative times. The fast-automatic 3D reconstruction from pre- and post in-painted images provided a relevant set of parameters to assess the spine surgery without any human intervention.",
		"email": [
			"benjamin.aubert@etsmtl.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_78",
		"source": "miccai",
		"year": 2017,
		"key": "dee84c05-30fe-495b-9942-20c8c618a26f",
		"use": 1
	},
	{
		"Title": "Deep Learning for Sensorless 3D Freehand Ultrasound Imaging",
		"Description": "Raphael Prevost, Mehrdad Salehi, Julian Sprung, Robert Bauer, Wolfgang Wein.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "3D freehand ultrasound imaging is a very promising imaging modality but its acquisition is often neither portable nor practical because of the required external tracking hardware. Building a sensorless solution that is fully based on image analysis would thus have many potential applications. However, previously proposed approaches rely on physical models whose assumptions only hold on synthetic or phantom datasets, failing to translate to actual clinical acquisitions with sufficient accuracy. In this paper, we investigate the alternative approach of using statistical learning to circumvent this problem. To that end, we are leveraging the unique modeling capabilities of convolutional neural networks in order to build an end-to-end system where we directly predict the ultrasound probe motion from the images themselves. Based on thorough experiments using both phantom acquisitions and a set of 100 in-vivo long ultrasound sweeps for vein mapping, we show that our novel approach significantly outperforms the standard method and has direct clinical applicability, with an average drift error of merely 7\\(\\%\\) over the whole length of each ultrasound clip.",
		"email": [
			"prevost@imfusion.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_71",
		"source": "miccai",
		"year": 2017,
		"key": "ace13139-cf9f-4ddb-b45f-31696c0b6290",
		"use": 1
	},
	{
		"Title": "Learning CNNs with Pairwise Domain Adaption for Real-Time 6DoF Ultrasound Transducer Detection and Tracking from X-Ray Images",
		"Description": "Jiannan Zheng, Shun Miao, Rui Liao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we present a fully automatic and real-time CNN-based system that achieves highly accurate and robust 6DoF pose estimation and tracking of Transesophageal Echocardiography (TEE) transducer from 2D X-ray images, a key enabler for integrating ultrasound and fluoroscopic image guidance in hybrid operating rooms for catheter-based procedures. Lightweight hierarchical CNNs are first pre-trained purely on a large number of synthetically-generated X-ray images with known ground truth poses. The pre-trained CNNs are then refined for generalization using only a small number of real X-ray images with annotated poses via our proposed pairwise domain adaptation scheme. To resolve the pose ambiguity caused by the self-symmetry of the TEE transducer and the translucent nature of X-ray imaging, a CNN classifier is trained to classify a correct pose from its flipped counterpart by seeing a large number of synthetically-generated pairs. The proposed system is validated on 1,663 fluoroscopic images from clinical studies, and achieves an error rate of 6.53% with a clinically relevant criteria (i.e., Projected Target Registration Error larger than 2.5 mm) and a frame rate of 83.3 frames per second in tracking mode, outperforming the state-of-the-art methods in terms of both accuracy and speed.",
		"email": [
			"jiannan.zheng@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_73",
		"source": "miccai",
		"year": 2017,
		"key": "54a4beff-634c-44b3-96fb-c06041b541c3",
		"use": 1
	},
	{
		"Title": "Precise Ultrasound Bone Registration with Learning-Based Segmentation and Speed of Sound Calibration",
		"Description": "Mehrdad Salehi, Raphael Prevost, Jos-Luis Moctezuma, Nassir Navab, Wolfgang Wein.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Ultrasound imaging is increasingly used in navigated surgery and registration-based applications. However, spatial information quality in ultrasound is relatively inferior to other modalities. Main limiting factors for an accurate registration between ultrasound and other modalities are tissue deformation and speed of sound variation throughout the body. The bone surface in ultrasound is a landmark which is less affected by such geometric distortions. In this paper, we present a workflow to accurately register intra-operative ultrasound images to a reference pre-operative CT volume based on an automatic and real-time image processing pipeline. We show that a convolutional neural network is able to produce robust, accurate and fast bone segmentation of such ultrasound images. We also develop a dedicated method to perform online speed of sound calibration by focusing on the bone area and optimizing the appearance of steered compounded images. We provide extensive validation on both phantom and real cadaver data obtaining overall errors under one millimeter.",
		"email": [
			"salehi@imfusion.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_77",
		"source": "miccai",
		"year": 2017,
		"key": "5469e7ff-6bd6-40df-82f7-2aa2dacd5a7c",
		"use": 1
	},
	{
		"Title": "Deep Correlational Learning for Survival Prediction from Multi-modality Data",
		"Description": "Jiawen Yao, Xinliang Zhu, Feiyun Zhu, Junzhou Huang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Technological advances have created a great opportunity to provide multi-view data for patients. However, due to the large discrepancy between different heterogeneous views, traditional survival models are unable to efficiently handle multiple modalities data as well as learn very complex interactions that can affect survival outcomes in various ways. In this paper, we develop a Deep Correlational Survival Model (DeepCorrSurv) for the integration of multi-view data. The proposed network consists of two sub-networks, view-specific and common sub-network. To remove the view discrepancy, the proposed DeepCorrSurv first explicitly maximizes the correlation among the views. Then it transfers feature hierarchies from view commonality and specifically fine-tunes on the survival regression task. Extensive experiments on real lung and brain tumor data sets demonstrated the effectiveness of the proposed DeepCorrSurv model using multiple modalities data across different tumor types.",
		"email": [
			"jzhuang@uta.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_46",
		"source": "miccai",
		"year": 2017,
		"key": "4b30511f-7832-4861-b4fc-8a4f03badeda",
		"use": 1
	},
	{
		"Title": "Intraoperative Organ Motion Models with an Ensemble of Conditional Generative Adversarial Networks",
		"Description": "Yipeng Hu, Eli Gibson, Tom Vercauteren, Hashim U. Ahmed, Mark Emberton, Caroline M. Moore, J. Alison Noble, Dean C. Barratt.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we describe how a patient-specific, ultrasound-probe-induced prostate motion model can be directly generated from a single preoperative MR image. Our motion model allows for sampling from the conditional distribution of dense displacement fields, is encoded by a generative neural network conditioned on a medical image, and accepts random noise as additional input. The generative network is trained by a minimax optimisation with a second discriminative neural network, tasked to distinguish generated samples from training motion data. In this work, we propose that (1) jointly optimising a third conditioning neural network that pre-processes the input image, can effectively extract patient-specific features for conditioning; and (2) combining multiple generative models trained separately with heuristically pre-disjointed training data sets can adequately mitigate the problem of mode collapse. Trained with diagnostic T2-weighted MR images from 143 real patients and 73,216 3D dense displacement fields from finite element simulations of intraoperative prostate motion due to transrectal ultrasound probe pressure, the proposed models produced physically-plausible patient-specific motion of prostate glands. The ability to capture biomechanically simulated motion was evaluated using two errors representing generalisability and specificity of the model. The median values, calculated from a 10-fold cross-validation, were 2.8  0.3 mm and 1.7  0.1 mm, respectively. We conclude that the introduced approach demonstrates the feasibility of applying state-of-the-art machine learning algorithms to generate organ motion models from patient images, and shows significant promise for future research.",
		"email": [
			"yipeng.hu@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_42",
		"source": "miccai",
		"year": 2017,
		"key": "3806257d-c1e4-4c0a-9439-17b2b0c1dabf",
		"use": 1
	},
	{
		"Title": "CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins from MRI Using Multi-view CNN",
		"Description": "Aliasghar Mortazi, Rashed Karim, Kawal Rhode, Jeremy Burt, Ulas Bagci.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Anatomical and biophysical modeling of left atrium (LA) and proximal pulmonary veins (PPVs) is important for clinical management of several cardiac diseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA and PPVs through visualization. However, there is a strong need for an advanced image segmentation method to be applied to cardiac MRI for quantitative analysis of LA and PPVs. In this study, we address this unmet clinical need by exploring a new deep learning-based segmentation strategy for quantification of LA and PPVs with high accuracy and heightened efficiency. Our approach is based on a multi-view convolutional neural network (CNN) with an adaptive fusion strategy and a new loss function that allows fast and more accurate convergence of the backpropagation based optimization. After training our network from scratch by using more than 60K 2D MRI images (slices), we have evaluated our segmentation strategy to the STACOM 2013 cardiac segmentation challenge benchmark. Qualitative and quantitative evaluations, obtained from the segmentation challenge, indicate that the proposed method achieved the state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10s in GPU, and 7.5 min in CPU).",
		"email": [
			"a.mortazi@knights.ucf.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_43",
		"source": "miccai",
		"year": 2017,
		"key": "1d85be89-a0d1-486d-a63a-c9ae4c15f834",
		"use": 1
	},
	{
		"Title": "Personalized Pancreatic Tumor Growth Prediction via Group Learning",
		"Description": "Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Tumor growth prediction, a highly challenging task, has long been viewed as a mathematical modeling problem, where the tumor growth pattern is personalized based on imaging and clinical data of a target patient. Though mathematical models yield promising results, their prediction accuracy may be limited by the absence of population trend data and personalized clinical characteristics. In this paper, we propose a statistical group learning approach to predict the tumor growth pattern that incorporates both the population trend and personalized data. In order to discover high-level features from multimodal imaging data, a deep convolutional neural network approach is developed to model the voxel-wise spatio-temporal tumor progression. The deep features are combined with the time intervals and the clinical factors to feed a process of feature selection. Our predictive model is pretrained on a group data set and personalized on the target patient data to estimate the future spatio-temporal progression of the patients tumor. Multimodal imaging data at multiple time points are used in the learning, personalization and inference stages. Our method achieves a Dice coefficient of \\(86.8\\%\\,\\pm \\,3.6\\%\\) and RVD of \\(7.9\\%\\,\\pm \\,5.4\\%\\) on a pancreatic tumor data set, outperforming the DSC of \\(84.4\\%\\,\\pm \\,4.0\\%\\) and RVD \\(13.9\\%\\,\\pm \\,9.8\\%\\) obtained by a previous state-of-the-art model-based method.",
		"email": [
			"jyao@cc.nih.gov"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_48",
		"source": "miccai",
		"year": 2017,
		"key": "4bb1bbe7-6be4-4c8d-bc9b-294585212c33",
		"use": 1
	},
	{
		"Title": "Boundary-Aware Fully Convolutional Network for Brain Tumor Segmentation",
		"Description": "Haocheng Shen, Ruixuan Wang, Jianguo Zhang, Stephen J. McKenna.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose a novel, multi-task, fully convolutional network (FCN) architecture for automatic segmentation of brain tumor. This network extracts multi-level contextual information by concatenating hierarchical feature representations extracted from multimodal MR images along with their symmetric-difference images. It achieves improved segmentation performance by incorporating boundary information directly into the loss function. The proposed method was evaluated on the BRATS13 and BRATS15 datasets and compared with competing methods on the BRATS13 testing set. Segmented tumor boundaries obtained were better than those obtained by single-task FCN and by FCN with CRF. The method is among the most accurate available and has relatively low computational cost at test time.",
		"email": [
			"h.y.shen@dundee.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_49",
		"source": "miccai",
		"year": 2017,
		"key": "fcd040cf-a478-4159-83bf-c3f669a91f1e",
		"use": 1
	},
	{
		"Title": "DARWIN: Deformable Patient Avatar Representation With Deep Image Network",
		"Description": "Vivek Singh, Kai Ma, Birgi Tamersoy, Yao-Jen Chang, Andreas Wimmer, Thomas ODonnell, Terrence Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we present a technical approach to robustly estimate the detailed patient body surface mesh under clothing cover from a single snapshot of a range sensor. Existing methods either lack level of detail of the estimated patient body model, fail to estimate the body model robustly under clothing cover, or lack sufficient evaluation over real patient datasets. In this work, we overcome these limitations by learning deep convolutional networks over real clinical dataset with large variation and augmentation. Our approach is validated with experiments conducted over 1063 human subjects from 3 different hospitals and surface errors are measured against groundtruth from CT data.",
		"email": [
			"vivek-singh@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_56",
		"source": "miccai",
		"year": 2017,
		"key": "a0742648-d548-427b-ad1e-e81aaf167724",
		"use": 1
	},
	{
		"Title": "Tracking and Segmentation of the Airways in Chest CT Using a Fully Convolutional Network",
		"Description": "Qier Meng, Holger R. Roth, Takayuki Kitasaka, Masahiro Oda, Junji Ueno, Kensaku Mori.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Airway segmentation plays an important role in analyzing chest computed tomography (CT) volumes such as lung cancer detection, chronic obstructive pulmonary disease (COPD), and surgical navigation. However, due to the complex tree-like structure of the airways, obtaining segmentation results with high accuracy for a complete 3D airway extraction remains a challenging task. In recent years, deep learning based methods, especially fully convolutional networks (FCN), have improved the state-of-the-art in many segmentation tasks. 3D U-Net is an example that optimized for 3D biomedical imaging. It consists of a contracting encoder part to analyze the input volume and a successive decoder part to generate integrated 3D segmentation results. While 3D U-Net can be trained for any 3D segmentation task, its direct application to airway segmentation is challenging due to differently sized airway branches. In this work, we combine 3D deep learning with image-based tracking in order to automatically extract the airways. Our method is driven by adaptive cuboidal volume of interest (VOI) analysis using a 3D U-Net model. We track the airways along their centerlines and set VOIs according to the diameter and running direction of each airway. After setting a VOI, the 3D U-Net is utilized to extract the airway region inside the VOI. All extracted candidate airway regions are unified to form an integrated airway tree. We trained on 30 cases and tested our method on an additional 20 cases. Compared with other state-of-the-art airway tracking and segmentation methods, our method can increase the detection rate by 5.6 while decreasing the false positives (FP) by 0.7 percentage points.",
		"email": [
			"kensaku@is.nagoya-u.ac.jp"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_23",
		"source": "miccai",
		"year": 2017,
		"key": "71035601-2a92-4327-9af3-64fd4269df53",
		"use": 1
	},
	{
		"Title": "Semi-supervised Learning for Network-Based Cardiac MR Image Segmentation",
		"Description": "Wenjia Bai, Ozan Oktay, Matthew Sinclair, Hideaki Suzuki, Martin Rajchl, Giacomo Tarroni, Ben Glocker, Andrew King, Paul M. Matthews, Daniel Rueckert.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Training a fully convolutional network for pixel-wise (or voxel-wise) image segmentation normally requires a large number of training images with corresponding ground truth label maps. However, it is a challenge to obtain such a large training set in the medical imaging domain, where expert annotations are time-consuming and difficult to obtain. In this paper, we propose a semi-supervised learning approach, in which a segmentation network is trained from both labelled and unlabelled data. The network parameters and the segmentations for the unlabelled data are alternately updated. We evaluate the method for short-axis cardiac MR image segmentation and it has demonstrated a high performance, outperforming a baseline supervised method. The mean Dice overlap metric is 0.92 for the left ventricular cavity, 0.85 for the myocardium and 0.89 for the right ventricular cavity. It also outperforms a state-of-the-art multi-atlas segmentation method by a large margin and the speed is substantially faster.",
		"email": [
			"w.bai@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_29",
		"source": "miccai",
		"year": 2017,
		"key": "e455daf2-73d3-4edf-97f7-970a1eedd2e5",
		"use": 1
	},
	{
		"Title": "Detection and Characterization of the Fetal Heartbeat in Free-hand Ultrasound Sweeps with Weakly-supervised Two-streams Convolutional Networks",
		"Description": "Yuan Gao, J. Alison Noble.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Assessment of fetal cardiac activity is essential to confirm pregnancy viability in obstetric ultrasound. However, automated detection and localization of a beating fetal heart, in free-hand ultrasound sweeps, is a very challenging task, due to high variation in heart appearance, scale and position (because of heart deformation, scanning orientations and artefacts). In this paper, we present a two-stream Convolutional Network (ConvNet) -a temporal sequence learning model- that recognizes heart frames and localizes the heart using only weak supervision. Our contribution is three-fold: (i) to the best of our knowledge, this is the first work to use two-stream spatio-temporal ConvNets in analysis of free-hand fetal ultrasound videos. The model is compact, and can be trained end-to-end with only image level labels, (ii) the model enforces rotation invariance, which does not require additional augmentation in the training data, and (iii) the model is particularly robust for heart detection, which is important in our application where there can be additional distracting textures, such as acoustic shadows. Our results demonstrate that the proposed two-stream ConvNet architecture significantly outperforms single stream spatial ConvNets (90.3% versus 74.9%), in terms of heart identification.",
		"email": [
			"Yuan.Gao2@eng.ox.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_35",
		"source": "miccai",
		"year": 2017,
		"key": "21202d19-2108-4da2-9d05-7fdfa413ed1c",
		"use": 1
	},
	{
		"Title": "Learning-Based Spatiotemporal Regularization and Integration of Tracking Methods for Regional 4D Cardiac Deformation Analysis",
		"Description": "Allen Lu, Maria Zontak, Nripesh Parajuli, John C. Stendahl, Nabil Boutagy, Melissa Eberle, Imran Alkhalil, Matthew ODonnell, Albert J. Sinusas, James S. Duncan.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Dense cardiac motion tracking and deformation analysis from echocardiography is important for detection and localization of myocardial dysfunction. However, tracking methods are often unreliable due to inherent ultrasound imaging properties. In this work, we propose a new data-driven spatiotemporal regularization strategy. We generate 4D Lagrangian displacement patches from different input sources as training data and learn the regularization procedure via a multi-layered perceptron (MLP) network. The learned regularization procedure is applied to initial noisy tracking results. We further propose a framework for integrating tracking methods to produce better overall estimations. We demonstrate the utility of this approach on block-matching, surface tracking, and free-form deformation-based methods. Finally, we quantitatively and qualitatively evaluate our performance on both tracking and strain accuracy using both synthetic and in vivo data.",
		"email": [
			"allen.lu@yale.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_37",
		"source": "miccai",
		"year": 2017,
		"key": "e1898ea9-fc69-48a5-a12b-e22d6166acf6",
		"use": 1
	},
	{
		"Title": "Retrospective Head Motion Estimation in Structural Brain MRI with 3D CNNs",
		"Description": "Juan Eugenio Iglesias, Garikoitz Lerma-Usabiaga, Luis C. Garcia-Peraza-Herrera, Sara Martinez, Pedro M. Paz-Alonso.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Head motion is one of the most important nuisance variables in neuroimaging, particularly in studies of clinical or special populations, such as children. However, the possibility of estimating motion in structural MRI is limited to a few specialized sites using advanced MRI acquisition techniques. Here we propose a supervised learning method to retrospectively estimate motion from plain MRI. Using sparsely labeled training data, we trained a 3D convolutional neural network to assess if voxels are corrupted by motion or not. The output of the network is a motion probability map, which we integrate across a region of interest (ROI) to obtain a scalar motion score. Using cross-validation on a dataset of \\(n=48\\) healthy children scanned at our center, and the cerebral cortex as ROI, we show that the proposed measure of motion explains away 37% of the variation in cortical thickness. We also show that the motion score is highly correlated with the results from human quality control of the scans. The proposed technique can not only be applied to current studies, but also opens up the possibility of reanalyzing large amounts of legacy datasets with motion into consideration: we applied the classifier trained on data from our center to the ABIDE dataset (autism), and managed to recover group differences that were confounded by motion.",
		"email": [
			"e.iglesias@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_36",
		"source": "miccai",
		"year": 2017,
		"key": "81bf7bc7-23e2-469e-b12f-c40c70e8aef3",
		"use": 1
	},
	{
		"Title": "Joint Craniomaxillofacial Bone Segmentation and Landmark Digitization by Context-Guided Fully Convolutional Networks",
		"Description": "Jun Zhang, Mingxia Liu, Li Wang, Si Chen, Peng Yuan, Jianfu Li, Steve Guo-Fang Shen, Zhen Tang, Ken-Chung Chen, James J. Xia, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Generating accurate 3D models from cone-beam computed tomography (CBCT) images is an important step in developing treatment plans for patients with craniomaxillofacial (CMF) deformities. This process often involves bone segmentation and landmark digitization. Since anatomical landmarks generally lie on the boundaries of segmented bone regions, the tasks of bone segmentation and landmark digitization could be highly correlated. However, most existing methods simply treat them as two standalone tasks, without considering their inherent association. In addition, these methods usually ignore the spatial context information (i.e., displacements from voxels to landmarks) in CBCT images. To this end, we propose a context-guided fully convolutional network (FCN) for joint bone segmentation and landmark digitization. Specifically, we first train an FCN to learn the displacement maps to capture the spatial context information in CBCT images. Using the learned displacement maps as guidance information, we further develop a multi-task FCN to jointly perform bone segmentation and landmark digitization. Our method has been evaluated on 107 subjects from two centers, and the experimental results show that our method is superior to the state-of-the-art methods in both bone segmentation and landmark digitization.",
		"email": [
			"jxia@houstonmethodist.org"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_81",
		"source": "miccai",
		"year": 2017,
		"key": "5ae0f71a-83b7-49e2-9435-123cab3c5b97",
		"use": 1
	},
	{
		"Title": "Neuron Segmentation Using Deep Complete Bipartite Networks",
		"Description": "Jianxu Chen, Sreya Banerjee, Abhinav Grama, Walter J. Scheirer, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we consider the problem of automatically segmenting neuronal cells in dual-color confocal microscopy images. This problem is a key task in various quantitative analysis applications in neuroscience, such as tracing cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using fully convolutional networks (FCN), has profoundly changed segmentation research in biomedical imaging. We face two major challenges in this problem. First, neuronal cells may form dense clusters, making it difficult to correctly identify all individual cells (even to human experts). Consequently, segmentation results of the known FCN-type models are not accurate enough. Second, pixel-wise ground truth is difficult to obtain. Only a limited amount of approximate instance-wise annotation can be collected, which makes the training of FCN models quite cumbersome. We propose a new FCN-type deep learning model, called deep complete bipartite networks (CB-Net), and a new scheme for leveraging approximate instance-wise annotation to train our pixel-wise prediction model. Evaluated using seven real datasets, our proposed new CB-Net model outperforms the state-of-the-art FCN models and produces neuron segmentation results of remarkable quality.",
		"email": [
			"jchen16@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_3",
		"source": "miccai",
		"year": 2017,
		"key": "c307f1c9-d5f1-462c-bb1d-d441462fb3a2",
		"use": 1
	},
	{
		"Title": "Semi-supervised Segmentation of Optic Cup in Retinal Fundus Images Using Variational Autoencoder",
		"Description": "Suman Sedai, Dwarikanath Mahapatra, Sajini Hewavitharanage, Stefan Maetschke, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Accurate segmentation of optic cup and disc in retinal fundus images is essential to compute the cup to disc ratio parameter, which is important for glaucoma assessment. The ill-defined boundaries of optic cup makes the segmentation a lot more challenging compared to optic disc. Existing approaches have mainly used fully supervised learning that requires many labeled samples to build a robust segmentation framework. In this paper, we propose a novel semi-supervised method to segment the optic cup, which can accurately localize the anatomy using limited number of labeled samples. The proposed method leverages the inherent feature similarity from a large number of unlabeled images to train the segmentation model from a smaller number of labeled images. It first learns the parameters of a generative model from unlabeled images using variational autoencoder. The trained generative model provides the feature embedding of the images which allows the clustering of the related observation in the latent feature space. We combine the feature embedding with the segmentation autoencoder which is trained on the labeled images for pixel-wise segmentation of the cup region. The main novelty of the proposed approach is in the utilization of generative models for semi-supervised segmentation. Experimental results show that the proposed method successfully segments optic cup with small number of labeled images, and unsupervised feature embedding learned from unlabeled data improves the segmentation accuracy. Given the challenge of access to annotated medical images in every clinical application, the proposed framework is a key contribution and applicable for segmentation of different anatomies across various medical imaging modalities.",
		"email": [
			"ssedai@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_9",
		"source": "miccai",
		"year": 2017,
		"key": "e1a37d47-1faf-439c-9217-dd6a05ef66a9",
		"use": 1
	},
	{
		"Title": "Active Learning and Proofreading for Delineation of Curvilinear Structures",
		"Description": "Agata Mosinska, Jakub Tarnawski, Pascal Fua.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Many state-of-the-art delineation methods rely on supervised machine learning algorithms. As a result, they require manually annotated training data, which is tedious to obtain. Furthermore, even minor classification errors may significantly affect the topology of the final result. In this paper we propose a generic approach to addressing both of these problems by taking into account the influence of a potential misclassification on the resulting delineation. In an Active Learning context, we identify parts of linear structures that should be annotated first in order to train a classifier effectively. In a proofreading context, we similarly find regions of the resulting reconstruction that should be verified in priority to obtain a nearly-perfect result. In both cases, by focusing the attention of the human expert on potential classification mistakes which are the most critical parts of the delineation, we reduce the amount of required supervision. We demonstrate the effectiveness of our approach on microscopy images depicting blood vessels and neurons.",
		"email": [
			"agata.mosinska@epfl.ch"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_19",
		"source": "miccai",
		"year": 2017,
		"key": "0e894b69-cccb-41c9-8182-06bad30c7224",
		"use": 1
	},
	{
		"Title": "Deep Learning for Isotropic Super-Resolution from Non-isotropic 3D Electron Microscopy",
		"Description": "Larissa Heinrich, John A. Bogovic, Stephan Saalfeld.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "The most sophisticated existing methods to generate 3D isotropic super-resolution (SR) from non-isotropic electron microscopy (EM) are based on learned dictionaries. Unfortunately, none of the existing methods generate practically satisfying results. For 2D natural images, recently developed super-resolution methods that use deep learning have been shown to significantly outperform the previous state of the art. We have adapted one of the most successful architectures (FSRCNN) for 3D super-resolution, and compared its performance to a 3D U-Net architecture that has not been used previously to generate super-resolution. We trained both architectures on artificially downscaled isotropic ground truth from focused ion beam milling scanning EM (FIB-SEM) and tested the performance for various hyperparameter settings.",
		"email": [
			"saalfelds@janelia.hhmi.org"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_16",
		"source": "miccai",
		"year": 2017,
		"key": "43d92205-c15e-43c9-b68a-59e45875b966",
		"use": 1
	},
	{
		"Title": "Isotropic Reconstruction of 3D Fluorescence Microscopy Images Using Convolutional Neural Networks",
		"Description": "Martin Weigert, Loic Royer, Florian Jug, Gene Myers.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to 3 synthetic and 3 real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.",
		"email": [
			"mweigert@mpi-cbg.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_15",
		"source": "miccai",
		"year": 2017,
		"key": "663b6a90-c643-4a0a-b4d3-9ce93f29dbc3",
		"use": 1
	},
	{
		"Title": "Fast Background Removal Method for 3D Multi-channel Deep Tissue Fluorescence Imaging",
		"Description": "Chenchen Li, Xiaowei Li, Hongji Cao, He Jiang, Xiaotie Deng, Danny Z. Chen, Lin Yang, Zhifeng Shao.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "The recent advances in tissue clearing and optical imaging have enabled us to obtain three-dimensional high-resolution images of various tissues. However, the severe background noise remains a major obstacle. In addition, there is an urgent need for fast background ground correction methods. In this paper, we present a fast background removal method for 3D multi-channel deep tissue fluorescence images, in which the objectives of different channels are well separated. We first conduct a window-based normalization to distinguish foreground signals from background noises in all channels. Then, we identify the pure background regions by conducting subtraction of images in different channels, which allow us to estimate the background noises of the whole images by interpolation. Experiments on real 3D datasets of mouse stomach show our method has superior performance and efficiency comparing with the current state-of-the-art background correction methods.",
		"email": [
			"lcc1992@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66185-8_11",
		"source": "miccai",
		"year": 2017,
		"key": "3ea80d61-7561-43ea-91eb-c447137df019",
		"use": 1
	},
	{
		"Title": "Maximum Mean Discrepancy Based Multiple Kernel Learning for Incomplete Multimodality Neuroimaging Data",
		"Description": "Xiaofeng Zhu, Kim-Han Thung, Ehsan Adeli, Yu Zhang, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "It is challenging to use incomplete multimodality data for Alzheimers Disease (AD) diagnosis. The current methods to address this challenge, such as low-rank matrix completion (i.e., imputing the missing values and unknown labels simultaneously) and multi-task learning (i.e., defining one regression task for each combination of modalities and then learning them jointly), are unable to model the complex data-to-label relationship in AD diagnosis and also ignore the heterogeneity among the modalities. In light of this, we propose a new Maximum Mean Discrepancy (MMD) based Multiple Kernel Learning (MKL) method for AD diagnosis using incomplete multimodality data. Specifically, we map all the samples from different modalities into a Reproducing Kernel Hilbert Space (RKHS), by devising a new MMD algorithm. The proposed MMD method incorporates data distribution matching, pair-wise sample matching and feature selection in an unified formulation, thus alleviating the modality heterogeneity issue and making all the samples comparable to share a common classifier in the RKHS. The resulting classifier obviously captures the nonlinear data-to-label relationship. We have tested our method using MRI and PET data from Alzheimers Disease Neuroimaging Initiative (ADNI) dataset for AD diagnosis. The experimental results show that our method outperforms other methods.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_9",
		"source": "miccai",
		"year": 2017,
		"key": "0e498ec5-7728-4ef8-af9d-a437f39bc73d",
		"use": 1
	},
	{
		"Title": "Unsupervised Feature Learning for Endomicroscopy Image Retrieval",
		"Description": "Yun Gu, Khushi Vyas, Jie Yang, Guang-Zhong Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Learning the visual representation for medical images is a critical task in computer-aided diagnosis. In this paper, we propose Unsupervised Multimodal Graph Mining (UMGM) to learn the discriminative features for probe-based confocal laser endomicroscopy (pCLE) mosaics of breast tissue. We build a multiscale multimodal graph based on both pCLE mosaics and histology images. The positive pairs are mined via cycle consistency and the negative pairs are extracted based on geodetic distance. Given the positive and negative pairs, the latent feature space is discovered by reconstructing the similarity between pCLE and histology images. Experiments on a database with 700 pCLE mosaics demonstrate that the proposed method outperforms previous works on pCLE feature learning. Specially, the top-1 accuracy in an eight-class retrieval task is 0.659 which leads to 10% improvement compared with the state-of-the-art method.",
		"email": [
			"jieyang@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_8",
		"source": "miccai",
		"year": 2017,
		"key": "2b529fbd-e443-473c-a8c9-ace301298bf2",
		"use": 1
	},
	{
		"Title": "Deep Multi-task Multi-channel Learning for Joint Classification and Regression of Brain Status",
		"Description": "Mingxia Liu, Jun Zhang, Ehsan Adeli, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Jointly identifying brain diseases and predicting clinical scores have attracted increasing attention in the domain of computer-aided diagnosis using magnetic resonance imaging (MRI) data, since these two tasks are highly correlated. Although several joint learning models have been developed, most existing methods focus on using human-engineered features extracted from MRI data. Due to the possible heterogeneous property between human-engineered features and subsequent classification/regression models, those methods may lead to sub-optimal learning performance. In this paper, we propose a deep multi-task multi-channel learning (DM\\(^2\\)L) framework for simultaneous classification and regression for brain disease diagnosis, using MRI data and personal information (i.e., age, gender, and education level) of subjects. Specifically, we first identify discriminative anatomical landmarks from MR images in a data-driven manner, and then extract multiple image patches around these detected landmarks. A deep multi-task multi-channel convolutional neural network is then developed for joint disease classification and clinical score regression. We train our model on a large multi-center cohort (i.e., ADNI-1) and test it on an independent cohort (i.e., ADNI-2). Experimental results demonstrate that DM\\(^2\\)L is superior to the state-of-the-art approaches in brain diasease diagnosis.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_1",
		"source": "miccai",
		"year": 2017,
		"key": "b4b7a742-dbfc-44f9-8d4e-49fd4053d644",
		"use": 1
	},
	{
		"Title": "Automatic Liver Segmentation Using an Adversarial Image-to-Image Network",
		"Description": "Dong Yang, Daguang Xu, S. Kevin Zhou, Bogdan Georgescu, Mingqing Chen, Sasa Grbic, Dimitris Metaxas, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.",
		"email": [
			"daguang.xu@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_58",
		"source": "miccai",
		"year": 2017,
		"key": "d5157b65-6abc-489d-958b-c2b36940bf42",
		"use": 1
	},
	{
		"Title": "Multi-level Multi-task Structured Sparse Learning for Diagnosis of Schizophrenia Disease",
		"Description": "Mingliang Wang, Xiaoke Hao, Jiashuang Huang, Kangcheng Wang, Xijia Xu, Daoqiang Zhang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In recent studies, it has attracted increasing attention in multi-frequency bands analysis for diagnosis of schizophrenia (SZ). However, most existing feature selection methods designed for multi-frequency bands analysis do not take into account the inherent structures (i.e., both frequency specificity and complementary information) from multi-frequency bands in the model, which are limited to identify the discriminative feature subset in a single step. To address this problem, we propose a multi-level multi-task structured sparse learning (MLMT-TS) framework to explicitly consider the common features with a hierarchical structure. Specifically, we introduce two regularization terms in the hierarchical framework to impose the common features across different bands and the specificity from individuals. Then, the selected features are used to construct multiple support vector machine (SVM) classifiers. Finally, we adopt an ensemble strategy to combine outputs of all SVM classifiers to achieve the final decision. Our method has been evaluated on 46 subjects, and the superior classification results demonstrate the effectiveness of our proposed method as compared to other methods.",
		"email": [
			"dqzhang@nuaa.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_6",
		"source": "miccai",
		"year": 2017,
		"key": "87a84867-e72d-400c-b559-33ff77dcc11a",
		"use": 1
	},
	{
		"Title": "Lesion Detection and Grading of Diabetic Retinopathy via Two-Stages Deep Convolutional Neural Networks",
		"Description": "Yehui Yang, Tao Li, Wensi Li, Haishan Wu, Wei Fan, Wensheng Zhang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose an automatic diabetic retinopathy (DR) analysis algorithm based on two-stages deep convolutional neural networks (DCNN). Compared to existing DCNN-based DR detection methods, the proposed algorithm has the following advantages: (1) Our algorithm can not only point out the lesions in fundus color images, but also give the severity grades of DR. (2) By introducing an imbalanced weighting scheme, more attentions will be payed on lesion patches for DR grading, which significantly improves the performance of DR grading under the same implementation setup. In this study, we label 12, 206 lesion patches and re-annotate the DR grades of 23, 595 fundus images from Kaggle competition dataset. Under the guidance of clinical ophthalmologists, the experimental results show that our lesion detection net achieves comparable performance with trained human observers, and the proposed imbalanced weighted scheme also be proved to significantly enhance the capability of our DCNN-based DR grading algorithm.",
		"email": [
			"yangyehuisw@126.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_61",
		"source": "miccai",
		"year": 2017,
		"key": "4bbbe962-87eb-47d4-814d-92d6f40718f2",
		"use": 1
	},
	{
		"Title": "Deep Reinforcement Learning for Active Breast Lesion Detection from DCE-MRI",
		"Description": "Gabriel Maicas, Gustavo Carneiro, Andrew P. Bradley, Jacinto C. Nascimento, Ian Reid.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We present a novel methodology for the automated detection of breast lesions from dynamic contrast-enhanced magnetic resonance volumes (DCE-MRI). Our method, based on deep reinforcement learning, significantly reduces the inference time for lesion detection compared to an exhaustive search, while retaining state-of-art accuracy.",
		"email": [
			"gabriel.maicas@adelaide.edu.au"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_76",
		"source": "miccai",
		"year": 2017,
		"key": "a07963fd-3c8a-4b87-9914-ba40f6b83b53",
		"use": 1
	},
	{
		"Title": "Transfer Learning for Domain Adaptation in MRI: Application in Brain Lesion Segmentation",
		"Description": "Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer, Elena Marchiori, Mehran Pesteie, Charles R. G. Guttmann, Frank-Erik de Leeuw, Clare M. Tempany, Bram van Ginneken, Andriy Fedorov, Purang Abolmaesumi, Bram Platel, William M. WellsIII.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis and treatment. However, variations in MRI acquisition protocols result in different appearances of normal and diseased tissue in the images. Convolutional neural networks (CNNs), which have shown to be successful in many medical image analysis tasks, are typically sensitive to the variations in imaging protocols. Therefore, in many cases, networks trained on data acquired with one MRI protocol, do not perform satisfactorily on data acquired with different protocols. This limits the use of models trained with large annotated legacy datasets on a new dataset with a different domain which is often a recurring situation in clinical settings. In this study, we aim to answer the following central questions regarding domain adaptation in medical image analysis: Given a fitted legacy model, (1) How much data from the new domain is required for a decent adaptation of the original network?; and, (2) What portion of the pre-trained model parameters should be retrained given a certain number of the new domain training samples? To address these questions, we conducted extensive experiments in white matter hyperintensity segmentation task. We trained a CNN on legacy MR images of brain and evaluated the performance of the domain-adapted network on the same task with images from a different domain. We then compared the performance of the model to the surrogate scenarios where either the same trained network is used or a new network is trained from scratch on the new dataset. The domain-adapted network tuned only by two training examples achieved a Dice score of 0.63 substantially outperforming a similar network trained on the same set of examples from scratch.",
		"email": [
			"mehrtash@bwh.harvard.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_59",
		"source": "miccai",
		"year": 2017,
		"key": "9b3fff6c-703f-4487-927a-0f1f9ed937ed",
		"use": 1
	},
	{
		"Title": "Pancreas Segmentation in MRI Using Graph-Based Decision Fusion on Convolutional Neural Networks",
		"Description": "Jinzheng Cai, Le Lu, Yuanpu Xie, Fuyong Xing, Lin Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.",
		"email": [
			"lin.yang@bme.ufl.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_77",
		"source": "miccai",
		"year": 2017,
		"key": "eb94c28c-6bbd-4d8e-aa48-e0081e6c7b06",
		"use": 1
	},
	{
		"Title": "BRIEFnet: Deep Pancreas Segmentation Using Binary Sparse Convolutions",
		"Description": "Mattias P. Heinrich, Ozan Oktay.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Dense prediction using deep convolutional neural networks (CNNs) has recently advanced the field of segmentation in computer vision and medical imaging. In contrast to patch-based classification, it requires only a single path through a deep network to segment every voxel in an image. However, it is difficult to incorporate contextual information without using contracting (pooling) layers, which would reduce the spatial accuracy for thinner structures. Consequently, huge receptive fields are required which might lead to disproportionate computational demand. Here, we propose to use binary sparse convolutions in the first layer as a particularly effective approach to reduce complexity while achieving high accuracy. The concept is inspired by the successful BRIEF descriptors and complemented with \\(1\\times 1\\) convolutions (cf. network in network) to further reduce the number of trainable parameters. Sparsity is in particular important for small datasets often found in medical imaging. Our experimental validation demonstrates accuracies for pancreas segmentation in CT that are comparable with state-of-the-art deep learning approaches and registration based multi-atlas segmentation with label fusion. The whole network, which also includes a classic CNN path to improve local details, can be trained in 10 min. Segmenting a new scan takes 3 s even without using a GPU.",
		"email": [
			"heinrich@imi.uni-luebeck.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_38",
		"source": "miccai",
		"year": 2017,
		"key": "3a05ee28-2b94-4cc1-9664-0be847a065c5",
		"use": 1
	},
	{
		"Title": "Discriminative Localization in CNNs for Weakly-Supervised Segmentation of Pulmonary Nodules",
		"Description": "Xinyang Feng, Jie Yang, Andrew F. Laine, Elsa D. Angelini.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automated detection and segmentation of pulmonary nodules on lung computed tomography (CT) scans can facilitate early lung cancer diagnosis. Existing supervised approaches for automated nodule segmentation on CT scans require voxel-based annotations for training, which are labor- and time-consuming to obtain. In this work, we propose a weakly-supervised method that generates accurate voxel-level nodule segmentation trained with image-level labels only. By adapting a convolutional neural network (CNN) trained for image classification, our proposed method learns discriminative regions from the activation maps of convolution units at different scales, and identifies the true nodule location with a novel candidate-screening framework. Experimental results on the public LIDC-IDRI dataset demonstrate that, our weakly-supervised nodule segmentation framework achieves competitive performance compared to a fully-supervised CNN-based segmentation method.",
		"email": [
			"e.angelini@imperial.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_65",
		"source": "miccai",
		"year": 2017,
		"key": "fe88ad87-2d26-494b-b23b-84f9cd1c26e1",
		"use": 1
	},
	{
		"Title": "Scalable Multimodal Convolutional Networks for Brain Tumour Segmentation",
		"Description": "Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Brain tumour segmentation plays a key role in computer-assisted surgery. Deep neural networks have increased the accuracy of automatic segmentation significantly, however these models tend to generalise poorly to different imaging modalities than those for which they have been designed, thereby limiting their applications. For example, a network architecture initially designed for brain parcellation of monomodal T1 MRI can not be easily translated into an efficient tumour segmentation network that jointly utilises T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable multimodal deep learning architecture using new nested structures that explicitly leverage deep features within or across modalities. This aims at making the early layers of the architecture structured and sparse so that the final architecture becomes scalable to the number of modalities. We evaluate the scalable architecture for brain tumour segmentation and give evidence of its regularisation effect compared to the conventional concatenation approach.",
		"email": [
			"l.fidon@cs.ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_33",
		"source": "miccai",
		"year": 2017,
		"key": "291fdb24-02c7-400e-9e95-dfcb9fcc34b3",
		"use": 1
	},
	{
		"Title": "Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness",
		"Description": "Wufeng Xue, Andrea Lum, Ashley Mercado, Mark Landis, James Warrington, Shuo Li.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: (1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; (2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190 mm\\(^2\\), 1.41 mm, 2.68 mm for average areas, RWT, dimensions and error rate of 10.4% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function.",
		"email": [
			"slishuo@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_32",
		"source": "miccai",
		"year": 2017,
		"key": "176eb12f-5900-4b6b-b54e-84c98b0e4456",
		"use": 1
	},
	{
		"Title": "Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection",
		"Description": "Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li, Xiaogang Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose a convolution neural network based algorithm for simultaneously diagnosing diabetic retinopathy and highlighting suspicious regions. Our contributions are two folds: (1) a network termed Zoom-in-Net which mimics the zoom-in process of a clinician to examine the retinal images. Trained with only image-level supervisions, Zoom-in-Net can generate attention maps which highlight suspicious regions, and predicts the disease level accurately based on both the whole image and its high resolution suspicious patches. (2) Only four bounding boxes generated from the automatically learned attention maps are enough to cover 80% of the lesions labeled by an experienced ophthalmologist, which shows good localization ability of the attention maps. By clustering features at high response locations on the attention maps, we discover meaningful clusters which contain potential lesions in diabetic retinopathy. Experiments show that our algorithm outperform the state-of-the-art methods on two datasets, EyePACS and Messidor.",
		"email": [
			"zwang@ee.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_31",
		"source": "miccai",
		"year": 2017,
		"key": "64ee7763-7ace-43ee-9ce1-3fd7ee528e94",
		"use": 1
	},
	{
		"Title": "Boundary Regularized Convolutional Neural Network for Layer Parsing of Breast Anatomy in Automated Whole Breast Ultrasound",
		"Description": "Cheng Bian, Ran Lee, Yi-Hong Chou, Jie-Zhi Cheng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "A boundary regularized deep convolutional encoder-decoder network (ConvEDNet) is developed in this study to address the difficult anatomical layer parsing problem in the noisy Automated Whole Breast Ultrasound (AWBUS) images. To achieve better network initialization, a two-stage adaptive domain transfer (2DT) is employed to land the VGG-16 encoder on the AWBUS domain with the bridge of network training for AWBUS edge detector. The knowledge transferred encoder is denoted as VGG-USEdge. To further augment the training of ConvEDNet, a deep boundary supervision (DBS) strategy is introduced to regularize the feature learning for better robustness to speckle noise and shadowing effect. We argue that simply counting on the image context cue, which can be learnt with the guidance of label maps, may not be sufficient to deal with the intrinsic noisy property of ultrasound images. With the regularization of boundary cue, the segmentation learning can be boosted. The efficacy of the proposed 2DT-DBS ConvEDNet is corroborated with the extensive comparison to the state-of-the-art deep learning segmentation methods. The segmentation results may assist the clinical image reading, particularly for junior medical doctors and residents and help to reduce false-positive findings from a computer-aided detection scheme.",
		"email": [
			"jzcheng@ntu.edu.tw"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_30",
		"source": "miccai",
		"year": 2017,
		"key": "5e68f182-4bd5-4135-ac19-76dcf097b03d",
		"use": 1
	},
	{
		"Title": "Semi-supervised Deep Learning for Fully Convolutional Networks",
		"Description": "Christoph Baur, Shadi Albarqouni, Nassir Navab.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.",
		"email": [
			"c.baur@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_36",
		"source": "miccai",
		"year": 2017,
		"key": "f308f036-f8b5-4d6a-b4a5-1a9a9be23bb2",
		"use": 1
	},
	{
		"Title": "Quality Assessment of Echocardiographic Cine Using Recurrent Neural Networks: Feasibility on Five Standard View Planes",
		"Description": "Amir H. Abdi, Christina Luong, Teresa Tsang, John Jue, Ken Gin, Darwin Yeung, Dale Hawley, Robert Rohling, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Echocardiography (echo) is a clinical imaging technique which is highly dependent on operator experience. We aim to reduce operator variability in data acquisition by automatically computing an echo quality score for real-time feedback. We achieve this with a deep neural network model, with convolutional layers to extract hierarchical features from the input echo cine and recurrent layers to leverage the sequential information in the echo cine loop. Using data from 509 separate patient studies, containing 2,450 echo cines across five standard echo imaging planes, we achieved a mean quality score accuracy of 85\\(\\%\\) compared to the gold-standard score assigned by experienced echosonographers. The proposed approach calculates the quality of a given 20 frame echo sequence within 10 ms, sufficient for real-time deployment.",
		"email": [
			"amirabdi@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_35",
		"source": "miccai",
		"year": 2017,
		"key": "ea74f4c1-f3b9-4aed-a137-81cd17249177",
		"use": 1
	},
	{
		"Title": "DOTE: Dual cOnvolutional filTer lEarning for Super-Resolution and Cross-Modality Synthesis in MRI",
		"Description": "Yawen Huang, Ling Shao, Alejandro F. Frangi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Cross-modal image synthesis is a topical problem in medical image computing. Existing methods for image synthesis are either tailored to a specific application, require large scale training sets, or are based on partitioning images into overlapping patches. In this paper, we propose a novel Dual cOnvolutional filTer lEarning (DOTE) approach to overcome the drawbacks of these approaches. We construct a closed loop joint filter learning strategy that generates informative feedback for model self-optimization. Our method can leverage data more efficiently thus reducing the size of the required training set. We extensively evaluate DOTE in two challenging tasks: image super-resolution and cross-modality synthesis. The experimental results demonstrate superior performance of our method over other state-of-the-art methods.",
		"email": [
			"yhuang36@sheffield.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_11",
		"source": "miccai",
		"year": 2017,
		"key": "6f138290-39d7-4003-afba-b0d98a5fa4ea",
		"use": 1
	},
	{
		"Title": "Classification of Pancreatic Cysts in Computed Tomography Images Using a Random Forest and Convolutional Neural Network Ensemble",
		"Description": "Konstantin Dmitriev, Arie E. Kaufman, Ammar A. Javed, Ralph H. Hruban, Elliot K. Fishman, Anne Marie Lennon, Joel H. Saltz.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "There are many different types of pancreatic cysts. These range from completely benign to malignant, and identifying the exact cyst type can be challenging in clinical practice. This work describes an automatic classification algorithm that classifies the four most common types of pancreatic cysts using computed tomography images. The proposed approach utilizes the general demographic information about a patient as well as the imaging appearance of the cyst. It is based on a Bayesian combination of the random forest classifier, which learns subclass-specific demographic, intensity, and shape features, and a new convolutional neural network that relies on the fine texture information. Quantitative assessment of the proposed method was performed using a 10-fold cross validation on 134 patients and reported a classification accuracy of 83.6%.",
		"email": [
			"kdmitriev@cs.stonybrook.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_18",
		"source": "miccai",
		"year": 2017,
		"key": "c65d8007-c3aa-4e0d-9d2e-a6843c59005a",
		"use": 1
	},
	{
		"Title": "Deep Image-to-Image Recurrent Network with Shape Basis Learning for Automatic Vertebra Labeling in Large-Scale 3D CT Volumes",
		"Description": "Dong Yang, Tao Xiong, Daguang Xu, S. Kevin Zhou, Zhoubing Xu, Mingqing Chen, JinHyeong Park, Sasa Grbic, Trac D. Tran, Sang Peter Chin, Dimitris Metaxas, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic vertebra localization and identification in 3D medical images plays an important role in many clinical tasks, including pathological diagnosis, surgical planning and postoperative assessment. In this paper, we propose an automatic and efficient algorithm to localize and label the vertebra centroids in 3D CT volumes. First, a deep image-to-image network (DI2IN) is deployed to initialize vertebra locations, employing the convolutional encoder-decoder architecture. Next, the centroid probability maps from DI2IN are modeled as a sequence according to the spatial relationship of vertebrae, and evolved with the convolutional long short-term memory (ConvLSTM) model. Finally, the landmark positions are further refined and regularized by another neural network with a learned shape basis. The whole pipeline can be conducted in the end-to-end manner. The proposed method outperforms other state-of-the-art methods on a public database of 302 spine CT volumes with various pathologies. To further boost the performance and validate that large labeled training data can benefit the deep learning algorithms, we leverage the knowledge of additional 1000 3D CT volumes from different patients. Our experimental results show that training with a large database improves the performance of proposed framework by a large margin and achieves an identification rate of 89%.",
		"email": [
			"daguang.xu@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_57",
		"source": "miccai",
		"year": 2017,
		"key": "1515925b-f924-416d-8da5-ef7d2ee2f6bc",
		"use": 1
	},
	{
		"Title": "Deep Convolutional Encoder-Decoders for Prostate Cancer Detection and Classification",
		"Description": "Atilla P. Kiraly, Clement Abi Nader, Ahmet Tuysuzoglu, Robert Grimm, Berthold Kiefer, Noha El-Zehiry, Ali Kamen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Prostate cancer accounts for approximately 11% of all cancer cases. Definitive diagnosis is made by histopathological examination of tissue biopsies. Recently, there have been strong correlations established between pre-biopsy multi-parametric MR image findings and the histopathology results. We investigate novel deep learning networks that provide tumor localization and classification solely based on prostate multi-parametric MR images using images with biopsy confirmed lesions. We propose to use a multi-channel image-to-image convolutional encoder-decoders where responses signify localized lesions and output channels represent different tumor classes. We take simple point locations in the labeled ground truth data and train networks to output Gaussian kernels around those points across multiple channels. This approach allows for both localization and classification within a single run. The input data consists of axial T2-weighted images, apparent diffusion coefficient maps, high b-value diffusion-weighted images, and K-trans parameter maps from 202 patients. The images were co-registered on a per patient basis and exhaustive comparisons were performed with 5-fold cross-validation across three different models with increasing complexity. The highest average classification area-under-the-curve (AUC) achieved was 83.4% using a medium complexity model, in which no skip-connection were used across layers. In individual k-folds, AUCs above 90% were achieved. The results demonstrate promise for directly determining tumor malignancy without performing an invasive biopsy procedure.",
		"email": [
			"atilla.kiraly@siemens-healthineers.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_56",
		"source": "miccai",
		"year": 2017,
		"key": "43caba14-1e94-4e6d-98f9-603544d92040",
		"use": 1
	},
	{
		"Title": "SD-Layer: Stain Deconvolutional Layer for CNNs in Medical Microscopic Imaging",
		"Description": "Rahul Duggal, Anubha Gupta, Ritu Gupta, Pramit Mallick.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Convolutional Neural Networks (CNNs) are typically trained in the RGB color space. However, in medical imaging, we believe that pixel stain quantities offer a fundamental view of the interaction between tissues and stain chemicals. Since the optical density (OD) colorspace allows to compute pixel stain quantities from pixel RGB intensities using the Beer-Lamberts law, we propose a stain deconvolutional layer, hereby named as SD-Layer, affixed at the front of CNN that performs two functions: (1) it transforms the input RGB microscopic images to Optical Density (OD) space and (2) this layer deconvolves OD image with the stain basis learned through backpropagation and provides tissue-specific stain absorption quantities as input to the following CNN layers. With the introduction of only nine additional learnable parameters in the proposed SD-Layer, we obtain a considerably improved performance on two standard CNN architectures: AlexNet and T-CNN. Using the T-CNN architecture prefixed with the proposed SD-Layer, we obtain 5-fold cross-validation accuracy of 93.2% in the problem of differentiating malignant immature White Blood Cells (WBCs) from normal immature WBCs for cancer detection.",
		"email": [
			"anubha@iiitd.ac.in"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_50",
		"source": "miccai",
		"year": 2017,
		"key": "eba03d18-9d4c-42b0-8198-5b752e48bb85",
		"use": 1
	},
	{
		"Title": "Quantification of Metabolites in Magnetic Resonance Spectroscopic Imaging Using Machine Learning",
		"Description": "Dhritiman Das, Eduardo Coello, Rolf F. Schulte, Bjoern H. Menze.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Magnetic Resonance Spectroscopic Imaging (MRSI) is a clinical imaging modality for measuring tissue metabolite levels in-vivo. An accurate estimation of spectral parameters allows for better assessment of spectral quality and metabolite concentration levels. The current gold standard quantification method is the LCModel - a commercial fitting tool. However, this fails for spectra having poor signal-to-noise ratio (SNR) or a large number of artifacts. This paper introduces a framework based on random forest regression for accurate estimation of the output parameters of a model based analysis of MR spectroscopy data. The goal of our proposed framework is to learn the spectral features from a training set comprising of different variations of both simulated and in-vivo brain spectra and then use this learning for the subsequent metabolite quantification. Experiments involve training and testing on simulated and in-vivo human brain spectra. We estimate parameters such as concentration of metabolites and compare our results with that from the LCModel.",
		"email": [
			"dhritiman.das@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_53",
		"source": "miccai",
		"year": 2017,
		"key": "df6cda8d-3457-4f65-9818-839b2fb2b291",
		"use": 1
	},
	{
		"Title": "Fast Prospective Detection of Contrast Inflow in X-ray Angiograms with Convolutional Neural Network and Recurrent Neural Network",
		"Description": "Hua Ma, Pierre Ambrosini, Theo van Walsum.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic detection of contrast inflow in X-ray angiographic sequences can facilitate image guidance in computer-assisted cardiac interventions. In this paper, we propose two different approaches for prospective contrast inflow detection. The methods were developed and evaluated to detect contrast frames from X-ray sequences. The first approach trains a convolutional neural network (CNN) to distinguish whether a frame has contrast agent or not. The second method extracts contrast features from images with enhanced vessel structures; the contrast frames are then detected based on changes in the feature curve using long short-term memory (LSTM), a recurrent neural network architecture. Our experiments show that both approaches achieve good performance on detection of the beginning contrast frame from X-ray sequences and are more robust than a state-of-the-art method. As the proposed methods work in prospective settings and run fast, they have the potential of being used in clinical practice.",
		"email": [
			"h.ma@erasmusmc.nl"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_52",
		"source": "miccai",
		"year": 2017,
		"key": "bd98712d-270c-407e-a8a7-3746c11f0580",
		"use": 1
	},
	{
		"Title": "Progressive and Multi-path Holistically Nested Neural Networks for Pathological Lung Segmentation from CT Images",
		"Description": "Adam P. Harrison, Ziyue Xu, Kevin George, Le Lu, Ronald M. Summers, Daniel J. Mollura.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Pathological lung segmentation (PLS) is an important, yet challenging, medical image application due to the wide variability of pathological lung appearance and shape. Because PLS is often a pre-requisite for other imaging analytics, methodological simplicity and generality are key factors in usability. Along those lines, we present a bottom-up deep-learning based approach that is expressive enough to handle variations in appearance, while remaining unaffected by any variations in shape. We incorporate the deeply supervised learning framework, but enhance it with a simple, yet effective, progressive multi-path scheme, which more reliably merges outputs from different network stages. The result is a deep model able to produce finer detailed masks, which we call progressive holistically-nested networks (P-HNNs). Using extensive cross-validation, our method is tested on a multi-institutional dataset comprising 929 CT scans (848 publicly available), of pathological lungs, reporting mean dice scores of 0.985 and demonstrating significant qualitative and quantitative improvements over state-of-the art approaches.",
		"email": [
			"ziyue.xu@nih.gov"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_71",
		"source": "miccai",
		"year": 2017,
		"key": "d4731df6-4d65-495f-a969-9b8d939fc8b0",
		"use": 1
	},
	{
		"Title": "Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample Filtering and Hybrid-Loss Residual Learning",
		"Description": "Qi Dou, Hao Chen, Yueming Jin, Huangjing Lin, Jing Qin, Pheng-Ann Heng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, we propose a novel framework with 3D convolutional networks (ConvNets) for automated detection of pulmonary nodules from low-dose CT scans, which is a challenging yet crucial task for lung cancer early diagnosis and treatment. Different from previous standard ConvNets, we try to tackle the severe hard/easy sample imbalance problem in medical datasets and explore the benefits of localized annotations to regularize the learning, and hence boost the performance of ConvNets to achieve more accurate detections. Our proposed framework consists of two stages: (1) candidate screening, and (2) false positive reduction. In the first stage, we establish a 3D fully convolutional network, effectively trained with an online sample filtering scheme, to sensitively and rapidly screen the nodule candidates. In the second stage, we design a hybrid-loss residual network which harnesses the location and size information as important cues to guide the nodule recognition procedure. Experimental results on the public large-scale LUNA16 dataset demonstrate superior performance of our proposed method compared with state-of-the-art approaches for the pulmonary nodule detection task.",
		"email": [
			"qdou@cse.cuhk.edu.hk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_72",
		"source": "miccai",
		"year": 2017,
		"key": "970ecb2d-fb9f-4435-903e-77935bde37c7",
		"use": 1
	},
	{
		"Title": "Manifold Learning of COPD",
		"Description": "Felix J. S. Bragman, Jamie R. McClelland, Joseph Jacob, John R. Hurst, David J. Hawkes.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Analysis of CT scans for studying Chronic Obstructive Pulmonary Disease (COPD) is generally limited to mean scores of disease extent. However, the evolution of local pulmonary damage may vary between patients with discordant effects on lung physiology. This limits the explanatory power of mean values in clinical studies. We present local disease and deformation distributions to address this limitation. The disease distribution aims to quantify two aspects of parenchymal damage: locally diffuse/dense disease and global homogeneity/heterogeneity. The deformation distribution links parenchymal damage to local volume change. These distributions are exploited to quantify inter-patient differences. We used manifold learning to model variations of these distributions in 743 patients from the COPDGene study. We applied manifold fusion to combine distinct aspects of COPD into a single model. We demonstrated the utility of the distributions by comparing associations between learned embeddings and measures of severity. We also illustrated the potential to identify trajectories of disease progression in a manifold space of COPD.",
		"email": [
			"f.bragman@ucl.ac.uk"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_67",
		"source": "miccai",
		"year": 2017,
		"key": "9cc30f03-39c3-488c-94f4-524d565c8ca4",
		"use": 1
	},
	{
		"Title": "Segmentation-Free Kidney Localization and Volume Estimation Using Aggregated Orthogonal Decision CNNs",
		"Description": "Mohammad Arafat Hussain, Alborz Amir-Khalili, Ghassan Hamarneh, Rafeef Abugharbieh.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Kidney volume is an important bio-marker in the clinical diagnosis of various renal diseases. For example, it plays an essential role in follow-up evaluation of kidney transplants. Most existing methods for volume estimation rely on kidney segmentation as a prerequisite step, which has various limitations such as initialization-sensitivity and computationally-expensive optimization. In this paper, we propose a hybrid localization-volume estimation deep learning approach capable of (i) localizing kidneys in abdominal CT images, and (ii) estimating renal volume without requiring segmentation. Our approach involves multiple levels of self-learning of image representation using convolutional neural layers, which we show better capture the rich and complex variability in kidney data, demonstrably outperforming hand-crafted feature representations. We validate our method on clinical data of 100 patients with a total of 200 kidney samples (left and right). Our results demonstrate a 55% increase in kidney boundary localization accuracy, and a 30% increase in volume estimation accuracy compared to recent state-of-the-art methods deploying regression-forest-based learning for the same tasks.",
		"email": [
			"arafat@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_70",
		"source": "miccai",
		"year": 2017,
		"key": "a0a15031-5367-45a4-8ac5-3b8968c590cc",
		"use": 1
	},
	{
		"Title": "Hierarchical Multimodal Fusion of Deep-Learned Lesion and Tissue Integrity Features in Brain MRIs for Distinguishing Neuromyelitis Optica from Multiple Sclerosis",
		"Description": "Youngjin Yoo, Lisa Y. W. Tang, Su-Hyun Kim, Ho Jin Kim, Lisa Eunyoung Lee, David K. B. Li, Shannon Kolind, Anthony Traboulsee, Roger Tam.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Neuromyelitis optica spectrum disorder (NMOSD) is a disease of the central nervous system that is often misdiagnosed as multiple sclerosis (MS) because they share similar clinical and radiological characteristics. Two key pathological signs of NMOSD and MS that are detectable on magnetic resonance imaging (MRI) are white matter lesions and alterations in tissue integrity as measured by fractional anisotropy (FA) values on diffusion tensor images (DTIs). This paper proposes a multimodal deep learning model that discovers latent features in brain lesion masks and DTIs for distinguishing NMOSD from MS. The main technical challenge is to optimally extract and integrate features from two very heterogeneous image types (lesion masks and FA maps). Our solution is to first build two modality-specific pathways, each designed to accommodate the expected feature density and scale, then integrate them into a hierarchical multimodal fusion (HMF) model. The HMF model contains two multimodal fusion layers operating at two different scales, which in turn are joined by a multi-scale fusion layer. We hypothesize that the HMF approach would allow the automatic extraction of joint-features of heterogeneous image types to be optimized with greater efficiency and accuracy than the traditional multimodal approach of combining only the top-layer modality-specific features with a single fusion layer. The proposed model gives an average diagnostic accuracy of 81.3% (85.3% sensitivity and 75.0% specificity) on 82 NMOSD patients and 52 MS patients in a seven-fold cross-validation, which significantly outperforms the user-defined MRI features previously used in clinical studies, as well as deep-learned features using the conventional fusion approach.",
		"email": [
			"youngjin.yoo@alumni.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_55",
		"source": "miccai",
		"year": 2017,
		"key": "28dd6038-c7e6-4eba-aee0-e790dbbfee3c",
		"use": 1
	},
	{
		"Title": "Accurate Pulmonary Nodule Detection in Computed Tomography Images Using Deep Convolutional Neural Networks",
		"Description": "Jia Ding, Aoxue Li, Zhiqiang Hu, Liwei Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Early detection of pulmonary cancer is the most promising way to enhance a patients chance for survival. Accurate pulmonary nodule detection in computed tomography (CT) images is a crucial step in diagnosing pulmonary cancer. In this paper, inspired by the successful use of deep convolutional neural networks (DCNNs) in natural image recognition, we propose a novel pulmonary nodule detection approach based on DCNNs. We first introduce a deconvolutional structure to Faster Region-based Convolutional Neural Network (Faster R-CNN) for candidate detection on axial slices. Then, a three-dimensional DCNN is presented for the subsequent false positive reduction. Experimental results of the LUng Nodule Analysis 2016 (LUNA16) Challenge demonstrate the superior detection performance of the proposed approach on nodule detection (average FROC-score of 0.893, ranking the 1st place over all submitted results), which outperforms the best result on the leaderboard of the LUNA16 Challenge (average FROC-score of 0.864).",
		"email": [
			"wanglw@cis.pku.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_64",
		"source": "miccai",
		"year": 2017,
		"key": "27a3ea1e-0999-45dc-b999-fd28614c3eb2",
		"use": 1
	},
	{
		"Title": "Liver Lesion Detection Based on Two-Stage Saliency Model with Modified Sparse Autoencoder",
		"Description": "Yixuan Yuan, Max Q.-H. Meng, Wenjian Qin, Lei Xing.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Liver lesion detection is an important task for diagnosis and surgical planning of focal liver disease. The large numbers of images in routine liver CT studies, in addition to their high diversity in appearance, have been hurdles for detecting all lesions by visual inspection. Automated methods for lesion identification are desirable, but the results of current approaches are limited due to the diversity of the training sets and the extensive tuning of parameters. In this paper, we propose a novel saliency model for lesion detection in CT images. First, we segment the image into multi-scale patch sizes. Then, a two-stage saliency model is proposed to detect liver lesions. In the first stage, we calculate the gray level contrast saliency map based on a prior knowledge to reduce the influence of blood vessels in CT images. In the second stage, we propose a modified sparse autoencoder (SAE) with neighbourhood information to learn discriminative features directly from raw patch features and adopt Locality-constrained Linear Coding (LLC) method to encode the obtained discriminative features of each patch. Then the second saliency map is calculated based on feature uniqueness and spatial distribution of patches. Followed by an appropriate mapping fusion, the liver lesions can be detected well. With \\(7\\times 7\\) sized patches, a 120 visual word dictionary, and 14 feature dimension, our model achieved 90.81% accuracy for lesion detection.",
		"email": [
			"yxyuan@stanford.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_66",
		"source": "miccai",
		"year": 2017,
		"key": "f62e0770-3aa9-4e5a-9b23-47d008dcaa29",
		"use": 1
	},
	{
		"Title": "Deep Multiple Instance Hashing for Scalable Medical Image Retrieval",
		"Description": "Sailesh Conjeti, Magdalini Paschali, Amin Katouzian, Nassir Navab.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "In this paper, for the first time, we introduce a multiple instance (MI) deep hashing technique for learning discriminative hash codes with weak bag-level supervision suited for large-scale retrieval. We learn such hash codes by aggregating deeply learnt hierarchical representations across bag members through an MI pool layer. For better trainability and retrieval quality, we propose a two-pronged approach that includes robust optimization and training with an auxiliary single instance hashing arm which is down-regulated gradually. We pose retrieval for tumor assessment as an MI problem because tumors often coexist with benign masses and could exhibit complementary signatures when scanned from different anatomical views. Experimental validations demonstrate improved retrieval performance over the state-of-the-art methods.",
		"email": [
			"sailesh.conjeti@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_63",
		"source": "miccai",
		"year": 2017,
		"key": "fae533be-d502-45c3-a404-cc414306cbda",
		"use": 1
	},
	{
		"Title": "Direct Detection of Pixel-Level Myocardial Infarction Areas via a Deep-Learning Algorithm",
		"Description": "Chenchu Xu, Lei Xu, Zhifan Gao, Shen Zhao, Heye Zhang, Yanping Zhang, Xiuquan Du, Shu Zhao, Dhanjoo Ghista, Shuo Li.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Accurate detection of the myocardial infarction (MI) area is crucial for early diagnosis planning and follow-up management. In this study, we propose an end-to-end deep-learning algorithm framework (OF-RNN) to accurately detect the MI area at the pixel level. Our OF-RNN consists of three different function layers: the heart localization layers, which can accurately and automatically crop the region-of-interest (ROI) sequences, including the left ventricle, using the whole cardiac magnetic resonance image sequences; the motion statistical layers, which are used to build a time-series architecture to capture two types of motion features (at the pixel-level) by integrating the local motion features generated by long short-term memory-recurrent neural networks and the global motion features generated by deep optical flows from the whole ROI sequence, which can effectively characterize myocardial physiologic function; and the fully connected discriminate layers, which use stacked auto-encoders to further learn these features, and they use a softmax classifier to build the correspondences from the motion features to the tissue identities (infarction or not) for each pixel. Through the seamless connection of each layer, our OF-RNN can obtain the area, position, and shape of the MI for each patient. Our proposed framework yielded an overall classification accuracy of 94.35% at the pixel level, from 114 clinical subjects. These results indicate the potential of our proposed method in aiding standardized MI assessments.",
		"email": [
			"hy.zhang@siat.ac.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_28",
		"source": "miccai",
		"year": 2017,
		"key": "7df387fb-fb35-43a1-b6f3-f88cfb00d6ed",
		"use": 1
	},
	{
		"Title": "Skin Disease Recognition Using Deep Saliency Features and Multimodal Learning of Dermoscopy and Clinical Images",
		"Description": "Zongyuan Ge, Sergey Demyanov, Rajib Chakravorty, Adrian Bowling, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Skin cancer is the most common cancer world-wide, among which Melanoma the most fatal cancer, accounts for more than 10,000 deaths annually in Australia and United States. The 5-year survival rate for Melanoma can be increased over 90% if detected in its early stage. However, intrinsic visual similarity across various skin conditions makes the diagnosis challenging both for clinicians and automated classification methods. Many automated skin cancer diagnostic systems have been proposed in literature, all of which consider solely dermoscopy images in their analysis. In reality, however, clinicians consider two modalities of imaging; an initial screening using clinical photography images to capture a macro view of the mole, followed by dermoscopy imaging which visualizes morphological structures within the skin lesion. Evidences show that these two modalities provide complementary visual features that can empower the decision making process. In this work, we propose a novel deep convolutional neural network (DCNN) architecture along with a saliency feature descriptor to capture discriminative features of the two modalities for skin lesions classification. The proposed DCNN accepts a pair images of clinical and dermoscopic view of a single lesion and is capable of learning single-modality and cross-modality representations, simultaneously. Using one of the largest collected skin lesion datasets, we demonstrate that the proposed multi-modality method significantly outperforms single-modality methods on three tasks; differentiation between 15 various skin diseases, distinguishing cancerous (3 cancer types including melanoma) from non-cancerous moles, and detecting melanoma from benign cases.",
		"email": [
			"zongyuan@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_29",
		"source": "miccai",
		"year": 2017,
		"key": "087bdfb5-b8e9-4dd7-8c90-704ec704213a",
		"use": 1
	},
	{
		"Title": "Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans",
		"Description": "Yuyin Zhou, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Automatic segmentation of an organ and its cystic region is a prerequisite of computer-aided diagnosis. In this paper, we focus on pancreatic cyst segmentation in abdominal CT scan. This task is important and very useful in clinical practice yet challenging due to the low contrast in boundary, the variability in location, shape and the different stages of the pancreatic cancer. Inspired by the high relevance between the location of a pancreas and its cystic region, we introduce extra deep supervision into the segmentation network, so that cyst segmentation can be improved with the help of relatively easier pancreas segmentation. Under a reasonable transformation function, our approach can be factorized into two stages, and each stage can be efficiently optimized via gradient back-propagation throughout the deep networks. We collect a new dataset with 131 pathological samples, which, to the best of our knowledge, is the largest set for pancreatic cyst segmentation. Without human assistance, our approach reports a \\(63.44\\%\\) average accuracy, measured by the Dice-Srensen coefficient (DSC), which is higher than the number (\\(60.46\\%\\)) without deep supervision.",
		"email": [
			"198808xc@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_26",
		"source": "miccai",
		"year": 2017,
		"key": "0dbb33e7-2532-499d-a7d7-de819a78611d",
		"use": 1
	},
	{
		"Title": "Error Corrective Boosting for Learning Fully Convolutional Networks with Limited Data",
		"Description": "Abhijit Guha Roy, Sailesh Conjeti, Debdoot Sheet, Amin Katouzian, Nassir Navab, Christian Wachinger.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Training deep fully convolutional neural networks (F-CNNs) for semantic image segmentation requires access to abundant labeled data. While large datasets of unlabeled image data are available in medical applications, access to manually labeled data is very limited. We propose to automatically create auxiliary labels on initially unlabeled data with existing tools and to use them for pre-training. For the subsequent fine-tuning of the network with manually labeled data, we introduce error corrective boosting (ECB), which emphasizes parameter updates on classes with lower accuracy. Furthermore, we introduce SkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that combines skip connections with the unpooling strategy for upsampling. The SD-Net addresses challenges of severe class imbalance and errors along boundaries. With application to whole-brain MRI T1 scan segmentation, we generate auxiliary labels on a large dataset with FreeSurfer and fine-tune on two datasets with manual annotations. Our results show that the inclusion of auxiliary labels and ECB yields significant improvements. SD-Net segments a 3D scan in 7 s in comparison to 30 h for the closest multi-atlas segmentation method, while reaching similar performance. It also outperforms the latest state-of-the-art F-CNN models.",
		"email": [
			"abhijit.guha-roy@tum.de"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_27",
		"source": "miccai",
		"year": 2017,
		"key": "113ea8ac-eb50-437b-a906-3d10f55ef3f7",
		"use": 1
	},
	{
		"Title": "Sparse Multi-kernel Based Multi-task Learning for Joint Prediction of Clinical Scores and Biomarker Identification in Alzheimers Disease",
		"Description": "Peng Cao, Xiaoli Liu, Jinzhu Yang, Dazhe Zhao, Osmar Zaiane.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Machine learning methods have been used to predict the clinical scores and identify the image biomarkers from individual MRI scans. Recently, the multi-task learning (MTL) with sparsity-inducing norm have been widely studied to investigate the prediction power of neuroimaging measures by incorporating inherent correlations among multiple clinical cognitive measures. However, most of the existing MTL algorithms are formulated linear sparse models, in which the response (e.g., cognitive score) is a linear function of predictors (e.g., neuroimaging measures). To exploit the nonlinear relationship between the neuroimaging measures and cognitive measures, we consider that tasks to be learned share a common subset of features in the kernel space as well as the kernel functions. Specifically, we propose a multi-kernel based multi-task learning with a mixed sparsity-inducing norm to better capture the complex relationship between the cognitive scores and the neuroimaging measures. The formation can be efficiently solved by mirror-descent optimization. Experiments on the Alzheimers Disease Neuroimaging Initiative (ADNI) database showed that the proposed algorithm achieved better prediction performance than state-of-the-art linear based methods both on single MRI and multiple modalities.",
		"email": [
			"caopeng@cse.neu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_23",
		"source": "miccai",
		"year": 2017,
		"key": "89b88d38-a064-4ef6-90f6-ddb7c7ab905b",
		"use": 1
	},
	{
		"Title": "Hybrid Mass Detection in Breast MRI Combining Unsupervised Saliency Analysis and Deep Learning",
		"Description": "Guy Amit, Omer Hadad, Sharon Alpert, Tal Tlusty, Yaniv Gur, Rami Ben-Ari, Sharbell Hashoul.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "\nTo interpret a breast MRI study, a radiologist has to examine over 1000 images, and integrate spatial and temporal information from multiple sequences. The automated detection and classification of suspicious lesions can help reduce the workload and improve accuracy. We describe a hybrid mass-detection algorithm that combines unsupervised candidate detection with deep learning-based classification. The detection algorithm first identifies image-salient regions, as well as regions that are cross-salient with respect to the contralateral breast image. We then use a convolutional neural network (CNN) to classify the detected candidates into true-positive and false-positive masses. The network uses a novel multi-channel image representation; this representation encompasses information from the anatomical and kinetic image features, as well as saliency maps. We evaluated our algorithm on a dataset of MRI studies from 171 patients, with 1957 annotated slices of malignant (59%) and benign (41%) masses. Unsupervised saliency-based detection provided a sensitivity of 0.96 with 9.7 false-positive detections per slice. Combined with CNN classification, the number of false positive detections dropped to 0.7 per slice, with 0.85 sensitivity. The multi-channel representation achieved higher classification performance compared to single-channel images. The combination of domain-specific unsupervised methods and general-purpose supervised learning offers advantages for medical imaging applications, and may improve the ability of automated algorithms to assist radiologists.",
		"email": [
			"guyam@il.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_68",
		"source": "miccai",
		"year": 2017,
		"key": "4ecc903a-b187-442a-a983-9895522960dc",
		"use": 1
	},
	{
		"Title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification",
		"Description": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations. (Code: https://github.com/wentaozhu/deep-mil-for-whole-mammogram-classification.git).",
		"email": [
			"wentaoz1@ics.uci.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_69",
		"source": "miccai",
		"year": 2017,
		"key": "f8a87eba-69bb-4d25-a975-ad1d19e3b786",
		"use": 1
	},
	{
		"Title": "Retinal Microaneurysm Detection Using Clinical Report Guided Multi-sieving CNN",
		"Description": "Ling Dai, Bin Sheng, Qiang Wu, Huating Li, Xuhong Hou, Weiping Jia, Ruogu Fang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Timely detection and treatment of microaneurysms (MA) is a critical step to prevent the development of vision-threatening eye diseases such as diabetic retinopathy. However, detecting MAs in fundus images is a highly challenging task due to the large variation of imaging conditions. In this paper, we focus on developing an interleaved deep mining technique to cope intelligently with the unbalanced MA detection problem. Specifically, we present a clinical report guided multi-sieving convolutional neural network (MS-CNN) which leverages a small amount of supervised information in clinical reports to identify the potential MA regions via a text-to-image mapping in the feature space. These potential MA regions are then interleaved with the fundus image information for multi-sieving deep mining in a highly unbalanced classification problem. Critically, the clinical reports are employed to bridge the semantic gap between low-level image features and high-level diagnostic information. Extensive evaluations show our framework achieves 99.7% precision and 87.8% recall, comparing favorably with the state-of-the-art algorithms. Integration of expert domain knowledge and image information demonstrates the feasibility to reduce the training difficulty of the classifiers under extremely unbalanced data distribution.",
		"email": [
			"shengbin@sjtu.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_60",
		"source": "miccai",
		"year": 2017,
		"key": "3a1c653c-f6b2-4506-a4dc-9fe4e989ecba",
		"use": 1
	},
	{
		"Title": "Medical Image Synthesis with Context-Aware Generative Adversarial Networks",
		"Description": "Dong Nie, Roger Trullo, Jun Lian, Caroline Petitjean, Su Ruan, Qian Wang, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Computed tomography (CT) is critical for various clinical applications, e.g., radiation treatment planning and also PET attenuation correction in MRI/PET scanner. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve radiations. Therefore, recently researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiation planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network (FCN) to generate CT given the MR image. To better model the nonlinear mapping from MRI to CT and produce more realistic images, we propose to use the adversarial training strategy to train the FCN. Moreover, we propose an image-gradient-difference based loss function to alleviate the blurriness of the generated CT. We further apply Auto-Context Model (ACM) to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MR images, and also outperforms three state-of-the-art methods under comparison.",
		"email": [
			"dgshen@med.unc.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_48",
		"source": "miccai",
		"year": 2017,
		"key": "a398a485-d15e-4f7d-b9f5-b927531f4bcb",
		"use": 1
	},
	{
		"Title": "Joint Detection and Diagnosis of Prostate Cancer in Multi-parametric MRI Based on Multimodal Convolutional Neural Networks",
		"Description": "Xin Yang, Zhiwei Wang, Chaoyue Liu, Hung Minh Le, Jingyu Chen, Kwang-Ting (Tim) Cheng, Liang Wang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "This paper presents an automated method for jointly localizing prostate cancer (PCa) in multi-parametric MRI (mp-MRI) images and assessing the aggressiveness of detected lesions. Our method employs multimodal multi-label convolutional neural networks (CNNs), which are trained in a weakly-supervised manner by providing a set of prostate images with image-level labels without priors of lesions locations. By distinguishing images with different labels, discriminative visual patterns related to indolent PCa and clinically significant (CS) PCa are automatically learned from clutters of prostate tissues. Cancer response maps (CRMs) with each pixel indicating the likelihood of being part of indolent/CS are explicitly generated at the last convolutional layer. We define new back-propagate error of CNN to enforce both optimized classification results and consistent CRMs for different modalities. Our method enables the feature learning processes of different modalities to mutually influence each other and, in turn yield more representative features. Comprehensive evaluation based on 402 lesions demonstrates superior performance of our method to the state-of-the-art method [13].",
		"email": [
			"xinyang2014@hust.edu.cn"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_49",
		"source": "miccai",
		"year": 2017,
		"key": "924756d4-cbb9-4f24-8b9a-8860f64241d4",
		"use": 1
	},
	{
		"Title": "Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation",
		"Description": "Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Image segmentation is a fundamental problem in biomedical image analysis. Recent advances in deep learning have achieved promising results on many biomedical image segmentation benchmarks. However, due to large variations in biomedical images (different modalities, image settings, objects, noise, etc.), to utilize deep learning on a new application, it usually needs a new set of training data. This can incur a great deal of annotation effort and cost, because only biomedical experts can annotate effectively, and often there are too many instances in images (e.g., cells) to annotate. In this paper, we aim to address the following question: With limited effort (e.g., time) for annotation, what instances should be annotated in order to attain the best performance? We present a deep active learning framework that combines fully convolutional network (FCN) and active learning to significantly reduce annotation effort by making judicious suggestions on the most effective annotation areas. We utilize uncertainty and similarity information provided by FCN and formulate a generalized version of the maximum set cover problem to determine the most representative and uncertain areas for annotation. Extensive experiments using the 2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image segmentation dataset show that, using annotation suggestions by our method, state-of-the-art segmentation performance can be achieved by using only 50% of training data.",
		"email": [
			"lyang5@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_46",
		"source": "miccai",
		"year": 2017,
		"key": "5b72a1c2-cd09-40b3-b308-6afc6b540e39",
		"use": 1
	},
	{
		"Title": "Deep Adversarial Networks for Biomedical Image Segmentation Utilizing Unannotated Images",
		"Description": "Yizhe Zhang, Lin Yang, Jianxu Chen, Maridel Fredericksen, David P. Hughes, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Semantic segmentation is a fundamental problem in biomedical image analysis. In biomedical practice, it is often the case that only limited annotated data are available for model training. Unannotated images, on the other hand, are easier to acquire. How to utilize unannotated images for training effective segmentation models is an important issue. In this paper, we propose a new deep adversarial network (DAN) model for biomedical image segmentation, aiming to attain consistently good segmentation results on both annotated and unannotated images. Our model consists of two networks: (1) a segmentation network (SN) to conduct segmentation; (2) an evaluation network (EN) to assess segmentation quality. During training, EN is encouraged to distinguish between segmentation results of unannotated images and annotated ones (by giving them different scores), while SN is encouraged to produce segmentation results of unannotated images such that EN cannot distinguish these from the annotated ones. Through an iterative adversarial training process, because EN is constantly criticizing the segmentation results of unannotated images, SN can be trained to produce more and more accurate segmentation for unannotated and unseen samples. Experiments show that our proposed DAN model is effective in utilizing unannotated image data to obtain considerably better segmentation.",
		"email": [
			"yzhang29@nd.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_47",
		"source": "miccai",
		"year": 2017,
		"key": "8ec36810-25e5-4c15-8fb7-1995fcadb418",
		"use": 1
	},
	{
		"Title": "Image Super Resolution Using Generative Adversarial Networks and Local Saliency Maps for Retinal Image Analysis",
		"Description": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Sajini Hewavitharanage, Rahil Garnavi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "We propose an image super resolution (ISR) method using generative adversarial networks (GANs) that takes a low resolution input fundus image and generates a high resolution super resolved (SR) image upto scaling factor of 16. This facilitates more accurate automated image analysis, especially for small or blurred landmarks and pathologies. Local saliency maps, which define each pixels importance, are used to define a novel saliency loss in the GAN cost function. Experimental results show the resulting SR images have perceptual quality very close to the original images and perform better than competing methods that do not weigh pixels according to their importance. When used for retinal vasculature segmentation, our SR images result in accuracy levels close to those obtained when using the original images.",
		"email": [
			"dwarim@au1.ibm.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_44",
		"source": "miccai",
		"year": 2017,
		"key": "e6de6d4e-f234-4585-90da-b98d3c8cb79b",
		"use": 1
	},
	{
		"Title": "Clinical Target-Volume Delineation in Prostate Brachytherapy Using Residual Neural Networks",
		"Description": "Emran Mohammad Abu Anas, Saman Nouranian, S. Sara Mahdavi, Ingrid Spadinger, William J. Morris, Septimu E. Salcudean, Parvin Mousavi, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Low dose-rate prostate brachytherapy is commonly used to treat early stage prostate cancer. This intervention involves implanting radioactive seeds inside a volume containing the prostate. Planning the intervention requires obtaining a series of ultrasound images from the prostate. This is followed by delineation of a clinical target volume, which mostly traces the prostate boundary in the ultrasound data, but can be modified based on institution-specific clinical guidelines. Here, we aim to automate the delineation of clinical target volume by using a new deep learning network based on residual neural nets and dilated convolution at deeper layers. In addition, we propose to include an exponential weight map in the optimization to improve local prediction. We train the network on 4,284 expert-labeled transrectal ultrasound images and test it on an independent set of 1,081 ultrasound images. With respect to the gold-standard delineation, we achieve a mean Dice similarity coefficient of 94%, a mean surface distance error of 1.05 mm and a mean Hausdorff distance error of 3.0 mm. The obtained results are statistically significantly better than two previous state-of-the-art techniques.",
		"email": [
			"emrana@ece.ubc.ca"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_42",
		"source": "miccai",
		"year": 2017,
		"key": "8b38fb51-6848-4097-9e1c-cc8fa674a754",
		"use": 1
	},
	{
		"Title": "Using Convolutional Neural Networks to Automatically Detect Eye-Blink Artifacts in Magnetoencephalography Without Resorting to Electrooculography",
		"Description": "Prabhat Garg, Elizabeth Davenport, Gowtham Murugesan, Ben Wagner, Christopher Whitlow, Joseph Maldjian, Albert Montillo.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "\nMagnetoencephelography (MEG) is a functional neuroimaging tool that records the magnetic fields induced by neuronal activity; however, signal from muscle activity often corrupts the data. Eye-blinks are one of the most common types of muscle artifact. They can be recorded by affixing eye proximal electrodes, as in electrooculography (EOG), however this complicates patient preparation and decreases comfort. Moreover, it can induce further muscular artifacts from facial twitching. We propose an EOG free, data driven approach. We begin with Independent Component Analysis (ICA), a well-known preprocessing approach that factors observed signal into statistically independent components. When applied to MEG, ICA can help separate neuronal components from non-neuronal ones, however, the components are randomly ordered. Thus, we develop a method to assign one of two labels, non-eye-blink or eye-blink, to each component.",
		"email": [
			"Albert.Montillo@UTSouthwestern.edu"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_43",
		"source": "miccai",
		"year": 2017,
		"key": "42544e51-8abd-46aa-b222-f6d09d574606",
		"use": 1
	},
	{
		"Title": "Segmentation of Intracranial Arterial Calcification with Deeply Supervised Residual Dropout Networks",
		"Description": "Gerda Bortsova, Gijs van Tulder, Florian Dubost, Tingying Peng, Nassir Navab, Aad van der Lugt, Daniel Bos, Marleen De Bruijne.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2017",
		"abstract": "Intracranial carotid artery calcification (ICAC) is a major risk factor for stroke, and might contribute to dementia and cognitive decline. Reliance on time-consuming manual annotation of ICAC hampers much demanded further research into the relationship between ICAC and neurological diseases. Automation of ICAC segmentation is therefore highly desirable, but difficult due to the proximity of the lesions to bony structures with a similar attenuation coefficient. In this paper, we propose a method for automatic segmentation of ICAC; the first to our knowledge. Our method is based on a 3D fully convolutional neural network that we extend with two regularization techniques. Firstly, we use deep supervision to encourage discriminative features in the hidden layers. Secondly, we augment the network with skip connections, as in the recently developed ResNet, and dropout layers, inserted in a way that skip connections circumvent them. We investigate the effect of skip connections and dropout. In addition, we propose a simple problem-specific modification of the network objective function that restricts the focus to the most important image regions and simplifies the optimization. We train and validate our model using 882 CT scans and test on 1,000. Our regularization techniques and objective improve the average Dice score by 7.1%, yielding an average Dice of 76.2% and 97.7% correlation between predicted ICAC volumes and manual annotations.",
		"email": [
			"gerdabortsova@gmail.com"
		],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-66179-7_41",
		"source": "miccai",
		"year": 2017,
		"key": "4f2409fc-ce04-4636-8803-d51ad1b4fe14",
		"use": 1
	},
	{
		"Title": "Modeling the Variability in Brain Morphology and Lesion Distribution in Multiple Sclerosis by Deep Learning",
		"Description": "Tom Brosch, Youngjin Yoo, David K. B. Li, Anthony Traboulsee, Roger Tam.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2014",
		"abstract": "Changes in brain morphology and white matter lesions are two hallmarks of multiple sclerosis (MS) pathology, but their variability beyond volumetrics is poorly characterized. To further our understanding of complex MS pathology, we aim to build a statistical model of brain images that can automatically discover spatial patterns of variability in brain morphology and lesion distribution. We propose building such a model using a deep belief network (DBN), a layered network whose parameters can be learned from training images. In contrast to other manifold learning algorithms, the DBN approach does not require a prebuilt proximity graph, which is particularly advantageous for modeling lesions, because their sparse and random nature makes defining a suitable distance measure between lesion images challenging. Our model consists of a morphology DBN, a lesion DBN, and a joint DBN that models concurring morphological and lesion patterns. Our results show that this model can automatically discover the classic patterns of MS pathology, as well as more subtle ones, and that the parameters computed have strong relationships to MS clinical scores.",
		"email": [],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-10470-6_58",
		"source": "miccai",
		"year": 2014,
		"key": "1f5bc38c-6b27-4ab5-8f36-af5cf8ff36ae",
		"use": 1
	},
	{
		"Title": "Segmenting Hippocampus from Infant Brains by Sparse Patch Matching with Deep-Learned Features",
		"Description": "Yanrong Guo, Guorong Wu, Leah A. Commander, Stephanie Szary, Valerie Jewells, Weili Lin, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2014",
		"abstract": "Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1- and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the inter-patch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods.",
		"email": [],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-10470-6_39",
		"source": "miccai",
		"year": 2014,
		"key": "a7deaf1d-ec1c-4260-b94d-ba834d712803",
		"use": 1
	},
	{
		"Title": "Deep Learning Based Imaging Data Completion for Improved Brain Disease Diagnosis",
		"Description": "Rongjian Li, Wenlu Zhang, Heung-Il Suk, Li Wang, Jiang Li, Dinggang Shen, Shuiwang Ji.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2014",
		"abstract": "Combining multi-modality brain data for disease diagnosis commonly leads to improved performance. A challenge in using multi-modality data is that the data are commonly incomplete; namely, some modality might be missing for some subjects. In this work, we proposed a deep learning based framework for estimating multi-modality imaging data. Our method takes the form of convolutional neural networks, where the input and output are two volumetric modalities. The network contains a large number of trainable parameters that capture the relationship between input and output modalities. When trained on subjects with all modalities, the network can estimate the output modality given the input modality. We evaluated our method on the Alzheimers Disease Neuroimaging Initiative (ADNI) database, where the input and output modalities are MRI and PET images, respectively. Results showed that our method significantly outperformed prior methods.",
		"email": [],
		"fullURL": "http://dx.doi.org/10.1007/978-3-319-10443-0_39",
		"source": "miccai",
		"year": 2014,
		"key": "0ecbc8a2-50d1-421e-9e38-5002ccdbdcf9",
		"use": 1
	},
	{
		"Title": "q-Space Deep Learning for Twelve-Fold Shorter and Model-Free Diffusion MRI Scans",
		"Description": "Vladimir Golkov, Alexey Dosovitskiy, Philipp Smann, Jonathan I. Sperl, Tim Sprenger, Michael Czisch, Marion I. Menzel, Pedro A. Gmez, Axel Haase, Thomas Brox, Daniel Cremers.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Diffusion MRI uses a multi-step data processing pipeline. With certain steps being prone to instabilities, the pipeline relies on considerable amounts of partly redundant input data, which requires long acquisition time. This leads to high scan costs and makes advanced diffusion models such as diffusion kurtosis imaging (DKI) and neurite orientation dispersion and density imaging (NODDI) inapplicable for children and adults who are uncooperative, uncomfortable or unwell. We demonstrate how deep learning, a group of algorithms in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This method allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models.",
		"email": [
			"golkov@cs.tum.edu"
		],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_5",
		"source": "miccai",
		"year": 2015,
		"key": "8266a579-87a8-4480-a7ed-5896b2b6732a",
		"use": 1
	},
	{
		"Title": "Automatic Localization and Identification of Vertebrae in Spine CT via a Joint Learning Model with Deep Neural Networks",
		"Description": "Hao Chen, Chiyao Shen, Jing Qin, Dong Ni, Lin Shi, Jack C. Y. Cheng, Pheng-Ann Heng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Accurate localization and identification of vertebrae in 3D spinal images is essential for many clinical tasks. However, automatic localization and identification of vertebrae remains challenging due to similar appearance of vertebrae, abnormal pathological curvatures and image artifacts induced by surgical implants. Traditional methods relying on hand-crafted low level features and/or a priori knowledge usually fail to overcome these challenges on arbitrary CT scans. We present a robust and efficient approach to automatically locating and identifying vertebrae in 3D CT volumes by exploiting high level feature representations with deep convolutional neural network (CNN). A novel joint learning model with CNN (J-CNN) is proposed by considering both the appearance of vertebrae and the pairwise conditional dependency of neighboring vertebrae. The J-CNN can effectively identify the type of vertebra and eliminate false detections based on a set of coarse vertebral centroids generated from a random forest classifier. Furthermore, the predicted centroids are refined by a shape regression model. Our approach was quantitatively evaluated on the dataset of MICCAI 2014 Computational Challenge on Vertebrae Localization and Identification. Compared with the state-of-the-art method [1], our approach achieved a large margin with 10.12% improvement of the identification rate and smaller localization errors.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_63",
		"source": "miccai",
		"year": 2015,
		"key": "1146e236-a2a7-4478-84c6-ea539431f0a4",
		"use": 1
	},
	{
		"Title": "3D Deep Learning for Efficient and Robust Landmark Detection in Volumetric Data",
		"Description": "Yefeng Zheng, David Liu, Bogdan Georgescu, Hien Nguyen, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Recently, deep learning has demonstrated great success in computer vision with the capability to learn powerful image features from a large training set. However, most of the published work has been confined to solving 2D problems, with a few limited exceptions that treated the 3D space as a composition of 2D orthogonal planes. The challenge of 3D deep learning is due to a much larger input vector, compared to 2D, which dramatically increases the computation time and the chance of over-fitting, especially when combined with limited training samples (hundreds to thousands), typical for medical imaging applications. To address this challenge, we propose an efficient and robust deep learning algorithm capable of full 3D detection in volumetric data. A two-step approach is exploited for efficient detection. A shallow network (with one hidden layer) is used for the initial testing of all voxels to obtain a small number of promising candidates, followed by more accurate classification with a deep network. In addition, we propose two approaches, i.e., separable filter decomposition and network sparsification, to speed up the evaluation of a network. To mitigate the over-fitting issue, thereby increasing detection robustness, we extract small 3D patches from a multi-resolution image pyramid. The deeply learned image features are further combined with Haar wavelet features to increase the detection accuracy. The proposed method has been quantitatively evaluated for carotid artery bifurcation detection on a head-neck CT dataset from 455 patients. Compared to the state-of-the-art, the mean error is reduced by more than half, from 5.97 mm to 2.64 mm, with a detection speed of less than 1 s/volume.",
		"email": [
			"yefeng.zheng@siemens.com"
		],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_69",
		"source": "miccai",
		"year": 2015,
		"key": "155a1da2-e0a4-496d-bb74-83a291dfeb89",
		"use": 1
	},
	{
		"Title": "A Hybrid of Deep Network and Hidden Markov Model for MCI Identification with Resting-State fMRI",
		"Description": "Heung-Il Suk, Seong-Whan Lee, Dinggang Shen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "In this paper, we propose a novel method for modelling functional dynamics in resting-state fMRI (rs-fMRI) for Mild Cognitive Impairment (MCI) identification. Specifically, we devise a hybrid architecture by combining Deep Auto-Encoder (DAE) and Hidden Markov Model (HMM). The roles of DAE and HMM are, respectively, to discover hierarchical non-linear relations among features, by which we transform the original features into a lower dimension space, and to model dynamic characteristics inherent in rs-fMRI, i.e., internal state changes. By building a generative model with HMMs for each class individually, we estimate the data likelihood of a test subject as MCI or normal healthy control, based on which we identify the clinical label. In our experiments, we achieved the maximal accuracy of 81.08% with the proposed method, outperforming state-of-the-art methods in the literature.",
		"email": [
			"hisuk@korea.ac.kr"
		],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_70",
		"source": "miccai",
		"year": 2015,
		"key": "652324b6-34ae-4c61-92ff-47e6589fb5be",
		"use": 1
	},
	{
		"Title": "Deep Learning and Structured Prediction for the Segmentation of Mass in Mammograms",
		"Description": "Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "In this paper, we explore the use of deep convolution and deep belief networks as potential functions in structured prediction models for the segmentation of breast masses from mammograms. In particular, the structured prediction models are estimated with loss minimization parameter learning algorithms, representing: a) conditional random field (CRF), and b) structured support vector machine (SSVM). For the CRF model, we use the inference algorithm based on tree re-weighted belief propagation with truncated fitting training, and for the SSVM model the inference is based on graph cuts with maximum margin training. We show empirically the importance of deep learning methods in producing state-of-the-art results for both structured prediction models. In addition, we show that our methods produce results that can be considered the best results to date on DDSM-BCRP and INbreast databases. Finally, we show that the CRF model is significantly faster than SSVM, both in terms of inference and training time, which suggests an advantage of CRF models when combined with deep learning potential functions.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_74",
		"source": "miccai",
		"year": 2015,
		"key": "86ac47a6-9ef7-489b-b19b-54c34366de04",
		"use": 1
	},
	{
		"Title": "Cross-Domain Synthesis of Medical Images Using Efficient Location-Sensitive Deep Network",
		"Description": "Hien Van Nguyen, Kevin Zhou, Raviteja Vemulapalli.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Cross-modality image synthesis has recently gained significant interest in the medical imaging community. In this paper, we propose a novel architecture called location-sensitive deep network (LSDN) for synthesizing images across domains. Our network integrates intensity feature from image voxels and spatial information in a principled manner. Specifically, LSDN models hidden nodes as products of features and spatial responses. We then propose a novel method, called ShrinkConnect, for reducing the computations of LSDN without sacrificing synthesis accuracy. ShrinkConnect enforces simultaneous sparsity to find a compact set of functions that accurately approximates the responses of all hidden nodes. Experimental results demonstrate that LSDN+ShrinkConnect outperforms the state of the art in cross-domain synthesis of MRI brain scans by a significant margin. Our approach is also computationally efficient, e.g. 26 faster than other sparse representation based methods.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_83",
		"source": "miccai",
		"year": 2015,
		"key": "a21da679-2f84-479f-bd7c-4c3d3543a3ef",
		"use": 1
	},
	{
		"Title": "Marginal Space Deep Learning: Efficient Architecture for Detection in Volumetric Image Data",
		"Description": "Florin C. Ghesu, Bogdan Georgescu, Yefeng Zheng, Joachim Hornegger, Dorin Comaniciu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Current state-of-the-art techniques for fast and robust parsing of volumetric medical image data exploit large annotated image databases and are typically based on machine learning methods. Two main challenges to be solved are the low efficiency in scanning large volumetric input images and the need for manual engineering of image features. This work proposes Marginal Space Deep Learning (MSDL) as an effective solution, that combines the strengths of efficient object parametrization in hierarchical marginal spaces with the automated feature design of Deep Learning (DL) network architectures. Representation learning through DL automatically identifies, disentangles and learns explanatory factors directly from low-level image data. However, the direct application of DL to volumetric data results in a very high complexity, due to the increased number of transformation parameters. For example, the number of parameters defining a similarity transformation increases to 9 in 3D (3 for location, 3 for orientation and 3 for scale). The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in high probability regions in spaces of gradually increasing dimensionality, for example starting from location only (3D) to location and orientation (6D) and full parameter space (9D). In addition, for parametrized feature computation, we propose to simplify the network by replacing the standard, pre-determined feature sampling pattern with a sparse, adaptive, self-learned pattern. The MSDL framework is evaluated on detecting the aortic heart valve in 3D ultrasound data. The dataset contains 3795 volumes from 150 patients. Our method outperforms the state-of-the-art with an improvement of 36 than one second. To our knowledge this is the first successful demonstration of the DL potential to detection in full 3D data with parametrized representations.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_87",
		"source": "miccai",
		"year": 2015,
		"key": "db069323-264d-4cfe-a896-41a9faebeee6",
		"use": 1
	},
	{
		"Title": "Detection of Glands and Villi by Collaboration of Domain Knowledge and Deep Learning",
		"Description": "Jiazhuo Wang, John D. MacKenzie, Rageshree Ramachandran, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Architecture distortions of glands and villi are indication of chronic inflammation. However, the duality nature of these two structures causes lots of ambiguity for their detection in H&E histology tissue images, especially when multiple instances are clustered together. Based on the observation that once such an object is detected for certain, the ambiguity in the neighborhood of the detected object can be reduced considerably, we propose to combine deep learning and domain knowledge in a unified framework, to simultaneously detect (the closely related) glands and villi in H&E histology tissue images. Our method iterates between exploring domain knowledge and performing deep learning classification, and the two components benefit from each other. (1) By exploring domain knowledge, the generated object proposals (to be fed to deep learning) form a more complete coverage of the true objects and the segmentation of object proposals can be more accurate, thus improving deep learnings performance on classification. (2) Deep learning can help verify the class of each object proposal, and provide feedback to repeatedly refresh and enhance domain knowledge so that more reliable object proposals can be generated later on. Experiments on clinical data validate our ideas and show that our method improves the state-of-the-art for gland detection in H&E histology tissue images (to our best knowledge, we are not aware of any method for villi detection).",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24571-3_3",
		"source": "miccai",
		"year": 2015,
		"key": "f54f7ef7-7d9b-4add-be84-bbcca8a541a5",
		"use": 1
	},
	{
		"Title": "Ultrasound-Based Detection of Prostate Cancer Using Automatic Feature Selection with Deep Belief Networks",
		"Description": "Shekoofeh Azizi, Farhad Imani, Bo Zhuang, Amir Tahmasebi, Jin Tae Kwak, Sheng Xu, Nishant Uniyal, Baris Turkbey, Peter Choyke, Peter Pinto, Bradford Wood, Mehdi Moradi, Parvin Mousavi, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "We propose an automatic feature selection framework for analyzing temporal ultrasound signals of prostate tissue. The framework consists of: 1) an unsupervised feature reduction step that uses Deep Belief Network (DBN) on spectral components of the temporal ultrasound data; 2) a supervised fine-tuning step that uses the histopathology of the tissue samples to further optimize the DBN; 3) a Support Vector Machine (SVM) classifier that uses the activation of the DBN as input and outputs a likelihood for the cancer. In leave-one-core-out cross-validation experiments using 35 biopsy cores, an area under the curve of 0.91 is obtained for cancer prediction. Subsequently, an independent group of 36 biopsy cores was used for validation of the model. The results show that the framework can predict 22 out of 23 benign, and all of cancerous cores correctly. We conclude that temporal analysis of ultrasound data can potentially complement multi-parametric Magnetic Resonance Imaging (mp-MRI) by improving the differentiation of benign and cancerous prostate tissue.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24571-3_9",
		"source": "miccai",
		"year": 2015,
		"key": "6d15c505-129d-4680-9c00-f41d840af6aa",
		"use": 1
	},
	{
		"Title": "Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation",
		"Description": "Tom Brosch, Youngjin Yoo, Lisa Y. W. Tang, David K. B. Li, Anthony Traboulsee, Roger Tam.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "We propose a novel segmentation approach based on deep convolutional encoder networks and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that has both convolutional and deconvolutional layers, and combines feature extraction and segmentation prediction in a single model. The joint training of the feature extraction and prediction layers allows the model to automatically learn features that are optimized for accuracy for any given combination of image types. In contrast to existing automatic feature learning approaches, which are typically patch-based, our model learns features from entire images, which eliminates patch selection and redundant calculations at the overlap of neighboring patches and thereby speeds up the training. Our network also uses a novel objective function that works well for segmenting underrepresented classes, such as MS lesions. We have evaluated our method on the publicly available labeled cases from the MS lesion segmentation challenge 2008 data set, showing that our method performs comparably to the state-of-theart. In addition, we have evaluated our method on the images of 500 subjects from an MS clinical trial and varied the number of training samples from 5 to 250 to show that the segmentation performance can be greatly improved by having a representative data set.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_1",
		"source": "miccai",
		"year": 2015,
		"key": "4f322ee2-b24a-4ab4-baed-99c6a7c4f5a0",
		"use": 1
	},
	{
		"Title": "Neutrophils Identification by Deep Learning and Voronoi Diagram of Clusters",
		"Description": "Jiazhuo Wang, John D. MacKenzie, Rageshree Ramachandran, Danny Z. Chen.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "Neutrophils are a primary type of immune cells, and their identification is critical in clinical diagnosis of active inflammation. However, in H&E histology tissue slides, the appearances of neutrophils are highly variable due to morphology, staining and locations. Further, the noisy and complex tissue environment causes artifacts resembling neutrophils. Thus, it is challenging to design, in a hand-crafted manner, computerized features that help identify neutrophils effectively. To better characterize neutrophils, we propose to extract their features in a learning manner, by constructing a deep convolutional neural network (CNN). In addition, in clinical practice, neutrophils are identified not only based on their individual appearance, but also on the context formed by multiple related cells. It is not quite straightforward for deep learning to capture precisely the rather complex cell context. Hence, we further propose to combine deep learning with Voronoi diagram of clusters (VDC), to extract needed context. Experiments on clinical data show that (1) the learned hierarchical representation of features by CNN outperforms hand-crafted features on characterizing neutrophils, and (2) the combination of CNN and VDC significantly improves over the state-of-the-art methods for neutrophil identification on H&E histology tissue images.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_27",
		"source": "miccai",
		"year": 2015,
		"key": "b17681c1-b132-4141-a5b1-eda440ca4a46",
		"use": 1
	},
	{
		"Title": "Unregistered Multiview Mammogram Analysis with Pre-trained Deep Learning Models",
		"Description": "Gustavo Carneiro, Jacinto Nascimento, Andrew P. Bradley.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "We show two important findings on the use of deep convolutional neural networks (CNN) in medical image analysis. First, we show that CNN models that are pre-trained using computer vision databases (e.g., Imagenet) are useful in medical image applications, despite the significant differences in image appearance. Second, we show that multiview classification is possible without the pre-registration of the input images. Rather, we use the high-level features produced by the CNNs trained in each view separately. Focusing on the classification of mammograms using craniocaudal (CC) and mediolateral oblique (MLO) views and their respective mass and micro-calcification segmentations of the same breast, we initially train a separate CNN model for each view and each segmentation map using an Imagenet pre-trained model. Then, using the features learned from each segmentation map and unregistered views, we train a final CNN classifier that estimates the patients risk of developing breast cancer using the Breast Imaging-Reporting and Data System (BI-RADS) score. We test our methodology in two publicly available datasets (InBreast and DDSM), containing hundreds of cases, and show that it produces a volume under ROC surface of over 0.9 and an area under ROC curve (for a 2-class problem - benign and malignant) of over 0.9. In general, our approach shows state-of-the-art classification results and demonstrates a new comprehensive way of addressing this challenging classification problem.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_78",
		"source": "miccai",
		"year": 2015,
		"key": "cd7a0718-ee72-4436-9fbd-7549aae72519",
		"use": 1
	},
	{
		"Title": "Automatic Feature Learning for Glaucoma Detection Based on Deep Learning",
		"Description": "Xiangyu Chen, Yanwu Xu, Shuicheng Yan, Damon Wing Kee Wong, Tien Yin Wong, Jiang Liu.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "Glaucoma is a chronic and irreversible eye disease in which the optic nerve is progressively damaged, leading to deterioration in vision and quality of life. In this paper, we present an Automatic feature Learning for glAucoma Detection based on Deep LearnINg (ALADDIN), with deep convolutional neural network (CNN) for feature learning. Different from the traditional convolutional layer that uses linear filters followed by a nonlinear activation function to scan the input, the adopted network embeds micro neural networks (multilayer perceptron) with more complex structures to abstract the data within the receptive field. Moreover, a contextualizing deep learning structure is proposed in order to obtain a hierarchical representation of fundus images to discriminate between glaucoma and non-glaucoma pattern, where the network takes the outputs from other CNN as the context information to boost the performance. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.838 and 0.898 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma diagnosis.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_80",
		"source": "miccai",
		"year": 2015,
		"key": "356cdaaa-be13-4e34-bf71-6f3938daeb7e",
		"use": 1
	},
	{
		"Title": "Fast Automatic Vertebrae Detection and Localization in Pathological CT Scans - A Deep Learning Approach",
		"Description": "Amin Suzani, Alexander Seitel, Yuan Liu, Sidney Fels, Robert N. Rohling, Purang Abolmaesumi.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "Automatic detection and localization of vertebrae in medical images are highly sought after techniques for computer-aided diagnosis systems of the spine. However, the presence of spine pathologies and surgical implants, and limited field-of-view of the spine anatomy in these images, make the development of these techniques challenging. This paper presents an automatic method for detection and localization of vertebrae in volumetric computed tomography (CT) scans. The method makes no assumptions about which section of the vertebral column is visible in the image. An efficient approach based on deep feed-forward neural networks is used to predict the location of each vertebra using its contextual information in the image. The method is evaluated on a public data set of 224 arbitrary-field-of-view CT scans of pathological cases and compared to two state-of-the-art methods. Our method can perform vertebrae detection at a rate of 96% with an overall run time of less than 3 seconds. Its fast and comparably accurate detection makes it appealing for clinical diagnosis and therapy applications.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_81",
		"source": "miccai",
		"year": 2015,
		"key": "1aa9f5cb-d6e8-43e5-9175-d76ad55e9a4a",
		"use": 1
	},
	{
		"Title": "Automatic Coronary Calcium Scoring in Cardiac CT Angiography Using Convolutional Neural Networks",
		"Description": "Jelmer M. Wolterink, Tim Leiner, Max A. Viergever, Ivana Igum.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "The amount of coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular events. Non-contrast enhanced cardiac CT is considered a reference for quantification of CAC. Recently, it has been shown that CAC may be quantified in cardiac CT angiography (CCTA). We present a pattern recognition method that automatically identifies and quantifies CAC in CCTA. The study included CCTA scans of 50 patients equally distributed over five cardiovascular risk categories. CAC in CCTA was identified in two stages. In the first stage, potential CAC voxels were identified using a convolutional neural network (CNN). In the second stage, candidate CAC lesions were extracted based on the CNN output for analyzed voxels and thereafter described with a set of features and classified using a Random Forest. Ten-fold stratified cross-validation experiments were performed. CAC volume was quantified per patient and compared with manual reference annotations in the CCTA scan. Bland-Altman bias and limits of agreement between reference and automatic annotations were -15 (-198168) after the first stage and -3 (-86  79) after the second stage. The results show that CAC can be automatically identified and quantified in CCTA using the proposed method. This might obviate the need for a dedicated non-contrast-enhanced CT scan for CAC scoring, which is regularly acquired prior to a CCTA scan, and thus reduce the CT radiation dose received by patients.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_72",
		"source": "miccai",
		"year": 2015,
		"key": "373e70f2-1a04-4db2-bb0d-7faab72278a3",
		"use": 1
	},
	{
		"Title": "Computer-Aided Pulmonary Embolism Detection Using a Novel Vessel-Aligned Multi-planar Image Representation and Convolutional Neural Networks",
		"Description": "Nima Tajbakhsh, Michael B. Gotway, Jianming Liang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Computer-aided detection (CAD) can play a major role in diagnosing pulmonary embolism (PE) at CT pulmonary angiography (CTPA). However, despite their demonstrated utility, to achieve a clinically acceptable sensitivity, existing PE CAD systems generate a high number of false positives, imposing extra burdens on radiologists to adjudicate these superfluous CAD findings. In this study, we investigate the feasibility of convolutional neural networks (CNNs) as an effective mechanism for eliminating false positives. A critical issue in successfully utilizing CNNs for detecting an object in 3D images is to develop a right image representation for the object. Toward this end, we have developed a vessel-aligned multi-planar image representation of emboli. Our image representation offers three advantages: (1) efficiency and compactnessconcisely summarizing the 3D contextual information around an embolus in only 2 image channels, (2) consistencyautomatically aligning the embolus in the 2-channel images according to the orientation of the affected vessel, and (3) expandabilitynaturally supporting data augmentation for training CNNs. We have evaluated our CAD approach using 121 CTPA datasets with a total of 326 emboli, achieving a sensitivity of 83% at 2 false positives per volume. This performance is superior to the best performing CAD system in the literature, which achieves a sensitivity of 71% at the same level of false positives. We have further evaluated our system using the entire 20 CTPA test datasets from the PE challenge. Our system outperforms the winning system from the challenge at 0mm localization error but is outperformed by it at 2mm and 5mm localization errors. In our view, the performance at 0mm localization error is more important than those at 2mm and 5mm localization errors.",
		"email": [
			"Nima.Tajbakhsh@asu.edu"
		],
		"fullURL": "https://doi.org/10.1007/978-3-319-24571-3_8",
		"source": "miccai",
		"year": 2015,
		"key": "4dd09bd5-6aff-49e2-b712-c21d835f7c2f",
		"use": 1
	},
	{
		"Title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
		"Description": "Olaf Ronneberger, Philipp Fischer, Thomas Brox.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
		"email": [
			"ronneber@informatik.uni-freiburg.de"
		],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_28",
		"source": "miccai",
		"year": 2015,
		"key": "a91ffaf6-ceec-46ff-8aa7-9c9b56a5de27",
		"use": 1
	},
	{
		"Title": "Beyond Classification: Structured Regression for Robust Cell Detection Using Convolutional Neural Network",
		"Description": "Yuanpu Xie, Fuyong Xing, Xiangfei Kong, Hai Su, Lin Yang.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention  MICCAI 2015",
		"abstract": "Robust cell detection serves as a critical prerequisite for many biomedical image analysis applications. In this paper, we present a novel convolutional neural network (CNN) based structured regression model, which is shown to be able to handle touching cells, inhomogeneous background noises, and large variations in sizes and shapes. The proposed method only requires a few training images with weak annotations (just one click near the center of the object). Given an input image patch, instead of providing a single class label like many traditional methods, our algorithm will generate the structured outputs (referred to as proximity patches). These proximity patches, which exhibit higher values for pixels near cell centers, will then be gathered from all testing image patches and fused to obtain the final proximity map, where the maximum positions indicate the cell centroids. The algorithm is tested using three data sets representing different image stains and modalities. The comparative experiments demonstrate the superior performance of this novel method over existing state-of-the-art.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24574-4_43",
		"source": "miccai",
		"year": 2015,
		"key": "4d68b421-3381-4f58-8db8-31e220cd9880",
		"use": 1
	},
	{
		"Title": "Automatic Fetal Ultrasound Standard Plane Detection Using Knowledge Transferred Recurrent Neural Networks",
		"Description": "Hao Chen, Qi Dou, Dong Ni, Jie-Zhi Cheng, Jing Qin, Shengli Li, Pheng-Ann Heng.",
		"ShortDetails": "Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
		"abstract": "Accurate acquisition of fetal ultrasound (US) standard planes is one of the most crucial steps in obstetric diagnosis. The conventional way of standard plane acquisition requires a thorough knowledge of fetal anatomy and intensive manual labors. Hence, automatic approaches are highly demanded in clinical practice. However, automatic detection of standard planes containing key anatomical structures from US videos remains a challenging problem due to the high intra-class variations of standard planes. Unlike previous studies that developed specific methods for different anatomical standard planes respectively, we present a general framework to detect standard planes from US videos automatically. Instead of utilizing hand-crafted visual features, our framework explores spatio-temporal feature learning with a novel knowledge transferred recurrent neural network (T-RNN), which incorporates a deep hierarchical visual feature extractor and a temporal sequence learning model. In order to extract visual features effectively, we propose a joint learning framework with knowledge transfer across multi-tasks to address the insufficiency issue of limited training data. Extensive experiments on different US standard planes with hundreds of videos corroborate that our method can achieve promising results, which outperform state-of-the-art methods.",
		"email": [],
		"fullURL": "https://doi.org/10.1007/978-3-319-24553-9_62",
		"source": "miccai",
		"year": 2015,
		"key": "c967a1c4-9683-49de-bc05-124f9860ccc8",
		"use": 1
	}
]