Title,URL,Description,Details,ShortDetails,Resource,Type,Identifiers,Db,EntrezUID,Properties,abstract,email,keywords,fullURL,source,year,key,use
deepbipolar: identifying genomic mutations for bipolar disorder via deep learning,/pubmed/28600868,"Laksshman S, Bhat RR, Viswanath V, Li X.",Hum Mutat. 2017 Sep;38(9):1217-1224. doi: 10.1002/humu.23272. Epub 2017 Aug 1.,Hum Mutat.  2017,PubMed,citation,PMID:28600868,pubmed,28600868,create date:2017/06/11 | first author:Laksshman S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Bipolar disorder, also known as manic depression, is a brain disorder that affects the brain structure of a patient. It results in extreme mood swings, severe states of depression, and overexcitement simultaneously. It is estimated that roughly 3% of the population of the United States (about 5.3 million adults) suffers from bipolar disorder. Recent research efforts like the Twin studies have demonstrated a high heritability factor for the disorder, making genomics a viable alternative for detecting and treating bipolar disorder, in addition to the conventional lengthy and costly postsymptom clinical diagnosis. Motivated by this study, leveraging several emerging deep learning algorithms, we design an end-to-end deep learning architecture (called DeepBipolar) to predict bipolar disorder based on limited genomic data. DeepBipolar adopts the Deep Convolutional Neural Network (DCNN) architecture that automatically extracts features from genotype information to predict the bipolar phenotype. We participated in the Critical Assessment of Genome Interpretation (CAGI) bipolar disorder challenge and DeepBipolar was considered the most successful by the independent assessor. In this work, we thoroughly evaluate the performance of DeepBipolar and analyze the type of signals we believe could have affected the classifier in distinguishing the case samples from the control set.</abstracttext></p><p class='copyright'>© 2017 Wiley Periodicals, Inc.</p></div></div>",,bipolar disorder; convolutional neural network; deep learning; exome single-nucleotide polymorphisms analysis,https://www.ncbi.nlm.nih.gov//pubmed/28600868,pubmed,2017,c3510786-340b-45d7-9272-eee0c8960971,1
"deep learning for healthcare: review, opportunities and challenges",/pubmed/28481991,"Miotto R, Wang F, Wang S, Jiang X, Dudley JT.",Brief Bioinform. 2017 May 6. doi: 10.1093/bib/bbx044. [Epub ahead of print],Brief Bioinform.  2017,PubMed,citation,PMID:28481991,pubmed,28481991,create date:2017/05/10 | first author:Miotto R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.</abstracttext></p><p class='copyright'>© The Author 2017. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.</p></div></div>",,biomedical informatics; deep learning; electronic health records; genomics; health care; translational bioinformatics,https://www.ncbi.nlm.nih.gov//pubmed/28481991,pubmed,2017,bae357bb-3bec-43da-8cc4-5095bc4d9bbe,1
hla class i binding prediction via convolutional neural networks,/pubmed/28444127,"Vang YS, Xie X.",Bioinformatics. 2017 Sep 1;33(17):2658-2665. doi: 10.1093/bioinformatics/btx264.,Bioinformatics.  2017,PubMed,citation,PMID:28444127,pubmed,28444127,create date:2017/04/27 | first author:Vang YS,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Many biological processes are governed by protein-ligand interactions. One such example is the recognition of self and non-self cells by the immune system. This immune response process is regulated by the major histocompatibility complex (MHC) protein which is encoded by the human leukocyte antigen (HLA) complex. Understanding the binding potential between MHC and peptides can lead to the design of more potent, peptide-based vaccines and immunotherapies for infectious autoimmune diseases.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>We apply machine learning techniques from the natural language processing (NLP) domain to address the task of MHC-peptide binding prediction. More specifically, we introduce a new distributed representation of amino acids, name HLA-Vec, that can be used for a variety of downstream proteomic machine learning tasks. We then propose a deep convolutional neural network architecture, name HLA-CNN, for the task of HLA class I-peptide binding prediction. Experimental results show combining the new distributed representation with our HLA-CNN architecture achieves state-of-the-art results in the majority of the latest two Immune Epitope Database (IEDB) weekly automated benchmark datasets. We further apply our model to predict binding on the human genome and identify 15 genes with potential for self binding.</abstracttext></p><h4>Availability and Implementation: </h4><p><abstracttext label='Availability and Implementation' nlmcategory='UNASSIGNED'>Codes to generate the HLA-Vec and HLA-CNN are publicly available at: https://github.com/uci-cbcl/HLA-bind .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>xhx@ics.uci.edu.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28444127,pubmed,2017,de37062a-9cfd-460b-9c90-e94689fefd04,1
deepcpg: accurate prediction of single-cell dna methylation states using deep learning,/pubmed/28395661,"Angermueller C, Lee HJ, Reik W, Stegle O.",Genome Biol. 2017 Apr 11;18(1):67. doi: 10.1186/s13059-017-1189-z. Erratum in: Genome Biol. 2017 May 12;18(1):90. ,Genome Biol.  2017,PubMed,citation,PMID:28395661 | PMCID:PMC5387360,pubmed,28395661,create date:2017/04/12 | first author:Angermueller C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent technological advances have enabled DNA methylation to be assayed at single-cell resolution. However, current protocols are limited by incomplete CpG coverage and hence methods to predict missing methylation states are critical to enable genome-wide analyses. We report DeepCpG, a computational approach based on deep neural networks to predict methylation states in single cells. We evaluate DeepCpG on single-cell methylation data from five cell types generated using alternative sequencing protocols. DeepCpG yields substantially more accurate predictions than previous methods. Additionally, we show that the model parameters can be interpreted, thereby providing insights into how sequence composition affects methylation variability.</abstracttext></p></div></div>",cangermueller@ebi.ac.uk,Artificial neural network; DNA methylation; Deep learning; Epigenetics; Machine learning; Single-cell genomics,https://www.ncbi.nlm.nih.gov//pubmed/28395661,pubmed,2017,bb3fc9dd-5fc2-4cd5-9173-485ab5481c7e,1
a multi-resolution approach for spinal metastasis detection using deep siamese neural networks,/pubmed/28364643,"Wang J, Fang Z, Lang N, Yuan H, Su MY, Baldi P.",Comput Biol Med. 2017 May 1;84:137-146. doi: 10.1016/j.compbiomed.2017.03.024. Epub 2017 Mar 27.,Comput Biol Med.  2017,PubMed,citation,PMID:28364643,pubmed,28364643,create date:2017/04/02 | first author:Wang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Spinal metastasis, a metastatic cancer of the spine, is the most common malignant disease in the spine. In this study, we investigate the feasibility of automated spinal metastasis detection in magnetic resonance imaging (MRI) by using deep learning methods. To accommodate the large variability in metastatic lesion sizes, we develop a Siamese deep neural network approach comprising three identical subnetworks for multi-resolution analysis and detection of spinal metastasis. At each location of interest, three image patches at three different resolutions are extracted and used as the input to the networks. To further reduce the false positives (FPs), we leverage the similarity between neighboring MRI slices, and adopt a weighted averaging strategy to aggregate the results obtained by the Siamese neural networks. The detection performance is evaluated on a set of 26 cases using a free-response receiver operating characteristic (FROC) analysis. The results show that the proposed approach correctly detects all the spinal metastatic lesions while producing only 0.40 FPs per case. At a true positive (TP) rate of 90%, the use of the aggregation reduces the FPs from 0.375 FPs per case to 0.207 FPs per case, a nearly 44.8% reduction. The results indicate that the proposed Siamese neural network method, combined with the aggregation strategy, provide a viable strategy for the automated detection of spinal metastasis in MRI images.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",pfbaldi@ics.uci.edu,Deep learning; Magnetic resonance imaging; Multi-resolution analysis; Siamese neural network; Spinal metastasis,https://www.ncbi.nlm.nih.gov//pubmed/28364643,pubmed,2017,f89cadf5-b09a-4213-8416-ae8448aec9bb,1
sequence-specific bias correction for rna-seq data using recurrent neural networks,/pubmed/28198674,"Zhang YZ, Yamaguchi R, Imoto S, Miyano S.",BMC Genomics. 2017 Jan 25;18(Suppl 1):1044. doi: 10.1186/s12864-016-3262-5.,BMC Genomics.  2017,PubMed,citation,PMID:28198674 | PMCID:PMC5310274,pubmed,28198674,create date:2017/02/16 | first author:Zhang YZ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>The recent success of deep learning techniques in machine learning and artificial intelligence has stimulated a great deal of interest among bioinformaticians, who now wish to bring the power of deep learning to bare on a host of bioinformatical problems. Deep learning is ideally suited for biological problems that require automatic or hierarchical feature representation for biological data when prior knowledge is limited. In this work, we address the sequence-specific bias correction problem for RNA-seq data redusing Recurrent Neural Networks (RNNs) to model nucleotide sequences without pre-determining sequence structures. The sequence-specific bias of a read is then calculated based on the sequence probabilities estimated by RNNs, and used in the estimation of gene abundance.</abstracttext></p><h4>RESULT: </h4><p><abstracttext label='RESULT' nlmcategory='RESULTS'>We explore the application of two popular RNN recurrent units for this task and demonstrate that RNN-based approaches provide a flexible way to model nucleotide sequences without knowledge of predetermined sequence structures. Our experiments show that training a RNN-based nucleotide sequence model is efficient and RNN-based bias correction methods compare well with the-state-of-the-art sequence-specific bias correction method on the commonly used MAQC-III data set.</abstracttext></p><h4>CONCLUSTIONS: </h4><p><abstracttext label='CONCLUSTIONS' nlmcategory='UNASSIGNED'>RNNs provides an alternative and flexible way to calculate sequence-specific bias without explicitly pre-determining sequence structures.</abstracttext></p></div></div>",miyano@ims.u-tokyo.ac.jp,Gene expression analysis; RNA-seq; Recurrent neural network; Sequence-specific bias,https://www.ncbi.nlm.nih.gov//pubmed/28198674,pubmed,2017,83147fad-3d7b-4cdc-a52a-75e0717553d7,1
deep artificial neural networks and neuromorphic chips for big data analysis: pharmaceutical and bioinformatics applications,/pubmed/27529225,"Pastur-Romay LA, Cedrón F, Pazos A, Porto-Pazos AB.",Int J Mol Sci. 2016 Aug 11;17(8). pii: E1313. doi: 10.3390/ijms17081313. Review.,Int J Mol Sci.  2016,PubMed,citation,PMID:27529225 | PMCID:PMC5000710,pubmed,27529225,create date:2016/08/17 | first author:Pastur-Romay LA,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Over the past decade, Deep Artificial Neural Networks (DNNs) have become the state-of-the-art algorithms in Machine Learning (ML), speech recognition, computer vision, natural language processing and many other tasks. This was made possible by the advancement in Big Data, Deep Learning (DL) and drastically increased chip processing abilities, especially general-purpose graphical processing units (GPGPUs). All this has created a growing interest in making the most of the potential offered by DNNs in almost every field. An overview of the main architectures of DNNs, and their usefulness in Pharmacology and Bioinformatics are presented in this work. The featured applications are: drug design, virtual screening (VS), Quantitative Structure-Activity Relationship (QSAR) research, protein structure prediction and genomics (and other omics) data mining. The future need of neuromorphic hardware for DNNs is also discussed, and the two most advanced chips are reviewed: IBM TrueNorth and SpiNNaker. In addition, this review points out the importance of considering not only neurons, as DNNs and neuromorphic chips should also include glial cells, given the proven importance of astrocytes, a type of glial cell which contributes to information processing in the brain. The Deep Artificial Neuron-Astrocyte Networks (DANAN) could overcome the difficulties in architecture design, learning process and scalability of the current ML methods. </abstracttext></p></div></div>",pastur90@gmail.com,Quantitative Structure–Activity Relationship; artificial neural networks; artificial neuron–astrocyte networks; big data; deep learning; drug design; genomic medicine; neuromorphic chips; protein structure prediction; tripartite synapses,https://www.ncbi.nlm.nih.gov//pubmed/27529225,pubmed,2016,9098a849-e62a-450c-9c1d-6ea296fb6830,1
cgbvs-dnn: prediction of compound-protein interactions based on deep learning,/pubmed/27515489,"Hamanaka M, Taneishi K, Iwata H, Ye J, Pei J, Hou J, Okuno Y.",Mol Inform. 2017 Jan;36(1-2). doi: 10.1002/minf.201600045. Epub 2016 Aug 12.,Mol Inform.  2017,PubMed,citation,PMID:27515489,pubmed,27515489,create date:2016/08/16 | first author:Hamanaka M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computational prediction of compound-protein interactions (CPIs) is of great importance for drug design as the first step in in-silico screening. We previously proposed chemical genomics-based virtual screening (CGBVS), which predicts CPIs by using a support vector machine (SVM). However, the CGBVS has problems when training using more than a million datasets of CPIs since SVMs require an exponential increase in the calculation time and computer memory. To solve this problem, we propose the CGBVS-DNN, in which we use deep neural networks, a kind of deep learning technique, instead of the SVM. Deep learning does not require learning all input data at once because the network can be trained with small mini-batches. Experimental results show that the CGBVS-DNN outperformed the original CGBVS with a quarter million CPIs. Results of cross-validation show that the accuracy of the CGBVS-DNN reaches up to 98.2 % (σ&lt;0.01) with 4 million CPIs.</abstracttext></p><p class='copyright'>© 2017 Wiley-VCH Verlag GmbH &amp; Co. KGaA, Weinheim.</p></div></div>",,chemical genomics-based virtual screening (cgbvs); compound-protein interactions (cpis); deep learning; in-silico screening; support vector machine,https://www.ncbi.nlm.nih.gov//pubmed/27515489,pubmed,2017,71b68fb4-caf0-4c6a-a4e2-2dbbbdce52dd,1
dl-adr: a novel deep learning model for classifying genomic variants into adverse drug reactions,/pubmed/27510822,"Liang Z, Huang JX, Zeng X, Zhang G.",BMC Med Genomics. 2016 Aug 10;9 Suppl 2:48. doi: 10.1186/s12920-016-0207-4.,BMC Med Genomics.  2016,PubMed,citation,PMID:27510822 | PMCID:PMC4980789,pubmed,27510822,create date:2016/08/12 | first author:Liang Z,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Genomic variations are associated with the metabolism and the occurrence of adverse reactions of many therapeutic agents. The polymorphisms on over 2000 locations of cytochrome P450 enzymes (CYP) due to many factors such as ethnicity, mutations, and inheritance attribute to the diversity of response and side effects of various drugs. The associations of the single nucleotide polymorphisms (SNPs), the internal pharmacokinetic patterns and the vulnerability of specific adverse reactions become one of the research interests of pharmacogenomics. The conventional genomewide association studies (GWAS) mainly focuses on the relation of single or multiple SNPs to a specific risk factors which are a one-to-many relation. However, there are no robust methods to establish a many-to-many network which can combine the direct and indirect associations between multiple SNPs and a serial of events (e.g. adverse reactions, metabolic patterns, prognostic factors etc.). In this paper, we present a novel deep learning model based on generative stochastic networks and hidden Markov chain to classify the observed samples with SNPs on five loci of two genes (CYP2D6 and CYP1A2) respectively to the vulnerable population of 14 types of adverse reactions.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A supervised deep learning model is proposed in this study. The revised generative stochastic networks (GSN) model with transited by the hidden Markov chain is used. The data of the training set are collected from clinical observation. The training set is composed of 83 observations of blood samples with the genotypes respectively on CYP2D6*2, *10, *14 and CYP1A2*1C, *1 F. The samples are genotyped by the polymerase chain reaction (PCR) method. A hidden Markov chain is used as the transition operator to simulate the probabilistic distribution. The model can perform learning at lower cost compared to the conventional maximal likelihood method because the transition distribution is conditional on the previous state of the hidden Markov chain. A least square loss (LASSO) algorithm and a k-Nearest Neighbors (kNN) algorithm are used as the baselines for comparison and to evaluate the performance of our proposed deep learning model.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>There are 53 adverse reactions reported during the observation. They are assigned to 14 categories. In the comparison of classification accuracy, the deep learning model shows superiority over the LASSO and kNN model with a rate over 80 %. In the comparison of reliability, the deep learning model shows the best stability among the three models.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Machine learning provides a new method to explore the complex associations among genomic variations and multiple events in pharmacogenomics studies. The new deep learning algorithm is capable of classifying various SNPs to the corresponding adverse reactions. We expect that as more genomic variations are added as features and more observations are made, the deep learning model can improve its performance and can act as a black-box but reliable verifier for other GWAS studies.</abstracttext></p></div></div>",jhuang@yorku.ca,Adverse drug reaction; Deep learning; Genomewide association study; Pharmacogenomics; Single nucleotide polymorphisms,https://www.ncbi.nlm.nih.gov//pubmed/27510822,pubmed,2016,dd4a641b-39ff-4f3e-8265-be740acf420f,1
ipminer: hidden ncrna-protein interaction sequential pattern mining with stacked autoencoder for accurate computational prediction,/pubmed/27506469,"Pan X, Fan YX, Yan J, Shen HB.",BMC Genomics. 2016 Aug 9;17:582. doi: 10.1186/s12864-016-2931-8.,BMC Genomics.  2016,PubMed,citation,PMID:27506469 | PMCID:PMC4979166,pubmed,27506469,create date:2016/08/11 | first author:Pan X,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Non-coding RNAs (ncRNAs) play crucial roles in many biological processes, such as post-transcription of gene regulation. ncRNAs mainly function through interaction with RNA binding proteins (RBPs). To understand the function of a ncRNA, a fundamental step is to identify which protein is involved into its interaction. Therefore it is promising to computationally predict RBPs, where the major challenge is that the interaction pattern or motif is difficult to be found.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In this study, we propose a computational method IPMiner (Interaction Pattern Miner) to predict ncRNA-protein interactions from sequences, which makes use of deep learning and further improves its performance using stacked ensembling. One of the IPMiner's typical merits is that it is able to mine the hidden sequential interaction patterns from sequence composition features of protein and RNA sequences using stacked autoencoder, and then the learned hidden features are fed into random forest models. Finally, stacked ensembling is used to integrate different predictors to further improve the prediction performance. The experimental results indicate that IPMiner achieves superior performance on the tested lncRNA-protein interaction dataset with an accuracy of 0.891, sensitivity of 0.939, specificity of 0.831, precision of 0.945 and Matthews correlation coefficient of 0.784, respectively. We further comprehensively investigate IPMiner on other RNA-protein interaction datasets, which yields better performance than the state-of-the-art methods, and the performance has an increase of over 20 % on some tested benchmarked datasets. In addition, we further apply IPMiner for large-scale prediction of ncRNA-protein network, that achieves promising prediction performance.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>By integrating deep neural network and stacked ensembling, from simple sequence composition features, IPMiner can automatically learn high-level abstraction features, which had strong discriminant ability for RNA-protein detection. IPMiner achieved high performance on our constructed lncRNA-protein benchmark dataset and other RNA-protein datasets. IPMiner tool is available at http://www.csbio.sjtu.edu.cn/bioinf/IPMiner .</abstracttext></p></div></div>",hbshen@sjtu.edu.cn,Deep learning; Stacked ensembing; ncRNA; ncRNA-protein,https://www.ncbi.nlm.nih.gov//pubmed/27506469,pubmed,2016,5df1da58-bc44-432d-9cf3-beb24cab99d2,1
deep learning for computational biology,/pubmed/27474269,"Angermueller C, Pärnamaa T, Parts L, Stegle O.",Mol Syst Biol. 2016 Jul 29;12(7):878. doi: 10.15252/msb.20156651. Review.,Mol Syst Biol.  2016,PubMed,citation,PMID:27474269 | PMCID:PMC4965871,pubmed,27474269,create date:2016/07/31 | first author:Angermueller C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology. </abstracttext></p><p class='copyright'>© 2016 The Authors. Published under the terms of the CC BY 4.0 license.</p></div></div>",leopold.parts@sanger.ac.uk,cellular imaging; computational biology; deep learning; machine learning; regulatory genomics,https://www.ncbi.nlm.nih.gov//pubmed/27474269,pubmed,2016,8a142cfd-9219-47fe-a310-50ec93f8a819,1
image analysis and machine learning in digital pathology: challenges and opportunities,/pubmed/27423409,"Madabhushi A, Lee G.",Med Image Anal. 2016 Oct;33:170-175. doi: 10.1016/j.media.2016.06.037. Epub 2016 Jul 4.,Med Image Anal.  2016,PubMed,citation,PMID:27423409 | PMCID:PMC5556681,pubmed,27423409,create date:2016/07/18 | first author:Madabhushi A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>With the rise in whole slide scanner technology, large numbers of tissue slides are being scanned and represented and archived digitally. While digital pathology has substantial implications for telepathology, second opinions, and education there are also huge research opportunities in image computing with this new source of 'big data'. It is well known that there is fundamental prognostic data embedded in pathology images. The ability to mine 'sub-visual' image features from digital pathology slide images, features that may not be visually discernible by a pathologist, offers the opportunity for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However the compelling opportunities in precision medicine offered by big digital pathology data come with their own set of computational challenges. Image analysis and computer assisted detection and diagnosis tools previously developed in the context of radiographic images are woefully inadequate to deal with the data density in high resolution digitized whole slide images. Additionally there has been recent substantial interest in combining and fusing radiologic imaging and proteomics and genomics based measurements with features extracted from digital pathology images for better prognostic prediction of disease aggressiveness and patient outcome. Again there is a paucity of powerful tools for combining disease specific features that manifest across multiple different length scales. The purpose of this review is to discuss developments in computational image analysis tools for predictive modeling of digital pathology images from a detection, segmentation, feature extraction, and tissue classification perspective. We discuss the emergence of new handcrafted feature approaches for improved predictive modeling of tissue appearance and also review the emergence of deep learning schemes for both object detection and tissue classification. We also briefly review some of the state of the art in fusion of radiology and pathology images and also combining digital pathology derived image measurements with molecular 'omics' features for better predictive modeling. The review ends with a brief discussion of some of the technical and computational challenges to be overcome and reflects on future opportunities for the quantitation of histopathology. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",anantm@case.edu,Deep learning; Digital pathology; Omics; Radiology,https://www.ncbi.nlm.nih.gov//pubmed/27423409,pubmed,2016,cb54fefc-2842-4ee0-9bb9-8169cd9aa3a7,1
basset: learning the regulatory code of the accessible genome with deep convolutional neural networks,/pubmed/27197224,"Kelley DR, Snoek J, Rinn JL.",Genome Res. 2016 Jul;26(7):990-9. doi: 10.1101/gr.200535.115. Epub 2016 May 3.,Genome Res.  2016,PubMed,citation,PMID:27197224 | PMCID:PMC4937568,pubmed,27197224,create date:2016/05/20 | first author:Kelley DR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The complex language of eukaryotic gene expression remains incompletely understood. Despite the importance suggested by many noncoding variants statistically associated with human disease, nearly all such variants have unknown mechanisms. Here, we address this challenge using an approach based on a recent machine learning advance-deep convolutional neural networks (CNNs). We introduce the open source package Basset to apply CNNs to learn the functional activity of DNA sequences from genomics data. We trained Basset on a compendium of accessible genomic sites mapped in 164 cell types by DNase-seq, and demonstrate greater predictive accuracy than previous methods. Basset predictions for the change in accessibility between variant alleles were far greater for Genome-wide association study (GWAS) SNPs that are likely to be causal relative to nearby SNPs in linkage disequilibrium with them. With Basset, a researcher can perform a single sequencing assay in their cell type of interest and simultaneously learn that cell's chromatin accessibility code and annotate every mutation in the genome with its influence on present accessibility and latent potential for accessibility. Thus, Basset offers a powerful computational approach to annotate and interpret the noncoding genome.</abstracttext></p><p class='copyright'>© 2016 Kelley et al.; Published by Cold Spring Harbor Laboratory Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27197224,pubmed,2016,296fc8f7-7ba1-4206-81ac-dcb92be688e2,1
deep patient: an unsupervised representation to predict the future of patients from the electronic health records,/pubmed/27185194,"Miotto R, Li L, Kidd BA, Dudley JT.",Sci Rep. 2016 May 17;6:26094. doi: 10.1038/srep26094.,Sci Rep.  2016,PubMed,citation,PMID:27185194 | PMCID:PMC4869115,pubmed,27185194,create date:2016/05/18 | first author:Miotto R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Secondary use of electronic health records (EHRs) promises to advance clinical research and better inform clinical decision making. Challenges in summarizing and representing patient data prevent widespread practice of predictive modeling using EHRs. Here we present a novel unsupervised deep feature learning method to derive a general-purpose patient representation from EHR data that facilitates clinical predictive modeling. In particular, a three-layer stack of denoising autoencoders was used to capture hierarchical regularities and dependencies in the aggregated EHRs of about 700,000 patients from the Mount Sinai data warehouse. The result is a representation we name 'deep patient'. We evaluated this representation as broadly predictive of health states by assessing the probability of patients to develop various diseases. We performed evaluation using 76,214 test patients comprising 78 diseases from diverse clinical domains and temporal windows. Our results significantly outperformed those achieved using representations based on raw EHR data and alternative feature learning strategies. Prediction performance for severe diabetes, schizophrenia, and various cancers were among the top performing. These findings indicate that deep learning applied to EHRs can derive patient representations that offer improved clinical predictions, and could provide a machine learning framework for augmenting clinical decision systems. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27185194,pubmed,2016,5ae321a4-15ef-48a3-8a40-99c022b5fb4e,1
deep learning for population genetic inference,/pubmed/27018908,"Sheehan S, Song YS.",PLoS Comput Biol. 2016 Mar 28;12(3):e1004845. doi: 10.1371/journal.pcbi.1004845. eCollection 2016 Mar.,PLoS Comput Biol.  2016,PubMed,citation,PMID:27018908 | PMCID:PMC4809617,pubmed,27018908,create date:2016/03/29 | first author:Sheehan S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Given genomic variation data from multiple individuals, computing the likelihood of complex population genetic models is often infeasible. To circumvent this problem, we introduce a novel likelihood-free inference framework by applying deep learning, a powerful modern technique in machine learning. Deep learning makes use of multilayer neural networks to learn a feature-based function from the input (e.g., hundreds of correlated summary statistics of data) to the output (e.g., population genetic parameters of interest). We demonstrate that deep learning can be effectively employed for population genetic inference and learning informative features of data. As a concrete application, we focus on the challenging problem of jointly inferring natural selection and demography (in the form of a population size change history). Our method is able to separate the global nature of demography from the local nature of selection, without sequential steps for these two factors. Studying demography and selection jointly is motivated by Drosophila, where pervasive selection confounds demographic analysis. We apply our method to 197 African Drosophila melanogaster genomes from Zambia to infer both their overall demography, and regions of their genome under selection. We find many regions of the genome that have experienced hard sweeps, and fewer under selection on standing variation (soft sweep) or balancing selection. Interestingly, we find that soft sweeps and balancing selection occur more frequently closer to the centromere of each chromosome. In addition, our demographic inference suggests that previously estimated bottlenecks for African Drosophila melanogaster are too extreme. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27018908,pubmed,2016,c156305f-fa0c-4fa9-bb45-f4d5e4b6e8a4,1
applications of deep learning in biomedicine,/pubmed/27007977,"Mamoshina P, Vieira A, Putin E, Zhavoronkov A.",Mol Pharm. 2016 May 2;13(5):1445-54. doi: 10.1021/acs.molpharmaceut.5b00982. Epub 2016 Mar 29.,Mol Pharm.  2016,PubMed,citation,PMID:27007977,pubmed,27007977,create date:2016/03/24 | first author:Mamoshina P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Increases in throughput and installed base of biomedical research equipment led to a massive accumulation of -omics data known to be highly variable, high-dimensional, and sourced from multiple often incompatible data platforms. While this data may be useful for biomarker identification and drug discovery, the bulk of it remains underutilized. Deep neural networks (DNNs) are efficient algorithms based on the use of compositional layers of neurons, with advantages well matched to the challenges -omics data presents. While achieving state-of-the-art results and even surpassing human accuracy in many challenging tasks, the adoption of deep learning in biomedicine has been comparatively slow. Here, we discuss key features of deep learning that may give this approach an edge over other machine learning methods. We then consider limitations and review a number of applications of deep learning in biomedical studies demonstrating proof of concept and practical utility. </abstracttext></p></div></div>",,RBM; artificial intelligence; biomarker development; deep learning; deep neural networks; genomics; transcriptomics,https://www.ncbi.nlm.nih.gov//pubmed/27007977,pubmed,2016,3a37bc1b-44fb-403c-8799-0a834a9358d4,1
deep learning in label-free cell classification,/pubmed/26975219,"Chen CL, Mahjoubfar A, Tai LC, Blaby IK, Huang A, Niazi KR, Jalali B.",Sci Rep. 2016 Mar 15;6:21471. doi: 10.1038/srep21471.,Sci Rep.  2016,PubMed,citation,PMID:26975219 | PMCID:PMC4791545,pubmed,26975219,create date:2016/03/16 | first author:Chen CL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Label-free cell analysis is essential to personalized genomics, cancer diagnostics, and drug development as it avoids adverse effects of staining reagents on cellular viability and cell signaling. However, currently available label-free cell assays mostly rely only on a single feature and lack sufficient differentiation. Also, the sample size analyzed by these assays is limited due to their low throughput. Here, we integrate feature extraction and deep learning with high-throughput quantitative imaging enabled by photonic time stretch, achieving record high accuracy in label-free cell classification. Our system captures quantitative optical phase and intensity images and extracts multiple biophysical features of individual cells. These biophysical measurements form a hyperdimensional feature space in which supervised learning is performed for cell classification. We compare various learning algorithms including artificial neural network, support vector machine, logistic regression, and a novel deep learning pipeline, which adopts global optimization of receiver operating characteristics. As a validation of the enhanced sensitivity and specificity of our system, we show classification of white blood T-cells against colon cancer cells, as well as lipid accumulating algal strains for biofuel production. This system opens up a new path to data-driven phenotypic diagnosis and better understanding of the heterogeneous gene expressions in cells. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26975219,pubmed,2016,4f4be733-94d4-4ea9-930f-860be5628aa4,1
continuous distributed representation of biological sequences for deep proteomics and genomics,/pubmed/26555596,"Asgari E, Mofrad MR.",PLoS One. 2015 Nov 10;10(11):e0141287. doi: 10.1371/journal.pone.0141287. eCollection 2015.,PLoS One.  2015,PubMed,citation,PMID:26555596 | PMCID:PMC4640716,pubmed,26555596,create date:2015/11/12 | first author:Asgari E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93%±0.06% is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. Importantly, this model needs to be trained only once and can then be applied to extract a comprehensive set of information regarding proteins of interest. Moreover, this representation can be considered as pre-training for various applications of deep learning in bioinformatics. The related data is available at Life Language Processing Website: http://llp.berkeley.edu and Harvard Dataverse: http://dx.doi.org/10.7910/DVN/JMFHTN. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26555596,pubmed,2015,5b81c5df-561b-47e5-8373-f853cb895d9a,1
de novo identification of replication-timing domains in the human genome by deep learning,/pubmed/26545821,"Liu F, Ren C, Li H, Zhou P, Bo X, Shu W.",Bioinformatics. 2016 Mar 1;32(5):641-9. doi: 10.1093/bioinformatics/btv643. Epub 2015 Nov 5.,Bioinformatics.  2016,PubMed,citation,PMID:26545821 | PMCID:PMC4795613,pubmed,26545821,create date:2015/11/08 | first author:Liu F,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>The de novo identification of the initiation and termination zones-regions that replicate earlier or later than their upstream and downstream neighbours, respectively-remains a key challenge in DNA replication.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Building on advances in deep learning, we developed a novel hybrid architecture combining a pre-trained, deep neural network and a hidden Markov model (DNN-HMM) for the de novo identification of replication domains using replication timing profiles. Our results demonstrate that DNN-HMM can significantly outperform strong, discriminatively trained Gaussian mixture model-HMM (GMM-HMM) systems and other six reported methods that can be applied to this challenge. We applied our trained DNN-HMM to identify distinct replication domain types, namely the early replication domain (ERD), the down transition zone (DTZ), the late replication domain (LRD) and the up transition zone (UTZ), using newly replicated DNA sequencing (Repli-Seq) data across 15 human cells. A subsequent integrative analysis revealed that these replication domains harbour unique genomic and epigenetic patterns, transcriptional activity and higher-order chromosomal structure. Our findings support the 'replication-domain' model, which states (1) that ERDs and LRDs, connected by UTZs and DTZs, are spatially compartmentalized structural and functional units of higher-order chromosomal structure, (2) that the adjacent DTZ-UTZ pairs form chromatin loops and (3) that intra-interactions within ERDs and LRDs tend to be short-range and long-range, respectively. Our model reveals an important chromatin organizational principle of the human genome and represents a critical step towards understanding the mechanisms regulating replication timing.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>Our DNN-HMM method and three additional algorithms can be freely accessed at https://github.com/wenjiegroup/DNN-HMM The replication domain regions identified in this study are available in GEO under the accession ID GSE53984.</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>shuwj@bmi.ac.cn or boxc@bmi.ac.cn</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2015. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26545821,pubmed,2016,d1a20cdd-b431-4ffd-9f0e-9c36c894e93c,1
multi-layer and recursive neural networks for metagenomic classification,/pubmed/26316190,"Ditzler G, Polikar R, Rosen G.",IEEE Trans Nanobioscience. 2015 Sep;14(6):608-16. doi: 10.1109/TNB.2015.2461219. Epub 2015 Aug 24.,IEEE Trans Nanobioscience.  2015,PubMed,citation,PMID:26316190,pubmed,26316190,create date:2015/09/01 | first author:Ditzler G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26316190,pubmed,2015,413754dd-01eb-41ca-8d7d-f08f32502076,1
predicting effects of noncoding variants with deep learning-based sequence model,/pubmed/26301843,"Zhou J, Troyanskaya OG.",Nat Methods. 2015 Oct;12(10):931-4. doi: 10.1038/nmeth.3547. Epub 2015 Aug 24.,Nat Methods.  2015,PubMed,citation,PMID:26301843 | PMCID:PMC4768299,pubmed,26301843,create date:2015/08/25 | first author:Zhou J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Identifying functional effects of noncoding variants is a major challenge in human genetics. To predict the noncoding-variant effects de novo from sequence, we developed a deep learning-based algorithmic framework, DeepSEA (http://deepsea.princeton.edu/), that directly learns a regulatory sequence code from large-scale chromatin-profiling data, enabling prediction of chromatin effects of sequence alterations with single-nucleotide sensitivity. We further used this capability to improve prioritization of functional variants including expression quantitative trait loci (eQTLs) and disease-associated variants. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26301843,pubmed,2015,1533b25b-86c8-4933-9623-659bd5905500,1
deep learning for regulatory genomics,/pubmed/26252139,"Park Y, Kellis M.",Nat Biotechnol. 2015 Aug;33(8):825-6. doi: 10.1038/nbt.3313. No abstract available. ,Nat Biotechnol.  2015,PubMed,citation,PMID:26252139,pubmed,26252139,create date:2015/08/08 | first author:Park Y,,,,https://www.ncbi.nlm.nih.gov//pubmed/26252139,pubmed,2015,2eac570f-5c76-4c94-b71a-ef22828ec739,1
predicting the sequence specificities of dna- and rna-binding proteins by deep learning,/pubmed/26213851,"Alipanahi B, Delong A, Weirauch MT, Frey BJ.",Nat Biotechnol. 2015 Aug;33(8):831-8. doi: 10.1038/nbt.3300. Epub 2015 Jul 27.,Nat Biotechnol.  2015,PubMed,citation,PMID:26213851,pubmed,26213851,create date:2015/07/28 | first author:Alipanahi B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Knowing the sequence specificities of DNA- and RNA-binding proteins is essential for developing models of the regulatory processes in biological systems and for identifying causal disease variants. Here we show that sequence specificities can be ascertained from experimental data with 'deep learning' techniques, which offer a scalable, flexible and unified computational approach for pattern discovery. Using a diverse array of experimental data and evaluation metrics, we find that deep learning outperforms other state-of-the-art methods, even when training on in vitro data and testing on in vivo data. We call this approach DeepBind and have built a stand-alone software tool that is fully automatic and handles millions of sequences per experiment. Specificities determined by DeepBind are readily visualized as a weighted ensemble of position weight matrices or as a 'mutation map' that indicates how variations affect binding within a specific sequence. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26213851,pubmed,2015,bbc94a21-f801-4ffc-88d1-2965d4ebb496,1
deep learning,/pubmed/26017442,"LeCun Y, Bengio Y, Hinton G.",Nature. 2015 May 28;521(7553):436-44. doi: 10.1038/nature14539. Review.,Nature.  2015,PubMed,citation,PMID:26017442,pubmed,26017442,create date:2015/05/29 | first author:LeCun Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26017442,pubmed,2015,e973e3a9-25a5-42e7-ba0f-4115400313ee,1
trans-species learning of cellular signaling systems with bimodal deep belief networks,/pubmed/25995230,"Chen L, Cai C, Chen V, Lu X.",Bioinformatics. 2015 Sep 15;31(18):3008-15. doi: 10.1093/bioinformatics/btv315. Epub 2015 May 20.,Bioinformatics.  2015,PubMed,citation,PMID:25995230 | PMCID:PMC4668779,pubmed,25995230,create date:2015/05/23 | first author:Chen L,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Model organisms play critical roles in biomedical research of human diseases and drug development. An imperative task is to translate information/knowledge acquired from model organisms to humans. In this study, we address a trans-species learning problem: predicting human cell responses to diverse stimuli, based on the responses of rat cells treated with the same stimuli.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We hypothesized that rat and human cells share a common signal-encoding mechanism but employ different proteins to transmit signals, and we developed a bimodal deep belief network and a semi-restricted bimodal deep belief network to represent the common encoding mechanism and perform trans-species learning. These 'deep learning' models include hierarchically organized latent variables capable of capturing the statistical structures in the observed proteomic data in a distributed fashion. The results show that the models significantly outperform two current state-of-the-art classification algorithms. Our study demonstrated the potential of using deep hierarchical models to simulate cellular signaling systems.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>The software is available at the following URL: http://pubreview.dbmi.pitt.edu/TransSpeciesDeepLearning/. The data are available through SBV IMPROVER website, https://www.sbvimprover.com/challenge-2/overview, upon publication of the report by the organizers.</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>xinghua@pitt.edu</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25995230,pubmed,2015,e7718c15-ef6b-4e68-8b44-24c8246d20ba,1
deep learning of the tissue-regulated splicing code,/pubmed/24931975,"Leung MK, Xiong HY, Lee LJ, Frey BJ.",Bioinformatics. 2014 Jun 15;30(12):i121-9. doi: 10.1093/bioinformatics/btu277.,Bioinformatics.  2014,PubMed,citation,PMID:24931975 | PMCID:PMC4058935,pubmed,24931975,create date:2014/06/17 | first author:Leung MK,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Alternative splicing (AS) is a regulated process that directs the generation of different transcripts from single genes. A computational model that can accurately predict splicing patterns based on genomic features and cellular context is highly desirable, both in understanding this widespread phenomenon, and in exploring the effects of genetic variations on AS.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Using a deep neural network, we developed a model inferred from mouse RNA-Seq data that can predict splicing patterns in individual tissues and differences in splicing patterns across tissues. Our architecture uses hidden variables that jointly represent features in genomic sequences and tissue types when making predictions. A graphics processing unit was used to greatly reduce the training time of our models with millions of parameters.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We show that the deep architecture surpasses the performance of the previous Bayesian method for predicting AS patterns. With the proper optimization procedure and selection of hyperparameters, we demonstrate that deep architectures can be beneficial, even with a moderately sparse dataset. An analysis of what the model has learned in terms of the genomic features is presented.</abstracttext></p><p class='copyright'>© The Author 2014. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/24931975,pubmed,2014,a6974808-f737-4f5a-be9a-1413996be04b,1
convolutional auto-encoder for image denoising of ultra-low-dose ct,/pubmed/28920094,"Nishio M, Nagashima C, Hirabayashi S, Ohnishi A, Sasaki K, Sagawa T, Hamada M, Yamashita T.",Heliyon. 2017 Aug 30;3(8):e00393. doi: 10.1016/j.heliyon.2017.e00393. eCollection 2017 Aug.,Heliyon.  2017,PubMed,citation,PMID:28920094 | PMCID:PMC5577435,pubmed,28920094,create date:2017/09/19 | first author:Nishio M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVES: </h4><p><abstracttext label='OBJECTIVES' nlmcategory='OBJECTIVE'>The purpose of this study was to validate a patch-based image denoising method for ultra-low-dose CT images. Neural network with convolutional auto-encoder and pairs of standard-dose CT and ultra-low-dose CT image patches were used for image denoising. The performance of the proposed method was measured by using a chest phantom.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>Standard-dose and ultra-low-dose CT images of the chest phantom were acquired. The tube currents for standard-dose and ultra-low-dose CT were 300 and 10 mA, respectively. Ultra-low-dose CT images were denoised with our proposed method using neural network, large-scale nonlocal mean, and block-matching and 3D filtering. Five radiologists and three technologists assessed the denoised ultra-low-dose CT images visually and recorded their subjective impressions of streak artifacts, noise other than streak artifacts, visualization of pulmonary vessels, and overall image quality.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>For the streak artifacts, noise other than streak artifacts, and visualization of pulmonary vessels, the results of our proposed method were statistically better than those of block-matching and 3D filtering (p-values &lt; 0.05). On the other hand, the difference in the overall image quality between our proposed method and block-matching and 3D filtering was not statistically significant (p-value = 0.07272). The p-values obtained between our proposed method and large-scale nonlocal mean were all less than 0.05.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Neural network with convolutional auto-encoder could be trained using pairs of standard-dose and ultra-low-dose CT image patches. According to the visual assessment by radiologists and technologists, the performance of our proposed method was superior to that of large-scale nonlocal mean and block-matching and 3D filtering.</abstracttext></p></div></div>",,Computer science; Medical imaging,https://www.ncbi.nlm.nih.gov//pubmed/28920094,pubmed,2017,89ee2f01-66d7-4d57-a492-305e54052013,1
automated arteriole and venule classification using deep learning for retinal images from the uk biobank cohort,/pubmed/28917120,"Welikala RA, Foster PJ, Whincup PH, Rudnicka AR, Owen CG, Strachan DP, Barman SA; UK Biobank Eye and Vision Consortium..",Comput Biol Med. 2017 Sep 8;90:23-32. doi: 10.1016/j.compbiomed.2017.09.005. [Epub ahead of print],Comput Biol Med.  2017,PubMed,citation,PMID:28917120,pubmed,28917120,create date:2017/09/17 | first author:Welikala RA,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The morphometric characteristics of the retinal vasculature are associated with future risk of many systemic and vascular diseases. However, analysis of data from large population based studies is needed to help resolve uncertainties in some of these associations. This requires automated systems that extract quantitative measures of vessel morphology from large numbers of retinal images. Associations between retinal vessel morphology and disease precursors/outcomes may be similar or opposing for arterioles and venules. Therefore, the accurate detection of the vessel type is an important element in such automated systems. This paper presents a deep learning approach for the automatic classification of arterioles and venules across the entire retinal image, including vessels located at the optic disc. This comprises of a convolutional neural network whose architecture contains six learned layers: three convolutional and three fully-connected. Complex patterns are automatically learnt from the data, which avoids the use of hand crafted features. The method is developed and evaluated using 835,914 centreline pixels derived from 100 retinal images selected from the 135,867 retinal images obtained at the UK Biobank (large population-based cohort study of middle aged and older adults) baseline examination. This is a challenging dataset in respect to image quality and hence arteriole/venule classification is required to be highly robust. The method achieves a significant increase in accuracy of 8.1% when compared to the baseline method, resulting in an arteriole/venule classification accuracy of 86.97% (per pixel basis) over the entire retinal image.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",R.Welikala@kingston.ac.uk,Arteriole/venule classification; Convolutional neural networks; Deep learning; Epidemiological studies; Retinal images; UK Biobank,https://www.ncbi.nlm.nih.gov//pubmed/28917120,pubmed,2017,d88c4041-9b67-47f5-9a56-55811fdd4a43,1
deep convolutional neural network with transfer learning for rectum toxicity prediction in cervical cancer radiotherapy: a feasibility study,/pubmed/28914611,"Zhen X, Chen J, Zhong Z, Hrycushko BA, Zhou L, Jiang SB, Albuquerque K, Gu X.",Phys Med Biol. 2017 Sep 15. doi: 10.1088/1361-6560/aa8d09. [Epub ahead of print],Phys Med Biol.  2017,PubMed,citation,PMID:28914611,pubmed,28914611,create date:2017/09/16 | first author:Zhen X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Better understanding of the dose-toxicity relationship is critical for safe dose escalation to improve local control in late-stage cervical cancer radiotherapy. In this study, we introduced a convolutional neural network (CNN) model to analyze rectum dose distribution and predict rectum toxicity. Forty-two cervical cancer patients treated with combined external beam radiotherapy (EBRT) and brachytherapy (BT) were retrospectively collected, including twelve toxicity patients and thirty non-toxicity patients. We adopted a transfer learning strategy to overcome the limited patient data issue. A 16-layers CNN developed by the visual geometry group (VGG-16) of the University of Oxford was pre-trained on a large-scale natural image database, ImageNet, and fine-tuned with patient rectum surface dose maps (RSDMs), which were accumulated EBRT+BT doses on the unfolded rectum surface. We used the adaptive synthetic sampling approach and the data augmentation method to address the two challenges, data imbalance and data scarcity. The gradient-weighted class activation maps (Grad-CAM) were also generated to highlight the discriminative regions on the RSDM along with the prediction model. We compare different CNN coefficients fine-tuning strategies, and compare the predictive performance using the traditional dose volume parameters, e.g., D0.1/1/2cc, and the texture features extracted from the RSDM. Satisfactory prediction performance was achieved with the proposed scheme, and we found that the mean Grad-CAM over the toxicity patient group has geometric consistence of distribution with the statistical analysis result, which indicates possible rectum toxicity location. The evaluation results have demonstrated the feasibility of building a CNN-based rectum dose-toxicity prediction model with transfer learning for cervical cancer radiotherapy.</abstracttext></p><p class='copyright'>© 2017 Institute of Physics and Engineering in Medicine.</p></div></div>",,Convolutional Neural Networks; Deformable Image Registration; Rectum Surface Dose Maps; Rectum Toxicity Prediction; Transfer Learning,https://www.ncbi.nlm.nih.gov//pubmed/28914611,pubmed,2017,b11157a6-6ac2-4f44-86aa-fda914f7ff1a,1
deep learning approach to bacterial colony classification,/pubmed/28910352,"Zieliński B, Plichta A, Misztal K, Spurek P, Brzychczy-Włoch M, Ochońska D.",PLoS One. 2017 Sep 14;12(9):e0184554. doi: 10.1371/journal.pone.0184554. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28910352,pubmed,28910352,create date:2017/09/15 | first author:Zieliński B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In microbiology it is diagnostically useful to recognize various genera and species of bacteria. It can be achieved using computer-aided methods, which make the recognition processes more automatic and thus significantly reduce the time necessary for the classification. Moreover, in case of diagnostic uncertainty (the misleading similarity in shape or structure of bacterial cells), such methods can minimize the risk of incorrect recognition. In this article, we apply the state of the art method for texture analysis to classify genera and species of bacteria. This method uses deep Convolutional Neural Networks to obtain image descriptors, which are then encoded and classified with Support Vector Machine or Random Forest. To evaluate this approach and to make it comparable with other approaches, we provide a new dataset of images. DIBaS dataset (Digital Image of Bacterial Species) contains 660 images with 33 different genera and species of bacteria.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28910352,pubmed,2017,1cd6171e-58bd-4abb-a14f-7bdcf3e60447,1
an automatic gastrointestinal polyp detection system in video endoscopy using fusion of color wavelet and convolutional neural network features,/pubmed/28894460,"Billah M, Waheed S, Rahman MM.",Int J Biomed Imaging. 2017;2017:9545920. doi: 10.1155/2017/9545920. Epub 2017 Aug 14.,Int J Biomed Imaging.  2017,PubMed,citation,PMID:28894460 | PMCID:PMC5574296,pubmed,28894460,create date:2017/09/13 | first author:Billah M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Gastrointestinal polyps are considered to be the precursors of cancer development in most of the cases. Therefore, early detection and removal of polyps can reduce the possibility of cancer. Video endoscopy is the most used diagnostic modality for gastrointestinal polyps. But, because it is an operator dependent procedure, several human factors can lead to misdetection of polyps. Computer aided polyp detection can reduce polyp miss detection rate and assists doctors in finding the most important regions to pay attention to. In this paper, an automatic system has been proposed as a support to gastrointestinal polyp detection. This system captures the video streams from endoscopic video and, in the output, it shows the identified polyps. Color wavelet (CW) features and convolutional neural network (CNN) features of video frames are extracted and combined together which are used to train a linear support vector machine (SVM). Evaluations on standard public databases show that the proposed system outperforms the state-of-the-art methods, gaining accuracy of 98.65%, sensitivity of 98.79%, and specificity of 98.52%.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28894460,pubmed,2017,6de40675-6363-4ad4-a453-3265fb8c037d,1
disease staging and prognosis in smokers using deep learning in chest computed tomography,/pubmed/28892454,"González G, Ash SY, Vegas Sanchez-Ferrero G, Onieva Onieva J, Rahaghi FN, Ross JC, Díaz A, San José Estépar R, Washko GR; COPDGene and ECLIPSE investigators..",Am J Respir Crit Care Med. 2017 Sep 11. doi: 10.1164/rccm.201705-0860OC. [Epub ahead of print],Am J Respir Crit Care Med.  2017,PubMed,citation,PMID:28892454,pubmed,28892454,create date:2017/09/12 | first author:González G,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>RATIONALE: </h4><p><abstracttext label='RATIONALE' nlmcategory='BACKGROUND'>Deep learning is a powerful tool that may allow for improved outcome prediction.</abstracttext></p><h4>OBJECTIVES: </h4><p><abstracttext label='OBJECTIVES' nlmcategory='OBJECTIVE'>To determine if deep learning, specifically convolutional neural network (CNN) analysis, could detect and stage chronic obstructive pulmonary disease (COPD) and predict acute respiratory disease events (ARD) and mortality in smokers.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A CNN was trained using CT scans from 7,983 COPDGene participants and evaluated using 1000 non-overlapping COPDGene participants and 1,672 ECLIPSE participants. Logistic regression (c-statistic and the Hosmer-Lemeshow test) was used to assess COPD diagnosis and ARD prediction. Cox regression (c-index and the Greenwood-Nam-D'Agnostino test) was used to assess mortality.</abstracttext></p><h4>MEASUREMENTS AND MAIN RESULTS: </h4><p><abstracttext label='MEASUREMENTS AND MAIN RESULTS' nlmcategory='RESULTS'>In COPDGene, the c-statistic for the detection of COPD was 0.856. 51.1% of participants in COPDGene were accurately staged and 74.95% were within one stage. In ECLIPSE, 29.4% were accurately staged and 74.6% were within one stage. In COPDGene and ECLIPSE the c-statistics for ARD events were 0.64 and 0.55 respectively and the Hosmer-Lemeshow p=0.502 and 0.380 respectively, suggesting no evidence of poor calibration. In COPDGene and ECLIPSE, CNN predicted mortality with fair discrimination (c-indices 0.72 and 0.60 respectively), and without evidence of poor calibration (Greenwood-Nam-D'Agnostino p-values of 0.307 and 0.331 respectively).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>A deep learning approach that uses only CT imaging data can identify those smokers who have COPD and predict who are most likely to have ARD events and those with the highest mortality. At a population level CNN analysis may be a powerful tool for risk assessment.</abstracttext></p></div></div>",german.gonzalez.serrano@gmail.com,"Artificial Intelligence (Computer Vision Systems); Neural Networks; Pulmonary Disease, Chronic Obstructive; X-Ray Computed",https://www.ncbi.nlm.nih.gov//pubmed/28892454,pubmed,2017,9a73a197-00e0-4246-bc5a-bcc5739e4373,1
decoding of visual activity patterns from fmri responses using multivariate pattern analyses and convolutional neural network,/pubmed/28891512,"Zafar R, Kamel N, Naufal M, Malik AS, Dass SC, Ahmad RF, Abdullah JM, Reza F.",J Integr Neurosci. 2017;16(3):275-289. doi: 10.3233/JIN-170016.,J Integr Neurosci.  2017,PubMed,citation,PMID:28891512,pubmed,28891512,create date:2017/09/12 | first author:Zafar R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Decoding of human brain activity has always been a primary goal in neuroscience especially with functional magnetic resonance imaging (fMRI) data. In recent years, Convolutional neural network (CNN) has become a popular method for the extraction of features due to its higher accuracy, however it needs a lot of computation and training data. In this study, an algorithm is developed using Multivariate pattern analysis (MVPA) and modified CNN to decode the behavior of brain for different images with limited data set. Selection of significant features is an important part of fMRI data analysis, since it reduces the computational burden and improves the prediction performance; significant features are selected using t-test. MVPA uses machine learning algorithms to classify different brain states and helps in prediction during the task. General linear model (GLM) is used to find the unknown parameters of every individual voxel and the classification is done using multi-class support vector machine (SVM). MVPA-CNN based proposed algorithm is compared with region of interest (ROI) based method and MVPA based estimated values. The proposed method showed better overall accuracy (68.6%) compared to ROI (61.88%) and estimation values (64.17%).</abstracttext></p></div></div>",,Convolutional neural network; GLM; MVPA; SVM; fMRI,https://www.ncbi.nlm.nih.gov//pubmed/28891512,pubmed,2017,d41a67e6-6d15-491f-a95d-bc70513b762e,1
automated classification of lung cancer types from cytological images using deep convolutional neural networks,/pubmed/28884120,"Teramoto A, Tsukamoto T, Kiriyama Y, Fujita H.",Biomed Res Int. 2017;2017:4067832. doi: 10.1155/2017/4067832. Epub 2017 Aug 13.,Biomed Res Int.  2017,PubMed,citation,PMID:28884120 | PMCID:PMC5572620,pubmed,28884120,create date:2017/09/09 | first author:Teramoto A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Lung cancer is a leading cause of death worldwide. Currently, in differential diagnosis of lung cancer, accurate classification of cancer types (adenocarcinoma, squamous cell carcinoma, and small cell carcinoma) is required. However, improving the accuracy and stability of diagnosis is challenging. In this study, we developed an automated classification scheme for lung cancers presented in microscopic images using a deep convolutional neural network (DCNN), which is a major deep learning technique. The DCNN used for classification consists of three convolutional layers, three pooling layers, and two fully connected layers. In evaluation experiments conducted, the DCNN was trained using our original database with a graphics processing unit. Microscopic images were first cropped and resampled to obtain images with resolution of 256 × 256 pixels and, to prevent overfitting, collected images were augmented via rotation, flipping, and filtering. The probabilities of three types of cancers were estimated using the developed scheme and its classification accuracy was evaluated using threefold cross validation. In the results obtained, approximately 71% of the images were classified correctly, which is on par with the accuracy of cytotechnologists and pathologists. Thus, the developed scheme is useful for classification of lung cancers from microscopic images.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28884120,pubmed,2017,0a530413-9f51-4c0e-bd6d-d60bd044fdc6,1
deep learning for magnetic resonance fingerprinting: a new approach for predicting quantitative parameter values from time series,/pubmed/28883201,"Hoppe E, Körzdörfer G, Würfl T, Wetzl J, Lugauer F, Pfeuffer J, Maier A.",Stud Health Technol Inform. 2017;243:202-206.,Stud Health Technol Inform.  2017,PubMed,citation,PMID:28883201,pubmed,28883201,create date:2017/09/09 | first author:Hoppe E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The purpose of this work is to evaluate methods from deep learning for application to Magnetic Resonance Fingerprinting (MRF). MRF is a recently proposed measurement technique for generating quantitative parameter maps. In MRF a non-steady state signal is generated by a pseudo-random excitation pattern. A comparison of the measured signal in each voxel with the physical model yields quantitative parameter maps. Currently, the comparison is done by matching a dictionary of simulated signals to the acquired signals. To accelerate the computation of quantitative maps we train a Convolutional Neural Network (CNN) on simulated dictionary data. As a proof of principle we show that the neural network implicitly encodes the dictionary and can replace the matching process.</abstracttext></p></div></div>",,Convolutional Neural Networks; Deep Learning; Machine Learning; Magnetic Resonance Fingerprinting; Supervised Machine Learning,https://www.ncbi.nlm.nih.gov//pubmed/28883201,pubmed,2017,f1deb1d9-3fb6-4d20-b968-eeaa9dee2699,1
denoising genome-wide histone chip-seq with convolutional neural networks,/pubmed/28881977,"Koh PW, Pierson E, Kundaje A.",Bioinformatics. 2017 Jul 15;33(14):i225-i233. doi: 10.1093/bioinformatics/btx243.,Bioinformatics.  2017,PubMed,citation,PMID:28881977,pubmed,28881977,create date:2017/09/09 | first author:Koh PW,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Chromatin immune-precipitation sequencing (ChIP-seq) experiments are commonly used to obtain genome-wide profiles of histone modifications associated with different types of functional genomic elements. However, the quality of histone ChIP-seq data is affected by many experimental parameters such as the amount of input DNA, antibody specificity, ChIP enrichment and sequencing depth. Making accurate inferences from chromatin profiling experiments that involve diverse experimental parameters is challenging.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>We introduce a convolutional denoising algorithm, Coda, that uses convolutional neural networks to learn a mapping from suboptimal to high-quality histone ChIP-seq data. This overcomes various sources of noise and variability, substantially enhancing and recovering signal when applied to low-quality chromatin profiling datasets across individuals, cell types and species. Our method has the potential to improve data quality at reduced costs. More broadly, this approach-using a high-dimensional discriminative model to encode a generative noise process-is generally applicable to other biological domains where it is easy to generate noisy data but difficult to analytically characterize the noise or underlying data distribution.</abstracttext></p><h4>Availability and implementation: </h4><p><abstracttext label='Availability and implementation' nlmcategory='UNASSIGNED'>https://github.com/kundajelab/coda .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>akundaje@stanford.edu.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28881977,pubmed,2017,a9d94a5f-47cd-4eda-8db1-1692ab6d6d58,1
chromatin accessibility prediction via convolutional long short-term memory networks with k-mer embedding,/pubmed/28881969,"Min X, Zeng W, Chen N, Chen T, Jiang R.",Bioinformatics. 2017 Jul 15;33(14):i92-i101. doi: 10.1093/bioinformatics/btx234.,Bioinformatics.  2017,PubMed,citation,PMID:28881969,pubmed,28881969,create date:2017/09/09 | first author:Min X,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Experimental techniques for measuring chromatin accessibility are expensive and time consuming, appealing for the development of computational approaches to predict open chromatin regions from DNA sequences. Along this direction, existing methods fall into two classes: one based on handcrafted k -mer features and the other based on convolutional neural networks. Although both categories have shown good performance in specific applications thus far, there still lacks a comprehensive framework to integrate useful k -mer co-occurrence information with recent advances in deep learning.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>We fill this gap by addressing the problem of chromatin accessibility prediction with a convolutional Long Short-Term Memory (LSTM) network with k -mer embedding. We first split DNA sequences into k -mers and pre-train k -mer embedding vectors based on the co-occurrence matrix of k -mers by using an unsupervised representation learning approach. We then construct a supervised deep learning architecture comprised of an embedding layer, three convolutional layers and a Bidirectional LSTM (BLSTM) layer for feature learning and classification. We demonstrate that our method gains high-quality fixed-length features from variable-length sequences and consistently outperforms baseline methods. We show that k -mer embedding can effectively enhance model performance by exploring different embedding strategies. We also prove the efficacy of both the convolution and the BLSTM layers by comparing two variations of the network architecture. We confirm the robustness of our model to hyper-parameters by performing sensitivity analysis. We hope our method can eventually reinforce our understanding of employing deep learning in genomic studies and shed light on research regarding mechanisms of chromatin accessibility.</abstracttext></p><h4>Availability and implementation: </h4><p><abstracttext label='Availability and implementation' nlmcategory='UNASSIGNED'>The source code can be downloaded from https://github.com/minxueric/ismb2017_lstm .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>tingchen@tsinghua.edu.cn or ruijiang@tsinghua.edu.cn.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary materials are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28881969,pubmed,2017,a5bc5f35-64a0-4db4-837d-22d5ab65e876,1
reconstructing cell cycle and disease progression using deep learning,/pubmed/28878212,"Eulenberg P, Köhler N, Blasi T, Filby A, Carpenter AE, Rees P, Theis FJ, Wolf FA.",Nat Commun. 2017 Sep 6;8(1):463. doi: 10.1038/s41467-017-00623-3.,Nat Commun.  2017,PubMed,citation,PMID:28878212 | PMCID:PMC5587733,pubmed,28878212,create date:2017/09/08 | first author:Eulenberg P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We show that deep convolutional neural networks combined with nonlinear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a sixfold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer.The interpretation of information-rich, high-throughput single-cell data is a challenge requiring sophisticated computational tools. Here the authors demonstrate a deep convolutional neural network that can classify cell cycle status on-the-fly.</abstracttext></p></div></div>",fabian.theis@helmholtz-muenchen.de,,https://www.ncbi.nlm.nih.gov//pubmed/28878212,pubmed,2017,d90f84eb-72be-4d95-9ccf-32101f4c29aa,1
very deep convolutional neural networks for morphologic classification of erythrocytes,/pubmed/28877918,"Durant TJS, Olson EM, Schulz WL, Torres R.",Clin Chem. 2017 Sep 6. pii: clinchem.2017.276345. doi: 10.1373/clinchem.2017.276345. [Epub ahead of print],Clin Chem.  2017,PubMed,citation,PMID:28877918,pubmed,28877918,create date:2017/09/08 | first author:Durant TJS,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Morphologic profiling of the erythrocyte population is a widely used and clinically valuable diagnostic modality, but one that relies on a slow manual process associated with significant labor cost and limited reproducibility. Automated profiling of erythrocytes from digital images by capable machine learning approaches would augment the throughput and value of morphologic analysis. To this end, we sought to evaluate the performance of leading implementation strategies for convolutional neural networks (CNNs) when applied to classification of erythrocytes based on morphology.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Erythrocytes were manually classified into 1 of 10 classes using a custom-developed Web application. Using recent literature to guide architectural considerations for neural network design, we implemented a 'very deep' CNN, consisting of &gt;150 layers, with dense shortcut connections.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The final database comprised 3737 labeled cells. Ensemble model predictions on unseen data demonstrated a harmonic mean of recall and precision metrics of 92.70% and 89.39%, respectively. Of the 748 cells in the test set, 23 misclassification errors were made, with a correct classification frequency of 90.60%, represented as a harmonic mean across the 10 morphologic classes.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>These findings indicate that erythrocyte morphology profiles could be measured with a high degree of accuracy with 'very deep' CNNs. Further, these data support future efforts to expand classes and optimize practical performance in a clinical environment as a prelude to full implementation as a clinical tool.</abstracttext></p><p class='copyright'>© 2017 American Association for Clinical Chemistry.</p></div></div>",richard.torres@yale.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28877918,pubmed,2017,4f8a8809-4f26-4b1a-a153-ac3679756fe4,1
deeppep: deep proteome inference from peptide profiles,/pubmed/28873403,"Kim M, Eetemadi A, Tagkopoulos I.",PLoS Comput Biol. 2017 Sep 5;13(9):e1005661. doi: 10.1371/journal.pcbi.1005661. eCollection 2017 Sep.,PLoS Comput Biol.  2017,PubMed,citation,PMID:28873403,pubmed,28873403,create date:2017/09/06 | first author:Kim M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Protein inference, the identification of the protein set that is the origin of a given peptide profile, is a fundamental challenge in proteomics. We present DeepPep, a deep-convolutional neural network framework that predicts the protein set from a proteomics mixture, given the sequence universe of possible proteins and a target peptide profile. In its core, DeepPep quantifies the change in probabilistic score of peptide-spectrum matches in the presence or absence of a specific protein, hence selecting as candidate proteins with the largest impact to the peptide profile. Application of the method across datasets argues for its competitive predictive ability (AUC of 0.80±0.18, AUPR of 0.84±0.28) in inferring proteins without need of peptide detectability on which the most competitive methods rely. We find that the convolutional neural network architecture outperforms the traditional artificial neural network architectures without convolution layers in protein inference. We expect that similar deep learning architectures that allow learning nonlinear patterns can be further extended to problems in metagenome profiling and cell type inference. The source code of DeepPep and the benchmark datasets used in this study are available at https://deeppep.github.io/DeepPep/.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28873403,pubmed,2017,e335d6a4-2ea2-4615-bb11-0e68a1185c70,1
a deep convolutional neural network model to classify heartbeats,/pubmed/28869899,"Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adam M, Gertych A, Tan RS.",Comput Biol Med. 2017 Aug 24;89:389-396. doi: 10.1016/j.compbiomed.2017.08.022. [Epub ahead of print],Comput Biol Med.  2017,PubMed,citation,PMID:28869899,pubmed,28869899,create date:2017/09/05 | first author:Acharya UR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The electrocardiogram (ECG) is a standard test used to monitor the activity of the heart. Many cardiac abnormalities will be manifested in the ECG including arrhythmia which is a general term that refers to an abnormal heart rhythm. The basis of arrhythmia diagnosis is the identification of normal versus abnormal individual heart beats, and their correct classification into different diagnoses, based on ECG morphology. Heartbeats can be sub-divided into five categories namely non-ectopic, supraventricular ectopic, ventricular ectopic, fusion, and unknown beats. It is challenging and time-consuming to distinguish these heartbeats on ECG as these signals are typically corrupted by noise. We developed a 9-layer deep convolutional neural network (CNN) to automatically identify 5 different categories of heartbeats in ECG signals. Our experiment was conducted in original and noise attenuated sets of ECG signals derived from a publicly available database. This set was artificially augmented to even out the number of instances the 5 classes of heartbeats and filtered to remove high-frequency noise. The CNN was trained using the augmented data and achieved an accuracy of 94.03% and 93.47% in the diagnostic classification of heartbeats in original and noise free ECGs, respectively. When the CNN was trained with highly imbalanced data (original dataset), the accuracy of the CNN reduced to 89.07%% and 89.3% in noisy and noise-free ECGs. When properly trained, the proposed CNN model can serve as a tool for screening of ECG to quickly identify different types and frequency of arrhythmic heartbeats.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",aru@np.edu.sg,Arrhythmia; Cardiovascular diseases; Convolutional neural network; Deep learning; Electrocardiogram signals; Heartbeat; PhysioBank MIT-BIH arrhythmia database,https://www.ncbi.nlm.nih.gov//pubmed/28869899,pubmed,2017,15a24873-8c8c-43a0-8656-26825c560e27,1
capturing complex 3d human motions with kernelized low-rank representation from monocular rgb camera,/pubmed/28869514,"Wang X, Wang F, Chen Y.",Sensors (Basel). 2017 Sep 3;17(9). pii: E2019. doi: 10.3390/s17092019.,Sensors (Basel).  2017,PubMed,citation,PMID:28869514,pubmed,28869514,create date:2017/09/05 | first author:Wang X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recovering 3D structures from the monocular image sequence is an inherently ambiguous problem that has attracted considerable attention from several research communities. To resolve the ambiguities, a variety of additional priors, such as low-rank shape basis, have been proposed. In this paper, we make two contributions. First, we introduce an assumption that 3D structures lie on the union of nonlinear subspaces. Based on this assumption, we propose a Non-Rigid Structure from Motion (NRSfM) method with kernelized low-rank representation. To be specific, we utilize the soft-inextensibility constraint to accurately recover 3D human motions. Second, we extend this NRSfM method to the marker-less 3D human pose estimation problem by combining with Convolutional Neural Network (CNN) based 2D human joint detectors. To evaluate the performance of our methods, we apply our marker-based method on several sequences from Utrecht Multi-Person Motion (UMPM) benchmark and CMU MoCap datasets, and then apply the marker-less method on the Human3.6M datasets. The experiments demonstrate that the kernelized low-rank representation is more suitable for modeling the complex deformation and the method consequently yields more accurate reconstructions. Benefiting from the CNN-based detector, the marker-less approach can be applied to more real-life applications.</abstracttext></p></div></div>",xwang.cv@gmail.com,3D human pose estimation; kernel low-rank representation; monocular reconstruction; non-rigid structure from motion,https://www.ncbi.nlm.nih.gov//pubmed/28869514,pubmed,2017,6e2738fd-baa1-4041-94d9-2a32fa248317,1
"automatic categorization and scoring of solid, part-solid and non-solid pulmonary nodules in ct images with convolutional neural network",/pubmed/28864824,"Tu X, Xie M, Gao J, Ma Z, Chen D, Wang Q, Finlayson SG, Ou Y, Cheng JZ.",Sci Rep. 2017 Sep 1;7(1):8533. doi: 10.1038/s41598-017-08040-8.,Sci Rep.  2017,PubMed,citation,PMID:28864824 | PMCID:PMC5581338,pubmed,28864824,create date:2017/09/03 | first author:Tu X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We present a computer-aided diagnosis system (CADx) for the automatic categorization of solid, part-solid and non-solid nodules in pulmonary computerized tomography images using a Convolutional Neural Network (CNN). Provided with only a two-dimensional region of interest (ROI) surrounding each nodule, our CNN automatically reasons from image context to discover informative computational features. As a result, no image segmentation processing is needed for further analysis of nodule attenuation, allowing our system to avoid potential errors caused by inaccurate image processing. We implemented two computerized texture analysis schemes, classification and regression, to automatically categorize solid, part-solid and non-solid nodules in CT scans, with hierarchical features in each case learned directly by the CNN model. To show the effectiveness of our CNN-based CADx, an established method based on histogram analysis (HIST) was implemented for comparison. The experimental results show significant performance improvement by the CNN model over HIST in both classification and regression tasks, yielding nodule classification and rating performance concordant with those of practicing radiologists. Adoption of CNN-based CADx systems may reduce the inter-observer variation among screening radiologists and provide a quantitative reference for further nodule analysis.</abstracttext></p></div></div>",mxie@uestc.edu.cn,,https://www.ncbi.nlm.nih.gov//pubmed/28864824,pubmed,2017,81d6c0eb-9cc3-4cb0-83d9-c67b86ef8c1c,1
classifying patient portal messages using convolutional neural networks,/pubmed/28864104,"Sulieman L, Gilmore D, French C, Cronin RM, Purcell Jackson G, Russell M, Fabbri D.",J Biomed Inform. 2017 Aug 29. pii: S1532-0464(17)30197-1. doi: 10.1016/j.jbi.2017.08.014. [Epub ahead of print],J Biomed Inform.  2017,PubMed,citation,PMID:28864104,pubmed,28864104,create date:2017/09/03 | first author:Sulieman L,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>atients communicate with healthcare providers via secure messaging in patient portals. As patient portal adoption increases, growing messaging volumes may overwhelm providers. Prior research has demonstrated promise in automating classification of patient portal messages into communication types to support message triage or answering. This paper examines if using semantic features and word context improves portal message classification.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>ortal messages were classified into the following categories: informational, medical, social, and logistical. We constructed features from portal messages including bag of words, bag of phrases, graph representations, and word embeddings. We trained one-versus-all random forest and logistic regression classifiers, and convolutional neural network (CNN) with a softmax output. We evaluated each classifier's performance using Area Under the Curve (AUC).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Representing the messages using bag of words, the random forest detected informational, medical, social, and logistical communications in patient portal messages with AUCs: 0.803, 0.884, 0.828, and 0.928, respectively. Graph representations of messages outperformed simpler features with AUCs: 0.837, 0.914, 0.846, 0.884 for informational, medical, social, and logistical communication, respectively. Representing words with Word2Vec embeddings, and mapping features using a CNN had the best performance with AUCs: 0.908 for informational, 0.917 for medical, 0.935 for social, and 0.943 for logistical categories.</abstracttext></p><h4>DISCUSSION AND CONCLUSION: </h4><p><abstracttext label='DISCUSSION AND CONCLUSION' nlmcategory='CONCLUSIONS'>Word2Vec and graph representations improved the accuracy of classifying portal messages compared to features that lacked semantic information such as bag of words, and bag of phrases. Furthermore, using Word2Vec along with a CNN model, which provide a higher order representation, improved the classification of portal messages.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Inc.</p></div></div>",lina.m.sulieman@vanderbilt.edu,Convolutional Neural Network; Patient portals; Text mining; Word Embedding; Word2Vec,https://www.ncbi.nlm.nih.gov//pubmed/28864104,pubmed,2017,713c1721-a686-444e-b69c-e6590a95cce1,1
automatic detection of new tumors and tumor burden evaluation in longitudinal liver ct scan studies,/pubmed/28856515,"Vivanti R, Szeskin A, Lev-Cohain N, Sosna J, Joskowicz L.",Int J Comput Assist Radiol Surg. 2017 Aug 30. doi: 10.1007/s11548-017-1660-z. [Epub ahead of print],Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28856515,pubmed,28856515,create date:2017/09/01 | first author:Vivanti R,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Radiological longitudinal follow-up of liver tumors in CT scans is the standard of care for disease progression assessment and for liver tumor therapy. Finding new tumors in the follow-up scan is essential to determine malignancy, to evaluate the total tumor burden, and to determine treatment efficacy. Since new tumors are typically small, they may be missed by examining radiologists.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We describe a new method for the automatic detection and segmentation of new tumors in longitudinal liver CT studies and for liver tumors burden quantification. Its inputs are the baseline and follow-up CT scans, the baseline tumors delineation, and a tumor appearance prior model. Its outputs are the new tumors segmentations in the follow-up scan, the tumor burden quantification in both scans, and the tumor burden change. Our method is the first comprehensive method that is explicitly designed to find new liver tumors. It integrates information from the scans, the baseline known tumors delineations, and a tumor appearance prior model in the form of a global convolutional neural network classifier. Unlike other deep learning-based methods, it does not require large tagged training sets.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our experimental results on 246 tumors, of which 97 were new tumors, from 37 longitudinal liver CT studies with radiologist approved ground-truth segmentations, yields a true positive new tumors detection rate of 86 versus 72% with stand-alone detection, and a tumor burden volume overlap error of 16%.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>New tumors detection and tumor burden volumetry are important for diagnosis and treatment. Our new method enables a simplified radiologist-friendly workflow that is potentially more accurate and reliable than the existing one by automatically and accurately following known tumors and detecting new tumors in the follow-up scan.</abstracttext></p></div></div>",refael.vivanti@mail.huji.ac.il,Liver tumors detection; Liver tumors segmentation; Longitudinal CT study; Radiological assessment,https://www.ncbi.nlm.nih.gov//pubmed/28856515,pubmed,2017,1f05d68c-ac7d-438d-8f24-29672b86f774,1
cnn based malaria diagnosis from focus-stack of blood smear images acquired using custom-built slide scanner,/pubmed/28851134,"Gopakumar G, Swetha M, Gorthi SS, Sai Subrahmanyam GRK.",J Biophotonics. 2017 Aug 29. doi: 10.1002/jbio.201700003. [Epub ahead of print],J Biophotonics.  2017,PubMed,citation,PMID:28851134,pubmed,28851134,create date:2017/08/30 | first author:Gopakumar G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper introduces a focus stacking based approach for automated quantitative detection of Plasmodium falciparum malaria from blood smear. For the detection, a custom designed convolutional neural network (CNN) operating on focus stack of images is used. The cell counting problem is addressed as the segmentation problem and we propose a two level segmentation strategy. Use of CNN operating on focus stack for the detection of malaria is first of its kind, and it not only improved the detection accuracy (both in terms of sensitivity (97.06%) and specificity (98.50%)) but also favoured the processing on cell patches and avoided the need for hand-engineered features. The slide images are acquired with a custom-built portable slide scanner made from low-cost, off-the-shelf components and is suitable for Point-of-Care Diagnostics. The proposed approach of employing sophisticated algorithmic processing together with inexpensive instrumentation can potentially benefit clinicians to enable malaria diagnosis.</abstracttext></p><p class='copyright'>This article is protected by copyright. All rights reserved.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28851134,pubmed,2017,bceca480-d8a9-4c6e-81dc-740cb9bf7964,1
co-trained convolutional neural networks for automated detection of prostate cancer in multi-parametric mri,/pubmed/28850876,"Yang X, Liu C, Wang Z, Yang J, Min HL, Wang L, Cheng KT.",Med Image Anal. 2017 Aug 24;42:212-227. doi: 10.1016/j.media.2017.08.006. [Epub ahead of print],Med Image Anal.  2017,PubMed,citation,PMID:28850876,pubmed,28850876,create date:2017/08/30 | first author:Yang X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Multi-parameter magnetic resonance imaging (mp-MRI) is increasingly popular for prostate cancer (PCa) detection and diagnosis. However, interpreting mp-MRI data which typically contains multiple unregistered 3D sequences, e.g. apparent diffusion coefficient (ADC) and T2-weighted (T2w) images, is time-consuming and demands special expertise, limiting its usage for large-scale PCa screening. Therefore, solutions to computer-aided detection of PCa in mp-MRI images are highly desirable. Most recent advances in automated methods for PCa detection employ a handcrafted feature based two-stage classification flow, i.e. voxel-level classification followed by a region-level classification. This work presents an automated PCa detection system which can concurrently identify the presence of PCa in an image and localize lesions based on deep convolutional neural network (CNN) features and a single-stage SVM classifier. Specifically, the developed co-trained CNNs consist of two parallel convolutional networks for ADC and T2w images respectively. Each network is trained using images of a single modality in a weakly-supervised manner by providing a set of prostate images with image-level labels indicating only the presence of PCa without priors of lesions' locations. Discriminative visual patterns of lesions can be learned effectively from clutters of prostate and surrounding tissues. A cancer response map with each pixel indicating the likelihood to be cancerous is explicitly generated at the last convolutional layer of the network for each modality. A new back-propagated error E is defined to enforce both optimized classification results and consistent cancer response maps for different modalities, which help capture highly representative PCa-relevant features during the CNN feature learning process. The CNN features of each modality are concatenated and fed into a SVM classifier. For images which are classified to contain cancers, non-maximum suppression and adaptive thresholding are applied to the corresponding cancer response maps for PCa foci localization. Evaluation based on 160 patient data with 12-core systematic TRUS-guided prostate biopsy as the reference standard demonstrates that our system achieves a sensitivity of 0.46, 0.92 and 0.97 at 0.1, 1 and 10 false positives per normal/benign patient which is significantly superior to two state-of-the-art CNN-based methods (Oquab et al., 2015; Zhou et al., 2015) and 6-core systematic prostate biopsies.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",zhiweiwang@hust.edu.cn,Cancer response map; Co-trained CNN; Convolutional neural network; Prostate biopsy; Prostate cancer detection,https://www.ncbi.nlm.nih.gov//pubmed/28850876,pubmed,2017,2a6422ec-65e1-4196-9d2b-9894fd85d2e4,1
3d convolutional neural network for automatic detection of lung nodules in chest ct,/pubmed/28845077,"Hamidian S, Sahiner B, Petrick N, Pezeshk A.",Proc SPIE Int Soc Opt Eng. 2017;10134. pii: 1013409. doi: 10.1117/12.2255795. Epub 2017 Mar 3.,Proc SPIE Int Soc Opt Eng.  2017,PubMed,citation,PMID:28845077 | PMCID:PMC5568782,pubmed,28845077,create date:2017/08/29 | first author:Hamidian S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep convolutional neural networks (CNNs) form the backbone of many state-of-the-art computer vision systems for classification and segmentation of 2D images. The same principles and architectures can be extended to three dimensions to obtain 3D CNNs that are suitable for volumetric data such as CT scans. In this work, we train a 3D CNN for automatic detection of pulmonary nodules in chest CT images using volumes of interest extracted from the LIDC dataset. We then convert the 3D CNN which has a fixed field of view to a 3D fully convolutional network (FCN) which can generate the score map for the entire volume efficiently in a single pass. Compared to the sliding window approach for applying a CNN across the entire input volume, the FCN leads to a nearly 800-fold speed-up, and thereby fast generation of output scores for a single case. This screening FCN is used to generate difficult negative examples that are used to train a new discriminant CNN. The overall system consists of the screening FCN for fast generation of candidate regions of interest, followed by the discrimination CNN.</abstracttext></p></div></div>",,Deep learning; chest CT; computer-aided diagnosis; convolutional neural networks,https://www.ncbi.nlm.nih.gov//pubmed/28845077,pubmed,2017,0fdb4264-a47d-4bce-8be1-37defde2dad8,1
constructing fine-granularity functional brain network atlases via deep convolutional autoencoder,/pubmed/28843214,"Zhao Y, Dong Q, Chen H, Iraji A, Li Y, Makkie M, Kou Z, Liu T.",Med Image Anal. 2017 Aug 18;42:200-211. doi: 10.1016/j.media.2017.08.005. [Epub ahead of print],Med Image Anal.  2017,PubMed,citation,PMID:28843214,pubmed,28843214,create date:2017/08/27 | first author:Zhao Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>State-of-the-art functional brain network reconstruction methods such as independent component analysis (ICA) or sparse coding of whole-brain fMRI data can effectively infer many thousands of volumetric brain network maps from a large number of human brains. However, due to the variability of individual brain networks and the large scale of such networks needed for statistically meaningful group-level analysis, it is still a challenging and open problem to derive group-wise common networks as network atlases. Inspired by the superior spatial pattern description ability of the deep convolutional neural networks (CNNs), a novel deep 3D convolutional autoencoder (CAE) network is designed here to extract spatial brain network features effectively, based on which an Apache Spark enabled computational framework is developed for fast clustering of larger number of network maps into fine-granularity atlases. To evaluate this framework, 10 resting state networks (RSNs) were manually labeled from the sparsely decomposed networks of Human Connectome Project (HCP) fMRI data and 5275 network training samples were obtained, in total. Then the deep CAE models are trained by these functional networks' spatial maps, and the learned features are used to refine the original 10 RSNs into 17 network atlases that possess fine-granularity functional network patterns. Interestingly, it turned out that some manually mislabeled outliers in training networks can be corrected by the deep CAE derived features. More importantly, fine granularities of networks can be identified and they reveal unique network patterns specific to different brain task states. By further applying this method to a dataset of mild traumatic brain injury study, it shows that the technique can effectively identify abnormal small networks in brain injury patients in comparison with controls. In general, our work presents a promising deep learning and big data analysis solution for modeling functional connectomes, with fine granularities, based on fMRI data.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",tliu@uga.edu,Deep learning; Functional brain networks; fMRI,https://www.ncbi.nlm.nih.gov//pubmed/28843214,pubmed,2017,0a168f1c-aa57-41d3-ba97-2326c731307e,1
personalized monitoring and advance warning system for cardiac arrhythmias,/pubmed/28839215,"Kiranyaz S, Ince T, Gabbouj M.",Sci Rep. 2017 Aug 24;7(1):9270. doi: 10.1038/s41598-017-09544-z.,Sci Rep.  2017,PubMed,citation,PMID:28839215 | PMCID:PMC5571226,pubmed,28839215,create date:2017/08/26 | first author:Kiranyaz S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Each year more than 7 million people die from cardiac arrhythmias. Yet no robust solution exists today to detect such heart anomalies right at the moment they occur. The purpose of this study was to design a personalized health monitoring system that can detect early occurrences of arrhythmias from an individual's electrocardiogram (ECG) signal. We first modelled the common causes of arrhythmias in the signal domain as a degradation of normal ECG beats to abnormal beats. Using the degradation models, we performed abnormal beat synthesis which created potential abnormal beats from the average normal beat of the individual. Finally, a Convolutional Neural Network (CNN) was trained using real normal and synthesized abnormal beats. As a personalized classifier, the trained CNN can monitor ECG beats in real time for arrhythmia detection. Over 34 patients' ECG records with a total of 63,341 ECG beats from the MIT-BIH arrhythmia benchmark database, we have shown that the probability of detecting one or more abnormal ECG beats among the first three occurrences is higher than 99.4% with a very low false-alarm rate.</abstracttext></p></div></div>",mkiranyaz@qu.edu.qa,,https://www.ncbi.nlm.nih.gov//pubmed/28839215,pubmed,2017,52301a73-3bbc-4ee3-874b-5c9545961396,1
identification and segmentation of myelinated nerve fibers in a cross-sectional optical microscopic image using a deep learning model,/pubmed/28837816,"Naito T, Nagashima Y, Taira K, Uchio N, Tsuji S, Shimizu J.",J Neurosci Methods. 2017 Aug 26;291:141-149. doi: 10.1016/j.jneumeth.2017.08.014. [Epub ahead of print],J Neurosci Methods.  2017,PubMed,citation,PMID:28837816,pubmed,28837816,create date:2017/08/25 | first author:Naito T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>The morphometric analysis of myelinated nerve fibers of peripheral nerves in cross-sectional optical microscopic images is valuable. Several automated methods for nerve fiber identification and segmentation have been reported. This paper presents a new method that uses a deep learning model of a convolutional neural network (CNN). We tested it for human sural nerve biopsy images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The method comprises four steps: normalization, clustering segmentation, myelinated nerve fiber identification, and clump splitting. A normalized sample image was separated into individual objects with clustering segmentation. Each object was applied to a CNN deep learning model that labeled myelinated nerve fibers as positive and other structures as negative. Only positives proceeded to the next step. For pretraining the model, 70,000 positive and negative data each from 39 samples were used. The accuracy of the proposed algorithm was evaluated using 10 samples that were not part of the training set. A P-value of &lt;0.05 was considered statistically significant.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The total true-positive rate (TPR) for the detection of myelinated fibers was 0.982, and the total false-positive rate was 0.016. The defined total area similarity (AS) and area overlap error of segmented myelin sheaths were 0.967 and 0.068, respectively. In all but one sample, there were no significant differences in estimated morphometric parameters obtained from our method and manual segmentation.</abstracttext></p><h4>COMPARISON WITH EXISTING METHODS: </h4><p><abstracttext label='COMPARISON WITH EXISTING METHODS' nlmcategory='UNASSIGNED'>The TPR and AS were higher than those obtained using previous methods.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>High-performance automated identification and segmentation of myelinated nerve fibers were achieved using a deep learning model.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",tanaitou-tky@umin.ac.jp,,https://www.ncbi.nlm.nih.gov//pubmed/28837816,pubmed,2017,fc542e66-8cc7-48ed-8473-8841be0d239e,1
deep neural network-based computer-assisted detection of cerebral aneurysms in mr angiography,/pubmed/28836310,"Nakao T, Hanaoka S, Nomura Y, Sato I, Nemoto M, Miki S, Maeda E, Yoshikawa T, Hayashi N, Abe O.",J Magn Reson Imaging. 2017 Aug 24. doi: 10.1002/jmri.25842. [Epub ahead of print],J Magn Reson Imaging.  2017,PubMed,citation,PMID:28836310,pubmed,28836310,create date:2017/08/25 | first author:Nakao T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>The usefulness of computer-assisted detection (CAD) for detecting cerebral aneurysms has been reported; therefore, the improved performance of CAD will help to detect cerebral aneurysms.</abstracttext></p><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>To develop a CAD system for intracranial aneurysms on unenhanced magnetic resonance angiography (MRA) images based on a deep convolutional neural network (CNN) and a maximum intensity projection (MIP) algorithm, and to demonstrate the usefulness of the system by training and evaluating it using a large dataset.</abstracttext></p><h4>STUDY TYPE: </h4><p><abstracttext label='STUDY TYPE' nlmcategory='METHODS'>Retrospective study.</abstracttext></p><h4>SUBJECTS: </h4><p><abstracttext label='SUBJECTS' nlmcategory='METHODS'>There were 450 cases with intracranial aneurysms. The diagnoses of brain aneurysms were made on the basis of MRA, which was performed as part of a brain screening program.</abstracttext></p><h4>FIELD STRENGTH/SEQUENCE: </h4><p><abstracttext label='FIELD STRENGTH/SEQUENCE' nlmcategory='UNASSIGNED'>Noncontrast-enhanced 3D time-of-flight (TOF) MRA on 3T MR scanners.</abstracttext></p><h4>ASSESSMENT: </h4><p><abstracttext label='ASSESSMENT' nlmcategory='RESULTS'>In our CAD, we used a CNN classifier that predicts whether each voxel is inside or outside aneurysms by inputting MIP images generated from a volume of interest (VOI) around the voxel. The CNN was trained in advance using manually inputted labels. We evaluated our method using 450 cases with intracranial aneurysms, 300 of which were used for training, 50 for parameter tuning, and 100 for the final evaluation.</abstracttext></p><h4>STATISTICAL TESTS: </h4><p><abstracttext label='STATISTICAL TESTS' nlmcategory='UNASSIGNED'>Free-response receiver operating characteristic (FROC) analysis.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our CAD system detected 94.2% (98/104) of aneurysms with 2.9 false positives per case (FPs/case). At a sensitivity of 70%, the number of FPs/case was 0.26.</abstracttext></p><h4>DATA CONCLUSION: </h4><p><abstracttext label='DATA CONCLUSION' nlmcategory='UNASSIGNED'>We showed that the combination of a CNN and an MIP algorithm is useful for the detection of intracranial aneurysms.</abstracttext></p><h4>LEVEL OF EVIDENCE: </h4><p><abstracttext label='LEVEL OF EVIDENCE' nlmcategory='METHODS'>4 Technical Efficacy Stage 1 J. Magn. Reson. Imaging 2017.</abstracttext></p><p class='copyright'>© 2017 International Society for Magnetic Resonance in Medicine.</p></div></div>",,cerebral aneurysm; computer-assisted detection; convolutional neural network,https://www.ncbi.nlm.nih.gov//pubmed/28836310,pubmed,2017,d2096183-d148-4100-8ea7-0c5518955128,1
detection of high-grade small bowel obstruction on conventional radiography with convolutional neural networks,/pubmed/28828625,"Cheng PM, Tejura TK, Tran KN, Whang G.",Abdom Radiol (NY). 2017 Aug 21. doi: 10.1007/s00261-017-1294-1. [Epub ahead of print],Abdom Radiol (NY).  2017,PubMed,citation,PMID:28828625,pubmed,28828625,create date:2017/08/23 | first author:Cheng PM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The purpose of this pilot study is to determine whether a deep convolutional neural network can be trained with limited image data to detect high-grade small bowel obstruction patterns on supine abdominal radiographs. Grayscale images from 3663 clinical supine abdominal radiographs were categorized into obstructive and non-obstructive categories independently by three abdominal radiologists, and the majority classification was used as ground truth; 74 images were found to be consistent with small bowel obstruction. Images were rescaled and randomized, with 2210 images constituting the training set (39 with small bowel obstruction) and 1453 images constituting the test set (35 with small bowel obstruction). Weight parameters for the final classification layer of the Inception v3 convolutional neural network, previously trained on the 2014 Large Scale Visual Recognition Challenge dataset, were retrained on the training set. After training, the neural network achieved an AUC of 0.84 on the test set (95% CI 0.78-0.89). At the maximum Youden index (sensitivity + specificity-1), the sensitivity of the system for small bowel obstruction is 83.8%, with a specificity of 68.1%. The results demonstrate that transfer learning with convolutional neural networks, even with limited training data, may be used to train a detector for high-grade small bowel obstruction gas patterns on supine radiographs.</abstracttext></p></div></div>",phillip.cheng@med.usc.edu,Artificial neural networks; Deep learning; Digital image processing; Machine learning; Small bowel obstruction,https://www.ncbi.nlm.nih.gov//pubmed/28828625,pubmed,2017,f3ccd87f-4027-4221-bd04-282cd684dd14,1
convolutional neural network for the detection of end-diastole and end-systole frames in free-breathing cardiac magnetic resonance imaging,/pubmed/28814965,"Yang F, He Y, Hussain M, Xie H, Lei P.",Comput Math Methods Med. 2017;2017:1640835. doi: 10.1155/2017/1640835. Epub 2017 Jul 26.,Comput Math Methods Med.  2017,PubMed,citation,PMID:28814965 | PMCID:PMC5549469,pubmed,28814965,create date:2017/08/18 | first author:Yang F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Free-breathing cardiac magnetic resonance (CMR) imaging has short examination time with high reproducibility. Detection of the end-diastole and the end-systole frames of the free-breathing cardiac magnetic resonance, supplemented by visual identification, is time consuming and laborious. We propose a novel method for automatic identification of both the end-diastole and the end-systole frames, in the free-breathing CMR imaging. The proposed technique utilizes the convolutional neural network to locate the left ventricle and to obtain the end-diastole and the end-systole frames from the respiratory motion signal. The proposed procedure works successfully on our free-breathing CMR data, and the results demonstrate a high degree of accuracy and stability. Convolutional neural network improves the postprocessing efficiency greatly and facilitates the clinical application of the free-breathing CMR imaging.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28814965,pubmed,2017,4e0cdfcb-2699-4a86-b58e-6b1c1578155f,1
a neural network multi-task learning approach to biomedical named entity recognition,/pubmed/28810903,"Crichton G, Pyysalo S, Chiu B, Korhonen A.",BMC Bioinformatics. 2017 Aug 15;18(1):368. doi: 10.1186/s12859-017-1776-8.,BMC Bioinformatics.  2017,PubMed,citation,PMID:28810903 | PMCID:PMC5558737,pubmed,28810903,create date:2017/08/16 | first author:Crichton G,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Named Entity Recognition (NER) is a key task in biomedical text mining. Accurate NER systems require task-specific, manually-annotated datasets, which are expensive to develop and thus limited in size. Since such datasets contain related but different information, an interesting question is whether it might be possible to use them together to improve NER performance. To investigate this, we develop supervised, multi-task, convolutional neural network models and apply them to a large number of varied existing biomedical named entity datasets. Additionally, we investigated the effect of dataset size on performance in both single- and multi-task settings.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We present a single-task model for NER, a Multi-output multi-task model and a Dependent multi-task model. We apply the three models to 15 biomedical datasets containing multiple named entities including Anatomy, Chemical, Disease, Gene/Protein and Species. Each dataset represent a task. The results from the single-task model and the multi-task models are then compared for evidence of benefits from Multi-task Learning. With the Multi-output multi-task model we observed an average F-score improvement of 0.8% when compared to the single-task model from an average baseline of 78.4%. Although there was a significant drop in performance on one dataset, performance improves significantly for five datasets by up to 6.3%. For the Dependent multi-task model we observed an average improvement of 0.4% when compared to the single-task model. There were no significant drops in performance on any dataset, and performance improves significantly for six datasets by up to 1.1%. The dataset size experiments found that as dataset size decreased, the multi-output model's performance increased compared to the single-task model's. Using 50, 25 and 10% of the training data resulted in an average drop of approximately 3.4, 8 and 16.7% respectively for the single-task model but approximately 0.2, 3.0 and 9.8% for the multi-task model.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Our results show that, on average, the multi-task models produced better NER results than the single-task models trained on a single NER dataset. We also found that Multi-task Learning is beneficial for small datasets. Across the various settings the improvements are significant, demonstrating the benefit of Multi-task Learning for this task.</abstracttext></p></div></div>",gkoc2@cam.ac.uk,Biomedical text mining; Convolutional neural networks; Multi-task learning; Named entity recognition,https://www.ncbi.nlm.nih.gov//pubmed/28810903,pubmed,2017,b5763cb0-475e-45de-be44-a67ca182fbdc,1
a new method for automatic sleep stage classification,/pubmed/28809709,"Zhang J, Wu Y.",IEEE Trans Biomed Circuits Syst. 2017 Aug 14. doi: 10.1109/TBCAS.2017.2719631. [Epub ahead of print],IEEE Trans Biomed Circuits Syst.  2017,PubMed,citation,PMID:28809709,pubmed,28809709,create date:2017/08/16 | first author:Zhang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Traditionally, automatic sleep stage classification is quite a challenging task because of the difficulty in translating open-textured standards to mathematical models and the limitations of handcrafted features. In this paper, a new system for automatic sleep stage classification is presented. Compared with existing sleep stage methods, our method can capture the sleep information hidden inside electroencephalography (EEG) signals and automatically extract features from raw data. To translate open sleep stage standards into machine rules recognized by computers, a new model named fast discriminative complex-valued convolutional neural network (FDCCNN) is proposed to extract features from raw EEG data and classify sleep stages. The new model combines complex-valued backpropagation and the Fisher criterion. It can learn discriminative features and overcome the negative effect of imbalance dataset. More importantly, the orthogonal decision boundaries for the real and imaginary parts of a complex-valued convolutional neuron are proven. A speed-up algorithm is proposed to reduce computational workload and yield improvements of over an order of magnitude compared to the normal convolution algorithm. The classification performances of handcrafted features and different convolutional neural networks are compared with that of the FDCCNN. The total accuracy and kappa coefficient of the proposed method are 92% and 0.84, respectively. Experiment results demonstrated that the performance of our system is comparable to those of human experts.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28809709,pubmed,2017,359617fb-292c-4463-a835-7ef7241e8396,1
pre-trained convolutional neural networks as feature extractors for tuberculosis detection,/pubmed/28800442,"Lopes UK, Valiati JF.",Comput Biol Med. 2017 Aug 4;89:135-143. doi: 10.1016/j.compbiomed.2017.08.001. [Epub ahead of print],Comput Biol Med.  2017,PubMed,citation,PMID:28800442,pubmed,28800442,create date:2017/08/12 | first author:Lopes UK,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>It is estimated that in 2015, approximately 1.8 million people infected by tuberculosis died, most of them in developing countries. Many of those deaths could have been prevented if the disease had been detected at an earlier stage, but the most advanced diagnosis methods are still cost prohibitive for mass adoption. One of the most popular tuberculosis diagnosis methods is the analysis of frontal thoracic radiographs; however, the impact of this method is diminished by the need for individual analysis of each radiography by properly trained radiologists. Significant research can be found on automating diagnosis by applying computational techniques to medical images, thereby eliminating the need for individual image analysis and greatly diminishing overall costs. In addition, recent improvements on deep learning accomplished excellent results classifying images on diverse domains, but its application for tuberculosis diagnosis remains limited. Thus, the focus of this work is to produce an investigation that will advance the research in the area, presenting three proposals to the application of pre-trained convolutional neural networks as feature extractors to detect the disease. The proposals presented in this work are implemented and compared to the current literature. The obtained results are competitive with published works demonstrating the potential of pre-trained convolutional networks as medical image feature extractors.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",joao.valiati@ai-engineers.com,Computer assisted diagnosis; Convolutional neural networks; Deep learning; Ensemble learning; Multiple instance learning; Tuberculosis,https://www.ncbi.nlm.nih.gov//pubmed/28800442,pubmed,2017,65a02558-a830-4b3b-a7df-90b686671914,1
holographic deep learning for rapid optical screening of anthrax spores,/pubmed/28798957,"Jo Y, Park S, Jung J, Yoon J, Joo H, Kim MH, Kang SJ, Choi MC, Lee SY, Park Y.",Sci Adv. 2017 Aug 4;3(8):e1700606. doi: 10.1126/sciadv.1700606. eCollection 2017 Aug.,Sci Adv.  2017,PubMed,citation,PMID:28798957 | PMCID:PMC5544395,pubmed,28798957,create date:2017/08/12 | first author:Jo Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Establishing early warning systems for anthrax attacks is crucial in biodefense. Despite numerous studies for decades, the limited sensitivity of conventional biochemical methods essentially requires preprocessing steps and thus has limitations to be used in realistic settings of biological warfare. We present an optical method for rapid and label-free screening of <i>Bacillus anthracis</i> spores through the synergistic application of holographic microscopy and deep learning. A deep convolutional neural network is designed to classify holographic images of unlabeled living cells. After training, the network outperforms previous techniques in all accuracy measures, achieving single-spore sensitivity and subgenus specificity. The unique 'representation learning' capability of deep learning enables direct training from raw images instead of manually extracted features. The method automatically recognizes key biological traits encoded in the images and exploits them as fingerprints. This remarkable learning ability makes the proposed method readily applicable to classifying various single cells in addition to <i>B. anthracis</i>, as demonstrated for the diagnosis of <i>Listeria monocytogenes</i>, without any modification. We believe that our strategy will make holographic microscopy more accessible to medical doctors and biomedical scientists for easy, rapid, and accurate point-of-care diagnosis of pathogens.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28798957,pubmed,2017,670821a2-7cb5-4108-a890-375510d7f34f,1
efficient and robust cell detection: a structured regression approach,/pubmed/28797548,"Xie Y, Xing F, Shi X, Kong X, Su H, Yang L.",Med Image Anal. 2017 Jul 26. pii: S1361-8415(17)30111-1. doi: 10.1016/j.media.2017.07.003. [Epub ahead of print],Med Image Anal.  2017,PubMed,citation,PMID:28797548,pubmed,28797548,create date:2017/08/12 | first author:Xie Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Efficient and robust cell detection serves as a critical prerequisite for many subsequent biomedical image analysis methods and computer-aided diagnosis (CAD). It remains a challenging task due to touching cells, inhomogeneous background noise, and large variations in cell sizes and shapes. In addition, the ever-increasing amount of available datasets and the high resolution of whole-slice scanned images pose a further demand for efficient processing algorithms. In this paper, we present a novel structured regression model based on a proposed fully residual convolutional neural network for efficient cell detection. For each testing image, our model learns to produce a dense proximity map that exhibits higher responses at locations near cell centers. Our method only requires a few training images with weak annotations (just one dot indicating the cell centroids). We have extensively evaluated our method using four different datasets, covering different microscopy staining methods (e.g., H &amp; E or Ki-67 staining) or image acquisition techniques (e.g., bright-filed image or phase contrast). Experimental results demonstrate the superiority of our method over existing state of the art methods in terms of both detection accuracy and running time.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier B.V.</p></div></div>",shampool@ufl.edu,Biomedical image analysis; Cell detection; Deep learning; Structured regression,https://www.ncbi.nlm.nih.gov//pubmed/28797548,pubmed,2017,6950ff7b-406c-44b5-ae61-df02bdf60dd6,1
automated breast ultrasound lesions detection using convolutional neural networks,/pubmed/28796627,"Yap MH, Pons G, Marti J, Ganau S, Sentis M, Zwiggelaar R, Davison AK, Marti R.",IEEE J Biomed Health Inform. 2017 Aug 7. doi: 10.1109/JBHI.2017.2731873. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28796627,pubmed,28796627,create date:2017/08/11 | first author:Yap MH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Breast lesion detection using ultrasound imaging is considered an important step of Computer-Aided Diagnosis systems. Over the past decade, researchers have demonstrated the possibilities to automate the initial lesion detection. However, the lack of a common dataset impedes research when comparing the performance of such algorithms. This paper proposes the use of deep learning approaches for breast ultrasound lesion detection and investigates three different methods: a Patch-based LeNet, a U-Net, and a transfer learning approach with a pretrained FCN-AlexNet. Their performance is compared against four state-of-the-art lesion detection algorithms (i.e. Radial Gradient Index, Multifractal Filtering, Rule-based Region Ranking and Deformable Part Models). In addition, this paper compares and contrasts two conventional ultrasound image datasets acquired from two different ultrasound systems. Dataset A comprises 306 (60 malignant and 246 benign) images and Dataset B comprises 163 (53 malignant and 110 benign) images. To overcome the lack of public datasets in this domain, Dataset B will be made available for research purposes. The results demonstrate an overall improvement by the deep learning approaches when assessed on both datasets in terms of True Positive Fraction, False Positives per image, and F-measure.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28796627,pubmed,2017,211d576b-be6a-4ba2-bded-41d6348174e4,1
high-quality plane wave compounding using convolutional neural networks,/pubmed/28792894,"Gasse M, Millioz F, Roux E, Garcia D, Liebgott H, Friboulet D.",IEEE Trans Ultrason Ferroelectr Freq Control. 2017 Aug 7. doi: 10.1109/TUFFC.2017.2736890. [Epub ahead of print],IEEE Trans Ultrason Ferroelectr Freq Control.  2017,PubMed,citation,PMID:28792894,pubmed,28792894,create date:2017/08/10 | first author:Gasse M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Single plane wave (PW) imaging produces ultrasound (US) images of poor quality at high frame rates (ultrafast). High-quality PW imaging usually relies on the coherent compounding of several successive steered emissions (typically more than ten), which in turn results in a decreased frame rate. We propose a new strategy to reduce the number of emitted PWs by learning a compounding operation from data, i.e. by training a convolutional neural network (CNN) to reconstruct high quality images using a small number of transmissions. We present experimental evidence that this approach is promising, as we were able to produce high-quality images from only 3 PWs, competing in terms of contrast ratio and lateral resolution with the standard compounding of 31 PWs (10x speed-up factor).</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28792894,pubmed,2017,fb2eb93c-de9f-481a-b8a6-c99f56d4a5f9,1
automatic phase aberration compensation for digital holographic microscopy based on deep learning background detection,/pubmed/28788938,"Nguyen T, Bui V, Lam V, Raub CB, Chang LC, Nehmetallah G.",Opt Express. 2017 Jun 26;25(13):15043-15057. doi: 10.1364/OE.25.015043.,Opt Express.  2017,PubMed,citation,PMID:28788938,pubmed,28788938,create date:2017/08/10 | first author:Nguyen T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a fully automatic technique to obtain aberration free quantitative phase imaging in digital holographic microscopy (DHM) based on deep learning. The traditional DHM solves the phase aberration compensation problem by manually detecting the background for quantitative measurement. This would be a drawback in real time implementation and for dynamic processes such as cell migration phenomena. A recent automatic aberration compensation approach using principle component analysis (PCA) in DHM avoids human intervention regardless of the cells' motion. However, it corrects spherical/elliptical aberration only and disregards the higher order aberrations. Traditional image segmentation techniques can be employed to spatially detect cell locations. Ideally, automatic image segmentation techniques make real time measurement possible. However, existing automatic unsupervised segmentation techniques have poor performance when applied to DHM phase images because of aberrations and speckle noise. In this paper, we propose a novel method that combines a supervised deep learning technique with convolutional neural network (CNN) and Zernike polynomial fitting (ZPF). The deep learning CNN is implemented to perform automatic background region detection that allows for ZPF to compute the self-conjugated phase to compensate for most aberrations.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28788938,pubmed,2017,a4d5b0c3-c98b-48e7-acee-366e1e30acf5,1
residual deep convolutional neural network predicts mgmt methylation status,/pubmed/28785873,"Korfiatis P, Kline TL, Lachance DH, Parney IF, Buckner JC, Erickson BJ.",J Digit Imaging. 2017 Aug 7. doi: 10.1007/s10278-017-0009-z. [Epub ahead of print],J Digit Imaging.  2017,PubMed,citation,PMID:28785873,pubmed,28785873,create date:2017/08/09 | first author:Korfiatis P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Predicting methylation of the O6-methylguanine methyltransferase (MGMT) gene status utilizing MRI imaging is of high importance since it is a predictor of response and prognosis in brain tumors. In this study, we compare three different residual deep neural network (ResNet) architectures to evaluate their ability in predicting MGMT methylation status without the need for a distinct tumor segmentation step. We found that the ResNet50 (50 layers) architecture was the best performing model, achieving an accuracy of 94.90% (+/- 3.92%) for the test set (classification of a slice as no tumor, methylated MGMT, or non-methylated). ResNet34 (34 layers) achieved 80.72% (+/- 13.61%) while ResNet18 (18 layers) accuracy was 76.75% (+/- 20.67%). ResNet50 performance was statistically significantly better than both ResNet18 and ResNet34 architectures (p &lt; 0.001). We report a method that alleviates the need of extensive preprocessing and acts as a proof of concept that deep neural architectures can be used to predict molecular biomarkers from routine medical images.</abstracttext></p></div></div>",bje@mayo.edu,Deep learning; MGMT methylation; MRI,https://www.ncbi.nlm.nih.gov//pubmed/28785873,pubmed,2017,ba4e3e81-d080-49a7-8d93-cfcfc5ef49ee,1
deep learning with convolutional neural networks for eeg decoding and visualization,/pubmed/28782865,"Schirrmeister RT, Springenberg JT, Fiederer LDJ, Glasstetter M, Eggensperger K, Tangermann M, Hutter F, Burgard W, Ball T.",Hum Brain Mapp. 2017 Aug 7. doi: 10.1002/hbm.23730. [Epub ahead of print],Hum Brain Mapp.  2017,PubMed,citation,PMID:28782865,pubmed,28782865,create date:2017/08/08 | first author:Schirrmeister RT,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end-to-end EEG analysis, but a better understanding of how to design and train ConvNets for end-to-end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean decoding accuracies 82.1% FBCSP, 84.0% deep ConvNets). While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta, and high gamma frequencies, and proved useful for spatially mapping the learned features by revealing the topography of the causal contributions of features in different frequency bands to the decoding decision. Our study thus shows how to design and train ConvNets to decode task-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping. Hum Brain Mapp, 2017. © 2017 Wiley Periodicals, Inc.</abstracttext></p><p class='copyright'>© 2017 The Authors Human Brain Mapping Published by Wiley Periodicals, Inc.</p></div></div>",,EEG analysis; brain mapping; brain-computer interface; brain-machine interface; electroencephalography; end-to-end learning; machine learning; model interpretability,https://www.ncbi.nlm.nih.gov//pubmed/28782865,pubmed,2017,8046022b-fe11-40f0-9a6a-9304b27cab65,1
"feature-free activity classification of inertial sensor data with machine vision techniques: method, development, and evaluation",/pubmed/28778851,"Dominguez Veiga JJ, O'Reilly M, Whelan D, Caulfield B, Ward TE.",JMIR Mhealth Uhealth. 2017 Aug 4;5(8):e115. doi: 10.2196/mhealth.7521.,JMIR Mhealth Uhealth.  2017,PubMed,citation,PMID:28778851 | PMCID:PMC5562934,pubmed,28778851,create date:2017/08/06 | first author:Dominguez Veiga JJ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Inertial sensors are one of the most commonly used sources of data for human activity recognition (HAR) and exercise detection (ED) tasks. The time series produced by these sensors are generally analyzed through numerical methods. Machine learning techniques such as random forests or support vector machines are popular in this field for classification efforts, but they need to be supported through the isolation of a potentially large number of additionally crafted features derived from the raw data. This feature preprocessing step can involve nontrivial digital signal processing (DSP) techniques. However, in many cases, the researchers interested in this type of activity recognition problems do not possess the necessary technical background for this feature-set development.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>The study aimed to present a novel application of established machine vision methods to provide interested researchers with an easier entry path into the HAR and ED fields. This can be achieved by removing the need for deep DSP skills through the use of transfer learning. This can be done by using a pretrained convolutional neural network (CNN) developed for machine vision purposes for exercise classification effort. The new method should simply require researchers to generate plots of the signals that they would like to build classifiers with, store them as images, and then place them in folders according to their training label before retraining the network.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We applied a CNN, an established machine vision technique, to the task of ED. Tensorflow, a high-level framework for machine learning, was used to facilitate infrastructure needs. Simple time series plots generated directly from accelerometer and gyroscope signals are used to retrain an openly available neural network (Inception), originally developed for machine vision tasks. Data from 82 healthy volunteers, performing 5 different exercises while wearing a lumbar-worn inertial measurement unit (IMU), was collected. The ability of the proposed method to automatically classify the exercise being completed was assessed using this dataset. For comparative purposes, classification using the same dataset was also performed using the more conventional approach of feature-extraction and classification using random forest classifiers.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>With the collected dataset and the proposed method, the different exercises could be recognized with a 95.89% (3827/3991) accuracy, which is competitive with current state-of-the-art techniques in ED.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The high level of accuracy attained with the proposed approach indicates that the waveform morphologies in the time-series plots for each of the exercises is sufficiently distinct among the participants to allow the use of machine vision approaches. The use of high-level machine learning frameworks, coupled with the novel use of machine vision techniques instead of complex manually crafted features, may facilitate access to research in the HAR field for individuals without extensive digital signal processing or machine learning backgrounds.</abstracttext></p></div></div>",,biofeedback; exercise; machine learning,https://www.ncbi.nlm.nih.gov//pubmed/28778851,pubmed,2017,8b1fbcfa-d7a1-4390-891e-11226abf1403,1
improving dense conditional random field for retinal vessel segmentation by discriminative feature learning and thin-vessel enhancement,/pubmed/28774435,"Zhou L, Yu Q, Xu X, Gu Y, Yang J.",Comput Methods Programs Biomed. 2017 Sep;148:13-25. doi: 10.1016/j.cmpb.2017.06.016. Epub 2017 Jun 24.,Comput Methods Programs Biomed.  2017,PubMed,citation,PMID:28774435,pubmed,28774435,create date:2017/08/05 | first author:Zhou L,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND AND OBJECTIVES: </h4><p><abstracttext label='BACKGROUND AND OBJECTIVES' nlmcategory='OBJECTIVE'>As retinal vessels in color fundus images are thin and elongated structures, standard pairwise based random fields, which always suffer the 'shrinking bias' problem, are not competent for such segmentation task. Recently, a dense conditional random field (CRF) model has been successfully used in retinal vessel segmentation. Its corresponding energy function is formulated as a linear combination of several unary features and a pairwise term. However, the hand-crafted unary features can be suboptimal in terms of linear models. Here we propose to learn discriminative unary features and enhance thin vessels for pairwise potentials to further improve the segmentation performance.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Our proposed method comprises four main steps: firstly, image preprocessing is applied to eliminate the strong edges around the field of view (FOV) and normalize the luminosity and contrast inside FOV; secondly, a convolutional neural network (CNN) is properly trained to generate discriminative features for linear models; thirdly, a combo of filters are applied to enhance thin vessels, reducing the intensity difference between thin and wide vessels; fourthly, by taking the discriminative features for unary potentials and the thin-vessel enhanced image for pairwise potentials, we adopt the dense CRF model to achieve the final retinal vessel segmentation. The segmentation performance is evaluated on four public datasets (i.e. DRIVE, STARE, CHASEDB1 and HRF).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Experimental results show that our proposed method improves the performance of the dense CRF model and outperforms other methods when evaluated in terms of F1-score, Matthews correlation coefficient (MCC) and G-mean, three effective metrics for the evaluation of imbalanced binary classification. Specifically, the F1-score, MCC and G-mean are 0.7942, 0.7656, 0.8835 for the DRIVE dataset respectively; 0.8017, 0.7830, 0.8859 for STARE respectively; 0.7644, 0.7398, 0.8579 for CHASEDB1 respectively; and 0.7627, 0.7402, 0.8812 for HRF respectively.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The discriminative features learned in CNNs are more effective than hand-crafted ones. Our proposed method performs well in retinal vessel segmentation. The architecture of our method is trainable and can be integrated into computer-aided diagnostic (CAD) systems in the future.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",minizon@sina.com,Convolutional neural network; Dense conditional random field; Feature learning; Image enhancement; Retinal vessel segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28774435,pubmed,2017,ccb9f5df-09bc-42ba-929e-da1514643c98,1
sinc: saliency-injected neural codes for representation and efficient retrieval of medical radiographs,/pubmed/28771497,"Ahmad J, Sajjad M, Mehmood I, Baik SW.",PLoS One. 2017 Aug 3;12(8):e0181707. doi: 10.1371/journal.pone.0181707. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28771497 | PMCID:PMC5542646,pubmed,28771497,create date:2017/08/05 | first author:Ahmad J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Medical image collections contain a wealth of information which can assist radiologists and medical experts in diagnosis and disease detection for making well-informed decisions. However, this objective can only be realized if efficient access is provided to semantically relevant cases from the ever-growing medical image repositories. In this paper, we present an efficient method for representing medical images by incorporating visual saliency and deep features obtained from a fine-tuned convolutional neural network (CNN) pre-trained on natural images. Saliency detector is employed to automatically identify regions of interest like tumors, fractures, and calcified spots in images prior to feature extraction. Neuronal activation features termed as neural codes from different CNN layers are comprehensively studied to identify most appropriate features for representing radiographs. This study revealed that neural codes from the last fully connected layer of the fine-tuned CNN are found to be the most suitable for representing medical images. The neural codes extracted from the entire image and salient part of the image are fused to obtain the saliency-injected neural codes (SiNC) descriptor which is used for indexing and retrieval. Finally, locality sensitive hashing techniques are applied on the SiNC descriptor to acquire short binary codes for allowing efficient retrieval in large scale image collections. Comprehensive experimental evaluations on the radiology images dataset reveal that the proposed framework achieves high retrieval accuracy and efficiency for scalable image retrieval applications and compares favorably with existing approaches.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28771497,pubmed,2017,468827ba-32f3-4e9c-8333-3c89e6688286,1
deep learning for single-molecule science,/pubmed/28762339,"Albrecht T, Slabaugh G, Alonso E, Al-Arif SMMR.",Nanotechnology. 2017 Aug 1;28(42):423001. doi: 10.1088/1361-6528/aa8334. [Epub ahead of print],Nanotechnology.  2017,PubMed,citation,PMID:28762339,pubmed,28762339,create date:2017/08/02 | first author:Albrecht T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Exploring and making predictions based on single-molecule data can be challenging, not only due to the sheer size of the datasets, but also because a priori knowledge about the signal characteristics is typically limited and poor signal-to-noise ratio. For example, hypothesis-driven data exploration, informed by an expectation of the signal characteristics, can lead to interpretation bias or loss of information. Equally, even when the different data categories are known, e.g., the four bases in DNA sequencing, it is often difficult to know how to make best use of the available information content. The latest developments in machine learning (ML), so-called deep learning (DL) offer interesting, new avenues to address such challenges. In some applications, such as speech and image recognition, DL has been able to outperform conventional ML strategies and even human performance. However, to date DL has not been applied much in single-molecule science, presumably in part because relatively little is known about the 'internal workings' of such DL tools within single-molecule science as a field. In this Tutorial, we make an attempt to illustrate in a step-by-step guide how one of those, a convolutional neural network (CNN), may be used for base calling in DNA sequencing applications. We compare it with a SVM as a more conventional ML method, and discuss some of the strengths and weaknesses of the approach. In particular, a 'deep' neural network has many features of a 'black box', which has important implications on how we look at and interpret data.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28762339,pubmed,2017,43ab22e1-db29-4f28-9988-02ecb4dcb85b,1
ultrasound image-based thyroid nodule automatic segmentation using convolutional neural networks,/pubmed/28762196,"Ma J, Wu F, Jiang T, Zhao Q, Kong D.",Int J Comput Assist Radiol Surg. 2017 Jul 31. doi: 10.1007/s11548-017-1649-7. [Epub ahead of print],Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28762196,pubmed,28762196,create date:2017/08/02 | first author:Ma J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Delineation of thyroid nodule boundaries from ultrasound images plays an important role in calculation of clinical indices and diagnosis of thyroid diseases. However, it is challenging for accurate and automatic segmentation of thyroid nodules because of their heterogeneous appearance and components similar to the background. In this study, we employ a deep convolutional neural network (CNN) to automatically segment thyroid nodules from ultrasound images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Our CNN-based method formulates a thyroid nodule segmentation problem as a patch classification task, where the relationship among patches is ignored. Specifically, the CNN used image patches from images of normal thyroids and thyroid nodules as inputs and then generated the segmentation probability maps as outputs. A multi-view strategy is used to improve the performance of the CNN-based model. Additionally, we compared the performance of our approach with that of the commonly used segmentation methods on the same dataset.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The experimental results suggest that our proposed method outperforms prior methods on thyroid nodule segmentation. Moreover, the results show that the CNN-based model is able to delineate multiple nodules in thyroid ultrasound images accurately and effectively. In detail, our CNN-based model can achieve an average of the overlap metric, dice ratio, true positive rate, false positive rate, and modified Hausdorff distance as [Formula: see text], [Formula: see text], [Formula: see text], [Formula: see text], [Formula: see text] on overall folds, respectively.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Our proposed method is fully automatic without any user interaction. Quantitative results also indicate that our method is so efficient and accurate that it can be good enough to replace the time-consuming and tedious manual segmentation approach, demonstrating the potential clinical applications.</abstracttext></p></div></div>",dkong@zju.edu.cn,Convolutional neural network; Segmentation; Thyroid nodule; Ultrasound image,https://www.ncbi.nlm.nih.gov//pubmed/28762196,pubmed,2017,ea64f0ff-c647-4ea3-8096-7f2a6f83622d,1
spinenet: automated classification and evidence visualization in spinal mris,/pubmed/28756059,"Jamaludin A, Kadir T, Zisserman A.",Med Image Anal. 2017 Oct;41:63-73. doi: 10.1016/j.media.2017.07.002. Epub 2017 Jul 21.,Med Image Anal.  2017,PubMed,citation,PMID:28756059,pubmed,28756059,create date:2017/08/02 | first author:Jamaludin A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",amirj@robots.ox.ac.uk,MRI analysis; Radiological classification; Spinal MRI,https://www.ncbi.nlm.nih.gov//pubmed/28756059,pubmed,2017,d6d9dc2a-125d-4041-b399-c9173622224d,1
topologynet: topology based deep convolutional and multi-task neural networks for biomolecular property predictions,/pubmed/28749969,"Cang Z, Wei GW.",PLoS Comput Biol. 2017 Jul 27;13(7):e1005690. doi: 10.1371/journal.pcbi.1005690. eCollection 2017 Jul.,PLoS Comput Biol.  2017,PubMed,citation,PMID:28749969 | PMCID:PMC5549771,pubmed,28749969,create date:2017/07/28 | first author:Cang Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Although deep learning approaches have had tremendous success in image, video and audio processing, computer vision, and speech recognition, their applications to three-dimensional (3D) biomolecular structural data sets have been hindered by the geometric and biological complexity. To address this problem we introduce the element-specific persistent homology (ESPH) method. ESPH represents 3D complex geometry by one-dimensional (1D) topological invariants and retains important biological information via a multichannel image-like representation. This representation reveals hidden structure-function relationships in biomolecules. We further integrate ESPH and deep convolutional neural networks to construct a multichannel topological neural network (TopologyNet) for the predictions of protein-ligand binding affinities and protein stability changes upon mutation. To overcome the deep learning limitations from small and noisy training sets, we propose a multi-task multichannel topological convolutional neural network (MM-TCNN). We demonstrate that TopologyNet outperforms the latest methods in the prediction of protein-ligand binding affinities, mutation induced globular protein folding free energy changes, and mutation induced membrane protein folding free energy changes.</abstracttext></p><h4>AVAILABILITY: </h4><p><abstracttext label='AVAILABILITY' nlmcategory='BACKGROUND'>weilab.math.msu.edu/TDL/.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28749969,pubmed,2017,52a56c50-059e-469d-80d9-06bb6b167f8b,1
fuzzy-c-means clustering based segmentation and cnn-classification for accurate segmentation of lung nodules,/pubmed/28749127,"K JD, R G, A M.",Asian Pac J Cancer Prev. 2017 Jul 27;18(7):1869-1874.,Asian Pac J Cancer Prev.  2017,PubMed,citation,PMID:28749127,pubmed,28749127,create date:2017/07/28 | first author:K JD,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Objective: Accurate segmentation of abnormal and healthy lungs is very crucial for a steadfast computer-aided
disease diagnostics. Methods: For this purpose a stack of chest CT scans are processed. In this paper, novel methods are
proposed for segmentation of the multimodal grayscale lung CT scan. In the conventional methods using Markov–Gibbs
Random Field (MGRF) model the required regions of interest (ROI) are identified. Result: The results of proposed FCM
and CNN based process are compared with the results obtained from the conventional method using MGRF model.
The results illustrate that the proposed method can able to segment the various kinds of complex multimodal medical
images precisely. Conclusion: However, in this paper, to obtain an exact boundary of the regions, every empirical
dispersion of the image is computed by Fuzzy C-Means Clustering segmentation. A classification process based on
the Convolutional Neural Network (CNN) classifier is accomplished to distinguish the normal tissue and the abnormal
tissue. The experimental evaluation is done using the Interstitial Lung Disease (ILD) database.</abstracttext></p><p class='copyright'>Creative Commons Attribution License</p></div></div>",jalaldeensit@gmail.com,Multimodal image; lung segmentation; Fuzzy-C-Means; CNN classifier; feature extraction,https://www.ncbi.nlm.nih.gov//pubmed/28749127,pubmed,2017,ac3f8958-7a1c-4ad5-99d1-57333727ac1b,1
open source software for automatic detection of cone photoreceptors in adaptive optics ophthalmoscopy using convolutional neural networks,/pubmed/28747737,"Cunefare D, Fang L, Cooper RF, Dubra A, Carroll J, Farsiu S.",Sci Rep. 2017 Jul 26;7(1):6620. doi: 10.1038/s41598-017-07103-0.,Sci Rep.  2017,PubMed,citation,PMID:28747737 | PMCID:PMC5529414,pubmed,28747737,create date:2017/07/28 | first author:Cunefare D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Imaging with an adaptive optics scanning light ophthalmoscope (AOSLO) enables direct visualization of the cone photoreceptor mosaic in the living human retina. Quantitative analysis of AOSLO images typically requires manual grading, which is time consuming, and subjective; thus, automated algorithms are highly desirable. Previously developed automated methods are often reliant on ad hoc rules that may not be transferable between different imaging modalities or retinal locations. In this work, we present a convolutional neural network (CNN) based method for cone detection that learns features of interest directly from training data. This cone-identifying algorithm was trained and validated on separate data sets of confocal and split detector AOSLO images with results showing performance that closely mimics the gold standard manual process. Further, without any need for algorithmic modifications for a specific AOSLO imaging system, our fully-automated multi-modality CNN-based cone detection method resulted in comparable results to previous automatic cone segmentation methods which utilized ad hoc rules for different applications. We have made free open-source software for the proposed method and the corresponding training and testing datasets available online.</abstracttext></p></div></div>",david.cunefare@duke.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28747737,pubmed,2017,fd8431c9-e190-45fb-88bc-3b21f4be9d2d,1
self-recalibrating surface emg pattern recognition for neuroprosthesis control based on convolutional neural network,/pubmed/28744189,"Zhai X, Jelfs B, Chan RHM, Tin C.",Front Neurosci. 2017 Jul 11;11:379. doi: 10.3389/fnins.2017.00379. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28744189 | PMCID:PMC5504564,pubmed,28744189,create date:2017/07/27 | first author:Zhai X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Hand movement classification based on surface electromyography (sEMG) pattern recognition is a promising approach for upper limb neuroprosthetic control. However, maintaining day-to-day performance is challenged by the non-stationary nature of sEMG in real-life operation. In this study, we propose a self-recalibrating classifier that can be automatically updated to maintain a stable performance over time without the need for user retraining. Our classifier is based on convolutional neural network (CNN) using short latency dimension-reduced sEMG spectrograms as inputs. The pretrained classifier is recalibrated routinely using a corrected version of the prediction results from recent testing sessions. Our proposed system was evaluated with the NinaPro database comprising of hand movement data of 40 intact and 11 amputee subjects. Our system was able to achieve ~10.18% (intact, 50 movement types) and ~2.99% (amputee, 10 movement types) increase in classification accuracy averaged over five testing sessions with respect to the unrecalibrated classifier. When compared with a support vector machine (SVM) classifier, our CNN-based system consistently showed higher absolute performance and larger improvement as well as more efficient training. These results suggest that the proposed system can be a useful tool to facilitate long-term adoption of prosthetics for amputees in real-life applications.</abstracttext></p></div></div>",,classification; convolutional neural network; hand gesture; myoelectric control; non-stationary EMG; pattern recognition,https://www.ncbi.nlm.nih.gov//pubmed/28744189,pubmed,2017,1577b78b-b266-4a56-b755-69f9d3d4db44,1
deep architecture neural network-based real-time image processing for image-guided radiotherapy,/pubmed/28743618,Mori S.,Phys Med. 2017 Aug;40:79-87. doi: 10.1016/j.ejmp.2017.07.013. Epub 2017 Jul 23.,Phys Med.  2017,PubMed,citation,PMID:28743618,pubmed,28743618,create date:2017/07/27 | first author:Mori S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>INTRODUCTION: </h4><p><abstracttext label='INTRODUCTION' nlmcategory='BACKGROUND'>To develop real-time image processing for image-guided radiotherapy, we evaluated several neural network models for use with different imaging modalities, including X-ray fluoroscopic image denoising.</abstracttext></p><h4>METHODS &amp; MATERIALS: </h4><p><abstracttext label='METHODS &amp; MATERIALS' nlmcategory='METHODS'>Setup images of prostate cancer patients were acquired with two oblique X-ray fluoroscopic units. Two types of residual network were designed: a convolutional autoencoder (rCAE) and a convolutional neural network (rCNN). We changed the convolutional kernel size and number of convolutional layers for both networks, and the number of pooling and upsampling layers for rCAE. The ground-truth image was applied to the contrast-limited adaptive histogram equalization (CLAHE) method of image processing. Network models were trained to keep the quality of the output image close to that of the ground-truth image from the input image without image processing. For image denoising evaluation, noisy input images were used for the training.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>More than 6 convolutional layers with convolutional kernels &gt;5×5 improved image quality. However, this did not allow real-time imaging. After applying a pair of pooling and upsampling layers to both networks, rCAEs with &gt;3 convolutions each and rCNNs with &gt;12 convolutions with a pair of pooling and upsampling layers achieved real-time processing at 30 frames per second (fps) with acceptable image quality.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Use of our suggested network achieved real-time image processing for contrast enhancement and image denoising by the use of a conventional modern personal computer.</abstracttext></p><p class='copyright'>Copyright © 2017 Associazione Italiana di Fisica Medica. Published by Elsevier Ltd. All rights reserved.</p></div></div>",mori.shinichiro@qst.go.jp,Computer-assisted; Fluoroscopy; Image processing; Neural network model; Radiation therapy,https://www.ncbi.nlm.nih.gov//pubmed/28743618,pubmed,2017,3b168276-17e1-4dc7-91e9-d6dcb969b3e6,1
brain tumor segmentation using holistically nested neural networks in mri images,/pubmed/28736864,"Zhuge Y, Krauze AV, Ning H, Cheng JY, Arora BC, Camphausen K, Miller RW.",Med Phys. 2017 Jul 24. doi: 10.1002/mp.12481. [Epub ahead of print],Med Phys.  2017,PubMed,citation,PMID:28736864,pubmed,28736864,create date:2017/07/25 | first author:Zhuge Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Gliomas are rapidly progressive, neurologically devastating, largely fatal brain tumors. Magnetic resonance imaging (MRI) is a widely used technique employed in the diagnosis and management of gliomas in clinical practice. MRI is also the standard imaging modality used to delineate the brain tumor target as part of treatment planning for the administration of radiation therapy. Despite more than 20 yr of research and development, computational brain tumor segmentation in MRI images remains a challenging task. We are presenting a novel method of automatic image segmentation based on holistically nested neural networks that could be employed for brain tumor segmentation of MRI images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Two preprocessing techniques were applied to MRI images. The N4ITK method was employed for correction of bias field distortion. A novel landmark-based intensity normalization method was developed so that tissue types have a similar intensity scale in images of different subjects for the same MRI protocol. The holistically nested neural networks (HNN), which extend from the convolutional neural networks (CNN) with a deep supervision through an additional weighted-fusion output layer, was trained to learn the multiscale and multilevel hierarchical appearance representation of the brain tumor in MRI images and was subsequently applied to produce a prediction map of the brain tumor on test images. Finally, the brain tumor was obtained through an optimum thresholding on the prediction map.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed method was evaluated on both the Multimodal Brain Tumor Image Segmentation (BRATS) Benchmark 2013 training datasets, and clinical data from our institute. A dice similarity coefficient (DSC) and sensitivity of 0.78 and 0.81 were achieved on 20 BRATS 2013 training datasets with high-grade gliomas (HGG), based on a two-fold cross-validation. The HNN model built on the BRATS 2013 training data was applied to ten clinical datasets with HGG from a locally developed database. DSC and sensitivity of 0.83 and 0.85 were achieved. A quantitative comparison indicated that the proposed method outperforms the popular fully convolutional network (FCN) method. In terms of efficiency, the proposed method took around 10 h for training with 50,000 iterations, and approximately 30 s for testing of a typical MRI image in the BRATS 2013 dataset with a size of 160 × 216 × 176, using a DELL PRECISION workstation T7400, with an NVIDIA Tesla K20c GPU.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>An effective brain tumor segmentation method for MRI images based on a HNN has been developed. The high level of accuracy and efficiency make this method practical in brain tumor segmentation. It may play a crucial role in both brain tumor diagnostic analysis and in the treatment planning of radiation therapy.</abstracttext></p><p class='copyright'>Published 2017. This article is a U.S. Government work and is in the public domain in the USA.</p></div></div>",,MRI image; brain tumor; convolutional neural networks; holistically nested neural networks; image segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28736864,pubmed,2017,ac01437e-df0b-45c8-af27-0f7f943a8a44,1
deep convolutional neural network and 3d deformable approach for tissue segmentation in musculoskeletal magnetic resonance imaging,/pubmed/28733975,"Liu F, Zhou Z, Jang H, Samsonov A, Zhao G, Kijowski R.",Magn Reson Med. 2017 Jul 21. doi: 10.1002/mrm.26841. [Epub ahead of print],Magn Reson Med.  2017,PubMed,citation,PMID:28733975,pubmed,28733975,create date:2017/07/25 | first author:Liu F,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>To describe and evaluate a new fully automated musculoskeletal tissue segmentation method using deep convolutional neural network (CNN) and three-dimensional (3D) simplex deformable modeling to improve the accuracy and efficiency of cartilage and bone segmentation within the knee joint.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A fully automated segmentation pipeline was built by combining a semantic segmentation CNN and 3D simplex deformable modeling. A CNN technique called SegNet was applied as the core of the segmentation method to perform high resolution pixel-wise multi-class tissue classification. The 3D simplex deformable modeling refined the output from SegNet to preserve the overall shape and maintain a desirable smooth surface for musculoskeletal structure. The fully automated segmentation method was tested using a publicly available knee image data set to compare with currently used state-of-the-art segmentation methods. The fully automated method was also evaluated on two different data sets, which include morphological and quantitative MR images with different tissue contrasts.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed fully automated segmentation method provided good segmentation performance with segmentation accuracy superior to most of state-of-the-art methods in the publicly available knee image data set. The method also demonstrated versatile segmentation performance on both morphological and quantitative musculoskeletal MR images with different tissue contrasts and spatial resolutions.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The study demonstrates that the combined CNN and 3D deformable modeling approach is useful for performing rapid and accurate cartilage and bone segmentation within the knee joint. The CNN has promising potential applications in musculoskeletal imaging. Magn Reson Med, 2017. © 2017 International Society for Magnetic Resonance in Medicine.</abstracttext></p><p class='copyright'>© 2017 International Society for Magnetic Resonance in Medicine.</p></div></div>",,CNN; MRI; deep learning; deformable model; musculoskeletal imaging; segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28733975,pubmed,2017,5adf95df-b744-476a-9738-34320b8a88e1,1
a deep convolutional neural network approach to single-particle recognition in cryo-electron microscopy,/pubmed/28732461,"Zhu Y, Ouyang Q, Mao Y.",BMC Bioinformatics. 2017 Jul 21;18(1):348. doi: 10.1186/s12859-017-1757-y.,BMC Bioinformatics.  2017,PubMed,citation,PMID:28732461 | PMCID:PMC5521087,pubmed,28732461,create date:2017/07/25 | first author:Zhu Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Single-particle cryo-electron microscopy (cryo-EM) has become a mainstream tool for the structural determination of biological macromolecular complexes. However, high-resolution cryo-EM reconstruction often requires hundreds of thousands of single-particle images. Particle extraction from experimental micrographs thus can be laborious and presents a major practical bottleneck in cryo-EM structural determination. Existing computational methods for particle picking often use low-resolution templates for particle matching, making them susceptible to reference-dependent bias. It is critical to develop a highly efficient template-free method for the automatic recognition of particle images from cryo-EM micrographs.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We developed a deep learning-based algorithmic framework, DeepEM, for single-particle recognition from noisy cryo-EM micrographs, enabling automated particle picking, selection and verification in an integrated fashion. The kernel of DeepEM is built upon a convolutional neural network (CNN) composed of eight layers, which can be recursively trained to be highly 'knowledgeable'. Our approach exhibits an improved performance and accuracy when tested on the standard KLH dataset. Application of DeepEM to several challenging experimental cryo-EM datasets demonstrated its ability to avoid the selection of un-wanted particles and non-particles even when true particles contain fewer features.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The DeepEM methodology, derived from a deep CNN, allows automated particle extraction from raw cryo-EM micrographs in the absence of a template. It demonstrates an improved performance, objectivity and accuracy. Application of this novel method is expected to free the labor involved in single-particle verification, significantly improving the efficiency of cryo-EM data processing.</abstracttext></p></div></div>",youdong_mao@dfci.harvard.edu,Convolutional neural network; Cryo-EM; Deep learning; Particle recognition; Single-particle reconstruction,https://www.ncbi.nlm.nih.gov//pubmed/28732461,pubmed,2017,1096829a-4770-4e33-ac4f-6d8f304b397a,1
de novo peptide sequencing by deep learning,/pubmed/28720701,"Tran NH, Zhang X, Xin L, Shan B, Li M.",Proc Natl Acad Sci U S A. 2017 Jul 18. pii: 201705691. doi: 10.1073/pnas.1705691114. [Epub ahead of print],Proc Natl Acad Sci U S A.  2017,PubMed,citation,PMID:28720701 | PMCID:PMC5547637,pubmed,28720701,create date:2017/07/20 | first author:Tran NH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>De novo peptide sequencing from tandem MS data is the key technology in proteomics for the characterization of proteins, especially for new sequences, such as mAbs. In this study, we propose a deep neural network model, DeepNovo, for de novo peptide sequencing. DeepNovo architecture combines recent advances in convolutional neural networks and recurrent neural networks to learn features of tandem mass spectra, fragment ions, and sequence patterns of peptides. The networks are further integrated with local dynamic programming to solve the complex optimization task of de novo sequencing. We evaluated the method on a wide variety of species and found that DeepNovo considerably outperformed state of the art methods, achieving 7.7-22.9% higher accuracy at the amino acid level and 38.1-64.0% higher accuracy at the peptide level. We further used DeepNovo to automatically reconstruct the complete sequences of antibody light and heavy chains of mouse, achieving 97.5-100% coverage and 97.2-99.5% accuracy, without assisting databases. Moreover, DeepNovo is retrainable to adapt to any sources of data and provides a complete end-to-end training and prediction solution to the de novo sequencing problem. Not only does our study extend the deep learning revolution to a new field, but it also shows an innovative approach in solving optimization problems by using deep learning and dynamic programming.</abstracttext></p></div></div>",mli@uwaterloo.ca,MS; de novo sequencing; deep learning,https://www.ncbi.nlm.nih.gov//pubmed/28720701,pubmed,2017,4a1f1d96-729c-4765-af27-ee077b4be218,1
"deep-learning based, automated segmentation of macular edema in optical coherence tomography",/pubmed/28717579,"Lee CS, Tyring AJ, Deruyter NP, Wu Y, Rokem A, Lee AY.",Biomed Opt Express. 2017 Jun 23;8(7):3440-3448. doi: 10.1364/BOE.8.003440. eCollection 2017 Jul 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28717579 | PMCID:PMC5508840,pubmed,28717579,create date:2017/07/19 | first author:Lee CS,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Evaluation of clinical images is essential for diagnosis in many specialties. Therefore the development of computer vision algorithms to help analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations of clinically relevant image features.</abstracttext></p></div></div>",,(110.4500) Optical coherence tomography; (150.1135) Algorithms,https://www.ncbi.nlm.nih.gov//pubmed/28717579,pubmed,2017,cc154b74-feca-47f3-b4cf-d008dce4944d,1
robust total retina thickness segmentation in optical coherence tomography images using convolutional neural networks,/pubmed/28717568,"Venhuizen FG, van Ginneken B, Liefers B, van Grinsven MJJP, Fauser S, Hoyng C, Theelen T, Sánchez CI.",Biomed Opt Express. 2017 Jun 16;8(7):3292-3316. doi: 10.1364/BOE.8.003292. eCollection 2017 Jul 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28717568 | PMCID:PMC5508829,pubmed,28717568,create date:2017/07/19 | first author:Venhuizen FG,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We developed a fully automated system using a convolutional neural network (CNN) for total retina segmentation in optical coherence tomography (OCT) that is robust to the presence of severe retinal pathology. A generalized U-net network architecture was introduced to include the large context needed to account for large retinal changes. The proposed algorithm outperformed qualitative and quantitatively two available algorithms. The algorithm accurately estimated macular thickness with an error of 14.0 ± 22.1 µm, substantially lower than the error obtained using the other algorithms (42.9 ± 116.0 µm and 27.1 ± 69.3 µm, respectively). These results highlighted the proposed algorithm's capability of modeling the wide variability in retinal appearance and obtained a robust and reliable retina segmentation even in severe pathological cases.</abstracttext></p></div></div>",,"(100.2960) Image analysis; (100.4996) Pattern recognition, neural networks; (110.4500) Optical coherence tomography; (170.1610) Clinical applications; (170.4470) Ophthalmology",https://www.ncbi.nlm.nih.gov//pubmed/28717568,pubmed,2017,dc71035f-82fd-4322-a570-f6956603aee0,1
deep learning based radiomics (dlr) and its usage in noninvasive idh1 prediction for low grade glioma,/pubmed/28710497,"Li Z, Wang Y, Yu J, Guo Y, Cao W.",Sci Rep. 2017 Jul 14;7(1):5467. doi: 10.1038/s41598-017-05848-2.,Sci Rep.  2017,PubMed,citation,PMID:28710497 | PMCID:PMC5511238,pubmed,28710497,create date:2017/07/16 | first author:Li Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning-based radiomics (DLR) was developed to extract deep information from multiple modalities of magnetic resonance (MR) images. The performance of DLR for predicting the mutation status of isocitrate dehydrogenase 1 (IDH1) was validated in a dataset of 151 patients with low-grade glioma. A modified convolutional neural network (CNN) structure with 6 convolutional layers and a fully connected layer with 4096 neurons was used to segment tumors. Instead of calculating image features from segmented images, as typically performed for normal radiomics approaches, image features were obtained by normalizing the information of the last convolutional layers of the CNN. Fisher vector was used to encode the CNN features from image slices of different sizes. High-throughput features with dimensionality greater than 1.6*10<sup>4</sup> were obtained from the CNN. Paired t-tests and F-scores were used to select CNN features that were able to discriminate IDH1. With the same dataset, the area under the operating characteristic curve (AUC) of the normal radiomics method was 86% for IDH1 estimation, whereas for DLR the AUC was 92%. The AUC of IDH1 estimation was further improved to 95% using DLR based on multiple-modality MR images. DLR could be a powerful way to extract deep information from medical images.</abstracttext></p></div></div>",yywang@fudan.edu.cn,,https://www.ncbi.nlm.nih.gov//pubmed/28710497,pubmed,2017,4c153b83-cd30-4680-9b55-d7bd857fdb2d,1
sononet: real-time detection and localisation of fetal standard scan planes in freehand ultrasound,/pubmed/28708546,"Baumgartner CF, Kamnitsas K, Matthew J, Fletcher TP, Smith S, Koch LM, Kainz B, Rueckert D.",IEEE Trans Med Imaging. 2017 Jul 11. doi: 10.1109/TMI.2017.2712367. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28708546,pubmed,28708546,create date:2017/07/15 | first author:Baumgartner CF,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Identifying and interpreting fetal standard scan planes during 2D ultrasound mid-pregnancy examinations are highly complex tasks which require years of training. Apart from guiding the probe to the correct location, it can be equally difficult for a non-expert to identify relevant structures within the image. Automatic image processing can provide tools to help experienced as well as inexperienced operators with these tasks. In this paper, we propose a novel method based on convolutional neural networks which can automatically detect 13 fetal standard views in freehand 2D ultrasound data as well as provide a localisation of the fetal structures via a bounding box. An important contribution is that the network learns to localise the target anatomy using weak supervision based on image-level labels only. The network architecture is designed to operate in real-time while providing optimal output for the localisation task. We present results for real-time annotation, retrospective frame retrieval from saved videos, and localisation on a very large and challenging dataset consisting of images and video recordings of full clinical anomaly screenings. We found that the proposed method achieved an average F1-score of 0.798 in a realistic classification experiment modelling real-time detection, and obtained a 90.09% accuracy for retrospective frame retrieval. Moreover, an accuracy of 77.8% was achieved on the localisation task.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28708546,pubmed,2017,6c85e956-416a-411e-99ca-4cb867622877,1
deep learning for fully-automated localization and segmentation of rectal cancer on multiparametric mr,/pubmed/28706185,"Trebeschi S, van Griethuysen JJM, Lambregts DMJ, Lahaye MJ, Parmer C, Bakers FCH, Peters NHGM, Beets-Tan RGH, Aerts HJWL.",Sci Rep. 2017 Jul 13;7(1):5301. doi: 10.1038/s41598-017-05728-9.,Sci Rep.  2017,PubMed,citation,PMID:28706185 | PMCID:PMC5509680,pubmed,28706185,create date:2017/07/15 | first author:Trebeschi S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Multiparametric Magnetic Resonance Imaging (MRI) can provide detailed information of the physical characteristics of rectum tumours. Several investigations suggest that volumetric analyses on anatomical and functional MRI contain clinically valuable information. However, manual delineation of tumours is a time consuming procedure, as it requires a high level of expertise. Here, we evaluate deep learning methods for automatic localization and segmentation of rectal cancers on multiparametric MR imaging. MRI scans (1.5T, T2-weighted, and DWI) of 140 patients with locally advanced rectal cancer were included in our analysis, equally divided between discovery and validation datasets. Two expert radiologists segmented each tumor. A convolutional neural network (CNN) was trained on the multiparametric MRIs of the discovery set to classify each voxel into tumour or non-tumour. On the independent validation dataset, the CNN showed high segmentation accuracy for reader1 (Dice Similarity Coefficient (DSC = 0.68) and reader2 (DSC = 0.70). The area under the curve (AUC) of the resulting probability maps was very high for both readers, AUC = 0.99 (SD = 0.05). Our results demonstrate that deep learning can perform accurate localization and segmentation of rectal cancer in MR imaging in the majority of patients. Deep learning technologies have the potential to improve the speed and accuracy of MRI-based rectum segmentations.</abstracttext></p></div></div>",Hugo_Aerts@dfci.harvard.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28706185,pubmed,2017,5ce3d49b-9d45-48f5-b58e-8ec32806b7b7,1
location sensitive deep convolutional neural networks for segmentation of white matter hyperintensities,/pubmed/28698556,"Ghafoorian M, Karssemeijer N, Heskes T, van Uden IWM, Sanchez CI, Litjens G, de Leeuw FE, van Ginneken B, Marchiori E, Platel B.",Sci Rep. 2017 Jul 11;7(1):5110. doi: 10.1038/s41598-017-05300-5.,Sci Rep.  2017,PubMed,citation,PMID:28698556 | PMCID:PMC5505987,pubmed,28698556,create date:2017/07/13 | first author:Ghafoorian M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The anatomical location of imaging features is of crucial importance for accurate diagnosis in many medical tasks. Convolutional neural networks (CNN) have had huge successes in computer vision, but they lack the natural ability to incorporate the anatomical location in their decision making process, hindering success in some medical image analysis tasks. In this paper, to integrate the anatomical location information into the network, we propose several deep CNN architectures that consider multi-scale patches or take explicit location features while training. We apply and compare the proposed architectures for segmentation of white matter hyperintensities in brain MR images on a large dataset. As a result, we observe that the CNNs that incorporate location information substantially outperform a conventional segmentation method with handcrafted features as well as CNNs that do not integrate location information. On a test set of 50 scans, the best configuration of our networks obtained a Dice score of 0.792, compared to 0.805 for an independent human observer. Performance levels of the machine and the independent human observer were not statistically significantly different (p-value = 0.06).</abstracttext></p></div></div>",mohsen.ghafoorian@radboudumc.nl,,https://www.ncbi.nlm.nih.gov//pubmed/28698556,pubmed,2017,79490e76-e058-4fb8-9f4f-5adb3dce5ba7,1
convolutional embedding of attributed molecular graphs for physical property prediction,/pubmed/28696688,"Coley CW, Barzilay R, Green WH, Jaakkola TS, Jensen KF.",J Chem Inf Model. 2017 Aug 28;57(8):1757-1772. doi: 10.1021/acs.jcim.6b00601. Epub 2017 Jul 25.,J Chem Inf Model.  2017,PubMed,citation,PMID:28696688,pubmed,28696688,create date:2017/07/12 | first author:Coley CW,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The task of learning an expressive molecular representation is central to developing quantitative structure-activity and property relationships. Traditional approaches rely on group additivity rules, empirical measurements or parameters, or generation of thousands of descriptors. In this paper, we employ a convolutional neural network for this embedding task by treating molecules as undirected graphs with attributed nodes and edges. Simple atom and bond attributes are used to construct atom-specific feature vectors that take into account the local chemical environment using different neighborhood radii. By working directly with the full molecular graph, there is a greater opportunity for models to identify important features relevant to a prediction task. Unlike other graph-based approaches, our atom featurization preserves molecule-level spatial information that significantly enhances model performance. Our models learn to identify important features of atom clusters for the prediction of aqueous solubility, octanol solubility, melting point, and toxicity. Extensions and limitations of this strategy are discussed.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28696688,pubmed,2017,88f79ea7-4527-4a5a-8e29-fba47041df06,1
thyroid nodule classification in ultrasound images by fine-tuning deep convolutional neural network,/pubmed/28695342,"Chi J, Walia E, Babyn P, Wang J, Groot G, Eramian M.",J Digit Imaging. 2017 Aug;30(4):477-486. doi: 10.1007/s10278-017-9997-y.,J Digit Imaging.  2017,PubMed,citation,PMID:28695342 | PMCID:PMC5537102,pubmed,28695342,create date:2017/07/12 | first author:Chi J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>With many thyroid nodules being incidentally detected, it is important to identify as many malignant nodules as possible while excluding those that are highly likely to be benign from fine needle aspiration (FNA) biopsies or surgeries. This paper presents a computer-aided diagnosis (CAD) system for classifying thyroid nodules in ultrasound images. We use deep learning approach to extract features from thyroid ultrasound images. Ultrasound images are pre-processed to calibrate their scale and remove the artifacts. A pre-trained GoogLeNet model is then fine-tuned using the pre-processed image samples which leads to superior feature extraction. The extracted features of the thyroid ultrasound images are sent to a Cost-sensitive Random Forest classifier to classify the images into 'malignant' and 'benign' cases. The experimental results show the proposed fine-tuned GoogLeNet model achieves excellent classification performance, attaining 98.29% classification accuracy, 99.10% sensitivity and 93.90% specificity for the images in an open access database (Pedraza et al. 16), while 96.34% classification accuracy, 86% sensitivity and 99% specificity for the images in our local health region database.</abstracttext></p></div></div>",chi.jianning@gmail.com,Computer vision; Convolutional neural network; Deep learning; Fine-tuning; Machine learning; Thyroid nodules; Ultrasonography,https://www.ncbi.nlm.nih.gov//pubmed/28695342,pubmed,2017,e42f4303-10cc-4ed4-ae84-f9c43037c5c8,1
overview of deep learning in medical imaging,/pubmed/28689314,Suzuki K.,Radiol Phys Technol. 2017 Jul 8. doi: 10.1007/s12194-017-0406-5. [Epub ahead of print] Review.,Radiol Phys Technol.  2017,PubMed,citation,PMID:28689314,pubmed,28689314,create date:2017/07/10 | first author:Suzuki K,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The use of machine learning (ML) has been increasing rapidly in the medical imaging field, including computer-aided diagnosis (CAD), radiomics, and medical image analysis. Recently, an ML area called deep learning emerged in the computer vision field and became very popular in many fields. It started from an event in late 2012, when a deep-learning approach based on a convolutional neural network (CNN) won an overwhelming victory in the best-known worldwide computer vision competition, ImageNet Classification. Since then, researchers in virtually all fields, including medical imaging, have started actively participating in the explosively growing field of deep learning. In this paper, the area of deep learning in medical imaging is overviewed, including (1) what was changed in machine learning before and after the introduction of deep learning, (2) what is the source of the power of deep learning, (3) two major deep-learning models: a massive-training artificial neural network (MTANN) and a convolutional neural network (CNN), (4) similarities and differences between the two models, and (5) their applications to medical imaging. This review shows that ML with feature input (or feature-based ML) was dominant before the introduction of deep learning, and that the major and essential difference between ML before and after deep learning is the learning of image data directly without object segmentation or feature extraction; thus, it is the source of the power of deep learning, although the depth of the model is an important attribute. The class of ML with image input (or image-based ML) including deep learning has a long history, but recently gained popularity due to the use of the new terminology, deep learning. There are two major models in this class of ML in medical imaging, MTANN and CNN, which have similarities as well as several differences. In our experience, MTANNs were substantially more efficient in their development, had a higher performance, and required a lesser number of training cases than did CNNs. 'Deep learning', or ML with image input, in medical imaging is an explosively growing, promising field. It is expected that ML with image input will be the mainstream area in the field of medical imaging in the next few decades.</abstracttext></p></div></div>",ksuzuki@iit.edu,Classification; Computer-aided diagnosis; Convolutional neural network; Deep learning; Massive-training artificial neural network; Medical image analysis,https://www.ncbi.nlm.nih.gov//pubmed/28689314,pubmed,2017,0387040c-985c-4751-8675-343df765199e,1
central focused convolutional neural networks: developing a data-driven model for lung nodule segmentation,/pubmed/28688283,"Wang S, Zhou M, Liu Z, Liu Z, Gu D, Zang Y, Dong D, Gevaert O, Tian J.",Med Image Anal. 2017 Aug;40:172-183. doi: 10.1016/j.media.2017.06.014. Epub 2017 Jun 30.,Med Image Anal.  2017,PubMed,citation,PMID:28688283,pubmed,28688283,create date:2017/07/09 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate lung nodule segmentation from computed tomography (CT) images is of great importance for image-driven lung cancer analysis. However, the heterogeneity of lung nodules and the presence of similar visual characteristics between nodules and their surroundings make it difficult for robust nodule segmentation. In this study, we propose a data-driven model, termed the Central Focused Convolutional Neural Networks (CF-CNN), to segment lung nodules from heterogeneous CT images. Our approach combines two key insights: 1) the proposed model captures a diverse set of nodule-sensitive features from both 3-D and 2-D CT images simultaneously; 2) when classifying an image voxel, the effects of its neighbor voxels can vary according to their spatial locations. We describe this phenomenon by proposing a novel central pooling layer retaining much information on voxel patch center, followed by a multi-scale patch learning strategy. Moreover, we design a weighted sampling to facilitate the model training, where training samples are selected according to their degree of segmentation difficulty. The proposed method has been extensively evaluated on the public LIDC dataset including 893 nodules and an independent dataset with 74 nodules from Guangdong General Hospital (GDGH). We showed that CF-CNN achieved superior segmentation performance with average dice scores of 82.15% and 80.02% for the two datasets respectively. Moreover, we compared our results with the inter-radiologists consistency on LIDC dataset, showing a difference in average dice score of only 1.98%.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier B.V.</p></div></div>",di.dong@ia.ac.cn,Computer-aided diagnosis; Convolutional neural networks; Deep learning; Lung nodule segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28688283,pubmed,2017,3685a379-c48a-4e08-b80b-008e5af55347,1
a deep feature fusion methodology for breast cancer diagnosis demonstrated on three imaging modality datasets,/pubmed/28681390,"Antropova N, Huynh BQ, Giger ML.",Med Phys. 2017 Jul 6. doi: 10.1002/mp.12453. [Epub ahead of print],Med Phys.  2017,PubMed,citation,PMID:28681390,pubmed,28681390,create date:2017/07/07 | first author:Antropova N,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Deep learning methods for radiomics/computer-aided diagnosis (CADx) are often prohibited by small datasets, long computation time, and the need for extensive image preprocessing.</abstracttext></p><h4>AIMS: </h4><p><abstracttext label='AIMS' nlmcategory='OBJECTIVE'>We aim to develop a breast CADx methodology that addresses the aforementioned issues by exploiting the efficiency of pre-trained convolutional neural networks (CNNs) and using pre-existing handcrafted CADx features.</abstracttext></p><h4>MATERIALS &amp; METHODS: </h4><p><abstracttext label='MATERIALS &amp; METHODS' nlmcategory='METHODS'>We present a methodology that extracts and pools low- to mid-level features using a pretrained CNN and fuses them with handcrafted radiomic features computed using conventional CADx methods. Our methodology is tested on three different clinical imaging modalities (dynamic contrast enhanced-MRI [690 cases], full-field digital mammography [245 cases], and ultrasound [1125 cases]).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>From ROC analysis, our fusion-based method demonstrates, on all three imaging modalities, statistically significant improvements in terms of AUC as compared to previous breast cancer CADx methods in the task of distinguishing between malignant and benign lesions. (DCE-MRI [AUC = 0.89 (se = 0.01)], FFDM [AUC = 0.86 (se = 0.01)], and ultrasound [AUC = 0.90 (se = 0.01)]).</abstracttext></p><h4>DISCUSSION/CONCLUSION: </h4><p><abstracttext label='DISCUSSION/CONCLUSION' nlmcategory='CONCLUSIONS'>We proposed a novel breast CADx methodology that can be used to more effectively characterize breast lesions in comparison to existing methods. Furthermore, our proposed methodology is computationally efficient and circumvents the need for image preprocessing.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,breast cancer; deep learning; feature extraction,https://www.ncbi.nlm.nih.gov//pubmed/28681390,pubmed,2017,bb7be244-d1a7-42f2-9e16-35e78f74ef4f,1
detecting anatomical landmarks from limited medical imaging data using two-stage task-oriented deep neural networks,/pubmed/28678706,"Zhang J, Liu M, Shen D.",IEEE Trans Image Process. 2017 Oct;26(10):4753-4764. doi: 10.1109/TIP.2017.2721106. Epub 2017 Jun 28.,IEEE Trans Image Process.  2017,PubMed,citation,PMID:28678706,pubmed,28678706,create date:2017/07/06 | first author:Zhang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28678706,pubmed,2017,72fca97c-1977-4adb-9af5-b0149409982b,1
auto-context convolutional neural network (auto-net) for brain extraction in magnetic resonance imaging,/pubmed/28678704,"Salehi SSM, Erdogmus D, Gholipour A.",IEEE Trans Med Imaging. 2017 Jun 28. doi: 10.1109/TMI.2017.2721362. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28678704,pubmed,28678704,create date:2017/07/06 | first author:Salehi SSM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. State-of-the-art brain extraction techniques rely heavily on the accuracy of alignment or registration between brain atlases and query brain anatomy, and/or make assumptions about the image geometry; therefore have limited success when these assumptions do not hold or image registration fails. With the aim of designing an accurate, learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. We consider two different architectures: 1) a voxelwise approach based on three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) that implicitly learn 3D image information without the need for computationally expensive 3D convolutions, and 2) a fully convolutional network based on the U-net architecture. Posterior probability maps generated by the networks are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain to extract it from non-brain tissue. The brain extraction results we have obtained from our CNNs are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.73% and 97.62%, respectively. Significant improvement was achieved via our auto-context algorithm. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fetal brain magnetic resonance imaging (MRI) datasets. In this application our voxelwise auto-context CNN performed much better than the other methods (Dice coefficient: 95.97%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Through training, our method can provide accurate brain extraction in challenging applications. This in-turn may reduce the problems associated with image registration in segmentation tasks.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28678704,pubmed,2017,49dcedc4-3818-4634-b2b2-f71046b699c9,1
automated critical test findings identification and online notification system using artificial intelligence in imaging,/pubmed/28678669,"Prevedello LM, Erdal BS, Ryu JL, Little KJ, Demirer M, Qian S, White RD.",Radiology. 2017 Jul 3:162664. doi: 10.1148/radiol.2017162664. [Epub ahead of print],Radiology.  2017,PubMed,citation,PMID:28678669,pubmed,28678669,create date:2017/07/06 | first author:Prevedello LM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Purpose To evaluate the performance of an artificial intelligence (AI) tool using a deep learning algorithm for detecting hemorrhage, mass effect, or hydrocephalus (HMH) at non-contrast material-enhanced head computed tomographic (CT) examinations and to determine algorithm performance for detection of suspected acute infarct (SAI). Materials and Methods This HIPAA-compliant retrospective study was completed after institutional review board approval. A training and validation dataset of noncontrast-enhanced head CT examinations that comprised 100 examinations of HMH, 22 of SAI, and 124 of noncritical findings was obtained resulting in 2583 representative images. Examinations were processed by using a convolutional neural network (deep learning) using two different window and level configurations (brain window and stroke window). AI algorithm performance was tested on a separate dataset containing 50 examinations with HMH findings, 15 with SAI findings, and 35 with noncritical findings. Results Final algorithm performance for HMH showed 90% (45 of 50) sensitivity (95% confidence interval [CI]: 78%, 97%) and 85% (68 of 80) specificity (95% CI: 76%, 92%), with area under the receiver operating characteristic curve (AUC) of 0.91 with the brain window. For SAI, the best performance was achieved with the stroke window showing 62% (13 of 21) sensitivity (95% CI: 38%, 82%) and 96% (27 of 28) specificity (95% CI: 82%, 100%), with AUC of 0.81. Conclusion AI using deep learning demonstrates promise for detecting critical findings at noncontrast-enhanced head CT. A dedicated algorithm was required to detect SAI. Detection of SAI showed lower sensitivity in comparison to detection of HMH, but showed reasonable performance. Findings support further investigation of the algorithm in a controlled and prospective clinical setting to determine whether it can independently screen noncontrast-enhanced head CT examinations and notify the interpreting radiologist of critical findings. <sup>©</sup> RSNA, 2017 Online supplemental material is available for this article.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28678669,pubmed,2017,944b5640-8afb-4a16-9703-ccf3075cc7a3,1
deep convolutional neural networks for automatic classification of gastric carcinoma using whole slide images in digital histopathology,/pubmed/28676295,"Sharma H, Zerbe N, Klempert I, Hellwich O, Hufnagl P.",Comput Med Imaging Graph. 2017 Jun 16. pii: S0895-6111(17)30050-2. doi: 10.1016/j.compmedimag.2017.06.001. [Epub ahead of print],Comput Med Imaging Graph.  2017,PubMed,citation,PMID:28676295,pubmed,28676295,create date:2017/07/06 | first author:Sharma H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning using convolutional neural networks is an actively emerging field in histological image analysis. This study explores deep learning methods for computer-aided classification in H&amp;E stained histopathological whole slide images of gastric carcinoma. An introductory convolutional neural network architecture is proposed for two computerized applications, namely, cancer classification based on immunohistochemical response and necrosis detection based on the existence of tumor necrosis in the tissue. Classification performance of the developed deep learning approach is quantitatively compared with traditional image analysis methods in digital histopathology requiring prior computation of handcrafted features, such as statistical measures using gray level co-occurrence matrix, Gabor filter-bank responses, LBP histograms, gray histograms, HSV histograms and RGB histograms, followed by random forest machine learning. Additionally, the widely known AlexNet deep convolutional framework is comparatively analyzed for the corresponding classification problems. The proposed convolutional neural network architecture reports favorable results, with an overall classification accuracy of 0.6990 for cancer classification and 0.8144 for necrosis detection.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",harshita.sharma@campus.tu-berlin.de,Cancer classification; Convolutional neural networks; Deep learning; Digital pathology; Gastric carcinoma; Histopathological image analysis; Necrosis detection,https://www.ncbi.nlm.nih.gov//pubmed/28676295,pubmed,2017,062df464-dc86-4d50-8ae9-e54acd12b0fb,1
deep learning in medical imaging: general overview,/pubmed/28670152,"Lee JG, Jun S, Cho YW, Lee H, Kim GB, Seo JB, Kim N.",Korean J Radiol. 2017 Jul-Aug;18(4):570-584. doi: 10.3348/kjr.2017.18.4.570. Epub 2017 May 19. Review.,Korean J Radiol.  2017,PubMed,citation,PMID:28670152 | PMCID:PMC5447633,pubmed,28670152,create date:2017/07/04 | first author:Lee JG,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The artificial neural network (ANN)-a machine learning technique inspired by the human neuronal synapse system-was introduced in the 1950s. However, the ANN was previously limited in its ability to solve actual problems, due to the vanishing gradient and overfitting problems with training of deep architecture, lack of computing power, and primarily the absence of sufficient data to train the computer system. Interest in this concept has lately resurfaced, due to the availability of big data, enhanced computing power with the current graphics processing units, and novel algorithms to train the deep neural network. Recent studies on this technology suggest its potentially to perform better than humans in some visual and auditory recognition tasks, which may portend its applications in medicine and healthcare, especially in medical imaging, in the foreseeable future. This review article offers perspectives on the history, development, and applications of deep learning technology, particularly regarding its applications in medical imaging.</abstracttext></p></div></div>",,Artificial intelligence; Computer-aided; Convolutional neural network; Machine learning; Precision medicine; Radiology; Recurrent Neural Network,https://www.ncbi.nlm.nih.gov//pubmed/28670152,pubmed,2017,fa78d776-da23-432a-87b8-809f8683e2c9,1
a hierarchical convolutional neural network for vesicle fusion event classification,/pubmed/28669577,"Li H, Mao Y, Yin Z, Xu Y.",Comput Med Imaging Graph. 2017 Sep;60:22-34. doi: 10.1016/j.compmedimag.2017.04.003. Epub 2017 Apr 20.,Comput Med Imaging Graph.  2017,PubMed,citation,PMID:28669577,pubmed,28669577,create date:2017/07/04 | first author:Li H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Quantitative analysis of vesicle exocytosis and classification of different modes of vesicle fusion from the fluorescence microscopy are of primary importance for biomedical researches. In this paper, we propose a novel Hierarchical Convolutional Neural Network (HCNN) method to automatically identify vesicle fusion events in time-lapse Total Internal Reflection Fluorescence Microscopy (TIRFM) image sequences. Firstly, a detection and tracking method is developed to extract image patch sequences containing potential fusion events. Then, a Gaussian Mixture Model (GMM) is applied on each image patch of the patch sequence with outliers rejected for robust Gaussian fitting. By utilizing the high-level time-series intensity change features introduced by GMM and the visual appearance features embedded in some key moments of the fusion process, the proposed HCNN architecture is able to classify each candidate patch sequence into three classes: full fusion event, partial fusion event and non-fusion event. Finally, we validate the performance of our method on 9 challenging datasets that have been annotated by cell biologists, and our method achieves better performances when comparing with three previous methods.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",hl87c@mst.edu,Hierarchical Convolutional Neural Network; TIRFM Image; Vesicle fusion event,https://www.ncbi.nlm.nih.gov//pubmed/28669577,pubmed,2017,bbabada0-ae6c-4d6d-818c-5ea0671e0fe0,1
fully automatic acute ischemic lesion segmentation in dwi using convolutional neural networks,/pubmed/28664034,"Chen L, Bentley P, Rueckert D.",Neuroimage Clin. 2017 Jun 13;15:633-643. doi: 10.1016/j.nicl.2017.06.016. eCollection 2017.,Neuroimage Clin.  2017,PubMed,citation,PMID:28664034 | PMCID:PMC5480013,pubmed,28664034,create date:2017/07/01 | first author:Chen L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Stroke is an acute cerebral vascular disease, which is likely to cause long-term disabilities and death. Acute ischemic lesions occur in most stroke patients. These lesions are treatable under accurate diagnosis and treatments. Although diffusion-weighted MR imaging (DWI) is sensitive to these lesions, localizing and quantifying them manually is costly and challenging for clinicians. In this paper, we propose a novel framework to automatically segment stroke lesions in DWI. Our framework consists of two convolutional neural networks (CNNs): one is an ensemble of two DeconvNets (Noh et al., 2015), which is the EDD Net; the second CNN is the multi-scale convolutional label evaluation net (MUSCLE Net), which aims to evaluate the lesions detected by the EDD Net in order to remove potential false positives. To the best of our knowledge, it is the first attempt to solve this problem and using both CNNs achieves very good results. Furthermore, we study the network architectures and key configurations in detail to ensure the best performance. It is validated on a large dataset comprising clinical acquired DW images from 741 subjects. A mean accuracy of Dice coefficient obtained is 0.67 in total. The mean Dice scores based on subjects with only small and large lesions are 0.61 and 0.83, respectively. The lesion detection rate achieved is 0.94.</abstracttext></p></div></div>",,Acute ischemic lesion segmentation; Convolutional neural networks; DWI; Deep learning,https://www.ncbi.nlm.nih.gov//pubmed/28664034,pubmed,2017,36900ed0-df8a-43a6-8616-40862b927681,1
malignancy detection on mammography using dual deep convolutional neural networks and genetically discovered false color input enhancement,/pubmed/28656455,"Teare P, Fishman M, Benzaquen O, Toledano E, Elnekave E.",J Digit Imaging. 2017 Aug;30(4):499-505. doi: 10.1007/s10278-017-9993-2.,J Digit Imaging.  2017,PubMed,citation,PMID:28656455 | PMCID:PMC5537100,pubmed,28656455,create date:2017/06/29 | first author:Teare P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Breast cancer is the most prevalent malignancy in the US and the third highest cause of cancer-related mortality worldwide. Regular mammography screening has been attributed with doubling the rate of early cancer detection over the past three decades, yet estimates of mammographic accuracy in the hands of experienced radiologists remain suboptimal with sensitivity ranging from 62 to 87% and specificity from 75 to 91%. Advances in machine learning (ML) in recent years have demonstrated capabilities of image analysis which often surpass those of human observers. Here we present two novel techniques to address inherent challenges in the application of ML to the domain of mammography. We describe the use of genetic search of image enhancement methods, leading us to the use of a novel form of false color enhancement through contrast limited adaptive histogram equalization (CLAHE), as a method to optimize mammographic feature representation. We also utilize dual deep convolutional neural networks at different scales, for classification of full mammogram images and derivative patches combined with a random forest gating network as a novel architectural solution capable of discerning malignancy with a specificity of 0.91 and a specificity of 0.80. To our knowledge, this represents the first automatic stand-alone mammography malignancy detection algorithm with sensitivity and specificity performance similar to that of expert radiologists.</abstracttext></p></div></div>",Eldad@zebra-med.com,Convolutional neural networks; Deep learning; Machine learning; Mammography,https://www.ncbi.nlm.nih.gov//pubmed/28656455,pubmed,2017,908ca1aa-aa16-416b-9ed0-74749616b280,1
deep convolutional neural networks for classifying head and neck cancer using hyperspectral imaging,/pubmed/28655055,"Halicek M, Lu G, Little JV, Wang X, Patel M, Griffith CC, El-Deiry MW, Chen AY, Fei B.",J Biomed Opt. 2017 Jun 1;22(6):60503. doi: 10.1117/1.JBO.22.6.060503.,J Biomed Opt.  2017,PubMed,citation,PMID:28655055 | PMCID:PMC5482930,pubmed,28655055,create date:2017/06/28 | first author:Halicek M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Surgical cancer resection requires an accurate and timely diagnosis of the cancer margins in order to achieve successful patient remission. Hyperspectral imaging (HSI) has emerged as a useful, noncontact technique for acquiring spectral and optical properties of tissue. A convolutional neural network (CNN) classifier is developed to classify excised, squamous-cell carcinoma, thyroid cancer, and normal head and neck tissue samples using HSI. The CNN classification was validated by the manual annotation of a pathologist specialized in head and neck cancer. The preliminary results of 50 patients indicate the potential of HSI and deep learning for automatic tissue-labeling of surgical specimens of head and neck patients.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28655055,pubmed,2017,8ca61012-a416-40bb-aeaf-11fb8dde5ad3,1
pixel-level deep segmentation: artificial intelligence quantifies muscle on computed tomography for body morphometric analysis,/pubmed/28653123,"Lee H, Troschel FM, Tajmir S, Fuchs G, Mario J, Fintelmann FJ, Do S.",J Digit Imaging. 2017 Aug;30(4):487-498. doi: 10.1007/s10278-017-9988-z.,J Digit Imaging.  2017,PubMed,citation,PMID:28653123 | PMCID:PMC5537099,pubmed,28653123,create date:2017/06/28 | first author:Lee H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Pretreatment risk stratification is key for personalized medicine. While many physicians rely on an 'eyeball test' to assess whether patients will tolerate major surgery or chemotherapy, 'eyeballing' is inherently subjective and difficult to quantify. The concept of morphometric age derived from cross-sectional imaging has been found to correlate well with outcomes such as length of stay, morbidity, and mortality. However, the determination of the morphometric age is time intensive and requires highly trained experts. In this study, we propose a fully automated deep learning system for the segmentation of skeletal muscle cross-sectional area (CSA) on an axial computed tomography image taken at the third lumbar vertebra. We utilized a fully automated deep segmentation model derived from an extended implementation of a fully convolutional network with weight initialization of an ImageNet pre-trained model, followed by post processing to eliminate intramuscular fat for a more accurate analysis. This experiment was conducted by varying window level (WL), window width (WW), and bit resolutions in order to better understand the effects of the parameters on the model performance. Our best model, fine-tuned on 250 training images and ground truth labels, achieves 0.93 ± 0.02 Dice similarity coefficient (DSC) and 3.68 ± 2.29% difference between predicted and ground truth muscle CSA on 150 held-out test cases. Ultimately, the fully automated segmentation system can be embedded into the clinical environment to accelerate the quantification of muscle and expanded to volume analysis of 3D datasets.</abstracttext></p></div></div>",sdo@mgh.harvard.edu,Artificial intelligence; Computed tomography; Computer-aided diagnosis (CAD); Convolutional neural networks; Deep learning; Muscle segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28653123,pubmed,2017,1a2f05ed-65fd-434a-a77f-209c6ecc5a64,1
incorporating deep learning with convolutional neural networks and position specific scoring matrices for identifying electron transport proteins,/pubmed/28643394,"Le NQ, Ho QT, Ou YY.",J Comput Chem. 2017 Sep 5;38(23):2000-2006. doi: 10.1002/jcc.24842. Epub 2017 Jun 22.,J Comput Chem.  2017,PubMed,citation,PMID:28643394,pubmed,28643394,create date:2017/06/24 | first author:Le NQ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In several years, deep learning is a modern machine learning technique using in a variety of fields with state-of-the-art performance. Therefore, utilization of deep learning to enhance performance is also an important solution for current bioinformatics field. In this study, we try to use deep learning via convolutional neural networks and position specific scoring matrices to identify electron transport proteins, which is an important molecular function in transmembrane proteins. Our deep learning method can approach a precise model for identifying of electron transport proteins with achieved sensitivity of 80.3%, specificity of 94.4%, and accuracy of 92.3%, with MCC of 0.71 for independent dataset. The proposed technique can serve as a powerful tool for identifying electron transport proteins and can help biologists understand the function of the electron transport proteins. Moreover, this study provides a basis for further research that can enrich a field of applying deep learning in bioinformatics. © 2017 Wiley Periodicals, Inc.</abstracttext></p><p class='copyright'>© 2017 Wiley Periodicals, Inc.</p></div></div>",,bioinformatics; convolutional neural network; deep learning; electron transport protein; position specific scoring matrix,https://www.ncbi.nlm.nih.gov//pubmed/28643394,pubmed,2017,27945803-9a38-4231-8256-8ba299734d1c,1
deep convolutional neural network for inverse problems in imaging,/pubmed/28641250,"Jin KH, McCann MT, Froustey E, Unser M.",IEEE Trans Image Process. 2017 Jun 15. doi: 10.1109/TIP.2017.2713099. [Epub ahead of print],IEEE Trans Image Process.  2017,PubMed,citation,PMID:28641250,pubmed,28641250,create date:2017/06/24 | first author:Jin KH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise nonlinearity) when the normal operator ( H*H where H* is the adjoint of the forward imaging operator, H ) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on the GPU.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28641250,pubmed,2017,5387c58d-343f-4f00-b746-d87448c0bd97,1
modeling task fmri data via deep convolutional autoencoder,/pubmed/28641247,"Huang H, Hu X, Zhao Y, Makkie M, Dong Q, Zhao S, Guo L, Liu T.",IEEE Trans Med Imaging. 2017 Jun 15. doi: 10.1109/TMI.2017.2715285. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28641247,pubmed,28641247,create date:2017/06/24 | first author:Huang H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Task-based fMRI (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, Independent Component Analysis (ICA) and Sparse Dictionary Learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this study, we developed a new neural network structure based on CNN, called Deep Convolutional Auto-Encoder (DCAE), in order to take the advantages of both data-driven approach and CNN's hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project (HCP) tfMRI datasets, and promising results are achieved.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28641247,pubmed,2017,ad09a4ce-15ba-490c-b0b5-b4a3687da3ad,1
cnndel: calling structural variations on low coverage data based on convolutional neural networks,/pubmed/28630866,"Wang J, Ling C, Gao J.",Biomed Res Int. 2017;2017:6375059. doi: 10.1155/2017/6375059. Epub 2017 May 28.,Biomed Res Int.  2017,PubMed,citation,PMID:28630866 | PMCID:PMC5467383,pubmed,28630866,create date:2017/06/21 | first author:Wang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Many structural variations (SVs) detection methods have been proposed due to the popularization of next-generation sequencing (NGS). These SV calling methods use different SV-property-dependent features; however, they all suffer from poor accuracy when running on low coverage sequences. The union of results from these tools achieves fairly high sensitivity but still produces low accuracy on low coverage sequence data. That is, these methods contain many false positives. In this paper, we present CNNdel, an approach for calling deletions from paired-end reads. CNNdel gathers SV candidates reported by multiple tools and then extracts features from aligned BAM files at the positions of candidates. With labeled feature-expressed candidates as a training set, CNNdel trains convolutional neural networks (CNNs) to distinguish true unlabeled candidates from false ones. Results show that CNNdel works well with NGS reads from 26 low coverage genomes of the 1000 Genomes Project. The paper demonstrates that convolutional neural networks can automatically assign the priority of SV features and reduce the false positives efficaciously.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28630866,pubmed,2017,1e06ee42-8244-436c-b5d7-d5781ba4fe5c,1
low-dose ct with a residual encoder-decoder convolutional neural network (red-cnn),/pubmed/28622671,"Chen H, Zhang Y, Kalra MK, Lin F, Chen Y, Liao P, Zhou J, Wang G.",IEEE Trans Med Imaging. 2017 Jun 13. doi: 10.1109/TMI.2017.2715284. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28622671,pubmed,28622671,create date:2017/06/18 | first author:Chen H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28622671,pubmed,2017,9099ea16-c811-4f60-ba04-f4d82b0a8e83,1
classification of clinical significance of mri prostate findings using 3d convolutional neural networks,/pubmed/28615793,"Mehrtash A, Sedghi A, Ghafoorian M, Taghipour M, Tempany CM, Wells WM 3rd, Kapur T, Mousavi P, Abolmaesumi P, Fedorov A.",Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10134. pii: 101342A. doi: 10.1117/12.2277123. Epub 2017 Mar 3.,Proc SPIE Int Soc Opt Eng.  2017,PubMed,citation,PMID:28615793 | PMCID:PMC5467889,pubmed,28615793,create date:2017/06/16 | first author:Mehrtash A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Prostate cancer (PCa) remains a leading cause of cancer mortality among American men. Multi-parametric magnetic resonance imaging (mpMRI) is widely used to assist with detection of PCa and characterization of its aggressiveness. Computer-aided diagnosis (CADx) of PCa in MRI can be used as clinical decision support system to aid radiologists in interpretation and reporting of mpMRI. We report on the development of a convolution neural network (CNN) model to support CADx in PCa based on the appearance of prostate tissue in mpMRI, conducted as part of the SPIE-AAPM-NCI PROSTATEx challenge. The performance of different combinations of mpMRI inputs to CNN was assessed and the best result was achieved using DWI and DCE-MRI modalities together with the zonal information of the finding. On the test set, the model achieved an area under the receiver operating characteristic curve of 0.80.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28615793,pubmed,2017,f1ce4b10-f0a2-4315-8a10-74258fd66bbf,1
3d deep convolutional neural networks for amino acid environment similarity analysis,/pubmed/28615003,"Torng W, Altman RB.",BMC Bioinformatics. 2017 Jun 14;18(1):302. doi: 10.1186/s12859-017-1702-0.,BMC Bioinformatics.  2017,PubMed,citation,PMID:28615003 | PMCID:PMC5472009,pubmed,28615003,create date:2017/06/16 | first author:Torng W,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Central to protein biology is the understanding of how structural elements give rise to observed function. The surfeit of protein structural data enables development of computational methods to systematically derive rules governing structural-functional relationships. However, performance of these methods depends critically on the choice of protein structural representation. Most current methods rely on features that are manually selected based on knowledge about protein structures. These are often general-purpose but not optimized for the specific application of interest. In this paper, we present a general framework that applies 3D convolutional neural network (3DCNN) technology to structure-based protein analysis. The framework automatically extracts task-specific features from the raw atom distribution, driven by supervised labels. As a pilot study, we use our network to analyze local protein microenvironments surrounding the 20 amino acids, and predict the amino acids most compatible with environments within a protein structure. To further validate the power of our method, we construct two amino acid substitution matrices from the prediction statistics and use them to predict effects of mutations in T4 lysozyme structures.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our deep 3DCNN achieves a two-fold increase in prediction accuracy compared to models that employ conventional hand-engineered features and successfully recapitulates known information about similar and different microenvironments. Models built from our predictions and substitution matrices achieve an 85% accuracy predicting outcomes of the T4 lysozyme mutation variants. Our substitution matrices contain rich information relevant to mutation analysis compared to well-established substitution matrices. Finally, we present a visualization method to inspect the individual contributions of each atom to the classification decisions.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>End-to-end trained deep learning networks consistently outperform methods using hand-engineered features, suggesting that the 3DCNN framework is well suited for analysis of protein microenvironments and may be useful for other protein structural analyses.</abstracttext></p></div></div>",russ.altman@stanford.edu,Amino acid similarities; Convolutional neural network; Deep learning; Mutation analysis; Protein structural analysis; Structural bioinformatics,https://www.ncbi.nlm.nih.gov//pubmed/28615003,pubmed,2017,15efe6a6-fa14-4c73-87d8-6cd070cc41a0,1
pattern recognition of momentary mental workload based on multi-channel electrophysiological data and ensemble convolutional neural networks,/pubmed/28611583,"Zhang J, Li S, Wang R.",Front Neurosci. 2017 May 30;11:310. doi: 10.3389/fnins.2017.00310. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28611583 | PMCID:PMC5447754,pubmed,28611583,create date:2017/06/15 | first author:Zhang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we deal with the Mental Workload (MWL) classification problem based on the measured physiological data. First we discussed the optimal depth (i.e., the number of hidden layers) and parameter optimization algorithms for the Convolutional Neural Networks (CNN). The base CNNs designed were tested according to five classification performance indices, namely Accuracy, Precision, F-measure, G-mean, and required training time. Then we developed an Ensemble Convolutional Neural Network (ECNN) to enhance the accuracy and robustness of the individual CNN model. For the ECNN design, three model aggregation approaches (weighted averaging, majority voting and stacking) were examined and a resampling strategy was used to enhance the diversity of individual CNN models. The results of MWL classification performance comparison indicated that the proposed ECNN framework can effectively improve MWL classification performance and is featured by entirely automatic feature extraction and MWL classification, when compared with traditional machine learning methods.</abstracttext></p></div></div>",,convolutional neural network; deep learning; electrophysiology; ensemble learning; mental workload; pattern classification,https://www.ncbi.nlm.nih.gov//pubmed/28611583,pubmed,2017,044d7338-b4a8-4c47-885b-f1163402e130,1
"predicting mental conditions based on ""history of present illness"" in psychiatric notes with deep neural networks",/pubmed/28606869,"Tran T, Kavuluru R.",J Biomed Inform. 2017 Jun 10. pii: S1532-0464(17)30133-8. doi: 10.1016/j.jbi.2017.06.010. [Epub ahead of print],J Biomed Inform.  2017,PubMed,citation,PMID:28606869,pubmed,28606869,create date:2017/06/14 | first author:Tran T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Applications of natural language processing to mental health notes are not common given the sensitive nature of the associated narratives. The CEGS N-GRID 2016 Shared Task in Clinical Natural Language Processing (NLP) changed this scenario by providing the first set of neuropsychiatric notes to participants. This study summarizes our efforts and results in proposing a novel data use case for this dataset as part of the third track in this shared task.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>We explore the feasibility and effectiveness of predicting a set of common mental conditions a patient has based on the short textual description of patient's history of present illness typically occurring in the beginning of a psychiatric initial evaluation note.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>We clean and process the 1000 records made available through the N-GRID clinical NLP task into a key-value dictionary and build a dataset of 986 examples for which there is a narrative for history of present illness as well as Yes/No responses with regards to presence of specific mental conditions. We propose two independent deep neural network models: one based on convolutional neural networks (CNN) and another based on recurrent neural networks with hierarchical attention (ReHAN), the latter of which allows for interpretation of model decisions. We conduct experiments to compare these methods to each other and to baselines based on linear models and named entity recognition (NER).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our CNN model with optimized thresholding of output probability estimates achieves best overall mean micro-F score of 63.144% for 11 common mental conditions with statistically significant gains (p&lt;0.05) over all other models. The ReHAN model with interpretable attention mechanism scored 61.904% mean micro-F1 score. Both models' improvements over baseline models (support vector machines and NER) are statistically significant. The ReHAN model additionally aids in interpretation of the results by surfacing important words and sentences that lead to a particular prediction for each instance.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Although the history of present illness is a short text segment averaging 300 words, it is a good predictor for a few conditions such as anxiety, depression, panic disorder, and attention deficit hyperactivity disorder. Proposed CNN and RNN models outperform baseline approaches and complement each other when evaluating on a per-label basis.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Inc.</p></div></div>",tung.tran@uky.edu,Convolutional and recurrent neural networks; Hierarchical attention networks; Multi-label text classification; Psychiatric condition prediction,https://www.ncbi.nlm.nih.gov//pubmed/28606869,pubmed,2017,973f5680-4b72-45e3-ae2e-b76541705028,1
exploring convolutional neural networks for drug-drug interaction extraction,/pubmed/28605776,"Suárez-Paniagua V, Segura-Bedmar I, Martínez P.",Database (Oxford). 2017 Jan 1;2017. doi: 10.1093/database/bax019.,Database (Oxford).  2017,PubMed,citation,PMID:28605776 | PMCID:PMC5467573,pubmed,28605776,create date:2017/06/13 | first author:Suárez-Paniagua V,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Drug-drug interaction (DDI), which is a specific type of adverse drug reaction, occurs when a drug influences the level or activity of another drug. Natural language processing techniques can provide health-care professionals with a novel way of reducing the time spent reviewing the literature for potential DDIs. The current state-of-the-art for the extraction of DDIs is based on feature-engineering algorithms (such as support vector machines), which usually require considerable time and effort. One possible alternative to these approaches includes deep learning. This technique aims to automatically learn the best feature representation from the input data for a given task. The purpose of this paper is to examine whether a convolutional neural network (CNN), which only uses word embeddings as input features, can be applied successfully to classify DDIs from biomedical texts. Proposed herein, is a CNN architecture with only one hidden layer, thus making the model more computationally efficient, and we perform detailed experiments in order to determine the best settings of the model. The goal is to determine the best parameter of this basic CNN that should be considered for future research. The experimental results show that the proposed approach is promising because it attained the second position in the 2013 rankings of the DDI extraction challenge. However, it obtained worse results than previous works using neural networks with more complex architectures.</abstracttext></p><p class='copyright'>© The Author(s) 2017. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28605776,pubmed,2017,2dde59af-5794-4088-bed1-6c43f19e0e76,1
multi-feature based benchmark for cervical dysplasia classification evaluation,/pubmed/28603299,"Xu T, Zhang H, Xin C, Kim E, Long LR, Xue Z, Antani S, Huang X.",Pattern Recognit. 2017 Mar;63:468-475. doi: 10.1016/j.patcog.2016.09.027. Epub 2016 Sep 22.,Pattern Recognit.  2017,PubMed,citation,PMID:28603299 | PMCID:PMC5464748,pubmed,28603299,create date:2017/06/13 | first author:Xu T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cervical cancer is one of the most common types of cancer in women worldwide. Most deaths due to the disease occur in less developed areas of the world. In this work, we introduce a new image dataset along with expert annotated diagnoses for evaluating image-based cervical disease classification algorithms. A large number of Cervigram<sup>®</sup> images are selected from a database provided by the US National Cancer Institute. For each image, we extract three complementary pyramid features: Pyramid histogram in L*A*B* color space (PLAB), Pyramid Histogram of Oriented Gradients (PHOG), and Pyramid histogram of Local Binary Patterns (PLBP). Other than hand-crafted pyramid features, we investigate the performance of convolutional neural network (CNN) features for cervical disease classification. Our experimental results demonstrate the effectiveness of both our hand-crafted and our deep features. We intend to release this multi-feature dataset and our extensive evaluations using seven classic classifiers can serve as the baseline.</abstracttext></p></div></div>",,Cervical cancer screening; computer aided diagnosis; convolutional neural network; image classification; local binary patterns; pyramid histogram,https://www.ncbi.nlm.nih.gov//pubmed/28603299,pubmed,2017,cd4cca4a-e0c9-450f-9263-5e7b634ec1fb,1
deep convolutional neural networks for endotracheal tube position and x-ray image classification: challenges and opportunities,/pubmed/28600640,Lakhani P.,J Digit Imaging. 2017 Aug;30(4):460-468. doi: 10.1007/s10278-017-9980-7.,J Digit Imaging.  2017,PubMed,citation,PMID:28600640 | PMCID:PMC5537094,pubmed,28600640,create date:2017/06/11 | first author:Lakhani P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The goal of this study is to evaluate the efficacy of deep convolutional neural networks (DCNNs) in differentiating subtle, intermediate, and more obvious image differences in radiography. Three different datasets were created, which included presence/absence of the endotracheal (ET) tube (n = 300), low/normal position of the ET tube (n = 300), and chest/abdominal radiographs (n = 120). The datasets were split into training, validation, and test. Both untrained and pre-trained deep neural networks were employed, including AlexNet and GoogLeNet classifiers, using the Caffe framework. Data augmentation was performed for the presence/absence and low/normal ET tube datasets. Receiver operating characteristic (ROC), area under the curves (AUC), and 95% confidence intervals were calculated. Statistical differences of the AUCs were determined using a non-parametric approach. The pre-trained AlexNet and GoogLeNet classifiers had perfect accuracy (AUC 1.00) in differentiating chest vs. abdominal radiographs, using only 45 training cases. For more difficult datasets, including the presence/absence and low/normal position endotracheal tubes, more training cases, pre-trained networks, and data-augmentation approaches were helpful to increase accuracy. The best-performing network for classifying presence vs. absence of an ET tube was still very accurate with an AUC of 0.99. However, for the most difficult dataset, such as low vs. normal position of the endotracheal tube, DCNNs did not perform as well, but achieved a reasonable AUC of 0.81.</abstracttext></p></div></div>",Paras.lakhani@jefferson.edu,Artificial intelligence; Artificial neural networks (ANNs); Classification; Machine learning; Radiography,https://www.ncbi.nlm.nih.gov//pubmed/28600640,pubmed,2017,4f1ffb01-099c-4b82-b32e-3f35295de368,1
dermoscopic image segmentation via multistage fully convolutional networks,/pubmed/28600236,"Bi L, Kim J, Ahn E, Kumar A, Fulham M, Feng D.",IEEE Trans Biomed Eng. 2017 Sep;64(9):2065-2074. doi: 10.1109/TBME.2017.2712771. Epub 2017 Jun 7.,IEEE Trans Biomed Eng.  2017,PubMed,citation,PMID:28600236,pubmed,28600236,create date:2017/06/11 | first author:Bi L,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Segmentation of skin lesions is an important step in the automated computer aided diagnosis of melanoma. However, existing segmentation methods have a tendency to over- or under-segment the lesions and perform poorly when the lesions have fuzzy boundaries, low contrast with the background, inhomogeneous textures, or contain artifacts. Furthermore, the performance of these methods are heavily reliant on the appropriate tuning of a large number of parameters as well as the use of effective preprocessing techniques, such as illumination correction and hair removal.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We propose to leverage fully convolutional networks (FCNs) to automatically segment the skin lesions. FCNs are a neural network architecture that achieves object detection by hierarchically combining low-level appearance information with high-level semantic information. We address the issue of FCN producing coarse segmentation boundaries for challenging skin lesions (e.g., those with fuzzy boundaries and/or low difference in the textures between the foreground and the background) through a multistage segmentation approach in which multiple FCNs learn complementary visual characteristics of different skin lesions; early stage FCNs learn coarse appearance and localization information while late-stage FCNs learn the subtle characteristics of the lesion boundaries. We also introduce a new parallel integration method to combine the complementary information derived from individual segmentation stages to achieve a final segmentation result that has accurate localization and well-defined lesion boundaries, even for the most challenging skin lesions.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We achieved an average Dice coefficient of 91.18% on the ISBI 2016 Skin Lesion Challenge dataset and 90.66% on the PH2 dataset.</abstracttext></p><h4>CONCLUSION AND SIGNIFICANCE: </h4><p><abstracttext label='CONCLUSION AND SIGNIFICANCE' nlmcategory='CONCLUSIONS'>Our extensive experimental results on two well-established public benchmark datasets demonstrate that our method is more effective than other state-of-the-art methods for skin lesion segmentation.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28600236,pubmed,2017,a03c8ecd-536b-4343-a643-40ad954b8bc1,1
detection and diagnosis of colitis on computed tomography using deep convolutional neural networks,/pubmed/28594460,"Liu J, Wang D, Lu L, Wei Z, Kim L, Turkbey EB, Sahiner B, Petrick NA, Summers RM.",Med Phys. 2017 Sep;44(9):4630-4642. doi: 10.1002/mp.12399. Epub 2017 Jul 18.,Med Phys.  2017,PubMed,citation,PMID:28594460,pubmed,28594460,create date:2017/06/09 | first author:Liu J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Colitis refers to inflammation of the inner lining of the colon that is frequently associated with infection and allergic reactions. In this paper, we propose deep convolutional neural networks methods for lesion-level colitis detection and a support vector machine (SVM) classifier for patient-level colitis diagnosis on routine abdominal CT scans.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The recently developed Faster Region-based Convolutional Neural Network (Faster RCNN) is utilized for lesion-level colitis detection. For each 2D slice, rectangular region proposals are generated by region proposal networks (RPN). Then, each region proposal is jointly classified and refined by a softmax classifier and bounding-box regressor. Two convolutional neural networks, eight layers of ZF net and 16 layers of VGG net are compared for colitis detection. Finally, for each patient, the detections on all 2D slices are collected and a SVM classifier is applied to develop a patient-level diagnosis. We trained and evaluated our method with 80 colitis patients and 80 normal cases using 4 × 4-fold cross validation.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>For lesion-level colitis detection, with ZF net, the mean of average precisions (mAP) were 48.7% and 50.9% for RCNN and Faster RCNN, respectively. The detection system achieved sensitivities of 51.4% and 54.0% at two false positives per patient for RCNN and Faster RCNN, respectively. With VGG net, Faster RCNN increased the mAP to 56.9% and increased the sensitivity to 58.4% at two false positive per patient. For patient-level colitis diagnosis, with ZF net, the average areas under the ROC curve (AUC) were 0.978 ± 0.009 and 0.984 ± 0.008 for RCNN and Faster RCNN method, respectively. The difference was not statistically significant with P = 0.18. At the optimal operating point, the RCNN method correctly identified 90.4% (72.3/80) of the colitis patients and 94.0% (75.2/80) of normal cases. The sensitivity improved to 91.6% (73.3/80) and the specificity improved to 95.0% (76.0/80) for the Faster RCNN method. With VGG net, Faster RCNN increased the AUC to 0.986 ± 0.007 and increased the diagnosis sensitivity to 93.7% (75.0/80) and specificity was unchanged at 95.0% (76.0/80).</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Colitis detection and diagnosis by deep convolutional neural networks is accurate and promising for future clinical application.</abstracttext></p><p class='copyright'>Published 2017. This article is a U.S. Government work and is in the public domain in the USA.</p></div></div>",,RPN; Region-based CNN; colitis detection; colitis diagnosis,https://www.ncbi.nlm.nih.gov//pubmed/28594460,pubmed,2017,53aaa619-cfc3-4fa7-93ad-a399933683d3,1
automated image quality evaluation of t(2) -weighted liver mri utilizing deep learning architecture,/pubmed/28577329,"Esses SJ, Lu X, Zhao T, Shanbhogue K, Dane B, Bruno M, Chandarana H.",J Magn Reson Imaging. 2017 Jun 3. doi: 10.1002/jmri.25779. [Epub ahead of print],J Magn Reson Imaging.  2017,PubMed,citation,PMID:28577329,pubmed,28577329,create date:2017/06/04 | first author:Esses SJ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>To develop and test a deep learning approach named Convolutional Neural Network (CNN) for automated screening of T<sub>2</sub> -weighted (T<sub>2</sub> WI) liver acquisitions for nondiagnostic images, and compare this automated approach to evaluation by two radiologists.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>We evaluated 522 liver magnetic resonance imaging (MRI) exams performed at 1.5T and 3T at our institution between November 2014 and May 2016 for CNN training and validation. The CNN consisted of an input layer, convolutional layer, fully connected layer, and output layer. 351 T<sub>2</sub> WI were anonymized for training. Each case was annotated with a label of being diagnostic or nondiagnostic for detecting lesions and assessing liver morphology. Another independently collected 171 cases were sequestered for a blind test. These 171 T<sub>2</sub> WI were assessed independently by two radiologists and annotated as being diagnostic or nondiagnostic. These 171 T<sub>2</sub> WI were presented to the CNN algorithm and image quality (IQ) output of the algorithm was compared to that of two radiologists.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>There was concordance in IQ label between Reader 1 and CNN in 79% of cases and between Reader 2 and CNN in 73%. The sensitivity and the specificity of the CNN algorithm in identifying nondiagnostic IQ was 67% and 81% with respect to Reader 1 and 47% and 80% with respect to Reader 2. The negative predictive value of the algorithm for identifying nondiagnostic IQ was 94% and 86% (relative to Readers 1 and 2).</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>We demonstrate a CNN algorithm that yields a high negative predictive value when screening for nondiagnostic T<sub>2</sub> WI of the liver.</abstracttext></p><h4>LEVEL OF EVIDENCE: </h4><p><abstracttext label='LEVEL OF EVIDENCE' nlmcategory='METHODS'>2 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2017.</abstracttext></p><p class='copyright'>© 2017 International Society for Magnetic Resonance in Medicine.</p></div></div>",,T2 weighted imaging; convolutional neuronal network; deep learning; image quality; liver MRI; machine learning,https://www.ncbi.nlm.nih.gov//pubmed/28577329,pubmed,2017,e0207c62-727f-403b-8a69-fb196762732b,1
multi-level deep supervised networks for retinal vessel segmentation,/pubmed/28577175,"Mo J, Zhang L.",Int J Comput Assist Radiol Surg. 2017 Jun 2. doi: 10.1007/s11548-017-1619-0. [Epub ahead of print],Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28577175,pubmed,28577175,create date:2017/06/04 | first author:Mo J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Changes in the appearance of retinal blood vessels are an important indicator for various ophthalmologic and cardiovascular diseases, including diabetes, hypertension, arteriosclerosis, and choroidal neovascularization. Vessel segmentation from retinal images is very challenging because of low blood vessel contrast, intricate vessel topology, and the presence of pathologies such as microaneurysms and hemorrhages. To overcome these challenges, we propose a neural network-based method for vessel segmentation.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A deep supervised fully convolutional network is developed by leveraging multi-level hierarchical features of the deep networks. To improve the discriminative capability of features in lower layers of the deep network and guide the gradient back propagation to overcome gradient vanishing, deep supervision with auxiliary classifiers is incorporated in some intermediate layers of the network. Moreover, the transferred knowledge learned from other domains is used to alleviate the issue of insufficient medical training data. The proposed approach does not rely on hand-crafted features and needs no problem-specific preprocessing or postprocessing, which reduces the impact of subjective factors.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We evaluate the proposed method on three publicly available databases, the DRIVE, STARE, and CHASE_DB1 databases. Extensive experiments demonstrate that our approach achieves better or comparable performance to state-of-the-art methods with a much faster processing speed, making it suitable for real-world clinical applications. The results of cross-training experiments demonstrate its robustness with respect to the training set.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The proposed approach segments retinal vessels accurately with a much faster processing speed and can be easily applied to other biomedical segmentation tasks.</abstracttext></p></div></div>",leizhang@scu.edu.cn,Deep supervision; Fully convolutional network; Retinal image; Vessel segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28577175,pubmed,2017,857a315b-8144-4198-8564-60c8a524dc53,1
deep learning for brain mri segmentation: state of the art and future directions,/pubmed/28577131,"Akkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ.",J Digit Imaging. 2017 Aug;30(4):449-459. doi: 10.1007/s10278-017-9983-4. Review.,J Digit Imaging.  2017,PubMed,citation,PMID:28577131 | PMCID:PMC5537095,pubmed,28577131,create date:2017/06/04 | first author:Akkus Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.</abstracttext></p></div></div>",bje@mayo.edu,Brain lesion segmentation; Convolutional neural network; Deep learning; Quantitative brain MRI,https://www.ncbi.nlm.nih.gov//pubmed/28577131,pubmed,2017,3de5bd0f-f317-472b-9c07-f0336f6c92bd,1
a preliminary examination of the diagnostic value of deep learning in hip osteoarthritis,/pubmed/28575070,"Xue Y, Zhang R, Deng Y, Chen K, Jiang T.",PLoS One. 2017 Jun 2;12(6):e0178992. doi: 10.1371/journal.pone.0178992. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28575070 | PMCID:PMC5456368,pubmed,28575070,create date:2017/06/03 | first author:Xue Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Hip Osteoarthritis (OA) is a common disease among the middle-aged and elderly people. Conventionally, hip OA is diagnosed by manually assessing X-ray images. This study took the hip joint as the object of observation and explored the diagnostic value of deep learning in hip osteoarthritis. A deep convolutional neural network (CNN) was trained and tested on 420 hip X-ray images to automatically diagnose hip OA. This CNN model achieved a balance of high sensitivity of 95.0% and high specificity of 90.7%, as well as an accuracy of 92.8% compared to the chief physicians. The CNN model performance is comparable to an attending physician with 10 years of experience. The results of this study indicate that deep learning has promising potential in the field of intelligent medical image diagnosis practice.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28575070,pubmed,2017,f19f9d38-6534-4fe8-9306-60203ad42ad2,1
generative adversarial networks for noise reduction in low-dose ct,/pubmed/28574346,"Wolterink JM, Leiner T, Viergever MA, Isgum I.",IEEE Trans Med Imaging. 2017 May 26. doi: 10.1109/TMI.2017.2708987. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28574346,pubmed,28574346,create date:2017/06/03 | first author:Wolterink JM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Noise is inherent to low-dose CT acquisition. We propose to train a convolutional neural network (CNN) jointly with an adversarial CNN to estimate routine-dose CT images from low-dose CT images and hence reduce noise. A generator CNN was trained to transform low-dose CT images into routine-dose CT images using voxel-wise loss minimization. An adversarial discriminator CNN was simultaneously trained to distinguish the output of the generator from routinedose CT images. The performance of this discriminator was used as an adversarial loss for the generator. Experiments were performed using CT images of an anthropomorphic phantom containing calcium inserts, as well as patient non-contrast-enhanced cardiac CT images. The phantom and patients were scanned at 20% and 100% routine clinical dose. Three training strategies were compared: the first used only voxel-wise loss, the second combined voxel-wise loss and adversarial loss, and the third used only adversarial loss. The results showed that training with only voxel-wise loss resulted in the highest peak signal-to-noise ratio with respect to reference routine-dose images. However, the CNNs trained with adversarial loss captured image statistics of routine-dose images better. Noise reduction improved quantification of low-density calcified inserts in phantom CT images and allowed coronary calcium scoring in low-dose patient CT images with high noise levels. Testing took less than 10 seconds per CT volume. CNN-based low-dose CT noise reduction in the image domain is feasible. Training with an adversarial network improves the CNN's ability to generate images with an appearance similar to that of reference routine-dose CT images.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28574346,pubmed,2017,ce12d341-8ca9-4e40-a75e-4b0de05ec215,1
classification of breast cancer histology images using convolutional neural networks,/pubmed/28570557,"Araújo T, Aresta G, Castro E, Rouco J, Aguiar P, Eloy C, Polónia A, Campilho A.",PLoS One. 2017 Jun 1;12(6):e0177544. doi: 10.1371/journal.pone.0177544. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28570557 | PMCID:PMC5453426,pubmed,28570557,create date:2017/06/02 | first author:Araújo T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Breast cancer is one of the main causes of cancer death worldwide. The diagnosis of biopsy tissue with hematoxylin and eosin stained images is non-trivial and specialists often disagree on the final diagnosis. Computer-aided Diagnosis systems contribute to reduce the cost and increase the efficiency of this process. Conventional classification approaches rely on feature extraction methods designed for a specific problem based on field-knowledge. To overcome the many difficulties of the feature-based approaches, deep learning methods are becoming important alternatives. A method for the classification of hematoxylin and eosin stained breast biopsy images using Convolutional Neural Networks (CNNs) is proposed. Images are classified in four classes, normal tissue, benign lesion, in situ carcinoma and invasive carcinoma, and in two classes, carcinoma and non-carcinoma. The architecture of the network is designed to retrieve information at different scales, including both nuclei and overall tissue organization. This design allows the extension of the proposed system to whole-slide histology images. The features extracted by the CNN are also used for training a Support Vector Machine classifier. Accuracies of 77.8% for four class and 83.3% for carcinoma/non-carcinoma are achieved. The sensitivity of our method for cancer cases is 95.6%.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28570557,pubmed,2017,c3191d8b-633c-4798-8df1-7038efa52062,1
assistive lesion-emphasis system: an assistive system for fundus image readers,/pubmed/28560245,"Rangrej SB, Sivaswamy J.",J Med Imaging (Bellingham). 2017 Apr;4(2):024503. doi: 10.1117/1.JMI.4.2.024503. Epub 2017 May 24.,J Med Imaging (Bellingham).  2017,PubMed,citation,PMID:28560245 | PMCID:PMC5443420,pubmed,28560245,create date:2017/06/01 | first author:Rangrej SB,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer-assisted diagnostic (CAD) tools are of interest as they enable efficient decision-making in clinics and the screening of diseases. The traditional approach to CAD algorithm design focuses on the automated detection of abnormalities independent of the end-user, who can be an image reader or an expert. We propose a reader-centric system design wherein a reader's attention is drawn to abnormal regions in a least-obtrusive yet effective manner, using saliency-based emphasis of abnormalities and without altering the appearance of the background tissues. We present an assistive lesion-emphasis system (ALES) based on the above idea, for fundus image-based diabetic retinopathy diagnosis. Lesion-saliency is learnt using a convolutional neural network (CNN), inspired by the saliency model of Itti and Koch. The CNN is used to fine-tune standard low-level filters and learn high-level filters for deriving a lesion-saliency map, which is then used to perform lesion-emphasis via a spatially variant version of gamma correction. The proposed system has been evaluated on public datasets and benchmarked against other saliency models. It was found to outperform other saliency models by 6% to 30% and boost the contrast-to-noise ratio of lesions by more than 30%. Results of a perceptual study also underscore the effectiveness and, hence, the potential of ALES as an assistive tool for readers.</abstracttext></p></div></div>",,color fundus image; computer-assisted diagnostic; convolutional neural network; gamma correction; saliency; selective enhancement,https://www.ncbi.nlm.nih.gov//pubmed/28560245,pubmed,2017,a7249e1c-0039-4739-8c4e-8f2fb4f5419e,1
spotting l3 slice in ct scans using deep convolutional network and transfer learning,/pubmed/28558319,"Belharbi S, Chatelain C, Hérault R, Adam S, Thureau S, Chastan M, Modzelewski R.",Comput Biol Med. 2017 Aug 1;87:95-103. doi: 10.1016/j.compbiomed.2017.05.018. Epub 2017 May 19.,Comput Biol Med.  2017,PubMed,citation,PMID:28558319,pubmed,28558319,create date:2017/05/31 | first author:Belharbi S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this article, we present a complete automated system for spotting a particular slice in a complete 3D Computed Tomography exam (CT scan). Our approach does not require any assumptions on which part of the patient's body is covered by the scan. It relies on an original machine learning regression approach. Our models are learned using the transfer learning trick by exploiting deep architectures that have been pre-trained on imageNet database, and therefore it requires very little annotation for its training. The whole pipeline consists of three steps: i) conversion of the CT scans into Maximum Intensity Projection (MIP) images, ii) prediction from a Convolutional Neural Network (CNN) applied in a sliding window fashion over the MIP image, and iii) robust analysis of the prediction sequence to predict the height of the desired slice within the whole CT scan. Our approach is applied to the detection of the third lumbar vertebra (L3) slice that has been found to be representative to the whole body composition. Our system is evaluated on a database collected in our clinical center, containing 642 CT scans from different patients. We obtained an average localization error of 1.91±2.69 slices (less than 5 mm) in an average time of less than 2.5 s/CT scan, allowing integration of the proposed system into daily clinical routines.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",Sebastien.Adam@univ-rouen.fr,Convolutional neural networks; Deep learning; Maximum intensity projection; Sarcopenia; Slice detection,https://www.ncbi.nlm.nih.gov//pubmed/28558319,pubmed,2017,69980397-2aa2-47e2-8229-287a3318817f,1
electroencephalogram-based decoding cognitive states using convolutional neural network and likelihood ratio based score fusion,/pubmed/28558002,"Zafar R, Dass SC, Malik AS.",PLoS One. 2017 May 30;12(5):e0178410. doi: 10.1371/journal.pone.0178410. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28558002 | PMCID:PMC5448783,pubmed,28558002,create date:2017/05/31 | first author:Zafar R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Electroencephalogram (EEG)-based decoding human brain activity is challenging, owing to the low spatial resolution of EEG. However, EEG is an important technique, especially for brain-computer interface applications. In this study, a novel algorithm is proposed to decode brain activity associated with different types of images. In this hybrid algorithm, convolutional neural network is modified for the extraction of features, a t-test is used for the selection of significant features and likelihood ratio-based score fusion is used for the prediction of brain activity. The proposed algorithm takes input data from multichannel EEG time-series, which is also known as multivariate pattern analysis. Comprehensive analysis was conducted using data from 30 participants. The results from the proposed method are compared with current recognized feature extraction and classification/prediction techniques. The wavelet transform-support vector machine method is the most popular currently used feature extraction and prediction method. This method showed an accuracy of 65.7%. However, the proposed method predicts the novel data with improved accuracy of 79.9%. In conclusion, the proposed algorithm outperformed the current feature extraction and prediction method.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28558002,pubmed,2017,f4ed5452-1cc2-4673-b15d-f0043da64741,1
"large scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features",/pubmed/28549410,"Xu Y, Jia Z, Wang LB, Ai Y, Zhang F, Lai M, Chang EI.",BMC Bioinformatics. 2017 May 26;18(1):281. doi: 10.1186/s12859-017-1685-x.,BMC Bioinformatics.  2017,PubMed,citation,PMID:28549410 | PMCID:PMC5446756,pubmed,28549410,create date:2017/05/28 | first author:Xu Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Histopathology image analysis is a gold standard for cancer recognition and diagnosis. Automatic analysis of histopathology images can help pathologists diagnose tumor and cancer subtypes, alleviating the workload of pathologists. There are two basic types of tasks in digital histopathology image analysis: image classification and image segmentation. Typical problems with histopathology images that hamper automatic analysis include complex clinical representations, limited quantities of training images in a dataset, and the extremely large size of singular images (usually up to gigapixels). The property of extremely large size for a single image also makes a histopathology image dataset be considered large-scale, even if the number of images in the dataset is limited.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In this paper, we propose leveraging deep convolutional neural network (CNN) activation features to perform classification, segmentation and visualization in large-scale tissue histopathology images. Our framework transfers features extracted from CNNs trained by a large natural image database, ImageNet, to histopathology images. We also explore the characteristics of CNN features by visualizing the response of individual neuron components in the last hidden layer. Some of these characteristics reveal biological insights that have been verified by pathologists. According to our experiments, the framework proposed has shown state-of-the-art performance on a brain tumor dataset from the MICCAI 2014 Brain Tumor Digital Pathology Challenge and a colon cancer histopathology image dataset.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The framework proposed is a simple, efficient and effective system for histopathology image automatic analysis. We successfully transfer ImageNet knowledge as deep convolutional activation features to the classification and segmentation of histopathology images with little training data. CNN features are significantly more powerful than expert-designed features.</abstracttext></p></div></div>",xuyan04@gmail.com,Classification; Deep convolution activation feature; Deep learning; Feature learning; Segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28549410,pubmed,2017,15048481-f94a-4d7f-8dac-38cfa9e5aca7,1
a deep convolutional neural network based framework for automatic fetal facial standard plane recognition,/pubmed/28534800,"Yu Z, Tan EL, Ni D, Qin J, Chen S, Li S, Lei B, Wang T.",IEEE J Biomed Health Inform. 2017 May 17. doi: 10.1109/JBHI.2017.2705031. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28534800,pubmed,28534800,create date:2017/05/24 | first author:Yu Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Ultrasound imaging has become a prevalent examination method in prenatal diagnosis. Accurate acquisition of fetal facial standard plane (FFSP) is the most important precondition for subsequent diagnosis and measurement. In the past few years, considerable effort has been devoted to FFSP recognition using various hand-crafted features, but the recognition performance is still unsatisfactory due to the high intra-class variation of FFSPs and the high degree of visual similarity between FFSPs and other non-FFSPs. To improve the recognition performance, we propose a method to automatically recognize FFSP via a deep convolutional neural network (DCNN) architecture. The proposed DCNN consists of 16 convolutional layers with small 3×3 size kernels and three fully connected layers. A global average pooling (GAP) is adopted in the last pooling layer to significantly reduce network parameters, which alleviates the overfitting problems and improves the performance under limited training data. Both the transfer learning strategy and a data augmentation technique tailored for FFSP are implemented to further boost the recognition performance. Extensive experiments demonstrate the advantage of our proposed method over traditional approaches and the effectiveness of DCNN to recognize FFSP for clinical diagnosis.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28534800,pubmed,2017,de8bba1a-edb8-4f19-8899-9c0301e96811,1
3d deeply supervised network for automated segmentation of volumetric medical images,/pubmed/28526212,"Dou Q, Yu L, Chen H, Jin Y, Yang X, Qin J, Heng PA.",Med Image Anal. 2017 Oct;41:40-54. doi: 10.1016/j.media.2017.05.001. Epub 2017 May 8.,Med Image Anal.  2017,PubMed,citation,PMID:28526212,pubmed,28526212,create date:2017/05/21 | first author:Dou Q,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>While deep convolutional neural networks (CNNs) have achieved remarkable success in 2D medical image segmentation, it is still a difficult task for CNNs to segment important organs or structures from 3D medical images owing to several mutually affected challenges, including the complicated anatomical environments in volumetric images, optimization difficulties of 3D networks and inadequacy of training samples. In this paper, we present a novel and efficient 3D fully convolutional network equipped with a 3D deep supervision mechanism to comprehensively address these challenges; we call it 3D DSN. Our proposed 3D DSN is capable of conducting volume-to-volume learning and inference, which can eliminate redundant computations and alleviate the risk of over-fitting on limited training data. More importantly, the 3D deep supervision mechanism can effectively cope with the optimization problem of gradients vanishing or exploding when training a 3D deep model, accelerating the convergence speed and simultaneously improving the discrimination capability. Such a mechanism is developed by deriving an objective function that directly guides the training of both lower and upper layers in the network, so that the adverse effects of unstable gradient changes can be counteracted during the training procedure. We also employ a fully connected conditional random field model as a post-processing step to refine the segmentation results. We have extensively validated the proposed 3D DSN on two typical yet challenging volumetric medical image segmentation tasks: (i) liver segmentation from 3D CT scans and (ii) whole heart and great vessels segmentation from 3D MR images, by participating two grand challenges held in conjunction with MICCAI. We have achieved competitive segmentation results to state-of-the-art approaches in both challenges with a much faster speed, corroborating the effectiveness of our proposed 3D DSN.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",harry.qin@polyu.edu.hk,3D deeply supervised networks; 3D fully convolutional networks; Deep learning; Volumetric medical image segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28526212,pubmed,2017,580d4ac7-1dbb-4137-8092-b20a65676047,1
transfer learning on fused multiparametric mr images for classifying histopathological subtypes of rhabdomyosarcoma,/pubmed/28515009,"Banerjee I, Crawley A, Bhethanabotla M, Daldrup-Link HE, Rubin DL.",Comput Med Imaging Graph. 2017 May 5. pii: S0895-6111(17)30045-9. doi: 10.1016/j.compmedimag.2017.05.002. [Epub ahead of print],Comput Med Imaging Graph.  2017,PubMed,citation,PMID:28515009,pubmed,28515009,create date:2017/05/19 | first author:Banerjee I,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper presents a deep-learning-based CADx for the differential diagnosis of embryonal (ERMS) and alveolar (ARMS) subtypes of rhabdomysarcoma (RMS) solely by analyzing multiparametric MR images. We formulated an automated pipeline that creates a comprehensive representation of tumor by performing a fusion of diffusion-weighted MR scans (DWI) and gadolinium chelate-enhanced T1-weighted MR scans (MRI). Finally, we adapted transfer learning approach where a pre-trained deep convolutional neural network has been fine-tuned based on the fused images for performing classification of the two RMS subtypes. We achieved 85% cross validation prediction accuracy from the fine-tuned deep CNN model. Our system can be exploited to provide a fast, efficient and reproducible diagnosis of RMS subtypes with less human interaction. The framework offers an efficient integration between advanced image processing methods and cutting-edge deep learning techniques which can be extended to deal with other clinical domains that involve multimodal imaging for disease diagnosis.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",imonb@stanford.edu,Computer aided diagnosis; Deep neural networks; Image fusion; Rhabdomyosarcoma; Transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/28515009,pubmed,2017,63e50da8-8b47-4ace-bb3a-d9953c6ba9d2,1
word sense disambiguation of medical terms via recurrent convolutional neural networks,/pubmed/28508773,"Festag S, Spreckelsen C.",Stud Health Technol Inform. 2017;236:8-15.,Stud Health Technol Inform.  2017,PubMed,citation,PMID:28508773,pubmed,28508773,create date:2017/05/17 | first author:Festag S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Tagging text data with codes representing biomedical concepts plays an important role in medical data management and analysis. A problem occurs if there are ambiguous words linked to several concepts.</abstracttext></p><h4>OBJECTIVES AND METHODS: </h4><p><abstracttext label='OBJECTIVES AND METHODS' nlmcategory='OBJECTIVE'>This study aims at investigating word sense disambiguation based on word embedding and recurrent convolutional neural networks. The study focuses on terms mapped to multiple concepts of the Unified Medical Language System (UMLS).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We created 20 text processing pipelines trained on a subset of the MeSH Word Sense Disambiguation (MSH WSD) data set, each pipeline disambiguating the sense of one word. The pipelines were then tested on a disjoint subset of MSH WSD data. Most pipelines achieved good or even excellent results (70% of the pipelines achieved at least 90% accuracy, 40% achieved at least 98% accuracy). One poor-performing outlier was detected.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The proposed approach can serve as a basis for an up-scaled system combining pipelines for many ambiguous words. The methods used here recently proved very successful in other fields of text understanding and can be expected to scale-up with improved availability of training data.</abstracttext></p></div></div>",,Neural Networks (Computer); Unified Medical Language System,https://www.ncbi.nlm.nih.gov//pubmed/28508773,pubmed,2017,d4ff4f68-d62b-48ed-b85c-4d757d9a6c69,1
ordinal convolutional neural networks for predicting rdoc positive valence psychiatric symptom severity scores,/pubmed/28506904,"Rios A, Kavuluru R.",J Biomed Inform. 2017 May 12. pii: S1532-0464(17)30103-X. doi: 10.1016/j.jbi.2017.05.008. [Epub ahead of print],J Biomed Inform.  2017,PubMed,citation,PMID:28506904,pubmed,28506904,create date:2017/05/17 | first author:Rios A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>The CEGS N-GRID 2016 Shared Task in Clinical Natural Language Processing (NLP) provided a set of 1000 neuropsychiatric notes to participants as part of a competition to predict psychiatric symptom severity scores. This paper summarizes our methods, results, and experiences based on our participation in the second track of the shared task.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Classical methods of text classification usually fall into one of three problem types: binary, multi-class, and multi-label classification. In this effort, we study ordinal regression problems with text data where misclassifications are penalized differently based on how far apart the ground truth and model predictions are on the ordinal scale. Specifically, we present our entries (methods and results) in the N-GRID shared task in predicting research domain criteria (RDoC) positive valence ordinal symptom severity scores (absent, mild, moderate, and severe) from psychiatric notes.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We propose a novel convolutional neural network (CNN) model designed to handle ordinal regression tasks on psychiatric notes. Broadly speaking, our model combines an ordinal loss function, a CNN, and conventional feature engineering (wide features) into a single model which is learned end-to-end. Given interpretability is an important concern with nonlinear models, we apply a recent approach called locally interpretable model-agnostic explanation (LIME) to identify important words that lead to instance specific predictions.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our best model entered into the shared task placed third among 24 teams and scored a macro mean absolute error (MMAE) based normalized score (100·(1-MMAE)) of 83.86. Since the competition, we improved our score (using basic ensembling) to 85.55, comparable with the winning shared task entry. Applying LIME to model predictions, we demonstrate the feasibility of instance specific prediction interpretation by identifying words that led to a particular decision.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>In this paper, we present a method that successfully uses wide features and an ordinal loss function applied to convolutional neural networks for ordinal text classification specifically in predicting psychiatric symptom severity scores. Our approach leads to excellent performance on the N-GRID shared task and is also amenable to interpretability using existing model-agnostic approaches.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",anthony.rios1@uky.edu,Convolutional neural networks; Model interpretability; Ordinal regression; Research domain criteria; Text classification,https://www.ncbi.nlm.nih.gov//pubmed/28506904,pubmed,2017,b52b3551-79b5-4c69-b2fc-d8d7aada8d63,1
pulmonary nodule classification with deep residual networks,/pubmed/28501942,"Nibali A, He Z, Wollersheim D.",Int J Comput Assist Radiol Surg. 2017 May 13. doi: 10.1007/s11548-017-1605-6. [Epub ahead of print],Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28501942,pubmed,28501942,create date:2017/05/16 | first author:Nibali A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>PURPOSE  : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains.</abstracttext></p></div></div>",anibali@students.latrobe.edu.au,CT images; Convolutional neural network; Lung nodule,https://www.ncbi.nlm.nih.gov//pubmed/28501942,pubmed,2017,8214158f-64ea-499d-af40-1ceaf72de02c,1
a two-step convolutional neural network based computer-aided detection scheme for automatically segmenting adipose tissue volume depicting on ct images,/pubmed/28495009,"Wang Y, Qiu Y, Thai T, Moore K, Liu H, Zheng B.",Comput Methods Programs Biomed. 2017 Jun;144:97-104. doi: 10.1016/j.cmpb.2017.03.017. Epub 2017 Mar 21.,Comput Methods Programs Biomed.  2017,PubMed,citation,PMID:28495009 | PMCID:PMC5441239,pubmed,28495009,create date:2017/05/13 | first author:Wang Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurately assessment of adipose tissue volume inside a human body plays an important role in predicting disease or cancer risk, diagnosis and prognosis. In order to overcome limitation of using only one subjectively selected CT image slice to estimate size of fat areas, this study aims to develop and test a computer-aided detection (CAD) scheme based on deep learning technique to automatically segment subcutaneous fat areas (SFA) and visceral fat areas (VFA) depicting on volumetric CT images. A retrospectively collected CT image dataset was divided into two independent training and testing groups. The proposed CAD framework consisted of two steps with two convolution neural networks (CNNs) namely, Selection-CNN and Segmentation-CNN. The first CNN was trained using 2,240 CT slices to select abdominal CT slices depicting SFA and VFA. The second CNN was trained with 84,000pixel patches and applied to the selected CT slices to identify fat-related pixels and assign them into SFA and VFA classes. Comparing to the manual CT slice selection and fat pixel segmentation results, the accuracy of CT slice selection using the Selection-CNN yielded 95.8%, while the accuracy of fat pixel segmentation using the Segmentation-CNN was 96.8%. This study demonstrated the feasibility of applying a new deep learning based CAD scheme to automatically recognize abdominal section of human body from CT scans and segment SFA and VFA from volumetric CT data with high accuracy or agreement with the manual segmentation results.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",yunzhi.wang-1@ou.edu,Computer-aided detection (CAD); Convolution neural network (CNN); Deep learning; Segmentation of adipose tissue; Subcutaneous fat area (SFA); Visceral fat area (VFA),https://www.ncbi.nlm.nih.gov//pubmed/28495009,pubmed,2017,16795d7d-98eb-44d4-bf0e-425a3ffac179,1
precision radiology: predicting longevity using feature engineering and deep learning methods in a radiomics framework,/pubmed/28490744,"Oakden-Rayner L, Carneiro G, Bessen T, Nascimento JC, Bradley AP, Palmer LJ.",Sci Rep. 2017 May 10;7(1):1648. doi: 10.1038/s41598-017-01931-w.,Sci Rep.  2017,PubMed,citation,PMID:28490744 | PMCID:PMC5431941,pubmed,28490744,create date:2017/05/12 | first author:Oakden-Rayner L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Precision medicine approaches rely on obtaining precise knowledge of the true state of health of an individual patient, which results from a combination of their genetic risks and environmental exposures. This approach is currently limited by the lack of effective and efficient non-invasive medical tests to define the full range of phenotypic variation associated with individual health. Such knowledge is critical for improved early intervention, for better treatment decisions, and for ameliorating the steadily worsening epidemic of chronic disease. We present proof-of-concept experiments to demonstrate how routinely acquired cross-sectional CT imaging may be used to predict patient longevity as a proxy for overall individual health and disease status using computer image analysis techniques. Despite the limitations of a modest dataset and the use of off-the-shelf machine learning methods, our results are comparable to previous 'manual' clinical methods for longevity prediction. This work demonstrates that radiomics techniques can be used to extract biomarkers relevant to one of the most widely used outcomes in epidemiological and clinical research - mortality, and that deep learning with convolutional neural networks can be usefully applied to radiomics research. Computer image analysis applied to routinely collected medical images offers substantial potential to enhance precision medicine initiatives.</abstracttext></p></div></div>",lukeoakdenrayner@gmail.com,,https://www.ncbi.nlm.nih.gov//pubmed/28490744,pubmed,2017,dbeda1cb-01cc-4b85-8ffb-a84bb0dde36d,1
two-phase deep convolutional neural network for reducing class skewness in histopathological images based breast cancer detection,/pubmed/28477446,"Wahab N, Khan A, Lee YS.",Comput Biol Med. 2017 Jun 1;85:86-97. doi: 10.1016/j.compbiomed.2017.04.012. Epub 2017 Apr 18.,Comput Biol Med.  2017,PubMed,citation,PMID:28477446,pubmed,28477446,create date:2017/05/10 | first author:Wahab N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Different types of breast cancer are affecting lives of women across the world. Common types include Ductal carcinoma in situ (DCIS), Invasive ductal carcinoma (IDC), Tubular carcinoma, Medullary carcinoma, and Invasive lobular carcinoma (ILC). While detecting cancer, one important factor is mitotic count - showing how rapidly the cells are dividing. But the class imbalance problem, due to the small number of mitotic nuclei in comparison to the overwhelming number of non-mitotic nuclei, affects the performance of classification models. This work presents a two-phase model to mitigate the class biasness issue while classifying mitotic and non-mitotic nuclei in breast cancer histopathology images through a deep convolutional neural network (CNN). First, nuclei are segmented out using blue ratio and global binary thresholding. In Phase-1 a CNN is then trained on the segmented out 80×80 pixel patches based on a standard dataset. Hard non-mitotic examples are identified and augmented; mitotic examples are oversampled by rotation and flipping; whereas non-mitotic examples are undersampled by blue ratio histogram based k-means clustering. Based on this information from Phase-1, the dataset is modified for Phase-2 in order to reduce the effects of class imbalance. The proposed CNN architecture and data balancing technique yielded an F-measure of 0.79, and outperformed all the methods relying on specific handcrafted features, as well as those using a combination of handcrafted and CNN-generated features.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",asif@pieas.edu.pk,Breast cancer; Class imbalance; Convolutional neural networks; Deep learning; Histopathology; Mitosis count,https://www.ncbi.nlm.nih.gov//pubmed/28477446,pubmed,2017,b78b9a03-763d-4018-8d1c-80b9ec57228c,1
deep learning for automated extraction of primary sites from cancer pathology reports,/pubmed/28475069,"Qiu J, Yoon HJ, Fearn PA, Tourassi GD.",IEEE J Biomed Health Inform. 2017 May 3. doi: 10.1109/JBHI.2017.2700722. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28475069,pubmed,28475069,create date:2017/05/06 | first author:Qiu J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study we investigated deep learning and a convolutional neural network (CNN), for extracting ICDO- 3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro and macro-F score increases of up to 0.132 and 0.226 respectively when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28475069,pubmed,2017,999d0200-4f5d-4e98-95e5-548ee7804485,1
automatic feature learning using multichannel roi based on deep structured algorithms for computerized lung cancer diagnosis,/pubmed/28473055,"Sun W, Zheng B, Qian W.",Comput Biol Med. 2017 Apr 13. pii: S0010-4825(17)30092-6. doi: 10.1016/j.compbiomed.2017.04.006. [Epub ahead of print],Comput Biol Med.  2017,PubMed,citation,PMID:28473055,pubmed,28473055,create date:2017/05/06 | first author:Sun W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This study aimed to analyze the ability of extracting automatically generated features using deep structured algorithms in lung nodule CT image diagnosis, and compare its performance with traditional computer aided diagnosis (CADx) systems using hand-crafted features. All of the 1018 cases were acquired from Lung Image Database Consortium (LIDC) public lung cancer database. The nodules were segmented according to four radiologists' markings, and 13,668 samples were generated by rotating every slice of nodule images. Three multichannel ROI based deep structured algorithms were designed and implemented in this study: convolutional neural network (CNN), deep belief network (DBN), and stacked denoising autoencoder (SDAE). For the comparison purpose, we also implemented a CADx system using hand-crafted features including density features, texture features and morphological features. The performance of every scheme was evaluated by using a 10-fold cross-validation method and an assessment index of the area under the receiver operating characteristic curve (AUC). The observed highest area under the curve (AUC) was 0.899±0.018 achieved by CNN, which was significantly higher than traditional CADx with the AUC=0.848±0.026. The results from DBN was also slightly higher than CADx, while SDAE was slightly lower. By visualizing the automatic generated features, we found some meaningful detectors like curvy stroke detectors from deep structured schemes. The study results showed the deep structured algorithms with automatically generated features can achieve desirable performance in lung nodule diagnosis. With well-tuned parameters and large enough dataset, the deep learning algorithms can have better performance than current popular CADx. We believe the deep learning algorithms with similar data preprocessing procedure can be used in other medical image analysis areas as well.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Ltd.</p></div></div>",wqian@utep.edu,Big data; Computer aided diagnosis; Deep learning; Lung cancer; Unsupervised feature learning,https://www.ncbi.nlm.nih.gov//pubmed/28473055,pubmed,2017,9cae9ed7-de63-41e1-897d-e531b45fa990,1
low data drug discovery with one-shot learning,/pubmed/28470045,"Altae-Tran H, Ramsundar B, Pappu AS, Pande V.",ACS Cent Sci. 2017 Apr 26;3(4):283-293. doi: 10.1021/acscentsci.6b00367. Epub 2017 Apr 3.,ACS Cent Sci.  2017,PubMed,citation,PMID:28470045 | PMCID:PMC5408335,pubmed,28470045,create date:2017/05/05 | first author:Altae-Tran H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds (Ma, J. et al. J. Chem. Inf.</abstracttext></p><h4>MODEL: </h4><p><abstracttext label='MODEL' nlmcategory='METHODS'>2015, 55, 263-274). However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the iterative refinement long short-term memory, that, when combined with graph convolutional neural networks, significantly improves learning of meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery (Ramsundar, B. deepchem.io. https://github.com/deepchem/deepchem, 2016).</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28470045,pubmed,2017,acfdf604-eda9-4a34-82c7-6f971ea1035d,1
deep learning-based artificial vision for grasp classification in myoelectric hands,/pubmed/28467317,"Ghazaei G, Alameer A, Degenaar P, Morgan G, Nazarpour K.",J Neural Eng. 2017 Jun;14(3):036025. doi: 10.1088/1741-2552/aa6802. Epub 2017 May 3.,J Neural Eng.  2017,PubMed,citation,PMID:28467317,pubmed,28467317,create date:2017/05/04 | first author:Ghazaei G,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Computer vision-based assistive technology solutions can revolutionise the quality of care for people with sensorimotor disorders. The goal of this work was to enable trans-radial amputees to use a simple, yet efficient, computer vision system to grasp and move common household objects with a two-channel myoelectric prosthetic hand.</abstracttext></p><h4>APPROACH: </h4><p><abstracttext label='APPROACH' nlmcategory='METHODS'>We developed a deep learning-based artificial vision system to augment the grasp functionality of a commercial prosthesis. Our main conceptual novelty is that we classify objects with regards to the grasp pattern without explicitly identifying them or measuring their dimensions. A convolutional neural network (CNN) structure was trained with images of over 500 graspable objects. For each object, 72 images, at [Formula: see text] intervals, were available. Objects were categorised into four grasp classes, namely: pinch, tripod, palmar wrist neutral and palmar wrist pronated. The CNN setting was first tuned and tested offline and then in realtime with objects or object views that were not included in the training set.</abstracttext></p><h4>MAIN RESULTS: </h4><p><abstracttext label='MAIN RESULTS' nlmcategory='RESULTS'>The classification accuracy in the offline tests reached [Formula: see text] for the seen and [Formula: see text] for the novel objects; reflecting the generalisability of grasp classification. We then implemented the proposed framework in realtime on a standard laptop computer and achieved an overall score of [Formula: see text] in classifying a set of novel as well as seen but randomly-rotated objects. Finally, the system was tested with two trans-radial amputee volunteers controlling an i-limb Ultra<sup>TM</sup> prosthetic hand and a motion control<sup>TM</sup> prosthetic wrist; augmented with a webcam. After training, subjects successfully picked up and moved the target objects with an overall success of up to [Formula: see text]. In addition, we show that with training, subjects' performance improved in terms of time required to accomplish a block of 24 trials despite a decreasing level of visual feedback.</abstracttext></p><h4>SIGNIFICANCE: </h4><p><abstracttext label='SIGNIFICANCE' nlmcategory='CONCLUSIONS'>The proposed design constitutes a substantial conceptual improvement for the control of multi-functional prosthetic hands. We show for the first time that deep-learning based computer vision systems can enhance the grip functionality of myoelectric hands considerably.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28467317,pubmed,2017,2b0dfb92-4188-4887-ab93-d4d967e6206b,1
global detection approach for clustered microcalcifications in mammograms using a deep learning network,/pubmed/28466029,"Wang J, Nishikawa RM, Yang Y.",J Med Imaging (Bellingham). 2017 Apr;4(2):024501. doi: 10.1117/1.JMI.4.2.024501. Epub 2017 Apr 22.,J Med Imaging (Bellingham).  2017,PubMed,citation,PMID:28466029 | PMCID:PMC5400890,pubmed,28466029,create date:2017/05/04 | first author:Wang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In computerized detection of clustered microcalcifications (MCs) from mammograms, the traditional approach is to apply a pattern detector to locate the presence of individual MCs, which are subsequently grouped into clusters. Such an approach is often susceptible to the occurrence of false positives (FPs) caused by local image patterns that resemble MCs. We investigate the feasibility of a direct detection approach to determining whether an image region contains clustered MCs or not. Toward this goal, we develop a deep convolutional neural network (CNN) as the classifier model to which the input consists of a large image window ([Formula: see text] in size). The multiple layers in the CNN classifier are trained to automatically extract image features relevant to MCs at different spatial scales. In the experiments, we demonstrated this approach on a dataset consisting of both screen-film mammograms and full-field digital mammograms. We evaluated the detection performance both on classifying image regions of clustered MCs using a receiver operating characteristic (ROC) analysis and on detecting clustered MCs from full mammograms by a free-response receiver operating characteristic analysis. For comparison, we also considered a recently developed MC detector with FP suppression. In classifying image regions of clustered MCs, the CNN classifier achieved 0.971 in the area under the ROC curve, compared to 0.944 for the MC detector. In detecting clustered MCs from full mammograms, at 90% sensitivity, the CNN classifier obtained an FP rate of 0.69 clusters/image, compared to 1.17 clusters/image by the MC detector. These results indicate that using global image features can be more effective in discriminating clustered MCs from FPs caused by various sources, such as linear structures, thereby providing a more accurate detection of clustered MCs on mammograms.</abstracttext></p></div></div>",,clustered microcalcifications; computer-aided detection; convolutional neural network; deep learning,https://www.ncbi.nlm.nih.gov//pubmed/28466029,pubmed,2017,95e648c8-3712-43c4-ab77-fda549bb6e49,1
tongue images classification based on constrained high dispersal network,/pubmed/28465706,"Meng D, Cao G, Duan Y, Zhu M, Tu L, Xu D, Xu J.",Evid Based Complement Alternat Med. 2017;2017:7452427. doi: 10.1155/2017/7452427. Epub 2017 Mar 30.,Evid Based Complement Alternat Med.  2017,PubMed,citation,PMID:28465706 | PMCID:PMC5390589,pubmed,28465706,create date:2017/05/04 | first author:Meng D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer aided tongue diagnosis has a great potential to play important roles in traditional Chinese medicine (TCM). However, the majority of the existing tongue image analyses and classification methods are based on the low-level features, which may not provide a holistic view of the tongue. Inspired by deep convolutional neural network (CNN), we propose a novel feature extraction framework called constrained high dispersal neural networks (CHDNet) to extract unbiased features and reduce human labor for tongue diagnosis in TCM. Previous CNN models have mostly focused on learning convolutional filters and adapting weights between them, but these models have two major issues: redundancy and insufficient capability in handling unbalanced sample distribution. We introduce high dispersal and local response normalization operation to address the issue of redundancy. We also add multiscale feature analysis to avoid the problem of sensitivity to deformation. Our proposed CHDNet learns high-level features and provides more classification information during training time, which may result in higher accuracy when predicting testing samples. We tested the proposed method on a set of 267 gastritis patients and a control group of 48 healthy volunteers. Test results show that CHDNet is a promising method in tongue image classification for the TCM study.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28465706,pubmed,2017,1b6a0c4e-5030-4755-acfa-bc764cad70c7,1
3d fully convolutional networks for subcortical segmentation in mri: a large-scale study,/pubmed/28450139,"Dolz J, Desrosiers C, Ben Ayed I.",Neuroimage. 2017 Apr 24. pii: S1053-8119(17)30332-4. doi: 10.1016/j.neuroimage.2017.04.039. [Epub ahead of print] Review.,Neuroimage.  2017,PubMed,citation,PMID:28450139,pubmed,28450139,create date:2017/04/30 | first author:Dolz J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This study investigates a 3D and fully convolutional neural network (CNN) for subcortical brain structure segmentation in MRI. 3D CNN architectures have been generally avoided due to their computational and memory requirements during inference. We address the problem via small kernels, allowing deeper architectures. We further model both local and global context by embedding intermediate-layer outputs in the final prediction, which encourages consistency between features extracted at different scales and embeds fine-grained information directly in the segmentation process. Our model is efficiently trained end-to-end on a graphics processing unit (GPU), in a single stage, exploiting the dense inference capabilities of fully CNNs. We performed comprehensive experiments over two publicly available datasets. First, we demonstrate a state-of-the-art performance on the ISBR dataset. Then, we report a large-scale multi-site evaluation over 1112 unregistered subject datasets acquired from 17 different sites (ABIDE dataset), with ages ranging from 7 to 64 years, showing that our method is robust to various acquisition protocols, demographics and clinical factors. Our method yielded segmentations that are highly consistent with a standard atlas-based approach, while running in a fraction of the time needed by atlas-based methods and avoiding registration/normalization steps. This makes it convenient for massive multi-site neuroanatomical imaging studies. To the best of our knowledge, our work is the first to study subcortical structure segmentation on such large-scale and heterogeneous data.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",jose.dolz.upv@gmail.com,3D CNN; Brain; Deep learning; Fully CNN; MRI segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28450139,pubmed,2017,385af1b2-7b38-4bfe-bc01-a80d2a3a0ae2,1
voxresnet: deep voxelwise residual networks for brain segmentation from 3d mr images,/pubmed/28445774,"Chen H, Dou Q, Yu L, Qin J, Heng PA.",Neuroimage. 2017 Apr 23. pii: S1053-8119(17)30334-8. doi: 10.1016/j.neuroimage.2017.04.041. [Epub ahead of print] Review.,Neuroimage.  2017,PubMed,citation,PMID:28445774,pubmed,28445774,create date:2017/04/27 | first author:Chen H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Segmentation of key brain tissues from 3D medical images is of great significance for brain disease diagnosis, progression assessment and monitoring of neurologic conditions. While manual segmentation is time-consuming, laborious, and subjective, automated segmentation is quite challenging due to the complicated anatomical environment of brain and the large variations of brain tissues. We propose a novel voxelwise residual network (VoxResNet) with a set of effective training schemes to cope with this challenging problem. The main merit of residual learning is that it can alleviate the degradation problem when training a deep network so that the performance gains achieved by increasing the network depth can be fully leveraged. With this technique, our VoxResNet is built with 25 layers, and hence can generate more representative features to deal with the large variations of brain tissues than its rivals using hand-crafted features or shallower networks. In order to effectively train such a deep network with limited training data for brain segmentation, we seamlessly integrate multi-modality and multi-level contextual information into our network, so that the complementary information of different modalities can be harnessed and features of different scales can be exploited. Furthermore, an auto-context version of the VoxResNet is proposed by combining the low-level image appearance features, implicit shape information, and high-level context together for further improving the segmentation performance. Extensive experiments on the well-known benchmark (i.e., MRBrainS) of brain segmentation from 3D magnetic resonance (MR) images corroborated the efficacy of the proposed VoxResNet. Our method achieved the first place in the challenge out of 37 competitors including several state-of-the-art brain segmentation methods. Our method is inherently general and can be readily applied as a powerful tool to many brain-related studies, where accurate segmentation of brain structures is critical.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",hchen@cse.cuhk.edu.hk,3D deep learning; Auto-context; Brain segmentation; Convolutional neural network; Multi-level contextual information; Multi-modality; Residual learning,https://www.ncbi.nlm.nih.gov//pubmed/28445774,pubmed,2017,c4036e3c-35ac-40f1-8936-dc8cb2112175,1
brainsegnet: a convolutional neural network architecture for automated segmentation of human brain structures,/pubmed/28439524,"Mehta R, Majumdar A, Sivaswamy J.",J Med Imaging (Bellingham). 2017 Apr;4(2):024003. doi: 10.1117/1.JMI.4.2.024003. Epub 2017 Apr 20.,J Med Imaging (Bellingham).  2017,PubMed,citation,PMID:28439524 | PMCID:PMC5397775,pubmed,28439524,create date:2017/04/26 | first author:Mehta R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated segmentation of cortical and noncortical human brain structures has been hitherto approached using nonrigid registration followed by label fusion. We propose an alternative approach for this using a convolutional neural network (CNN) which classifies a voxel into one of many structures. Four different kinds of two-dimensional and three-dimensional intensity patches are extracted for each voxel, providing local and global (context) information to the CNN. The proposed approach is evaluated on five different publicly available datasets which differ in the number of labels per volume. The obtained mean Dice coefficient varied according to the number of labels, for example, it is [Formula: see text] and [Formula: see text] for datasets with the least (32) and the most (134) number of labels, respectively. These figures are marginally better or on par with those obtained with the current state-of-the-art methods on nearly all datasets, at a reduced computational time. The consistently good performance of the proposed method across datasets and no requirement for registration make it attractive for many applications where reduced computational time is necessary.</abstracttext></p></div></div>",,brain MRI; convolutional neural networks; multiatlas segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28439524,pubmed,2017,8ab770fc-7d86-45c0-8284-ad5cd752ff9f,1
convolutional neural network regression for short-axis left ventricle segmentation in cardiac cine mr sequences,/pubmed/28437634,"Tan LK, Liew YM, Lim E, McLaughlin RA.",Med Image Anal. 2017 Jul;39:78-86. doi: 10.1016/j.media.2017.04.002. Epub 2017 Apr 12.,Med Image Anal.  2017,PubMed,citation,PMID:28437634,pubmed,28437634,create date:2017/04/25 | first author:Tan LK,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated left ventricular (LV) segmentation is crucial for efficient quantification of cardiac function and morphology to aid subsequent management of cardiac pathologies. In this paper, we parameterize the complete (all short axis slices and phases) LV segmentation task in terms of the radial distances between the LV centerpoint and the endo- and epicardial contours in polar space. We then utilize convolutional neural network regression to infer these parameters. Utilizing parameter regression, as opposed to conventional pixel classification, allows the network to inherently reflect domain-specific physical constraints. We have benchmarked our approach primarily against the publicly-available left ventricle segmentation challenge (LVSC) dataset, which consists of 100 training and 100 validation cardiac MRI cases representing a heterogeneous mix of cardiac pathologies and imaging parameters across multiple centers. Our approach attained a .77 Jaccard index, which is the highest published overall result in comparison to other automated algorithms. To test general applicability, we also evaluated against the Kaggle Second Annual Data Science Bowl, where the evaluation metric was the indirect clinical measures of LV volume rather than direct myocardial contours. Our approach attained a Continuous Ranked Probability Score (CRPS) of .0124, which would have ranked tenth in the original challenge. With this we demonstrate the effectiveness of convolutional neural network regression paired with domain-specific features in clinical segmentation.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",liewym@um.edu.my,Cardiac MRI; Convolutional neural networks; Deep learning; LV segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28437634,pubmed,2017,2e486f47-2c44-4593-be03-daf8a6dc369b,1
deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks,/pubmed/28436741,"Lakhani P, Sundaram B.",Radiology. 2017 Aug;284(2):574-582. doi: 10.1148/radiol.2017162326. Epub 2017 Apr 24.,Radiology.  2017,PubMed,citation,PMID:28436741,pubmed,28436741,create date:2017/04/25 | first author:Lakhani P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Purpose To evaluate the efficacy of deep convolutional neural networks (DCNNs) for detecting tuberculosis (TB) on chest radiographs. Materials and Methods Four deidentified HIPAA-compliant datasets were used in this study that were exempted from review by the institutional review board, which consisted of 1007 posteroanterior chest radiographs. The datasets were split into training (68.0%), validation (17.1%), and test (14.9%). Two different DCNNs, AlexNet and GoogLeNet, were used to classify the images as having manifestations of pulmonary TB or as healthy. Both untrained and pretrained networks on ImageNet were used, and augmentation with multiple preprocessing techniques. Ensembles were performed on the best-performing algorithms. For cases where the classifiers were in disagreement, an independent board-certified cardiothoracic radiologist blindly interpreted the images to evaluate a potential radiologist-augmented workflow. Receiver operating characteristic curves and areas under the curve (AUCs) were used to assess model performance by using the DeLong method for statistical comparison of receiver operating characteristic curves. Results The best-performing classifier had an AUC of 0.99, which was an ensemble of the AlexNet and GoogLeNet DCNNs. The AUCs of the pretrained models were greater than that of the untrained models (P &lt; .001). Augmenting the dataset further increased accuracy (P values for AlexNet and GoogLeNet were .03 and .02, respectively). The DCNNs had disagreement in 13 of the 150 test cases, which were blindly reviewed by a cardiothoracic radiologist, who correctly interpreted all 13 cases (100%). This radiologist-augmented approach resulted in a sensitivity of 97.3% and specificity 100%. Conclusion Deep learning with DCNNs can accurately classify TB at chest radiography with an AUC of 0.99. A radiologist-augmented approach for cases where there was disagreement among the classifiers further improved accuracy. <sup>©</sup> RSNA, 2017.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28436741,pubmed,2017,80262690-ca74-4789-b58f-d648f35b39b8,1
improving automated multiple sclerosis lesion segmentation with a cascaded 3d convolutional neural network approach,/pubmed/28435096,"Valverde S, Cabezas M, Roura E, González-Villà S, Pareto D, Vilanova JC, Ramió-Torrentà L, Rovira À, Oliver A, Lladó X.",Neuroimage. 2017 Jul 15;155:159-168. doi: 10.1016/j.neuroimage.2017.04.034. Epub 2017 Apr 19.,Neuroimage.  2017,PubMed,citation,PMID:28435096,pubmed,28435096,create date:2017/04/25 | first author:Valverde S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we present a novel automated method for White Matter (WM) lesion segmentation of Multiple Sclerosis (MS) patient images. Our approach is based on a cascade of two 3D patch-wise convolutional neural networks (CNN). The first network is trained to be more sensitive revealing possible candidate lesion voxels while the second network is trained to reduce the number of misclassified voxels coming from the first network. This cascaded CNN architecture tends to learn well from a small (n≤35) set of labeled data of the same MRI contrast, which can be very interesting in practice, given the difficulty to obtain manual label annotations and the large amount of available unlabeled Magnetic Resonance Imaging (MRI) data. We evaluate the accuracy of the proposed method on the public MS lesion segmentation challenge MICCAI2008 dataset, comparing it with respect to other state-of-the-art MS lesion segmentation tools. Furthermore, the proposed method is also evaluated on two private MS clinical datasets, where the performance of our method is also compared with different recent public available state-of-the-art MS lesion segmentation methods. At the time of writing this paper, our method is the best ranked approach on the MICCAI2008 challenge, outperforming the rest of 60 participant methods when using all the available input modalities (T1-w, T2-w and FLAIR), while still in the top-rank (3rd position) when using only T1-w and FLAIR modalities. On clinical MS data, our approach exhibits a significant increase in the accuracy segmenting of WM lesions when compared with the rest of evaluated methods, highly correlating (r≥0.97) also with the expected lesion volume.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",svalverde@eia.udg.edu,Automatic lesion segmentation; Brain; Convolutional neural networks; MRI; Multiple sclerosis,https://www.ncbi.nlm.nih.gov//pubmed/28435096,pubmed,2017,a23455d2-1371-4273-af83-0a64b85b53b1,1
medical text classification using convolutional neural networks,/pubmed/28423791,"Hughes M, Li I, Kotoulas S, Suzumura T.",Stud Health Technol Inform. 2017;235:246-250.,Stud Health Technol Inform.  2017,PubMed,citation,PMID:28423791,pubmed,28423791,create date:2017/04/21 | first author:Hughes M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We present an approach to automatically classify clinical text at a sentence level. We are using deep convolutional neural networks to represent complex features. We train the network on a dataset providing a broad categorization of health information. Through a detailed evaluation, we demonstrate that our method outperforms several approaches widely used in natural language processing tasks by about 15%.</abstracttext></p></div></div>",,Clinical text; convolutional neural network; semantic clinical classification; sentence classification,https://www.ncbi.nlm.nih.gov//pubmed/28423791,pubmed,2017,d4f1fc42-15c8-42a2-b52f-1c257df71954,1
automated analysis of high-content microscopy data with deep learning,/pubmed/28420678,"Kraus OZ, Grys BT, Ba J, Chong Y, Frey BJ, Boone C, Andrews BJ.",Mol Syst Biol. 2017 Apr 18;13(4):924. doi: 10.15252/msb.20177551.,Mol Syst Biol.  2017,PubMed,citation,PMID:28420678 | PMCID:PMC5408780,pubmed,28420678,create date:2017/04/20 | first author:Kraus OZ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Existing computational pipelines for quantitative analysis of high-content microscopy data rely on traditional machine learning approaches that fail to accurately classify more than a single dataset without substantial tuning and training, requiring extensive analysis. Here, we demonstrate that the application of deep learning to biological image data can overcome the pitfalls associated with conventional machine learning classifiers. Using a deep convolutional neural network (DeepLoc) to analyze yeast cell images, we show improved performance over traditional approaches in the automated classification of protein subcellular localization. We also demonstrate the ability of DeepLoc to classify highly divergent image sets, including images of pheromone-arrested cells with abnormal cellular morphology, as well as images generated in different genetic backgrounds and in different laboratories. We offer an open-source implementation that enables updating DeepLoc on new microscopy datasets. This study highlights deep learning as an important tool for the expedited analysis of high-content microscopy data.</abstracttext></p><p class='copyright'>© 2017 The Authors. Published under the terms of the CC BY 4.0 license.</p></div></div>",charlie.boone@utoronto.ca,"
Saccharomyces cerevisiae
; deep learning; high‐content screening; image analysis; machine learning",https://www.ncbi.nlm.nih.gov//pubmed/28420678,pubmed,2017,bc15836c-6b3a-42b8-9362-bdbe94fee41e,1
accurate and reproducible invasive breast cancer detection in whole-slide images: a deep learning approach for quantifying tumor extent,/pubmed/28418027,"Cruz-Roa A, Gilmore H, Basavanhally A, Feldman M, Ganesan S, Shih NNC, Tomaszewski J, González FA, Madabhushi A.",Sci Rep. 2017 Apr 18;7:46450. doi: 10.1038/srep46450.,Sci Rep.  2017,PubMed,citation,PMID:28418027 | PMCID:PMC5394452,pubmed,28418027,create date:2017/04/19 | first author:Cruz-Roa A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>With the increasing ability to routinely and rapidly digitize whole slide images with slide scanners, there has been interest in developing computerized image analysis algorithms for automated detection of disease extent from digital pathology images. The manual identification of presence and extent of breast cancer by a pathologist is critical for patient management for tumor staging and assessing treatment response. However, this process is tedious and subject to inter- and intra-reader variability. For computerized methods to be useful as decision support tools, they need to be resilient to data acquired from different sources, different staining and cutting protocols and different scanners. The objective of this study was to evaluate the accuracy and robustness of a deep learning-based method to automatically identify the extent of invasive tumor on digitized images. Here, we present a new method that employs a convolutional neural network for detecting presence of invasive tumor on whole slide images. Our approach involves training the classifier on nearly 400 exemplars from multiple different sites, and scanners, and then independently validating on almost 200 cases from The Cancer Genome Atlas. Our approach yielded a Dice coefficient of 75.86%, a positive predictive value of 71.62% and a negative predictive value of 96.77% in terms of pixel-by-pixel evaluation compared to manually annotated regions of invasive ductal carcinoma.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28418027,pubmed,2017,687f0e2c-89fa-4f90-a785-8a50dd80f326,1
chemical-induced disease relation extraction via convolutional neural network,/pubmed/28415073,"Gu J, Sun F, Qian L, Zhou G.",Database (Oxford). 2017 Jan 1;2017(1). doi: 10.1093/database/bax024.,Database (Oxford).  2017,PubMed,citation,PMID:28415073 | PMCID:PMC5467558,pubmed,28415073,create date:2017/04/18 | first author:Gu J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This article describes our work on the BioCreative-V chemical-disease relation (CDR) extraction task, which employed a maximum entropy (ME) model and a convolutional neural network model for relation extraction at inter- and intra-sentence level, respectively. In our work, relation extraction between entity concepts in documents was simplified to relation extraction between entity mentions. We first constructed pairs of chemical and disease mentions as relation instances for training and testing stages, then we trained and applied the ME model and the convolutional neural network model for inter- and intra-sentence level, respectively. Finally, we merged the classification results from mention level to document level to acquire the final relations between chemical and disease concepts. The evaluation on the BioCreative-V CDR corpus shows the effectiveness of our proposed approach.</abstracttext></p><h4>Database URL: </h4><p><abstracttext label='Database URL' nlmcategory='UNASSIGNED'>http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28415073,pubmed,2017,2fe06977-2153-46ba-9aaa-71f70b4e4199,1
identification of cell cycle-regulated genes by convolutional neural network,/pubmed/28413974,"Liu C, Cui P, Huang T.",Comb Chem High Throughput Screen. 2017 Apr 17. doi: 10.2174/1386207320666170417144937. [Epub ahead of print],Comb Chem High Throughput Screen.  2017,PubMed,citation,PMID:28413974,pubmed,28413974,create date:2017/04/18 | first author:Liu C,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>The cell cycle-regulated genes express periodically with the cell cycle stages, and the identification and study of these genes can provide a deep understanding of the cell cycle process. Large false positives and low overlaps are big problems in cell cycle-regulated gene detection.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Here, a computational framework called DLGene was proposed for cell cycle-regulated gene detection. It is based on the convolutional neural network, a deep learning algorithm representing raw form of data pattern without assumption of their distribution. First, the expression data was transformed to categorical state data to denote the changing state of gene expression, and four different expression patterns were revealed for the reported cell cycle-regulated genes. Then, DLGene was applied to discriminate the non-cell cycle gene and the four subtypes of cell cycle genes. Its performances were compared with six traditional machine learning methods. At last, the biological functions of representative cell cycle genes for each subtype were analyzed.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our method showed better and more balanced performance of sensitivity and specificity comparing to other machine learning algorithms. The cell cycle genes had very different expression pattern with non-cell cycle genes and among the cell-cycle genes, there were four subtypes. Our method not only detects the cell cycle genes, but also describes its expression pattern, such as when its highest expression level is reached and how it changes with time. For each type, we analyzed the biological functions of the representative genes and such results provided novel insight of the cell cycle mechanisms.</abstracttext></p><p class='copyright'>Copyright© Bentham Science Publishers; For any queries, please email at epub@benthamscience.org.</p></div></div>",,cell cycle; cell cycle-regulated genes; classification; convolutional neural network; deep learning; machine learning,https://www.ncbi.nlm.nih.gov//pubmed/28413974,pubmed,2017,d1145abc-18c6-4f9f-bf43-f759ef393d3d,1
quantitative analysis of patients with celiac disease by video capsule endoscopy: a deep learning method,/pubmed/28412572,"Zhou T, Han G, Li BN, Lin Z, Ciaccio EJ, Green PH, Qin J.",Comput Biol Med. 2017 Jun 1;85:1-6. doi: 10.1016/j.compbiomed.2017.03.031. Epub 2017 Apr 8.,Comput Biol Med.  2017,PubMed,citation,PMID:28412572,pubmed,28412572,create date:2017/04/17 | first author:Zhou T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Celiac disease is one of the most common diseases in the world. Capsule endoscopy is an alternative way to visualize the entire small intestine without invasiveness to the patient. It is useful to characterize celiac disease, but hours are need to manually analyze the retrospective data of a single patient. Computer-aided quantitative analysis by a deep learning method helps in alleviating the workload during analysis of the retrospective videos.</abstracttext></p><h4>METHOD: </h4><p><abstracttext label='METHOD' nlmcategory='METHODS'>Capsule endoscopy clips from 6 celiac disease patients and 5 controls were preprocessed for training. The frames with a large field of opaque extraluminal fluid or air bubbles were removed automatically by using a pre-selection algorithm. Then the frames were cropped and the intensity was corrected prior to frame rotation in the proposed new method. The GoogLeNet is trained with these frames. Then, the clips of capsule endoscopy from 5 additional celiac disease patients and 5 additional control patients are used for testing. The trained GoogLeNet was able to distinguish the frames from capsule endoscopy clips of celiac disease patients vs controls. Quantitative measurement with evaluation of the confidence was developed to assess the severity level of pathology in the subjects.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Relying on the evaluation confidence, the GoogLeNet achieved 100% sensitivity and specificity for the testing set. The t-test confirmed the evaluation confidence is significant to distinguish celiac disease patients from controls. Furthermore, it is found that the evaluation confidence may also relate to the severity level of small bowel mucosal lesions.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>A deep convolutional neural network was established for quantitative measurement of the existence and degree of pathology throughout the small intestine, which may improve computer-aided clinical techniques to assess mucosal atrophy and other etiologies in real-time with videocapsule endoscopy.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",bingoon@ieee.org,Celiac disease; Deep learning; GoogLeNet; Quantitative analysis; Videocapsule endoscopy,https://www.ncbi.nlm.nih.gov//pubmed/28412572,pubmed,2017,d34e1d10-1db4-495e-9bbc-eab6c3a5430c,1
epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images,/pubmed/28410112,"Huang Y, Zheng H, Liu C, Ding X, Rohde G.",IEEE J Biomed Health Inform. 2017 Apr 6. doi: 10.1109/JBHI.2017.2691738. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28410112,pubmed,28410112,create date:2017/04/15 | first author:Huang Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28410112,pubmed,2017,361c7ffa-8455-4deb-a7cf-37fc6604adcb,1
automatic quality assessment of echocardiograms using convolutional neural networks: feasibility on the apical four-chamber view,/pubmed/28391191,"Abdi AH, Luong C, Tsang T, Allan G, Nouranian S, Jue J, Hawley D, Fleming S, Gin K, Swift J, Rohling R, Abolmaesumi P.",IEEE Trans Med Imaging. 2017 Jun;36(6):1221-1230. doi: 10.1109/TMI.2017.2690836. Epub 2017 Apr 4.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28391191,pubmed,28391191,create date:2017/04/10 | first author:Abdi AH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 ± 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28391191,pubmed,2017,3db1dca1-3ee6-416e-ac50-74359d00efb7,1
ultrasound standard plane detection using a composite neural network framework,/pubmed/28371793,"Chen H, Wu L, Dou Q, Qin J, Li S, Cheng JZ, Ni D, Heng PA.",IEEE Trans Cybern. 2017 Jun;47(6):1576-1586. doi: 10.1109/TCYB.2017.2685080. Epub 2017 Mar 30.,IEEE Trans Cybern.  2017,PubMed,citation,PMID:28371793,pubmed,28371793,create date:2017/04/04 | first author:Chen H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28371793,pubmed,2017,83ba711f-401a-4d6a-a2b9-6f0117f6369d,1
protein-ligand scoring with convolutional neural networks,/pubmed/28368587,"Ragoza M, Hochuli J, Idrobo E, Sunseri J, Koes DR.",J Chem Inf Model. 2017 Apr 24;57(4):942-957. doi: 10.1021/acs.jcim.6b00740. Epub 2017 Apr 11.,J Chem Inf Model.  2017,PubMed,citation,PMID:28368587 | PMCID:PMC5479431,pubmed,28368587,create date:2017/04/04 | first author:Ragoza M,<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computational approaches to drug discovery can reduce the time and cost associated with experimental assays and enable the screening of novel chemotypes. Structure-based drug design methods rely on scoring functions to rank and predict binding affinities and poses. The ever-expanding amount of protein-ligand binding and structural data enables the use of deep machine learning techniques for protein-ligand scoring. We describe convolutional neural network (CNN) scoring functions that take as input a comprehensive three-dimensional (3D) representation of a protein-ligand interaction. A CNN scoring function automatically learns the key features of protein-ligand interactions that correlate with binding. We train and optimize our CNN scoring functions to discriminate between correct and incorrect binding poses and known binders and nonbinders. We find that our CNN scoring function outperforms the AutoDock Vina scoring function when ranking poses both for pose prediction and virtual screening.</abstracttext></p></div></div>,,,https://www.ncbi.nlm.nih.gov//pubmed/28368587,pubmed,2017,95671ca5-27e8-4de1-893f-912af7d787a8,1
joint multiple fully connected convolutional neural network with extreme learning machine for hepatocellular carcinoma nuclei grading,/pubmed/28365546,"Li S, Jiang H, Pang W.",Comput Biol Med. 2017 May 1;84:156-167. doi: 10.1016/j.compbiomed.2017.03.017. Epub 2017 Mar 22.,Comput Biol Med.  2017,PubMed,citation,PMID:28365546,pubmed,28365546,create date:2017/04/04 | first author:Li S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate cell grading of cancerous tissue pathological image is of great importance in medical diagnosis and treatment. This paper proposes a joint multiple fully connected convolutional neural network with extreme learning machine (MFC-CNN-ELM) architecture for hepatocellular carcinoma (HCC) nuclei grading. First, in preprocessing stage, each grayscale image patch with the fixed size is obtained using center-proliferation segmentation (CPS) method and the corresponding labels are marked under the guidance of three pathologists. Next, a multiple fully connected convolutional neural network (MFC-CNN) is designed to extract the multi-form feature vectors of each input image automatically, which considers multi-scale contextual information of deep layer maps sufficiently. After that, a convolutional neural network extreme learning machine (CNN-ELM) model is proposed to grade HCC nuclei. Finally, a back propagation (BP) algorithm, which contains a new up-sample method, is utilized to train MFC-CNN-ELM architecture. The experiment comparison results demonstrate that our proposed MFC-CNN-ELM has superior performance compared with related works for HCC nuclei grading. Meanwhile, external validation using ICPR 2014 HEp-2 cell dataset shows the good generalization of our MFC-CNN-ELM architecture.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",lisiqi881020@163.com,Back propagation; Convolutional neural network; Extreme learning machine; Hepatocellular carcinoma nuclei grading; Multiple fully connected layers,https://www.ncbi.nlm.nih.gov//pubmed/28365546,pubmed,2017,22dcc695-925c-4c98-b4b3-e63c1f089aec,1
fuiqa: fetal ultrasound image quality assessment with deep convolutional networks,/pubmed/28362600,"Wu L, Cheng JZ, Li S, Lei B, Wang T, Ni D.",IEEE Trans Cybern. 2017 May;47(5):1336-1349. doi: 10.1109/TCYB.2017.2671898. Epub 2017 Mar 9.,IEEE Trans Cybern.  2017,PubMed,citation,PMID:28362600,pubmed,28362600,create date:2017/04/01 | first author:Wu L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The quality of ultrasound (US) images for the obstetric examination is crucial for accurate biometric measurement. However, manual quality control is a labor intensive process and often impractical in a clinical setting. To improve the efficiency of examination and alleviate the measurement error caused by improper US scanning operation and slice selection, a computerized fetal US image quality assessment (FUIQA) scheme is proposed to assist the implementation of US image quality control in the clinical obstetric examination. The proposed FUIQA is realized with two deep convolutional neural network models, which are denoted as L-CNN and C-CNN, respectively. The L-CNN aims to find the region of interest (ROI) of the fetal abdominal region in the US image. Based on the ROI found by the L-CNN, the C-CNN evaluates the image quality by assessing the goodness of depiction for the key structures of stomach bubble and umbilical vein. To further boost the performance of the L-CNN, we augment the input sources of the neural network with the local phase features along with the original US data. It will be shown that the heterogeneous input sources will help to improve the performance of the L-CNN. The performance of the proposed FUIQA is compared with the subjective image quality evaluation results from three medical doctors. With comprehensive experiments, it will be illustrated that the computerized assessment with our FUIQA scheme can be comparable to the subjective ratings from medical doctors.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28362600,pubmed,2017,18fbe570-1370-4d2a-bce7-a740a6a20ec5,1
slide: automatic spine level identification system using a deep convolutional neural network,/pubmed/28361323,"Hetherington J, Lessoway V, Gunka V, Abolmaesumi P, Rohling R.",Int J Comput Assist Radiol Surg. 2017 Jul;12(7):1189-1198. doi: 10.1007/s11548-017-1575-8. Epub 2017 Mar 30.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28361323,pubmed,28361323,create date:2017/04/01 | first author:Hetherington J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Percutaneous spinal needle insertion procedures often require proper identification of the vertebral level to effectively and safely deliver analgesic agents. The current clinical method involves 'blind' identification of the vertebral level through manual palpation of the spine, which has only 30% reported accuracy. Therefore, there is a need for better anatomical identification prior to needle insertion.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A real-time system was developed to identify the vertebral level from a sequence of ultrasound images, following a clinical imaging protocol. The system uses a deep convolutional neural network (CNN) to classify transverse images of the lower spine. Several existing CNN architectures were implemented, utilizing transfer learning, and compared for adequacy in a real-time system. In the system, the CNN output is processed, using a novel state machine, to automatically identify vertebral levels as the transducer moves up the spine. Additionally, a graphical display was developed and integrated within 3D Slicer. Finally, an augmented reality display, projecting the level onto the patient's back, was also designed. A small feasibility study [Formula: see text] evaluated performance.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed CNN successfully discriminates ultrasound images of the sacrum, intervertebral gaps, and vertebral bones, achieving 88% 20-fold cross-validation accuracy. Seventeen of 20 test ultrasound scans had successful identification of all vertebral levels, processed at real-time speed (40 frames/s).</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>A machine learning system is presented that successfully identifies lumbar vertebral levels. The small study on human subjects demonstrated real-time performance. A projection-based augmented reality display was used to show the vertebral level directly on the subject adjacent to the puncture site.</abstracttext></p></div></div>",jordenh@ece.ubc.ca,Machine learning; Needle guidance; Ultrasound; Vertebral level,https://www.ncbi.nlm.nih.gov//pubmed/28361323,pubmed,2017,b72bc794-69d1-4cf6-a2ef-f3a1f3d87796,1
structured pyramidal neural networks,/pubmed/28359221,"Soares AM, Fernandes BJ, Bastos-Filho CJ.",Int J Neural Syst. 2017 Feb 9:1750021. doi: 10.1142/S0129065717500216. [Epub ahead of print],Int J Neural Syst.  2017,PubMed,citation,PMID:28359221,pubmed,28359221,create date:2017/04/01 | first author:Soares AM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The Pyramidal Neural Networks (PNN) are an example of a successful recently proposed model inspired by the human visual system and deep learning theory. PNNs are applied to computer vision and based on the concept of receptive fields. This paper proposes a variation of PNN, named here as Structured Pyramidal Neural Network (SPNN). SPNN has self-adaptive variable receptive fields, while the original PNNs rely on the same size for the fields of all neurons, which limits the model since it is not possible to put more computing resources in a particular region of the image. Another limitation of the original approach is the need to define values for a reasonable number of parameters, which can turn difficult the application of PNNs in contexts in which the user does not have experience. On the other hand, SPNN has a fewer number of parameters. Its structure is determined using a novel method with Delaunay Triangulation and k-means clustering. SPNN achieved better results than PNNs and similar performance when compared to Convolutional Neural Network (CNN) and Support Vector Machine (SVM), but using lower memory capacity and processing time.</abstracttext></p></div></div>",,Delaunay triangulation; Pyramidal neural networks; clustering; receptive fields,https://www.ncbi.nlm.nih.gov//pubmed/28359221,pubmed,2017,5ee9e24a-b606-44ab-b758-c63dddb6669a,1
addressing multi-label imbalance problem of surgical tool detection using cnn,/pubmed/28357628,"Sahu M, Mukhopadhyay A, Szengel A, Zachow S.",Int J Comput Assist Radiol Surg. 2017 Jun;12(6):1013-1020. doi: 10.1007/s11548-017-1565-x. Epub 2017 Mar 29.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28357628,pubmed,28357628,create date:2017/03/31 | first author:Sahu M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>A fully automated surgical tool detection framework is proposed for endoscopic video streams. State-of-the-art surgical tool detection methods rely on supervised one-vs-all or multi-class classification techniques, completely ignoring the co-occurrence relationship of the tools and the associated class imbalance.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>In this paper, we formulate tool detection as a multi-label classification task where tool co-occurrences are treated as separate classes. In addition, imbalance on tool co-occurrences is analyzed and stratification techniques are employed to address the imbalance during convolutional neural network (CNN) training. Moreover, temporal smoothing is introduced as an online post-processing step to enhance runtime prediction.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Quantitative analysis is performed on the M2CAI16 tool detection dataset to highlight the importance of stratification, temporal smoothing and the overall framework for tool detection.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The analysis on tool imbalance, backed by the empirical results, indicates the need and superiority of the proposed framework over state-of-the-art techniques.</abstracttext></p></div></div>",sahu@zib.de,CNN; Laparoscopic videos; Multi-label learning; Surgical tool detection; Transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/28357628,pubmed,2017,259e06f5-df9e-4167-a84d-6762af3f4fa2,1
extraction of skin lesions from non-dermoscopic images for surgical excision of melanoma,/pubmed/28342106,"Jafari MH, Nasr-Esfahani E, Karimi N, Soroushmehr SMR, Samavi S, Najarian K.",Int J Comput Assist Radiol Surg. 2017 Jun;12(6):1021-1030. doi: 10.1007/s11548-017-1567-8. Epub 2017 Mar 24.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28342106,pubmed,28342106,create date:2017/03/28 | first author:Jafari MH,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Computerized prescreening of suspicious moles and lesions for malignancy is of great importance for assessing the need and the priority of the removal surgery. Detection can be done by images captured by standard cameras, which are more preferable due to low cost and availability. One important step in computerized evaluation is accurate detection of lesion's region, i.e., segmentation of an image into two regions as lesion and normal skin.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>In this paper, a new method based on deep neural networks is proposed for accurate extraction of a lesion region. The input image is preprocessed, and then, its patches are fed to a convolutional neural network. Local texture and global structure of the patches are processed in order to assign pixels to lesion or normal classes. A method for effective selection of training patches is proposed for more accurate detection of a lesion's border.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our results indicate that the proposed method could reach the accuracy of 98.7% and the sensitivity of 95.2% in segmentation of lesion regions over the dataset of clinical images.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The experimental results of qualitative and quantitative evaluations demonstrate that our method can outperform other state-of-the-art algorithms exist in the literature.</abstracttext></p></div></div>",samavi@mcmaster.ca,Convolutional neural network; Deep learning; Medical image segmentation; Melanoma excision; Skin cancer,https://www.ncbi.nlm.nih.gov//pubmed/28342106,pubmed,2017,5af47d3d-a094-476c-8bfa-95247a590193,1
predicting the impact of non-coding variants on dna methylation,/pubmed/28334830,"Zeng H, Gifford DK.",Nucleic Acids Res. 2017 Jun 20;45(11):e99. doi: 10.1093/nar/gkx177.,Nucleic Acids Res.  2017,PubMed,citation,PMID:28334830 | PMCID:PMC5499808,pubmed,28334830,create date:2017/03/24 | first author:Zeng H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>DNA methylation plays a crucial role in the establishment of tissue-specific gene expression and the regulation of key biological processes. However, our present inability to predict the effect of genome sequence variation on DNA methylation precludes a comprehensive assessment of the consequences of non-coding variation. We introduce CpGenie, a sequence-based framework that learns a regulatory code of DNA methylation using a deep convolutional neural network and uses this network to predict the impact of sequence variation on proximal CpG site DNA methylation. CpGenie produces allele-specific DNA methylation prediction with single-nucleotide sensitivity that enables accurate prediction of methylation quantitative trait loci (meQTL). We demonstrate that CpGenie prioritizes validated GWAS SNPs, and contributes to the prediction of functional non-coding variants, including expression quantitative trait loci (eQTL) and disease-associated mutations. CpGenie is publicly available to assist in identifying and interpreting regulatory non-coding variants.</abstracttext></p><p class='copyright'>© The Author(s) 2017. Published by Oxford University Press on behalf of Nucleic Acids Research.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28334830,pubmed,2017,046586a6-3a5f-42ff-9875-0b75de03df4b,1
multi-scale rotation-invariant convolutional neural networks for lung texture classification,/pubmed/28333649,"Wang Q, Zheng Y, Yang G, Jin W, Chen X, Yin Y.",IEEE J Biomed Health Inform. 2017 Mar 21. doi: 10.1109/JBHI.2017.2685586. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28333649,pubmed,28333649,create date:2017/03/24 | first author:Wang Q,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a new Multi-scale Rotation-invariant Convolutional Neural Network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography (HRCT). MRCNN employs Gabor-local binary pattern (Gabor-LBP) which introduces a good property in image analysis - invariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public Interstitial Lung Disease (ILD) database show a superior performance of the proposed method to state-of-the-art.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28333649,pubmed,2017,9722e358-1cc4-4032-8361-c3efcb70e157,1
longitudinal analysis of discussion topics in an online breast cancer community using convolutional neural networks,/pubmed/28323113,"Zhang S, Grave E, Sklar E, Elhadad N.",J Biomed Inform. 2017 May;69:1-9. doi: 10.1016/j.jbi.2017.03.012. Epub 2017 Mar 18.,J Biomed Inform.  2017,PubMed,citation,PMID:28323113,pubmed,28323113,create date:2017/03/23 | first author:Zhang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Identifying topics of discussions in online health communities (OHC) is critical to various information extraction applications, but can be difficult because topics of OHC content are usually heterogeneous and domain-dependent. In this paper, we provide a multi-class schema, an annotated dataset, and supervised classifiers based on convolutional neural network (CNN) and other models for the task of classifying discussion topics. We apply the CNN classifier to the most popular breast cancer online community, and carry out cross-sectional and longitudinal analyses to show topic distributions and topic dynamics throughout members' participation. Our experimental results suggest that CNN outperforms other classifiers in the task of topic classification and identify several patterns and trajectories. For example, although members discuss mainly disease-related topics, their interest may change through time and vary with their disease severities.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Inc.</p></div></div>",sz2338@columbia.edu,Breast cancer; Convolutional neural network; Deep learning; Longitudinal analysis; Online health community; Topic,https://www.ncbi.nlm.nih.gov//pubmed/28323113,pubmed,2017,e78af411-5d42-421f-bbb1-0836b4f71fd9,1
toolkits and libraries for deep learning,/pubmed/28315069,"Erickson BJ, Korfiatis P, Akkus Z, Kline T, Philbrick K.",J Digit Imaging. 2017 Aug;30(4):400-405. doi: 10.1007/s10278-017-9965-6. Review.,J Digit Imaging.  2017,PubMed,citation,PMID:28315069 | PMCID:PMC5537091,pubmed,28315069,create date:2017/03/21 | first author:Erickson BJ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning is an important new area of machine learning which encompasses a wide range of neural network architectures designed to complete various tasks. In the medical imaging domain, example tasks include organ segmentation, lesion detection, and tumor classification. The most popular network architecture for deep learning for images is the convolutional neural network (CNN). Whereas traditional machine learning requires determination and calculation of features from which the algorithm learns, deep learning approaches learn the important features as well as the proper weighting of those features to make predictions for new data. In this paper, we will describe some of the libraries and tools that are available to aid in the construction and efficient execution of deep learning as applied to medical images.</abstracttext></p></div></div>",bje@Mayo.edu,Artificial intelligence; Convolutional neural network; Deep learning; Machine learning,https://www.ncbi.nlm.nih.gov//pubmed/28315069,pubmed,2017,7dfd66f9-e0fa-402d-831e-d76634c45885,1
localization and diagnosis framework for pediatric cataracts based on slit-lamp images using deep features of a convolutional neural network,/pubmed/28306716,"Liu X, Jiang J, Zhang K, Long E, Cui J, Zhu M, An Y, Zhang J, Liu Z, Lin Z, Li X, Chen J, Cao Q, Li J, Wu X, Wang D, Lin H.",PLoS One. 2017 Mar 17;12(3):e0168606. doi: 10.1371/journal.pone.0168606. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28306716 | PMCID:PMC5356999,pubmed,28306716,create date:2017/03/18 | first author:Liu X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Slit-lamp images play an essential role for diagnosis of pediatric cataracts. We present a computer vision-based framework for the automatic localization and diagnosis of slit-lamp images by identifying the lens region of interest (ROI) and employing a deep learning convolutional neural network (CNN). First, three grading degrees for slit-lamp images are proposed in conjunction with three leading ophthalmologists. The lens ROI is located in an automated manner in the original image using two successive applications of Candy detection and the Hough transform, which are cropped, resized to a fixed size and used to form pediatric cataract datasets. These datasets are fed into the CNN to extract high-level features and implement automatic classification and grading. To demonstrate the performance and effectiveness of the deep features extracted in the CNN, we investigate the features combined with support vector machine (SVM) and softmax classifier and compare these with the traditional representative methods. The qualitative and quantitative experimental results demonstrate that our proposed method offers exceptional mean accuracy, sensitivity and specificity: classification (97.07%, 97.28%, and 96.83%) and a three-degree grading area (89.02%, 86.63%, and 90.75%), density (92.68%, 91.05%, and 93.94%) and location (89.28%, 82.70%, and 93.08%). Finally, we developed and deployed a potential automatic diagnostic software for ophthalmologists and patients in clinical applications to implement the validated model.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28306716,pubmed,2017,8b06fb8c-6800-4830-b65b-8ea7dec2f509,1
3d scattering transforms for disease classification in neuroimaging,/pubmed/28289601,"Adel T, Cohen T, Caan M, Welling M; AGEhIV study group and the Alzheimer's Disease Neuroimaging Initiative..",Neuroimage Clin. 2017 Feb 10;14:506-517. doi: 10.1016/j.nicl.2017.02.004. eCollection 2017.,Neuroimage Clin.  2017,PubMed,citation,PMID:28289601 | PMCID:PMC5338908,pubmed,28289601,create date:2017/03/16 | first author:Adel T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Classifying neurodegenerative brain diseases in MRI aims at correctly assigning discrete labels to MRI scans. Such labels usually refer to a diagnostic decision a learner infers based on what it has learned from a training sample of MRI scans. Classification from MRI voxels separately typically does not provide independent evidence towards or against a class; the information relevant for classification is only present in the form of complicated multivariate patterns (or 'features'). Deep learning solves this problem by learning a sequence of non-linear transformations that result in feature representations that are better suited to classification. Such learned features have been shown to drastically outperform hand-engineered features in computer vision and audio analysis domains. However, applying the deep learning approach to the task of MRI classification is extremely challenging, because it requires a very large amount of data which is currently not available. We propose to instead use a three dimensional scattering transform, which resembles a deep convolutional neural network but has no learnable parameters. Furthermore, the scattering transform linearizes diffeomorphisms (due to e.g. residual anatomical variability in MRI scans), making the different disease states more easily separable using a linear classifier. In experiments on brain morphometry in Alzheimer's disease, and on white matter microstructural damage in HIV, scattering representations are shown to be highly effective for the task of disease classification. For instance, in semi-supervised learning of progressive versus stable MCI, we reach an accuracy of 82.7%. We also present a visualization method to highlight areas that provide evidence for or against a certain class, both on an individual and group level.</abstracttext></p></div></div>",,Feature extraction; MRI classification; Scattering representation,https://www.ncbi.nlm.nih.gov//pubmed/28289601,pubmed,2017,cafc18e7-3d13-4edd-928e-bb7a64ef3977,1
automatic quantification of tumour hypoxia from multi-modal microscopy images using weakly-supervised learning methods,/pubmed/28278461,"Carneiro G, Peng T, Bayer C, Navab N.",IEEE Trans Med Imaging. 2017 Jul;36(7):1405-1417. doi: 10.1109/TMI.2017.2677479. Epub 2017 Mar 2.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28278461,pubmed,28278461,create date:2017/03/10 | first author:Carneiro G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitative results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28278461,pubmed,2017,9d23f008-cb09-4c27-baea-32874663bac4,1
fully automated deep learning system for bone age assessment,/pubmed/28275919,"Lee H, Tajmir S, Lee J, Zissen M, Yeshiwas BA, Alkasab TK, Choy G, Do S.",J Digit Imaging. 2017 Aug;30(4):427-441. doi: 10.1007/s10278-017-9955-8.,J Digit Imaging.  2017,PubMed,citation,PMID:28275919 | PMCID:PMC5537090,pubmed,28275919,create date:2017/03/10 | first author:Lee H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Skeletal maturity progresses through discrete phases, a fact that is used routinely in pediatrics where bone age assessments (BAAs) are compared to chronological age in the evaluation of endocrine and metabolic disorders. While central to many disease evaluations, little has changed to improve the tedious process since its introduction in 1950. In this study, we propose a fully automated deep learning pipeline to segment a region of interest, standardize and preprocess input radiographs, and perform BAA. Our models use an ImageNet pretrained, fine-tuned convolutional neural network (CNN) to achieve 57.32 and 61.40% accuracies for the female and male cohorts on our held-out test images. Female test radiographs were assigned a BAA within 1 year 90.39% and within 2 years 98.11% of the time. Male test radiographs were assigned 94.18% within 1 year and 99.00% within 2 years. Using the input occlusion method, attention maps were created which reveal what features the trained model uses to perform BAA. These correspond to what human experts look at when manually performing BAA. Finally, the fully automated BAA system was deployed in the clinical environment as a decision supporting system for more accurate and efficient BAAs at much faster interpretation time (&lt;2 s) than the conventional method.</abstracttext></p></div></div>",sdo@mgh.harvard.edu,Artificial intelligence; Artificial neural networks (ANNs); Automated measurement; Automated object detection; Bone-age; Classification; Clinical workflow; Computer vision; Computer-aided diagnosis (CAD); Data collection; Decision support; Digital X-ray radiogrammetry; Efficiency; Machine learning; Structured reporting,https://www.ncbi.nlm.nih.gov//pubmed/28275919,pubmed,2017,922b4ea4-4ae6-4039-8b04-1f85c1537e13,1
deep multi-scale location-aware 3d convolutional neural networks for automated detection of lacunes of presumed vascular origin,/pubmed/28271039,"Ghafoorian M, Karssemeijer N, Heskes T, Bergkamp M, Wissink J, Obels J, Keizer K, Leeuw FE, Ginneken BV, Marchiori E, Platel B.",Neuroimage Clin. 2017 Feb 4;14:391-399. doi: 10.1016/j.nicl.2017.01.033. eCollection 2017.,Neuroimage Clin.  2017,PubMed,citation,PMID:28271039 | PMCID:PMC5322213,pubmed,28271039,create date:2017/03/09 | first author:Ghafoorian M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Lacunes of presumed vascular origin (lacunes) are associated with an increased risk of stroke, gait impairment, and dementia and are a primary imaging feature of the small vessel disease. Quantification of lacunes may be of great importance to elucidate the mechanisms behind neuro-degenerative disorders and is recommended as part of study standards for small vessel disease research. However, due to the different appearance of lacunes in various brain regions and the existence of other similar-looking structures, such as perivascular spaces, manual annotation is a difficult, elaborative and subjective task, which can potentially be greatly improved by reliable and consistent computer-aided detection (CAD) routines. In this paper, we propose an automated two-stage method using deep convolutional neural networks (CNN). We show that this method has good performance and can considerably benefit readers. We first use a fully convolutional neural network to detect initial candidates. In the second step, we employ a 3D CNN as a false positive reduction tool. As the location information is important to the analysis of candidate structures, we further equip the network with contextual information using multi-scale analysis and integration of explicit location features. We trained, validated and tested our networks on a large dataset of 1075 cases obtained from two different studies. Subsequently, we conducted an observer study with four trained observers and compared our method with them using a free-response operating characteristic analysis. Shown on a test set of 111 cases, the resulting CAD system exhibits performance similar to the trained human observers and achieves a sensitivity of 0.974 with 0.13 false positives per slice. A feasibility study also showed that a trained human observer would considerably benefit once aided by the CAD system.</abstracttext></p></div></div>",,Automated detection; Convolutional neural networks; Deep learning; Lacunes; Location-aware; Multi-scale,https://www.ncbi.nlm.nih.gov//pubmed/28271039,pubmed,2017,222c6e74-462c-4912-b361-2461ef672150,1
deep feature learning for automatic tissue classification of coronary artery using optical coherence tomography,/pubmed/28271012,"Abdolmanafi A, Duong L, Dahdah N, Cheriet F.",Biomed Opt Express. 2017 Jan 30;8(2):1203-1220. doi: 10.1364/BOE.8.001203. eCollection 2017 Feb 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28271012 | PMCID:PMC5330543,pubmed,28271012,create date:2017/03/09 | first author:Abdolmanafi A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Kawasaki disease (KD) is an acute childhood disease complicated by coronary artery aneurysms, intima thickening, thrombi, stenosis, lamellar calcifications, and disappearance of the media border. Automatic classification of the coronary artery layers (intima, media, and scar features) is important for analyzing optical coherence tomography (OCT) images recorded in pediatric patients. OCT has been known as an intracoronary imaging modality using near-infrared light which has recently been used to image the inner coronary artery tissues of pediatric patients, providing high spatial resolution (ranging from 10 to 20 <i>μ</i>m). This study aims to develop a robust and fully automated tissue classification method by using the convolutional neural networks (CNNs) as feature extractor and comparing the predictions of three state-of-the-art classifiers, CNN, random forest (RF), and support vector machine (SVM). The results show the robustness of CNN as the feature extractor and random forest as the classifier with classification rate up to 96%, especially to characterize the second layer of coronary arteries (media), which is a very thin layer and it is challenging to be recognized and specified from other tissues.</abstracttext></p></div></div>",,"(100.0100) Image processing; (100.2960) Image analysis; (100.4996) Pattern recognition, neural networks; (110.0110) Imaging systems; (110.2960) Image analysis; (110.4500) Optical coherence tomography",https://www.ncbi.nlm.nih.gov//pubmed/28271012,pubmed,2017,72a2c8ad-3007-45ec-be80-51f54e7c8271,1
low-dose ct via convolutional neural network,/pubmed/28270976,"Chen H, Zhang Y, Zhang W, Liao P, Li K, Zhou J, Wang G.",Biomed Opt Express. 2017 Jan 9;8(2):679-694. doi: 10.1364/BOE.8.000679. eCollection 2017 Feb 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28270976 | PMCID:PMC5330597,pubmed,28270976,create date:2017/03/09 | first author:Chen H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In order to reduce the potential radiation risk, low-dose CT has attracted an increasing attention. However, simply lowering the radiation dose will significantly degrade the image quality. In this paper, we propose a new noise reduction method for low-dose CT via deep learning without accessing original projection data. A deep convolutional neural network is here used to map low-dose CT images towards its corresponding normal-dose counterparts in a patch-by-patch fashion. Qualitative results demonstrate a great potential of the proposed method on artifact reduction and structure preservation. In terms of the quantitative metrics, the proposed method has showed a substantial improvement on PSNR, RMSE and SSIM than the competing state-of-art methods. Furthermore, the speed of our method is one order of magnitude faster than the iterative reconstruction and patch-based image denoising methods.</abstracttext></p></div></div>",,(100.3190) Inverse problems; (100.6950) Tomographic image processing; (340.7440) X-ray imaging,https://www.ncbi.nlm.nih.gov//pubmed/28270976,pubmed,2017,caf7dd2e-d637-4fd8-b04b-3542614aee21,1
transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration,/pubmed/28270969,"Karri SP, Chakraborty D, Chatterjee J.",Biomed Opt Express. 2017 Jan 4;8(2):579-592. doi: 10.1364/BOE.8.000579. eCollection 2017 Feb 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28270969 | PMCID:PMC5330546,pubmed,28270969,create date:2017/03/09 | first author:Karri SP,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We present an algorithm for identifying retinal pathologies given retinal optical coherence tomography (OCT) images. Our approach fine-tunes a pre-trained convolutional neural network (CNN), GoogLeNet, to improve its prediction capability (compared to random initialization training) and identifies salient responses during prediction to understand learned filter characteristics. We considered a data set containing subjects with diabetic macular edema, or dry age-related macular degeneration, or no pathology. The fine-tuned CNN could effectively identify pathologies in comparison to classical learning. Our algorithm aims to demonstrate that models trained on non-medical images can be fine-tuned for classifying OCT images with limited training data.</abstracttext></p></div></div>",,(070.5010) Pattern recognition; (100.2960) Image analysis; (110.4500) Optical coherence tomography; (170.1610) Clinical applications; (170.4470) Ophthalmology,https://www.ncbi.nlm.nih.gov//pubmed/28270969,pubmed,2017,a5f8a210-8604-4dcb-a048-84e6e8769e96,1
predicting seizures from local field potentials recorded via intracortical microelectrode arrays,/pubmed/28269702,"Aghagolzadeh M, Hochberg LR, Cash SS, Truccolo W.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:6353-6356. doi: 10.1109/EMBC.2016.7592181.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28269702,pubmed,28269702,create date:2017/03/09 | first author:Aghagolzadeh M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The need for new therapeutic interventions to treat pharmacologically resistant focal epileptic seizures has led recently to the development of closed-loop systems for seizure control. Once a seizure is predicted/detected by the system, electrical stimulation is delivered to prevent seizure initiation or spread. So far, seizure prediction/detection has been limited to tracking non-invasive electroencephalogram (EEG) or intracranial EEG (iEEG) signals. Here, we examine seizure prediction based on local field potentials (LFPs) from a small neocortical patch recorded via a 10×10 microelectrode array implanted in a patient with focal seizures. We formulate the seizure (ictal) prediction problem in terms of discriminating between interictal and preictal neural activity. Using deep Convolutional Neural Networks (CNNs), we show that periods of preictal activity can be successfully discriminated (80% detection; no false positives) from periods of interictal activity several (2-18) minutes prior to seizure onset. CNN input features consisted of the spectral power of LFP channels (1-second time windows) computed in 50 frequency bands (0-100 Hz; 2 Hz steps). Our preliminary results show that intracortical LFPs may be a promising neural signal for seizure prediction in focal epilepsy.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28269702,pubmed,2016,a9bd06f0-6951-4443-9476-397babc6c50f,1
thorax disease diagnosis using deep convolutional neural network,/pubmed/28268784,"Jie Chen, Xianbiao Qi, Tervonen O, Silven O, Guoying Zhao, Pietikainen M.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:2287-2290. doi: 10.1109/EMBC.2016.7591186.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268784,pubmed,28268784,create date:2017/03/09 | first author:Jie Chen,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer aided diagnosis (CAD) is an important issue, which can significantly improve the efficiency of doctors. In this paper, we propose a deep convolutional neural network (CNN) based method for thorax disease diagnosis. We firstly align the images by matching the interest points between the images, and then enlarge the dataset by using Gaussian scale space theory. After that we use the enlarged dataset to train a deep CNN model and apply the obtained model for the diagnosis of new test data. Our experimental results show our method achieves very promising results.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268784,pubmed,2016,1e6b154a-1f97-4944-a253-2eb89d7019be,1
melanoma detection by analysis of clinical images using convolutional neural network,/pubmed/28268581,"Nasr-Esfahani E, Samavi S, Karimi N, Soroushmehr SM, Jafari MH, Ward K, Najarian K.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:1373-1376. doi: 10.1109/EMBC.2016.7590963.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268581,pubmed,28268581,create date:2017/03/09 | first author:Nasr-Esfahani E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Melanoma, most threatening type of skin cancer, is on the rise. In this paper an implementation of a deep-learning system on a computer server, equipped with graphic processing unit (GPU), is proposed for detection of melanoma lesions. Clinical (non-dermoscopic) images are used in the proposed system, which could assist a dermatologist in early diagnosis of this type of skin cancer. In the proposed system, input clinical images, which could contain illumination and noise effects, are preprocessed in order to reduce such artifacts. Afterward, the enhanced images are fed to a pre-trained convolutional neural network (CNN) which is a member of deep learning models. The CNN classifier, which is trained by large number of training samples, distinguishes between melanoma and benign cases. Experimental results show that the proposed method is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268581,pubmed,2016,cc86efaa-0b58-4b3a-b950-67394a56e082,1
a cnn based neurobiology inspired approach for retinal image quality assessment,/pubmed/28268565,"Mahapatra D, Roy PK, Sedai S, Garnavi R.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:1304-1307. doi: 10.1109/EMBC.2016.7590946.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268565,pubmed,28268565,create date:2017/03/09 | first author:Mahapatra D,<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Retinal image quality assessment (IQA) algorithms use different hand crafted features for training classifiers without considering the working of the human visual system (HVS) which plays an important role in IQA. We propose a convolutional neural network (CNN) based approach that determines image quality using the underlying principles behind the working of the HVS. CNNs provide a principled approach to feature learning and hence higher accuracy in decision making. Experimental results demonstrate the superior performance of our proposed algorithm over competing methods.</abstracttext></p></div></div>,,,https://www.ncbi.nlm.nih.gov//pubmed/28268565,pubmed,2016,4c430cc0-fb4f-44b2-894f-ed5a10f0b722,1
encoding physiological signals as images for affective state recognition using convolutional neural networks,/pubmed/28268449,"Guangliang Yu, Xiang Li, Dawei Song, Xiaozhao Zhao, Peng Zhang, Yuexian Hou, Bin Hu.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:812-815. doi: 10.1109/EMBC.2016.7590825.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268449,pubmed,28268449,create date:2017/03/09 | first author:Guangliang Yu,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Affective state recognition based on multiple modalities of physiological signals has been a hot research topic. Traditional methods require designing hand-crafted features based on domain knowledge, which is time-consuming and has not achieved a satisfactory performance. On the other hand, conducting classification on raw signals directly can also cause some problems, such as the interference of noise and the curse of dimensionality. To address these problems, we propose a novel approach that encodes different modalities of data as images and use convolutional neural networks (CNN) to perform the affective state recognition task. We validate our aproach on the DECAF dataset in comparison with two state-of-the-art methods, i.e., the Support Vector Machines (SVM) and Random Forest (RF). Experimental results show that our aproach outperforms the baselines by 5% to 9%.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268449,pubmed,2016,568b78c7-b1c2-4893-a232-93048ff40b72,1
automatic lumbar vertebrae detection based on feature fusion deep learning for partial occluded c-arm x-ray images,/pubmed/28268411,"Yang Li, Wei Liang, Yinlong Zhang, Haibo An, Jindong Tan.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:647-650. doi: 10.1109/EMBC.2016.7590785.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268411,pubmed,28268411,create date:2017/03/09 | first author:Yang Li,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268411,pubmed,2016,6edc4ee0-e1b2-49ff-a924-54942412779f,1
vessel extraction in x-ray angiograms using deep learning,/pubmed/28268410,"Nasr-Esfahani E, Samavi S, Karimi N, Soroushmehr SM, Ward K, Jafari MH, Felfeliyan B, Nallamothu B, Najarian K.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:643-646. doi: 10.1109/EMBC.2016.7590784.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268410,pubmed,28268410,create date:2017/03/09 | first author:Nasr-Esfahani E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Coronary artery disease (CAD) is the most common type of heart disease which is the leading cause of death all over the world. X-ray angiography is currently the gold standard imaging technique for CAD diagnosis. These images usually suffer from low quality and presence of noise. Therefore, vessel enhancement and vessel segmentation play important roles in CAD diagnosis. In this paper a deep learning approach using convolutional neural networks (CNN) is proposed for detecting vessel regions in angiography images. Initially, an input angiogram is preprocessed to enhance its contrast. Afterward, the image is evaluated using patches of pixels and the network determines the vessel and background regions. A set of 1,040,000 patches is used in order to train the deep CNN. Experimental results on angiography images of a dataset show that our proposed method has a superior performance in extraction of vessel regions.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268410,pubmed,2016,c7b75dde-0d3d-4e37-a9f5-ec8eba89f498,1
a deep convolutional neural network for bleeding detection in wireless capsule endoscopy images,/pubmed/28268409,"Xiao Jia, Meng MQ.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:639-642. doi: 10.1109/EMBC.2016.7590783.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268409,pubmed,28268409,create date:2017/03/09 | first author:Xiao Jia,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268409,pubmed,2016,19fe9d15-1a95-44c4-be60-812ccfb9437c,1
fetal facial standard plane recognition via very deep convolutional networks,/pubmed/28268406,"Zhen Yu, Dong Ni, Siping Chen, Shengli Li, Tianfu Wang, Baiying Lei.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:627-630. doi: 10.1109/EMBC.2016.7590780.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268406,pubmed,28268406,create date:2017/03/09 | first author:Zhen Yu,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3×3) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268406,pubmed,2016,32724446-98f2-4545-a25b-b6d70f0f41fe,1
wearable device-based gait recognition using angle embedded gait dynamic images and a convolutional neural network,/pubmed/28264503,"Zhao Y, Zhou S.",Sensors (Basel). 2017 Feb 28;17(3). pii: E478. doi: 10.3390/s17030478.,Sensors (Basel).  2017,PubMed,citation,PMID:28264503 | PMCID:PMC5375764,pubmed,28264503,create date:2017/03/08 | first author:Zhao Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The widespread installation of inertial sensors in smartphones and other wearable devices provides a valuable opportunity to identify people by analyzing their gait patterns, for either cooperative or non-cooperative circumstances. However, it is still a challenging task to reliably extract discriminative features for gait recognition with noisy and complex data sequences collected from casually worn wearable devices like smartphones. To cope with this problem, we propose a novel image-based gait recognition approach using the Convolutional Neural Network (CNN) without the need to manually extract discriminative features. The CNN's input image, which is encoded straightforwardly from the inertial sensor data sequences, is called Angle Embedded Gait Dynamic Image (AE-GDI). AE-GDI is a new two-dimensional representation of gait dynamics, which is invariant to rotation and translation. The performance of the proposed approach in gait authentication and gait labeling is evaluated using two datasets: (1) the McGill University dataset, which is collected under realistic conditions; and (2) the Osaka University dataset with the largest number of subjects. Experimental results show that the proposed approach achieves competitive recognition accuracy over existing approaches and provides an effective parametric solution for identification among a large number of subjects by gait patterns.</abstracttext></p></div></div>",zhaoyongjia@buaa.edu.cn,angle embedded gait dynamic image; biometrics; convolutional neural network; gait authentication; gait labeling; gait recognition; wearable devices,https://www.ncbi.nlm.nih.gov//pubmed/28264503,pubmed,2017,7ce310fd-c3ad-482a-aa81-096d31187001,1
a novel end-to-end classifier using domain transferred deep convolutional neural networks for biomedical images,/pubmed/28254085,"Pang S, Yu Z, Orgun MA.",Comput Methods Programs Biomed. 2017 Mar;140:283-293. doi: 10.1016/j.cmpb.2016.12.019. Epub 2017 Jan 6.,Comput Methods Programs Biomed.  2017,PubMed,citation,PMID:28254085,pubmed,28254085,create date:2017/03/04 | first author:Pang S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND AND OBJECTIVES: </h4><p><abstracttext label='BACKGROUND AND OBJECTIVES' nlmcategory='OBJECTIVE'>Highly accurate classification of biomedical images is an essential task in the clinical diagnosis of numerous medical diseases identified from those images. Traditional image classification methods combined with hand-crafted image feature descriptors and various classifiers are not able to effectively improve the accuracy rate and meet the high requirements of classification of biomedical images. The same also holds true for artificial neural network models directly trained with limited biomedical images used as training data or directly used as a black box to extract the deep features based on another distant dataset. In this study, we propose a highly reliable and accurate end-to-end classifier for all kinds of biomedical images via deep learning and transfer learning.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We first apply domain transferred deep convolutional neural network for building a deep model; and then develop an overall deep learning architecture based on the raw pixels of original biomedical images using supervised training. In our model, we do not need the manual design of the feature space, seek an effective feature vector classifier or segment specific detection object and image patches, which are the main technological difficulties in the adoption of traditional image classification methods. Moreover, we do not need to be concerned with whether there are large training sets of annotated biomedical images, affordable parallel computing resources featuring GPUs or long times to wait for training a perfect deep model, which are the main problems to train deep neural networks for biomedical image classification as observed in recent works.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>With the utilization of a simple data augmentation method and fast convergence speed, our algorithm can achieve the best accuracy rate and outstanding classification ability for biomedical images. We have evaluated our classifier on several well-known public biomedical datasets and compared it with several state-of-the-art approaches.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>We propose a robust automated end-to-end classifier for biomedical images based on a domain transferred deep convolutional neural network model that shows a highly reliable and accurate performance which has been confirmed on several public biomedical image datasets.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ireland Ltd. All rights reserved.</p></div></div>",pangshuchao1212@sina.com,Biomedical image classification; Convolutional neural network; Data augmentation; Deep learning; Transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/28254085,pubmed,2017,633eb771-27f2-45f5-8658-a1c9872aeb48,1
convnet-based localization of anatomical structures in 3-d medical images,/pubmed/28252392,"de Vos BD, Wolterink JM, de Jong PA, Leiner T, Viergever MA, Isgum I.",IEEE Trans Med Imaging. 2017 Jul;36(7):1470-1481. doi: 10.1109/TMI.2017.2673121. Epub 2017 Feb 23.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28252392,pubmed,28252392,create date:2017/03/03 | first author:de Vos BD,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28252392,pubmed,2017,7b73f5b5-3436-4146-b15e-58b526c3696b,1
automated synaptic connectivity inference for volume electron microscopy,/pubmed/28250467,"Dorkenwald S, Schubert PJ, Killinger MF, Urban G, Mikula S, Svara F, Kornfeld J.",Nat Methods. 2017 Apr;14(4):435-442. doi: 10.1038/nmeth.4206. Epub 2017 Feb 27.,Nat Methods.  2017,PubMed,citation,PMID:28250467,pubmed,28250467,create date:2017/03/03 | first author:Dorkenwald S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Teravoxel volume electron microscopy data sets from neural tissue can now be acquired in weeks, but data analysis requires years of manual labor. We developed the SyConn framework, which uses deep convolutional neural networks and random forest classifiers to infer a richly annotated synaptic connectivity matrix from manual neurite skeleton reconstructions by automatically identifying mitochondria, synapses and their types, axons, dendrites, spines, myelin, somata and cell types. We tested our approach on serial block-face electron microscopy data sets from zebrafish, mouse and zebra finch, and computed the synaptic wiring of songbird basal ganglia. We found that, for example, basal-ganglia cell types with high firing rates in vivo had higher densities of mitochondria and vesicles and that synapse sizes and quantities scaled systematically, depending on the innervated postsynaptic cell types.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28250467,pubmed,2017,66a77de5-b8f0-4d09-ba0c-3c83d8b19014,1
cell segmentation in histopathological images with deep learning algorithms by utilizing spatial relationships,/pubmed/28247185,"Hatipoglu N, Bilgin G.",Med Biol Eng Comput. 2017 Feb 28. doi: 10.1007/s11517-017-1630-1. [Epub ahead of print],Med Biol Eng Comput.  2017,PubMed,citation,PMID:28247185,pubmed,28247185,create date:2017/03/02 | first author:Hatipoglu N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In many computerized methods for cell detection, segmentation, and classification in digital histopathology that have recently emerged, the task of cell segmentation remains a chief problem for image processing in designing computer-aided diagnosis (CAD) systems. In research and diagnostic studies on cancer, pathologists can use CAD systems as second readers to analyze high-resolution histopathological images. Since cell detection and segmentation are critical for cancer grade assessments, cellular and extracellular structures should primarily be extracted from histopathological images. In response, we sought to identify a useful cell segmentation approach with histopathological images that uses not only prominent deep learning algorithms (i.e., convolutional neural networks, stacked autoencoders, and deep belief networks), but also spatial relationships, information of which is critical for achieving better cell segmentation results. To that end, we collected cellular and extracellular samples from histopathological images by windowing in small patches with various sizes. In experiments, the segmentation accuracies of the methods used improved as the window sizes increased due to the addition of local spatial and contextual information. Once we compared the effects of training sample size and influence of window size, results revealed that the deep learning algorithms, especially convolutional neural networks and partly stacked autoencoders, performed better than conventional methods in cell segmentation.</abstracttext></p></div></div>",gbilgin@yildiz.edu.tr,Computer-aided diagnosis systems; Deep learning algorithms; Histopathological images; Segmentation; Spatial relationships,https://www.ncbi.nlm.nih.gov//pubmed/28247185,pubmed,2017,63791715-04a1-407e-8b26-d50db85c74a6,1
rna-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach,/pubmed/28245811,"Pan X, Shen HB.",BMC Bioinformatics. 2017 Feb 28;18(1):136. doi: 10.1186/s12859-017-1561-8.,BMC Bioinformatics.  2017,PubMed,citation,PMID:28245811 | PMCID:PMC5331642,pubmed,28245811,create date:2017/03/02 | first author:Pan X,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>RNAs play key roles in cells through the interactions with proteins known as the RNA-binding proteins (RBP) and their binding motifs enable crucial understanding of the post-transcriptional regulation of RNAs. How the RBPs correctly recognize the target RNAs and why they bind specific positions is still far from clear. Machine learning-based algorithms are widely acknowledged to be capable of speeding up this process. Although many automatic tools have been developed to predict the RNA-protein binding sites from the rapidly growing multi-resource data, e.g. sequence, structure, their domain specific features and formats have posed significant computational challenges. One of current difficulties is that the cross-source shared common knowledge is at a higher abstraction level beyond the observed data, resulting in a low efficiency of direct integration of observed data across domains. The other difficulty is how to interpret the prediction results. Existing approaches tend to terminate after outputting the potential discrete binding sites on the sequences, but how to assemble them into the meaningful binding motifs is a topic worth of further investigation.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In viewing of these challenges, we propose a deep learning-based framework (iDeep) by using a novel hybrid convolutional neural network and deep belief network to predict the RBP interaction sites and motifs on RNAs. This new protocol is featured by transforming the original observed data into a high-level abstraction feature space using multiple layers of learning blocks, where the shared representations across different domains are integrated. To validate our iDeep method, we performed experiments on 31 large-scale CLIP-seq datasets, and our results show that by integrating multiple sources of data, the average AUC can be improved by 8% compared to the best single-source-based predictor; and through cross-domain knowledge integration at an abstraction level, it outperforms the state-of-the-art predictors by 6%. Besides the overall enhanced prediction performance, the convolutional neural network module embedded in iDeep is also able to automatically capture the interpretable binding motifs for RBPs. Large-scale experiments demonstrate that these mined binding motifs agree well with the experimentally verified results, suggesting iDeep is a promising approach in the real-world applications.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The iDeep framework not only can achieve promising performance than the state-of-the-art predictors, but also easily capture interpretable binding motifs. iDeep is available at http://www.csbio.sjtu.edu.cn/bioinf/iDeep.</abstracttext></p></div></div>",xypan172436@gmail.com,CLIP-seq; Convolutional neural network; Deep belief network; Multimodal deep learning; RNA-binding protein,https://www.ncbi.nlm.nih.gov//pubmed/28245811,pubmed,2017,75b0e7b9-844b-485d-81b6-32a590c5dc5a,1
a convolutional neural network approach to calibrating the rotation axis for x-ray computed tomography,/pubmed/28244442,"Yang X, De Carlo F, Phatak C, Gürsoy D.",J Synchrotron Radiat. 2017 Mar 1;24(Pt 2):469-475. doi: 10.1107/S1600577516020117. Epub 2017 Jan 24.,J Synchrotron Radiat.  2017,PubMed,citation,PMID:28244442,pubmed,28244442,create date:2017/03/01 | first author:Yang X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper presents an algorithm to calibrate the center-of-rotation for X-ray tomography by using a machine learning approach, the Convolutional Neural Network (CNN). The algorithm shows excellent accuracy from the evaluation of synthetic data with various noise ratios. It is further validated with experimental data of four different shale samples measured at the Advanced Photon Source and at the Swiss Light Source. The results are as good as those determined by visual inspection and show better robustness than conventional methods. CNN has also great potential for reducing or removing other artifacts caused by instrument instability, detector non-linearity, etc. An open-source toolbox, which integrates the CNN methods described in this paper, is freely available through GitHub at tomography/xlearn and can be easily integrated into existing computational pipelines available at various synchrotron facilities. Source code, documentation and information on how to contribute are also provided.</abstracttext></p></div></div>",,Python; convolutional neural network; open-source; rotation axis; tomography reconstruction,https://www.ncbi.nlm.nih.gov//pubmed/28244442,pubmed,2017,2d307094-9799-4a51-b93e-8158d39184ac,1
deepnat: deep convolutional neural network for segmenting neuroanatomy,/pubmed/28223187,"Wachinger C, Reuter M, Klein T.",Neuroimage. 2017 Feb 20. pii: S1053-8119(17)30146-5. doi: 10.1016/j.neuroimage.2017.02.035. [Epub ahead of print],Neuroimage.  2017,PubMed,citation,PMID:28223187 | PMCID:PMC5563492,pubmed,28223187,create date:2017/02/23 | first author:Wachinger C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We introduce DeepNAT, a 3D Deep convolutional neural network for the automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance images. DeepNAT is an end-to-end learning-based approach to brain segmentation that jointly learns an abstract feature representation and a multi-class classification. We propose a 3D patch-based approach, where we do not only predict the center voxel of the patch but also neighbors, which is formulated as multi-task learning. To address a class imbalance problem, we arrange two networks hierarchically, where the first one separates foreground from background, and the second one identifies 25 brain structures on the foreground. Since patches lack spatial context, we augment them with coordinates. To this end, we introduce a novel intrinsic parameterization of the brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As network architecture, we use three convolutional layers with pooling, batch normalization, and non-linearities, followed by fully connected layers with dropout. The final segmentation is inferred from the probabilistic output of the network with a 3D fully connected conditional random field, which ensures label agreement between close voxels. The roughly 2.7million parameters in the network are learned with stochastic gradient descent. Our results show that DeepNAT compares favorably to state-of-the-art methods. Finally, the purely learning-based method may have a high potential for the adaptation to young, old, or diseased brains by fine-tuning the pre-trained network with a small training sample on the target application, where the availability of larger datasets with manual annotations may boost the overall segmentation accuracy in the future.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",christian.wachinger@med.uni-muenchen.de,Brain segmentation; Conditional random field; Convolutional neural networks; Deep learning; Multi-task learning,https://www.ncbi.nlm.nih.gov//pubmed/28223187,pubmed,2017,84bcfb66-242a-4b6e-93f7-5aed2298bf58,1
automatic sleep stage classification of single-channel eeg by using complex-valued convolutional neural network,/pubmed/28222011,"Zhang J, Wu Y.",Biomed Tech (Berl). 2017 Feb 21. pii: /j/bmte.ahead-of-print/bmt-2016-0156/bmt-2016-0156.xml. doi: 10.1515/bmt-2016-0156. [Epub ahead of print],Biomed Tech (Berl).  2017,PubMed,citation,PMID:28222011,pubmed,28222011,create date:2017/02/22 | first author:Zhang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Many systems are developed for automatic sleep stage classification. However, nearly all models are based on handcrafted features. Because of the large feature space, there are so many features that feature selection should be used. Meanwhile, designing handcrafted features is a difficult and time-consuming task because the feature designing needs domain knowledge of experienced experts. Results vary when different sets of features are chosen to identify sleep stages. Additionally, many features that we may be unaware of exist. However, these features may be important for sleep stage classification. Therefore, a new sleep stage classification system, which is based on the complex-valued convolutional neural network (CCNN), is proposed in this study. Unlike the existing sleep stage methods, our method can automatically extract features from raw electroencephalography data and then classify sleep stage based on the learned features. Additionally, we also prove that the decision boundaries for the real and imaginary parts of a complex-valued convolutional neuron intersect orthogonally. The classification performances of handcrafted features are compared with those of learned features via CCNN. Experimental results show that the proposed method is comparable to the existing methods. CCNN obtains a better classification performance and considerably faster convergence speed than convolutional neural network. Experimental results also show that the proposed method is a useful decision-support tool for automatic sleep stage classification.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28222011,pubmed,2017,f61fa658-74f7-434c-849b-11025364e0c4,1
segmentation of organs-at-risks in head and neck ct images using convolutional neural networks,/pubmed/28205307,"Ibragimov B, Xing L.",Med Phys. 2017 Feb;44(2):547-557. doi: 10.1002/mp.12045.,Med Phys.  2017,PubMed,citation,PMID:28205307 | PMCID:PMC5383420,pubmed,28205307,create date:2017/02/17 | first author:Ibragimov B,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Accurate segmentation of organs-at-risks (OARs) is the key step for efficient planning of radiation therapy for head and neck (HaN) cancer treatment. In the work, we proposed the first deep learning-based algorithm, for segmentation of OARs in HaN CT images, and compared its performance against state-of-the-art automated segmentation algorithms, commercial software, and interobserver variability.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Convolutional neural networks (CNNs)-a concept from the field of deep learning-were used to study consistent intensity patterns of OARs from training CT images and to segment the OAR in a previously unseen test CT image. For CNN training, we extracted a representative number of positive intensity patches around voxels that belong to the OAR of interest in training CT images, and negative intensity patches around voxels that belong to the surrounding structures. These patches then passed through a sequence of CNN layers that captured local image features such as corners, end-points, and edges, and combined them into more complex high-order features that can efficiently describe the OAR. The trained network was applied to classify voxels in a region of interest in the test image where the corresponding OAR is expected to be located. We then smoothed the obtained classification results by using Markov random fields algorithm. We finally extracted the largest connected component of the smoothed voxels classified as the OAR by CNN, performed dilate-erode operations to remove cavities of the component, which resulted in segmentation of the OAR in the test image.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The performance of CNNs was validated on segmentation of spinal cord, mandible, parotid glands, submandibular glands, larynx, pharynx, eye globes, optic nerves, and optic chiasm using 50 CT images. The obtained segmentation results varied from 37.4% Dice coefficient (DSC) for chiasm to 89.5% DSC for mandible. We also analyzed the performance of state-of-the-art algorithms and commercial software reported in the literature, and observed that CNNs demonstrate similar or superior performance on segmentation of spinal cord, mandible, parotid glands, larynx, pharynx, eye globes, and optic nerves, but inferior performance on segmentation of submandibular glands and optic chiasm.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>We concluded that convolution neural networks can accurately segment most of OARs using a representative database of 50 HaN CT images. At the same time, inclusion of additional information, for example, MR images, may be beneficial to some OARs with poorly visible boundaries.</abstracttext></p><p class='copyright'>© 2016 American Association of Physicists in Medicine.</p></div></div>",,convolutional neural networks; deep learning; head and neck; radiotherapy; segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28205307,pubmed,2017,a274c952-6989-478c-bd5c-47dd0902015f,1
a multi-scale convolutional neural network for phenotyping high-content cellular images,/pubmed/28203779,"Godinez WJ, Hossain I, Lazic SE, Davies JW, Zhang X.",Bioinformatics. 2017 Jul 1;33(13):2010-2019. doi: 10.1093/bioinformatics/btx069.,Bioinformatics.  2017,PubMed,citation,PMID:28203779,pubmed,28203779,create date:2017/02/17 | first author:Godinez WJ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Identifying phenotypes based on high-content cellular images is challenging. Conventional image analysis pipelines for phenotype identification comprise multiple independent steps, with each step requiring method customization and adjustment of multiple parameters.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>Here, we present an approach based on a multi-scale convolutional neural network (M-CNN) that classifies, in a single cohesive step, cellular images into phenotypes by using directly and solely the images' pixel intensity values. The only parameters in the approach are the weights of the neural network, which are automatically optimized based on training images. The approach requires no a priori knowledge or manual customization, and is applicable to single- or multi-channel images displaying single or multiple cells. We evaluated the classification performance of the approach on eight diverse benchmark datasets. The approach yielded overall a higher classification accuracy compared with state-of-the-art results, including those of other deep CNN architectures. In addition to using the network to simply obtain a yes-or-no prediction for a given phenotype, we use the probability outputs calculated by the network to quantitatively describe the phenotypes. This study shows that these probability values correlate with chemical treatment concentrations. This finding validates further our approach and enables chemical treatment potency estimation via CNNs.</abstracttext></p><h4>Availability and Implementation: </h4><p><abstracttext label='Availability and Implementation' nlmcategory='UNASSIGNED'>The network specifications and solver definitions are provided in Supplementary Software 1.</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>william_jose.godinez_navarro@novartis.com or xian-1.zhang@novartis.com.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28203779,pubmed,2017,a030b956-134d-4221-8a7d-81d0b80a3c6e,1
mr-based synthetic ct generation using a deep convolutional neural network method,/pubmed/28192624,Han X.,Med Phys. 2017 Apr;44(4):1408-1419. doi: 10.1002/mp.12155. Epub 2017 Mar 21.,Med Phys.  2017,PubMed,citation,PMID:28192624,pubmed,28192624,create date:2017/02/14 | first author:Han X,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Interests have been rapidly growing in the field of radiotherapy to replace CT with magnetic resonance imaging (MRI), due to superior soft tissue contrast offered by MRI and the desire to reduce unnecessary radiation dose. MR-only radiotherapy also simplifies clinical workflow and avoids uncertainties in aligning MR with CT. Methods, however, are needed to derive CT-equivalent representations, often known as synthetic CT (sCT), from patient MR images for dose calculation and DRR-based patient positioning. Synthetic CT estimation is also important for PET attenuation correction in hybrid PET-MR systems. We propose in this work a novel deep convolutional neural network (DCNN) method for sCT generation and evaluate its performance on a set of brain tumor patient images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The proposed method builds upon recent developments of deep learning and convolutional neural networks in the computer vision literature. The proposed DCNN model has 27 convolutional layers interleaved with pooling and unpooling layers and 35 million free parameters, which can be trained to learn a direct end-to-end mapping from MR images to their corresponding CTs. Training such a large model on our limited data is made possible through the principle of transfer learning and by initializing model weights from a pretrained model. Eighteen brain tumor patients with both CT and T1-weighted MR images are used as experimental data and a sixfold cross-validation study is performed. Each sCT generated is compared against the real CT image of the same patient on a voxel-by-voxel basis. Comparison is also made with respect to an atlas-based approach that involves deformable atlas registration and patch-based atlas fusion.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed DCNN method produced a mean absolute error (MAE) below 85 HU for 13 of the 18 test subjects. The overall average MAE was 84.8 ± 17.3 HU for all subjects, which was found to be significantly better than the average MAE of 94.5 ± 17.8 HU for the atlas-based method. The DCNN method also provided significantly better accuracy when being evaluated using two other metrics: the mean squared error (188.6 ± 33.7 versus 198.3 ± 33.0) and the Pearson correlation coefficient(0.906 ± 0.03 versus 0.896 ± 0.03). Although training a DCNN model can be slow, training only need be done once. Applying a trained model to generate a complete sCT volume for each new patient MR image only took 9 s, which was much faster than the atlas-based approach.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>A DCNN model method was developed, and shown to be able to produce highly accurate sCT estimations from conventional, single-sequence MR images in near real time. Quantitative results also showed that the proposed method competed favorably with an atlas-based method, in terms of both accuracy and computation speed at test time. Further validation on dose computation accuracy and on a larger patient cohort is warranted. Extensions of the method are also possible to further improve accuracy or to handle multi-sequence MR images.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,"
MRI
; convolutional neural network; deep learning; radiation therapy; synthetic CT",https://www.ncbi.nlm.nih.gov//pubmed/28192624,pubmed,2017,e1096c87-1ba3-42bd-9c50-f87ffa2fcf43,1
three-class mammogram classification based on descriptive cnn features,/pubmed/28191461,"Jadoon MM, Zhang Q, Haq IU, Butt S, Jadoon A.",Biomed Res Int. 2017;2017:3640901. doi: 10.1155/2017/3640901. Epub 2017 Jan 15.,Biomed Res Int.  2017,PubMed,citation,PMID:28191461 | PMCID:PMC5274695,pubmed,28191461,create date:2017/02/14 | first author:Jadoon MM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, a novel classification technique for large data set of mammograms using a deep learning method is proposed. The proposed model targets a three-class classification study (normal, malignant, and benign cases). In our model we have presented two methods, namely, convolutional neural network-discrete wavelet (CNN-DW) and convolutional neural network-curvelet transform (CNN-CT). An augmented data set is generated by using mammogram patches. To enhance the contrast of mammogram images, the data set is filtered by contrast limited adaptive histogram equalization (CLAHE). In the CNN-DW method, enhanced mammogram images are decomposed as its four subbands by means of two-dimensional discrete wavelet transform (2D-DWT), while in the second method discrete curvelet transform (DCT) is used. In both methods, dense scale invariant feature (DSIFT) for all subbands is extracted. Input data matrix containing these subband features of all the mammogram patches is created that is processed as input to convolutional neural network (CNN). Softmax layer and support vector machine (SVM) layer are used to train CNN for classification. Proposed methods have been compared with existing methods in terms of accuracy rate, error rate, and various validation assessment measures. CNN-DW and CNN-CT have achieved accuracy rate of 81.83% and 83.74%, respectively. Simulation results clearly validate the significance and impact of our proposed model as compared to other well-known existing techniques.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28191461,pubmed,2017,95f4504c-be68-4776-80e2-f029aa7bc6fb,1
detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection,/pubmed/28186883,"Sarikaya D, Corso JJ, Guru KA.",IEEE Trans Med Imaging. 2017 Jul;36(7):1542-1549. doi: 10.1109/TMI.2017.2665671. Epub 2017 Feb 8.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28186883,pubmed,28186883,create date:2017/02/12 | first author:Sarikaya D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28186883,pubmed,2017,b20585d6-9d30-48e3-a824-172f9ec4fb0f,1
cascade convolutional neural networks for automatic detection of thyroid nodules in ultrasound images,/pubmed/28186630,"Ma J, Wu F, Jiang T, Zhu J, Kong D.",Med Phys. 2017 May;44(5):1678-1691. doi: 10.1002/mp.12134. Epub 2017 Apr 17.,Med Phys.  2017,PubMed,citation,PMID:28186630,pubmed,28186630,create date:2017/02/12 | first author:Ma J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>It is very important for calculation of clinical indices and diagnosis to detect thyroid nodules from ultrasound images. However, this task is a challenge mainly due to heterogeneous thyroid nodules with distinct components are similar to background in ultrasound images. In this study, we employ cascade deep convolutional neural networks (CNNs) to develop and evaluate a fully automatic detection of thyroid nodules from 2D ultrasound images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Our cascade CNNs are a type of hybrid model, consisting of two different CNNs and a new splitting method. Specifically, it employs a deep CNN to learn the segmentation probability maps from the ground true data. Then, all the segmentation probability maps are split into different connected regions by the splitting method. Finally, another deep CNN is used to automatically detect the thyroid nodules from ultrasound thyroid images.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Experiment results illustrate the cascade CNNs are very effective in detection of thyroid nodules. Specially, the value of area under the curve of receiver operating characteristic is 98.51%. The Free-response receiver operating characteristic (FROC) and jackknife alternative FROC (JAFROC) analyses show a significant improvement in the performance of our cascade CNNs compared to that of other methods. The multi-view strategy can improve the performance of cascade CNNs. Moreover, our special splitting method can effectively separate different connected regions so that the second CNN can correctively gain the positive and negative samples according to the automatic labels.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The experiment results demonstrate the potential clinical applications of this proposed method. This technique can offer physicians an objective second opinion, and reduce their heavy workload so as to avoid misdiagnosis causes because of excessive fatigue. In addition, it is easy and reproducible for a person without medical expertise to diagnose thyroid nodules.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,convolutional neural network; detection; feature extraction; segmentation; thyroid nodule; ultrasound image,https://www.ncbi.nlm.nih.gov//pubmed/28186630,pubmed,2017,25ea889e-c9ad-43bf-a920-06d0456c0800,1
comparative validation of polyp detection methods in video colonoscopy: results from the miccai 2015 endoscopic vision challenge,/pubmed/28182555,"Bernal J, Tajkbaksh N, Sanchez FJ, Matuszewski BJ, Hao Chen, Lequan Yu, Angermann Q, Romain O, Rustad B, Balasingham I, Pogorelov K, Sungbin Choi, Debard Q, Maier-Hein L, Speidel S, Stoyanov D, Brandao P, Cordova H, Sanchez-Montes C, Gurudu SR, Fernandez-Esparrach G, Dray X, et al.",IEEE Trans Med Imaging. 2017 Jun;36(6):1231-1249. doi: 10.1109/TMI.2017.2664042. Epub 2017 Feb 2.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28182555,pubmed,28182555,create date:2017/02/10 | first author:Bernal J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Colonoscopy is the gold standard for colon cancer screening though some polyps are still missed, thus preventing early disease detection and treatment. Several computational systems have been proposed to assist polyp detection during colonoscopy but so far without consistent evaluation. The lack of publicly available annotated databases has made it difficult to compare methods and to assess if they achieve performance levels acceptable for clinical use. The Automatic Polyp Detection sub-challenge, conducted as part of the Endoscopic Vision Challenge (http://endovis.grand-challenge.org) at the international conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in 2015, was an effort to address this need. In this paper, we report the results of this comparative evaluation of polyp detection methods, as well as describe additional experiments to further explore differences between methods. We define performance metrics and provide evaluation databases that allow comparison of multiple methodologies. Results show that convolutional neural networks are the state of the art. Nevertheless, it is also demonstrated that combining different methodologies can lead to an improved overall performance.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28182555,pubmed,2017,2e766f27-7e40-4519-a898-b9bbbde4e20a,1
issls prize in bioengineering science 2017: automation of reading of radiological features from magnetic resonance images (mris) of the lumbar spine without human intervention is comparable with an expert radiologist,/pubmed/28168339,"Jamaludin A, Lootus M, Kadir T, Zisserman A, Urban J, Battié MC, Fairbank J, McCall I; Genodisc Consortium..",Eur Spine J. 2017 May;26(5):1374-1383. doi: 10.1007/s00586-017-4956-3. Epub 2017 Feb 6.,Eur Spine J.  2017,PubMed,citation,PMID:28168339,pubmed,28168339,create date:2017/02/09 | first author:Jamaludin A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>STUDY DESIGN: </h4><p><abstracttext label='STUDY DESIGN' nlmcategory='METHODS'>Investigation of the automation of radiological features from magnetic resonance images (MRIs) of the lumbar spine.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>To automate the process of grading lumbar intervertebral discs and vertebral bodies from MRIs. MR imaging is the most common imaging technique used in investigating low back pain (LBP). Various features of degradation, based on MRIs, are commonly recorded and graded, e.g., Modic change and Pfirrmann grading of intervertebral discs. Consistent scoring and grading is important for developing robust clinical systems and research. Automation facilitates this consistency and reduces the time of radiological analysis considerably and hence the expense.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>12,018 intervertebral discs, from 2009 patients, were graded by a radiologist and were then used to train: (1) a system to detect and label vertebrae and discs in a given scan, and (2) a convolutional neural network (CNN) model that predicts several radiological gradings. The performance of the model, in terms of class average accuracy, was compared with the intra-observer class average accuracy of the radiologist.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The detection system achieved 95.6% accuracy in terms of disc detection and labeling. The model is able to produce predictions of multiple pathological gradings that consistently matched those of the radiologist. The model identifies 'Evidence Hotspots' that are the voxels that most contribute to the degradation scores.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Automation of radiological grading is now on par with human performance. The system can be beneficial in aiding clinical diagnoses in terms of objectivity of gradings and the speed of analysis. It can also draw the attention of a radiologist to regions of degradation. This objectivity and speed is an important stepping stone in the investigation of the relationship between MRIs and clinical diagnoses of back pain in large cohorts.</abstracttext></p><h4>LEVEL OF EVIDENCE: </h4><p><abstracttext label='LEVEL OF EVIDENCE' nlmcategory='METHODS'>Level 3.</abstracttext></p></div></div>",jeremy.fairbank@ndorms.ox.ac.uk,Automated grading; Deep learning; Disc analysis; Disc bulge; Disc classification; Disc detection; Disc herniation; Modic changes; Pfirrmann grading; Spondylolisthesis; Vertebrae analysis,https://www.ncbi.nlm.nih.gov//pubmed/28168339,pubmed,2017,63445a19-e533-4ce3-b13b-93619b6c4846,1
comparing humans and deep learning performance for grading amd: a study in using universal deep features and transfer learning for automated amd analysis,/pubmed/28167406,"Burlina P, Pacheco KD, Joshi N, Freund DE, Bressler NM.",Comput Biol Med. 2017 Mar 1;82:80-86. doi: 10.1016/j.compbiomed.2017.01.018. Epub 2017 Jan 27.,Comput Biol Med.  2017,PubMed,citation,PMID:28167406 | PMCID:PMC5373654,pubmed,28167406,create date:2017/02/09 | first author:Burlina P,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>When left untreated, age-related macular degeneration (AMD) is the leading cause of vision loss in people over fifty in the US. Currently it is estimated that about eight million US individuals have the intermediate stage of AMD that is often asymptomatic with regard to visual deficit. These individuals are at high risk for progressing to the advanced stage where the often treatable choroidal neovascular form of AMD can occur. Careful monitoring to detect the onset and prompt treatment of the neovascular form as well as dietary supplementation can reduce the risk of vision loss from AMD, therefore, preferred practice patterns recommend identifying individuals with the intermediate stage in a timely manner.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Past automated retinal image analysis (ARIA) methods applied on fundus imagery have relied on engineered and hand-designed visual features. We instead detail the novel application of a machine learning approach using deep learning for the problem of ARIA and AMD analysis. We use transfer learning and universal features derived from deep convolutional neural networks (DCNN). We address clinically relevant 4-class, 3-class, and 2-class AMD severity classification problems.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Using 5664 color fundus images from the NIH AREDS dataset and DCNN universal features, we obtain values for accuracy for the (4-, 3-, 2-) class classification problem of (79.4%, 81.5%, 93.4%) for machine vs. (75.8%, 85.0%, 95.2%) for physician grading.</abstracttext></p><h4>DISCUSSION: </h4><p><abstracttext label='DISCUSSION' nlmcategory='CONCLUSIONS'>This study demonstrates the efficacy of machine grading based on deep universal features/transfer learning when applied to ARIA and is a promising step in providing a pre-screener to identify individuals with intermediate AMD and also as a tool that can facilitate identifying such individuals for clinical studies aimed at developing improved therapies. It also demonstrates comparable performance between computer and physician grading.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",David.Freund@jhuapl.edu,"Age-related macular degeneration, (AMD); Deep Convolutional Neural Networks, (DCNNs); Deep learning; Retinal image analysis; Transfer learning; Universal features",https://www.ncbi.nlm.nih.gov//pubmed/28167406,pubmed,2017,b2e9cbca-0d43-4550-9da2-0397c86576d1,1
deep ensemble learning of sparse regression models for brain disease diagnosis,/pubmed/28167394,"Suk HI, Lee SW, Shen D; Alzheimer’s Disease Neuroimaging Initiative..",Med Image Anal. 2017 Apr;37:101-113. doi: 10.1016/j.media.2017.01.008. Epub 2017 Jan 24.,Med Image Anal.  2017,PubMed,citation,PMID:28167394,pubmed,28167394,create date:2017/02/09 | first author:Suk HI,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent studies on brain imaging analysis witnessed the core roles of machine learning techniques in computer-assisted intervention for brain disease diagnosis. Of various machine-learning techniques, sparse regression models have proved their effectiveness in handling high-dimensional data but with a small number of training samples, especially in medical problems. In the meantime, deep learning methods have been making great successes by outperforming the state-of-the-art performances in various applications. In this paper, we propose a novel framework that combines the two conceptually different methods of sparse regression and deep learning for Alzheimer's disease/mild cognitive impairment diagnosis and prognosis. Specifically, we first train multiple sparse regression models, each of which is trained with different values of a regularization control parameter. Thus, our multiple sparse regression models potentially select different feature subsets from the original feature set; thereby they have different powers to predict the response values, i.e., clinical label and clinical scores in our work. By regarding the response values from our sparse regression models as target-level representations, we then build a deep convolutional neural network for clinical decision making, which thus we call 'Deep Ensemble Sparse Regression Network.' To our best knowledge, this is the first work that combines sparse regression models with deep neural network. In our experiments with the ADNI cohort, we validated the effectiveness of the proposed method by achieving the highest diagnostic accuracies in three classification tasks. We also rigorously analyzed our results and compared with the previous studies on the ADNI cohort in the literature.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",hisuk@korea.ac.kr,Alzheimer’s disease; Convolutional neural network; Deep ensemble learning; Sparse regression model,https://www.ncbi.nlm.nih.gov//pubmed/28167394,pubmed,2017,346aec69-dffe-4049-a1d6-da9aeba8976c,1
recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks,/pubmed/28158264,"Umarov RK, Solovyev VV.",PLoS One. 2017 Feb 3;12(2):e0171410. doi: 10.1371/journal.pone.0171410. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28158264 | PMCID:PMC5291440,pubmed,28158264,create date:2017/02/06 | first author:Umarov RK,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate computational identification of promoters remains a challenge as these key DNA regulatory regions have variable structures composed of functional motifs that provide gene-specific initiation of transcription. In this paper we utilize Convolutional Neural Networks (CNN) to analyze sequence characteristics of prokaryotic and eukaryotic promoters and build their predictive models. We trained a similar CNN architecture on promoters of five distant organisms: human, mouse, plant (Arabidopsis), and two bacteria (Escherichia coli and Bacillus subtilis). We found that CNN trained on sigma70 subclass of Escherichia coli promoter gives an excellent classification of promoters and non-promoter sequences (Sn = 0.90, Sp = 0.96, CC = 0.84). The Bacillus subtilis promoters identification CNN model achieves Sn = 0.91, Sp = 0.95, and CC = 0.86. For human, mouse and Arabidopsis promoters we employed CNNs for identification of two well-known promoter classes (TATA and non-TATA promoters). CNN models nicely recognize these complex functional regions. For human promoters Sn/Sp/CC accuracy of prediction reached 0.95/0.98/0,90 on TATA and 0.90/0.98/0.89 for non-TATA promoter sequences, respectively. For Arabidopsis we observed Sn/Sp/CC 0.95/0.97/0.91 (TATA) and 0.94/0.94/0.86 (non-TATA) promoters. Thus, the developed CNN models, implemented in CNNProm program, demonstrated the ability of deep learning approach to grasp complex promoter sequence characteristics and achieve significantly higher accuracy compared to the previously developed promoter prediction programs. We also propose random substitution procedure to discover positionally conserved promoter functional elements. As the suggested approach does not require knowledge of any specific promoter features, it can be easily extended to identify promoters and other complex functional regions in sequences of many other and especially newly sequenced genomes. The CNNProm program is available to run at web server http://www.softberry.com.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28158264,pubmed,2017,af2c888d-2615-47c3-9bcd-7aad385d4791,1
a deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images,/pubmed/28154470,"Xu J, Luo X, Wang G, Gilmore H, Madabhushi A.",Neurocomputing. 2016 May 26;191:214-223. doi: 10.1016/j.neucom.2016.01.034. Epub 2016 Feb 17.,Neurocomputing.  2016,PubMed,citation,PMID:28154470 | PMCID:PMC5283391,pubmed,28154470,create date:2017/02/06 | first author:Xu J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Epithelial (EP) and stromal (ST) are two types of tissues in histological images. Automated segmentation or classification of EP and ST tissues is important when developing computerized system for analyzing the tumor microenvironment. In this paper, a Deep Convolutional Neural Networks (DCNN) based feature learning is presented to automatically segment or classify EP and ST regions from digitized tumor tissue microarrays (TMAs). Current approaches are based on handcraft feature representation, such as color, texture, and Local Binary Patterns (LBP) in classifying two regions. Compared to handcrafted feature based approaches, which involve task dependent representation, DCNN is an end-to-end feature extractor that may be directly learned from the raw pixel intensity value of EP and ST tissues in a data driven fashion. These high-level features contribute to the construction of a supervised classifier for discriminating the two types of tissues. In this work we compare DCNN based models with three handcraft feature extraction based approaches on two different datasets which consist of 157 Hematoxylin and Eosin (H&amp;E) stained images of breast cancer and 1376 immunohistological (IHC) stained images of colorectal cancer, respectively. The DCNN based feature learning approach was shown to have a F1 classification score of 85%, 89%, and 100%, accuracy (ACC) of 84%, 88%, and 100%, and Matthews Correlation Coefficient (MCC) of 86%, 77%, and 100% on two H&amp;E stained (NKI and VGH) and IHC stained data, respectively. Our DNN based approach was shown to outperform three handcraft feature extraction based approaches in terms of the classification of EP and ST regions.</abstracttext></p></div></div>",,Breast histopathology; Colorectal cancer; Deep Convolutional Neural Networks; Feature representation; The classification of epithelial and stromal regions,https://www.ncbi.nlm.nih.gov//pubmed/28154470,pubmed,2016,6429810d-0d56-4d3c-8d30-e290517a2495,1
3d deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients,/pubmed/28149967,"Nie D, Zhang H, Adeli E, Liu L, Shen D.",Med Image Comput Comput Assist Interv. 2016 Oct;9901:212-220. doi: 10.1007/978-3-319-46723-8_25. Epub 2016 Oct 2.,Med Image Comput Comput Assist Interv.  2016,PubMed,citation,PMID:28149967 | PMCID:PMC5278791,pubmed,28149967,create date:2017/02/06 | first author:Nie D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>High-grade glioma is the most aggressive and severe brain tumor that leads to death of almost 50% patients in 1-2 years. Thus, accurate prognosis for glioma patients would provide essential guidelines for their treatment planning. Conventional survival prediction generally utilizes clinical information and limited handcrafted features from magnetic resonance images (MRI), which is often time consuming, laborious and subjective. In this paper, we propose using deep learning frameworks to automatically extract features from multi-modal preoperative brain images (i.e., T1 MRI, fMRI and DTI) of high-grade glioma patients. Specifically, we adopt 3D convolutional neural networks (CNNs) and also propose a new network architecture for using multi-channel data and learning supervised features. Along with the pivotal clinical features, we finally train a support vector machine to predict if the patient has a long or short overall survival (OS) time. Experimental results demonstrate that our methods can achieve an accuracy as high as 89.9% We also find that the learned features from fMRI and DTI play more important roles in accurately predicting the OS time, which provides valuable insights into functional neuro-oncological applications.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28149967,pubmed,2016,9a087af8-ea14-4ca7-88bf-664776c39b8e,1
comparison of machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer from (18)f-fdg pet/ct images,/pubmed/28130689,"Wang H, Zhou Z, Li Y, Chen Z, Lu P, Wang W, Liu W, Yu L.",EJNMMI Res. 2017 Dec;7(1):11. doi: 10.1186/s13550-017-0260-9. Epub 2017 Jan 28.,EJNMMI Res.  2017,PubMed,citation,PMID:28130689 | PMCID:PMC5272853,pubmed,28130689,create date:2017/01/29 | first author:Wang H,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>This study aimed to compare one state-of-the-art deep learning method and four classical machine learning methods for classifying mediastinal lymph node metastasis of non-small cell lung cancer (NSCLC) from <sup>18</sup>F-FDG PET/CT images. Another objective was to compare the discriminative power of the recently popular PET/CT texture features with the widely used diagnostic features such as tumor size, CT value, SUV, image contrast, and intensity standard deviation. The four classical machine learning methods included random forests, support vector machines, adaptive boosting, and artificial neural network. The deep learning method was the convolutional neural networks (CNN). The five methods were evaluated using 1397 lymph nodes collected from PET/CT images of 168 patients, with corresponding pathology analysis results as gold standard. The comparison was conducted using 10 times 10-fold cross-validation based on the criterion of sensitivity, specificity, accuracy (ACC), and area under the ROC curve (AUC). For each classical method, different input features were compared to select the optimal feature set. Based on the optimal feature set, the classical methods were compared with CNN, as well as with human doctors from our institute.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>For the classical methods, the diagnostic features resulted in 81~85% ACC and 0.87~0.92 AUC, which were significantly higher than the results of texture features. CNN's sensitivity, specificity, ACC, and AUC were 84, 88, 86, and 0.91, respectively. There was no significant difference between the results of CNN and the best classical method. The sensitivity, specificity, and ACC of human doctors were 73, 90, and 82, respectively. All the five machine learning methods had higher sensitivities but lower specificities than human doctors.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The present study shows that the performance of CNN is not significantly different from the best classical methods and human doctors for classifying mediastinal lymph node metastasis of NSCLC from PET/CT images. Because CNN does not need tumor segmentation or feature calculation, it is more convenient and more objective than the classical methods. However, CNN does not make use of the import diagnostic features, which have been proved more discriminative than the texture features for classifying small-sized lymph nodes. Therefore, incorporating the diagnostic features into CNN is a promising direction for future research.</abstracttext></p></div></div>",yulijuan2003@126.com,Computer-aided diagnosis; Deep learning; Machine learning; Non-small cell lung cancer; Positron-emission tomography,https://www.ncbi.nlm.nih.gov//pubmed/28130689,pubmed,2017,744e89bc-802b-4eb2-ab80-e88652025569,1
dermatologist-level classification of skin cancer with deep neural networks,/pubmed/28117445,"Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S.",Nature. 2017 Feb 2;542(7639):115-118. doi: 10.1038/nature21056. Epub 2017 Jan 25. Erratum in: Nature. 2017 Jun 28;546(7660):686. ,Nature.  2017,PubMed,citation,PMID:28117445,pubmed,28117445,create date:2017/01/25 | first author:Esteva A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28117445,pubmed,2017,5c96104c-1601-4d48-8441-7606d3d02309,1
integrating online and offline three-dimensional deep learning for automated polyp detection in colonoscopy videos,/pubmed/28114049,"Lequan Yu, Hao Chen, Qi Dou, Jing Qin, Pheng Ann Heng.",IEEE J Biomed Health Inform. 2017 Jan;21(1):65-75. doi: 10.1109/JBHI.2016.2637004. Epub 2016 Dec 7.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28114049,pubmed,28114049,create date:2017/01/24 | first author:Lequan Yu,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28114049,pubmed,2017,9b21f857-fa06-4a8b-8a33-f60e62a58ccb,1
multisource transfer learning with convolutional neural networks for lung pattern analysis,/pubmed/28114048,"Christodoulidis S, Anthimopoulos M, Ebner L, Christe A, Mougiakakou S.",IEEE J Biomed Health Inform. 2017 Jan;21(1):76-84. doi: 10.1109/JBHI.2016.2636929. Epub 2016 Dec 7.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28114048,pubmed,28114048,create date:2017/01/24 | first author:Christodoulidis S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28114048,pubmed,2017,dc01682e-3bfb-4ca1-9d5a-4c517062fa95,1
an ensemble of fine-tuned convolutional neural networks for medical image classification,/pubmed/28114041,"Kumar A, Kim J, Lyndon D, Fulham M, Feng D.",IEEE J Biomed Health Inform. 2017 Jan;21(1):31-40. doi: 10.1109/JBHI.2016.2635663. Epub 2016 Dec 5.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28114041,pubmed,28114041,create date:2017/01/24 | first author:Kumar A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than methods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28114041,pubmed,2017,0d387812-850a-4e84-8a57-cc701279a3bf,1
automatic detection and classification of colorectal polyps by transferring low-level cnn features from nonmedical domain,/pubmed/28114040,"Zhang R, Zheng Y, Mak TW, Yu R, Wong SH, Lau JY, Poon CC.",IEEE J Biomed Health Inform. 2017 Jan;21(1):41-47. doi: 10.1109/JBHI.2016.2635662. Epub 2016 Dec 5.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28114040,pubmed,28114040,create date:2017/01/24 | first author:Zhang R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28114040,pubmed,2017,3bd582af-d98e-4f9e-8985-040bca4e4925,1
beyond object proposals: random crop pooling for multi-label image recognition,/pubmed/28113973,"Wang M, Luo C, Hong R, Tang J, Feng J.",IEEE Trans Image Process. 2016 Sep 22. doi: 10.1109/TIP.2016.2612829. [Epub ahead of print],IEEE Trans Image Process.  2016,PubMed,citation,PMID:28113973,pubmed,28113973,create date:2017/01/24 | first author:Wang M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Learning high-level image representations using object proposals has achieved remarkable success in multilabel image recognition. However, most object proposals provide merely coarse information about the objects, and only carefully selected proposals can be helpful for boosting the performance of multi-label image recognition. In this work, we propose an object-proposal-free framework for multi-label image recognition: random crop pooling (RCP). Basically, RCP performs stochastic scaling and cropping over images before feeding them to a standard convolutional neural network, which works quite well with a max-pooling operation for recognizing the complex contents of multi-label images. To better fit the multi-label image recognition task, we further develop a new loss function - the dynamic weighted Euclidean loss - for the training of the deep network. Our RCP approach is amazingly simple yet effective. It can achieve significantly better image recognition performance than the approaches using object proposals. Moreover, our adapted network can be easily trained in an end-to-end manner. Extensive experiments are conducted on two representative multilabel image recognition datasets (i.e., PASCAL VOC 2007 and PASCAL VOC 2012), and the results clearly demonstrate the superiority of our approach.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28113973,pubmed,2016,0f559068-eb61-4c4a-b29a-f45604b92d1e,1
automatic scoring of multiple semantic attributes with multi-task feature leverage: a study on pulmonary nodules in ct images,/pubmed/28113928,"Sihong Chen, Jing Qin, Xing Ji, Baiying Lei, Tianfu Wang, Dong Ni, Jie-Zhi Cheng.",IEEE Trans Med Imaging. 2017 Mar;36(3):802-814. doi: 10.1109/TMI.2016.2629462. Epub 2016 Nov 16.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28113928,pubmed,28113928,create date:2017/01/24 | first author:Sihong Chen,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of 'spiculation', 'texture', 'margin', etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28113928,pubmed,2017,a651e0dd-0966-43ee-ad3e-b83f2795b6ef,1
adaptive estimation of active contour parameters using convolutional neural networks and texture analysis,/pubmed/28113927,"Hoogi A, Subramaniam A, Veerapaneni R, Rubin DL.",IEEE Trans Med Imaging. 2017 Mar;36(3):781-791. doi: 10.1109/TMI.2016.2628084. Epub 2016 Nov 11.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28113927 | PMCID:PMC5510759,pubmed,28113927,create date:2017/01/24 | first author:Hoogi A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we propose a generalization of the level set segmentation approach by supplying a novel method for adaptive estimation of active contour parameters. The presented segmentation method is fully automatic once the lesion has been detected. First, the location of the level set contour relative to the lesion is estimated using a convolutional neural network (CNN). The CNN has two convolutional layers for feature extraction, which lead into dense layers for classification. Second, the output CNN probabilities are then used to adaptively calculate the parameters of the active contour functional during the segmentation process. Finally, the adaptive window size surrounding each contour point is re-estimated by an iterative process that considers lesion size and spatial texture. We demonstrate the capabilities of our method on a dataset of 164 MRI and 112 CT images of liver lesions that includes low contrast and heterogeneous lesions as well as noisy images. To illustrate the strength of our method, we evaluated it against state of the art CNN-based and active contour techniques. For all cases, our method, as assessed by Dice similarity coefficients, performed significantly better than currently available methods. An average Dice improvement of 0.27 was found across the entire dataset over all comparisons. We also analyzed two challenging subsets of lesions and obtained a significant Dice improvement of 0.24 with our method (p &lt;;0.001, Wilcoxon).</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28113927,pubmed,2017,d0d864c6-5fa8-4f54-9a22-3eed407e5d4e,1
detecting cardiovascular disease from mammograms with deep learning,/pubmed/28113340,"Wang J, Ding H, Bidgoli FA, Zhou B, Iribarren C, Molloi S, Baldi P.",IEEE Trans Med Imaging. 2017 May;36(5):1172-1181. doi: 10.1109/TMI.2017.2655486. Epub 2017 Jan 19.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28113340 | PMCID:PMC5522710,pubmed,28113340,create date:2017/01/24 | first author:Wang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected inmammograms, can be useful riskmarkers associated with the disease. We investigate the feasibility of automated and accurate detection ofBACsinmammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-responsereceiveroperatingcharacteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection inmammograms to help identify and assess patients with cardiovascular risks.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28113340,pubmed,2017,e1883ad9-8d0d-431c-a724-ede3c867221b,1
a comprehensive study on cross-view gait based human identification with deep cnns,/pubmed/28113278,"Wu Z, Huang Y, Wang L, Wang X, Tan T.",IEEE Trans Pattern Anal Mach Intell. 2016 Mar 23. doi: 10.1109/TPAMI.2016.2545669. [Epub ahead of print],IEEE Trans Pattern Anal Mach Intell.  2016,PubMed,citation,PMID:28113278,pubmed,28113278,create date:2017/01/24 | first author:Wu Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper studies an approach to gait based human identification via similarity learning by deep convolutional neural networks (CNNs).With a pretty small group of labeled multi-view human walking videos, we can train deep networks to recognize the most discriminative changes of gait patterns which suggest the change of human identity. To the best of our knowledge, this is the first work based on deep CNNs for gait recognition in the literature. Here, we provide an extensive empirical evaluation in terms of various scenarios, namely, cross-view and cross-walkingcondition, with different preprocessing approaches and network architectures. The method is first evaluated on the challenging CASIA-B dataset in terms of cross-view gait recognition. Experimental results show that it outperforms the previous state-of-theart methods by a significant margin. In particular, our method shows advantages when the cross-view angle is large, i.e., no less than 36. And the average recognition rate can reach 94.1%, much better than the previous best result (less than 65%). The method is further evaluated on the OU-ISIR gait dataset to test its generalization ability to larger data. OU-ISIR is currently the largest dataset available in the literature for gait recognition, with 4,007 subjects. On this dataset, the average accuracy of our method under identical view conditions is above 98%, and the one for cross-view scenarios is above 91%. Finally, the method also performs the best on the USF gait dataset, whose gait sequences are imaged in a real outdoor scene. These results show great potential of this method for practical applications.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28113278,pubmed,2016,23337bf0-3eb2-4853-95a4-70cceb4ee99a,1
cnn-svm for microvascular morphological type recognition with data augmentation,/pubmed/28111532,"Xue DX, Zhang R, Feng H, Wang YL.",J Med Biol Eng. 2016;36(6):755-764. doi: 10.1007/s40846-016-0182-4. Epub 2016 Dec 10.,J Med Biol Eng.  2016,PubMed,citation,PMID:28111532 | PMCID:PMC5216097,pubmed,28111532,create date:2017/01/24 | first author:Xue DX,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper focuses on the problem of feature extraction and the classification of microvascular morphological types to aid esophageal cancer detection. We present a patch-based system with a hybrid SVM model with data augmentation for intraepithelial papillary capillary loop recognition. A greedy patch-generating algorithm and a specialized CNN named NBI-Net are designed to extract hierarchical features from patches. We investigate a series of data augmentation techniques to progressively improve the prediction invariance of image scaling and rotation. For classifier boosting, SVM is used as an alternative to softmax to enhance generalization ability. The effectiveness of CNN feature representation ability is discussed for a set of widely used CNN models, including AlexNet, VGG-16, and GoogLeNet. Experiments are conducted on the NBI-ME dataset. The recognition rate is up to 92.74% on the patch level with data augmentation and classifier boosting. The results show that the combined CNN-SVM model beats models of traditional features with SVM as well as the original CNN with softmax. The synthesis results indicate that our system is able to assist clinical diagnosis to a certain extent.</abstracttext></p></div></div>",,Convolutional neural network; Data augmentation; Feature representation; Microvascular type classification; Support vector machine (SVM),https://www.ncbi.nlm.nih.gov//pubmed/28111532,pubmed,2016,0af67ac3-e63b-4eed-89cf-829c6dbe0203,1
detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion,/pubmed/28110732,"Prentašić P, Lončarić S.",Comput Methods Programs Biomed. 2016 Dec;137:281-292. doi: 10.1016/j.cmpb.2016.09.018. Epub 2016 Oct 6.,Comput Methods Programs Biomed.  2016,PubMed,citation,PMID:28110732,pubmed,28110732,create date:2017/01/24 | first author:Prentašić P,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND AND OBJECTIVE: </h4><p><abstracttext label='BACKGROUND AND OBJECTIVE' nlmcategory='OBJECTIVE'>Diabetic retinopathy is one of the leading disabling chronic diseases and one of the leading causes of preventable blindness in developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into automated population screening programs. Detection of exudates in color fundus photographs is very important for early diagnosis of diabetic retinopathy.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We use deep convolutional neural networks for exudate detection. In order to incorporate high level anatomical knowledge about potential exudate locations, output of the convolutional neural network is combined with the output of the optic disc detection and vessel detection procedures.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In the validation step using a manually segmented image database we obtain a maximum F<sub>1</sub> measure of 0.78.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>As manually segmenting and counting exudate areas is a tedious task, having a reliable automated output, such as automated segmentation using convolutional neural networks in combination with other landmark detectors, is an important step in creating automated screening programs for early detection of diabetic retinopathy.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ireland Ltd. All rights reserved.</p></div></div>",pavle.prentasic@fer.hr,Convolutional neural networks; Diabetic retinopathy; Exudates; Fundus photographs; Machine learning,https://www.ncbi.nlm.nih.gov//pubmed/28110732,pubmed,2016,15de50b5-be30-42fc-b6e0-ab0f0770d7d0,1
sensor-based gait parameter extraction with deep convolutional neural networks,/pubmed/28103196,"Hannink J, Kautz T, Pasluosta CF, Gasmann KG, Klucken J, Eskofier BM.",IEEE J Biomed Health Inform. 2017 Jan;21(1):85-93. doi: 10.1109/JBHI.2016.2636456. Epub 2016 Dec 8.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28103196,pubmed,28103196,create date:2017/01/20 | first author:Hannink J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Measurement of stride-related, biomechanical parameters is the common rationale for objective gait impairment scoring. State-of-the-art double-integration approaches to extract these parameters from inertial sensor data are, however, limited in their clinical applicability due to the underlying assumptions. To overcome this, we present a method to translate the abstract information provided by wearable sensors to context-related expert features based on deep convolutional neural networks. Regarding mobile gait analysis, this enables integration-free and data-driven extraction of a set of eight spatio-temporal stride parameters. To this end, two modeling approaches are compared: a combined network estimating all parameters of interest and an ensemble approach that spawns less complex networks for each parameter individually. The ensemble approach is outperforming the combined modeling in the current application. On a clinically relevant and publicly available benchmark dataset, we estimate stride length, width and medio-lateral change in foot angle up to -0.15 ± 6.09 cm, -0.09 ± 4.22 cm and 0.13 ± 3.78° respectively. Stride, swing and stance time as well as heel and toe contact times are estimated up to ±0.07, ±0.05, ±0.07, ±0.07 and ±0.12 s respectively. This is comparable to and in parts outperforming or defining state of the art. Our results further indicate that the proposed change in the methodology could substitute assumption-driven double-integration methods and enable mobile assessment of spatio-temporal stride parameters in clinically critical situations as, e.g., in the case of spastic gait impairments.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28103196,pubmed,2017,a31777df-0909-47f3-b9ad-e9b36d0da386,1
learning to diagnose cirrhosis with liver capsule guided ultrasound image classification,/pubmed/28098774,"Liu X, Song JL, Wang SH, Zhao JW, Chen YQ.",Sensors (Basel). 2017 Jan 13;17(1). pii: E149. doi: 10.3390/s17010149.,Sensors (Basel).  2017,PubMed,citation,PMID:28098774 | PMCID:PMC5298722,pubmed,28098774,create date:2017/01/19 | first author:Liu X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper proposes a computer-aided cirrhosis diagnosis system to diagnose cirrhosis based on ultrasound images. We first propose a method to extract a liver capsule on an ultrasound image, then, based on the extracted liver capsule, we fine-tune a deep convolutional neural network (CNN) model to extract features from the image patches cropped around the liver capsules. Finally, a trained support vector machine (SVM) classifier is applied to classify the sample into normal or abnormal cases. Experimental results show that the proposed method can effectively extract the liver capsules and accurately classify the ultrasound images.</abstracttext></p></div></div>",xiangliu09@fudan.edu.cn,cirrhosis; computer-aided diagnosis; convolutional neural network; ultrasound imaging,https://www.ncbi.nlm.nih.gov//pubmed/28098774,pubmed,2017,27e3518a-0d6b-435c-91d2-eb9793d63185,1
discriminating solitary cysts from soft tissue lesions in mammography using a pretrained deep convolutional neural network,/pubmed/28094850,"Kooi T, van Ginneken B, Karssemeijer N, den Heeten A.",Med Phys. 2017 Mar;44(3):1017-1027. doi: 10.1002/mp.12110.,Med Phys.  2017,PubMed,citation,PMID:28094850,pubmed,28094850,create date:2017/01/18 | first author:Kooi T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>It is estimated that 7% of women in the western world will develop palpable breast cysts in their lifetime. Even though cysts have been correlated with risk of developing breast cancer, many of them are benign and do not require follow-up. We develop a method to discriminate benign solitary cysts from malignant masses in digital mammography. We think a system like this can have merit in the clinic as a decision aid or complementary to specialized modalities.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We employ a deep convolutional neural network (CNN) to classify cyst and mass patches. Deep CNNs have been shown to be powerful classifiers, but need a large amount of training data for which medical problems are often difficult to come by. The key contribution of this paper is that we show good performance can be obtained on a small dataset by pretraining the network on a large dataset of a related task. We subsequently investigate the following: (a) when a mammographic exam is performed, two different views of the same breast are recorded. We investigate the merit of combining the output of the classifier from these two views. (b) We evaluate the importance of the resolution of the patches fed to the network. (c) A method dubbed tissue augmentation is subsequently employed, where we extract normal tissue from normal patches and superimpose this onto the actual samples aiming for a classifier invariant to occluding tissue. (d) We combine the representation extracted using the deep CNN with our previously developed features.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We show that using the proposed deep learning method, an area under the ROC curve (AUC) value of 0.80 can be obtained on a set of benign solitary cysts and malignant mass findings recalled in screening. We find that it works significantly better than our previously developed approach by comparing the AUC of the ROC using bootstrapping. By combining views, the results can be further improved, though this difference was not found to be significant. We find no significant difference between using a resolution of 100 versus 200 micron. The proposed tissue augmentations give a small improvement in performance, but this improvement was also not found to be significant. The final system obtained an AUC of 0.80 with 95% confidence interval [0.78, 0.83], calculated using bootstrapping. The system works best for lesions larger than 27 mm where it obtains an AUC value of 0.87.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>We have presented a computer-aided diagnosis (CADx) method to discriminate cysts from solid lesion in mammography using features from a deep CNN trained on a large set of mass candidates, obtaining an AUC of 0.80 on a set of diagnostic exams recalled from screening. We believe the system shows great potential and comes close to the performance of recently developed spectral mammography. We think the system can be further improved when more data and computational power becomes available.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,breast cancer; computer aided diagnosis; deep learning; solitary cysts; transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/28094850,pubmed,2017,0f9f30f5-727b-4239-892a-7e17273f81d4,1
pancreas segmentation in mri using graph-based decision fusion on convolutional neural networks,/pubmed/28083570,"Cai J, Lu L, Zhang Z, Xing F, Yang L, Yin Q.",Med Image Comput Comput Assist Interv. 2016 Oct;9901:442-450. doi: 10.1007/978-3-319-46723-8_51. Epub 2016 Oct 2.,Med Image Comput Comput Assist Interv.  2016,PubMed,citation,PMID:28083570 | PMCID:PMC5223591,pubmed,28083570,create date:2017/01/14 | first author:Cai J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated pancreas segmentation in medical images is a prerequisite for many clinical applications, such as diabetes inspection, pancreatic cancer diagnosis, and surgical planing. In this paper, we formulate pancreas segmentation in magnetic resonance imaging (MRI) scans as a graph based decision fusion process combined with deep convolutional neural networks (CNN). Our approach conducts pancreatic detection and boundary segmentation with two types of CNN models respectively: 1) the tissue detection step to differentiate pancreas and non-pancreas tissue with spatial intensity context; 2) the boundary detection step to allocate the semantic boundaries of pancreas. Both detection results of the two networks are fused together as the initialization of a conditional random field (CRF) framework to obtain the final segmentation output. Our approach achieves the mean dice similarity coefficient (DSC) 76.1% with the standard deviation of 8.7% in a dataset containing 78 abdominal MRI scans. The proposed algorithm achieves the best results compared with other state of the arts.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28083570,pubmed,2016,f8c4679b-3fc7-4868-a45c-b09eef7b63ae,1
latent feature representation with depth directional long-term recurrent learning for breast masses in digital breast tomosynthesis,/pubmed/28081006,"Kim DH, Kim ST, Chang JM, Ro YM.",Phys Med Biol. 2017 Feb 7;62(3):1009-1031. doi: 10.1088/1361-6560/aa504e. Epub 2017 Jan 12.,Phys Med Biol.  2017,PubMed,citation,PMID:28081006,pubmed,28081006,create date:2017/01/13 | first author:Kim DH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Characterization of masses in computer-aided detection systems for digital breast tomosynthesis (DBT) is an important step to reduce false positive (FP) rates. To effectively differentiate masses from FPs in DBT, discriminative mass feature representation is required. In this paper, we propose a new latent feature representation boosted by depth directional long-term recurrent learning for characterizing malignant masses. The proposed network is designed to encode mass characteristics in two parts. First, 2D spatial image characteristics of DBT slices are encoded as a slice feature representation by convolutional neural network (CNN). Then, depth directional characteristics of masses among the slice feature representations are encoded by the proposed depth directional long-term recurrent learning. In addition, to further improve the class discriminability of latent feature representation, we have devised three objective functions aiming to (a) minimize classification error, (b) minimize intra-class variation within the same class, and (c) preserve feature representation consistency in a central slice. Experimental results have demonstrated that the proposed latent feature representation achieves a higher level of classification performance in terms of receiver operating characteristic (ROC) curves and the area under the ROC curve values compared to performance with feature representation learned by conventional CNN and hand-crafted features.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28081006,pubmed,2017,94275f34-5706-433b-bf51-de5430b7adce,1
pulmonary nodule classification with deep convolutional neural networks on computed tomography images,/pubmed/28070212,"Li W, Cao P, Zhao D, Wang J.",Comput Math Methods Med. 2016;2016:6215085. doi: 10.1155/2016/6215085. Epub 2016 Dec 14.,Comput Math Methods Med.  2016,PubMed,citation,PMID:28070212 | PMCID:PMC5192289,pubmed,28070212,create date:2017/01/11 | first author:Li W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer aided detection (CAD) systems can assist radiologists by offering a second opinion on early diagnosis of lung cancer. Classification and feature representation play critical roles in false-positive reduction (FPR) in lung nodule CAD. We design a deep convolutional neural networks method for nodule classification, which has an advantage of autolearning representation and strong generalization ability. A specified network structure for nodule images is proposed to solve the recognition of three types of nodules, that is, solid, semisolid, and ground glass opacity (GGO). Deep convolutional neural networks are trained by 62,492 regions-of-interest (ROIs) samples including 40,772 nodules and 21,720 nonnodules from the Lung Image Database Consortium (LIDC) database. Experimental results demonstrate the effectiveness of the proposed method in terms of sensitivity and overall accuracy and that it consistently outperforms the competing methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28070212,pubmed,2016,679b0a2c-5d78-4a93-9e25-d6b575108534,1
deep feature transfer learning in combination with traditional features predicts survival among patients with lung adenocarcinoma,/pubmed/28066809,"Paul R, Hawkins SH, Balagurunathan Y, Schabath MB, Gillies RJ, Hall LO, Goldgof DB.",Tomography. 2016 Dec;2(4):388-395. doi: 10.18383/j.tom.2016.00211.,Tomography.  2016,PubMed,citation,PMID:28066809 | PMCID:PMC5218828,pubmed,28066809,create date:2017/01/10 | first author:Paul R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Lung cancer is the most common cause of cancer-related deaths in the USA. It can be detected and diagnosed using computed tomography images. For an automated classifier, identifying predictive features from medical images is a key concern. Deep feature extraction using pretrained convolutional neural networks (CNNs) has recently been successfully applied in some image domains. Here, we applied a pretrained CNN to extract deep features from 40 computed tomography images, with contrast, of non-small cell adenocarcinoma lung cancer, and combined deep features with traditional image features and trained classifiers to predict short- and long-term survivors. We experimented with several pretrained CNNs and several feature selection strategies. The best previously reported accuracy when using traditional quantitative features was 77.5% (area under the curve [AUC], 0.712), which was achieved by a decision tree classifier. The best reported accuracy from transfer learning and deep features was 77.5% (AUC, 0.713) using a decision tree classifier. When extracted deep neural network features were combined with traditional quantitative features, we obtained an accuracy of 90% (AUC, 0.935) with the 5 best post-rectified linear unit features extracted from a vgg-f pretrained CNN and the 5 best traditional features. The best results were achieved with the symmetric uncertainty feature ranking algorithm followed by a random forests classifier.</abstracttext></p></div></div>",,adenocarcinoma; computed tomography; deep features; deep neural network; lung cancer; pre-trained CNN; symmetric uncertainty; transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/28066809,pubmed,2016,6ddf3ec1-3d23-47f7-a199-a078434b696a,1
automated segmentation of hyperintense regions in flair mri using deep learning,/pubmed/28066806,"Korfiatis P, Kline TL, Erickson BJ.",Tomography. 2016 Dec;2(4):334-340. doi: 10.18383/j.tom.2016.00166.,Tomography.  2016,PubMed,citation,PMID:28066806 | PMCID:PMC5215737,pubmed,28066806,create date:2017/01/10 | first author:Korfiatis P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We present a deep convolutional neural network application based on autoencoders aimed at segmentation of increased signal regions in fluid-attenuated inversion recovery magnetic resonance imaging images. The convolutional autoencoders were trained on the publicly available Brain Tumor Image Segmentation Benchmark (BRATS) data set, and the accuracy was evaluated on a data set where 3 expert segmentations were available. The simultaneous truth and performance level estimation (STAPLE) algorithm was used to provide the ground truth for comparison, and Dice coefficient, Jaccard coefficient, true positive fraction, and false negative fraction were calculated. The proposed technique was within the interobserver variability with respect to Dice, Jaccard, and true positive fraction. The developed method can be used to produce automatic segmentations of tumor regions corresponding to signal-increased fluid-attenuated inversion recovery regions.</abstracttext></p></div></div>",,FLAIR; autoencoders; convolution; segmentation,https://www.ncbi.nlm.nih.gov//pubmed/28066806,pubmed,2016,205244c6-eb6b-454b-a9c4-ae1280fd1330,1
cytopathological image analysis using deep-learning networks in microfluidic microscopy,/pubmed/28059233,"Gopakumar G, Hari Babu K, Mishra D, Gorthi SS, Sai Subrahmanyam GR.",J Opt Soc Am A Opt Image Sci Vis. 2017 Jan 1;34(1):111-121. doi: 10.1364/JOSAA.34.000111.,J Opt Soc Am A Opt Image Sci Vis.  2017,PubMed,citation,PMID:28059233,pubmed,28059233,create date:2017/01/07 | first author:Gopakumar G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cytopathologic testing is one of the most critical steps in the diagnosis of diseases, including cancer. However, the task is laborious and demands skill. Associated high cost and low throughput drew considerable interest in automating the testing process. Several neural network architectures were designed to provide human expertise to machines. In this paper, we explore and propose the feasibility of using deep-learning networks for cytopathologic analysis by performing the classification of three important unlabeled, unstained leukemia cell lines (K562, MOLT, and HL60). The cell images used in the classification are captured using a low-cost, high-throughput cell imaging technique: microfluidics-based imaging flow cytometry. We demonstrate that without any conventional fine segmentation followed by explicit feature extraction, the proposed deep-learning algorithms effectively classify the coarsely localized cell lines. We show that the designed deep belief network as well as the deeply pretrained convolutional neural network outperform the conventionally used decision systems and are important in the medical domain, where the availability of labeled data is limited for training. We hope that our work enables the development of a clinically significant high-throughput microfluidic microscopy-based tool for disease screening/triaging, especially in resource-limited settings.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28059233,pubmed,2017,e234692d-38f4-4678-af84-08ec65abdde5,1
accurate de novo prediction of protein contact map by ultra-deep learning model,/pubmed/28056090,"Wang S, Sun S, Li Z, Zhang R, Xu J.",PLoS Comput Biol. 2017 Jan 5;13(1):e1005324. doi: 10.1371/journal.pcbi.1005324. eCollection 2017 Jan.,PLoS Comput Biol.  2017,PubMed,citation,PMID:28056090 | PMCID:PMC5249242,pubmed,28056090,create date:2017/01/06 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Protein contacts contain key information for the understanding of protein structure and function and thus, contact prediction from sequence is an important problem. Recently exciting progress has been made on this problem, but the predicted contacts for proteins without many sequence homologs is still of low quality and not very useful for de novo structure prediction.</abstracttext></p><h4>METHOD: </h4><p><abstracttext label='METHOD' nlmcategory='METHODS'>This paper presents a new deep learning method that predicts contacts by integrating both evolutionary coupling (EC) and sequence conservation information through an ultra-deep neural network formed by two deep residual neural networks. The first residual network conducts a series of 1-dimensional convolutional transformation of sequential features; the second residual network conducts a series of 2-dimensional convolutional transformation of pairwise information including output of the first residual network, EC information and pairwise potential. By using very deep residual networks, we can accurately model contact occurrence patterns and complex sequence-structure relationship and thus, obtain higher-quality contact prediction regardless of how many sequence homologs are available for proteins in question.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our method greatly outperforms existing methods and leads to much more accurate contact-assisted folding. Tested on 105 CASP11 targets, 76 past CAMEO hard targets, and 398 membrane proteins, the average top L long-range prediction accuracy obtained by our method, one representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21 and 0.30, respectively; the average top L/10 long-range accuracy of our method, CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding using our predicted contacts as restraints but without any force fields can yield correct folds (i.e., TMscore&gt;0.6) for 203 of the 579 test proteins, while that using MetaPSICOV- and CCMpred-predicted contacts can do so for only 79 and 62 of them, respectively. Our contact-assisted models also have much better quality than template-based models especially for membrane proteins. The 3D models built from our contact prediction have TMscore&gt;0.5 for 208 of the 398 membrane proteins, while those from homology modeling have TMscore&gt;0.5 for only 10 of them. Further, even if trained mostly by soluble proteins, our deep learning method works very well on membrane proteins. In the recent blind CAMEO benchmark, our fully-automated web server implementing this method successfully folded 6 targets with a new fold and only 0.3L-2.3L effective sequence homologs, including one β protein of 182 residues, one α+β protein of 125 residues, one α protein of 140 residues, one α protein of 217 residues, one α/β of 260 residues and one α protein of 462 residues. Our method also achieved the highest F1 score on free-modeling targets in the latest CASP (Critical Assessment of Structure Prediction), although it was not fully implemented back then.</abstracttext></p><h4>AVAILABILITY: </h4><p><abstracttext label='AVAILABILITY' nlmcategory='BACKGROUND'>http://raptorx.uchicago.edu/ContactMap/.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28056090,pubmed,2017,b9d79512-2e85-4ebd-922f-3bef76c46ac4,1
mirtdl: a deep learning approach for mirna target prediction,/pubmed/28055894,"Shuang Cheng, Maozu Guo, Chunyu Wang, Xiaoyan Liu, Yang Liu, Xuejian Wu.",IEEE/ACM Trans Comput Biol Bioinform. 2016 Nov;13(6):1161-1169. doi: 10.1109/TCBB.2015.2510002. Epub 2015 Dec 22.,IEEE/ACM Trans Comput Biol Bioinform.  2016,PubMed,citation,PMID:28055894,pubmed,28055894,create date:2017/01/06 | first author:Shuang Cheng,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28055894,pubmed,2016,7ef814c0-1066-429b-9b03-7717bee1a3bf,1
multichannel convolutional neural network for biological relation extraction,/pubmed/28053977,"Quan C, Hua L, Sun X, Bai W.",Biomed Res Int. 2016;2016:1850404. doi: 10.1155/2016/1850404. Epub 2016 Dec 7.,Biomed Res Int.  2016,PubMed,citation,PMID:28053977 | PMCID:PMC5174749,pubmed,28053977,create date:2017/01/06 | first author:Quan C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The plethora of biomedical relations which are embedded in medical logs (records) demands researchers' attention. Previous theoretical and practical focuses were restricted on traditional machine learning techniques. However, these methods are susceptible to the issues of <i>'vocabulary gap'</i> and data sparseness and the unattainable automation process in feature extraction. To address aforementioned issues, in this work, we propose a multichannel convolutional neural network (MCCNN) for automated biomedical relation extraction. The proposed model has the following two contributions: (1) it enables the fusion of multiple (e.g., five) versions in word embeddings; (2) the need for manual feature engineering can be obviated by automated feature learning with convolutional neural network (CNN). We evaluated our model on two biomedical relation extraction tasks: drug-drug interaction (DDI) extraction and protein-protein interaction (PPI) extraction. For DDI task, our system achieved an overall <i>f</i>-score of 70.2% compared to the standard linear SVM based system (e.g., 67.0%) on DDIExtraction 2013 challenge dataset. And for PPI task, we evaluated our system on Aimed and BioInfer PPI corpus; our system exceeded the state-of-art ensemble SVM system by 2.7% and 5.6% on <i>f</i>-scores.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28053977,pubmed,2016,785da0cd-1e4a-4f0a-ad25-cfe81105b7fb,1
automated melanoma recognition in dermoscopy images via very deep residual networks,/pubmed/28026754,"Yu L, Chen H, Dou Q, Qin J, Heng PA.",IEEE Trans Med Imaging. 2017 Apr;36(4):994-1004. doi: 10.1109/TMI.2016.2642839. Epub 2016 Dec 21.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28026754,pubmed,28026754,create date:2016/12/28 | first author:Yu L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, respectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28026754,pubmed,2017,32e73a89-63e2-417d-9e25-1a18552080d8,1
boosting docking-based virtual screening with deep learning,/pubmed/28024405,"Pereira JC, Caffarena ER, Dos Santos CN.",J Chem Inf Model. 2016 Dec 27;56(12):2495-2506. doi: 10.1021/acs.jcim.6b00355. Epub 2016 Nov 29.,J Chem Inf Model.  2016,PubMed,citation,PMID:28024405,pubmed,28024405,create date:2016/12/28 | first author:Pereira JC,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this work, we propose a deep learning approach to improve docking-based virtual screening. The deep neural network that is introduced, DeepVS, uses the output of a docking program and learns how to extract relevant features from basic data such as atom and residues types obtained from protein-ligand complexes. Our approach introduces the use of atom and amino acid embeddings and implements an effective way of creating distributed vector representations of protein-ligand complexes by modeling the compound as a set of atom contexts that is further processed by a convolutional layer. One of the main advantages of the proposed method is that it does not require feature engineering. We evaluate DeepVS on the Directory of Useful Decoys (DUD), using the output of two docking programs: Autodock Vina1.1.2 and Dock 6.6. Using a strict evaluation with leave-one-out cross-validation, DeepVS outperforms the docking programs, with regard to both AUC ROC and enrichment factor. Moreover, using the output of Autodock Vina1.1.2, DeepVS achieves an AUC ROC of 0.81, which, to the best of our knowledge, is the best AUC reported so far for virtual screening using the 40 receptors from the DUD.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28024405,pubmed,2016,83badea7-db5c-486a-8379-bf4c618f0ba2,1
intelligent and automatic in vivo detection and quantification of transplanted cells in mri,/pubmed/28019017,"Afridi MJ, Ross A, Liu X, Bennewitz MF, Shuboni DD, Shapiro EM.",Magn Reson Med. 2016 Dec 26. doi: 10.1002/mrm.26571. [Epub ahead of print],Magn Reson Med.  2016,PubMed,citation,PMID:28019017,pubmed,28019017,create date:2016/12/27 | first author:Afridi MJ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Magnetic resonance imaging (MRI)-based cell tracking has emerged as a useful tool for identifying the location of transplanted cells, and even their migration. Magnetically labeled cells appear as dark contrast in T2*-weighted MRI, with sensitivities of individual cells. One key hurdle to the widespread use of MRI-based cell tracking is the inability to determine the number of transplanted cells based on this contrast feature. In the case of single cell detection, manual enumeration of spots in three-dimensional (3D) MRI in principle is possible; however, it is a tedious and time-consuming task that is prone to subjectivity and inaccuracy on a large scale. This research presents the first comprehensive study on how a computer-based intelligent, automatic, and accurate cell quantification approach can be designed for spot detection in MRI scans.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Magnetically labeled mesenchymal stem cells (MSCs) were transplanted into rats using an intracardiac injection, accomplishing single cell seeding in the brain. T2*-weighted MRI of these rat brains were performed where labeled MSCs appeared as spots. Using machine learning and computer vision paradigms, approaches were designed to systematically explore the possibility of automatic detection of these spots in MRI. Experiments were validated against known in vitro scenarios.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Using the proposed deep convolutional neural network (CNN) architecture, an in vivo accuracy up to 97.3% and in vitro accuracy of up to 99.8% was achieved for automated spot detection in MRI data.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The proposed approach for automatic quantification of MRI-based cell tracking will facilitate the use of MRI in large-scale cell therapy studies. Magn Reson Med, 2016. © 2016 International Society for Magnetic Resonance in Medicine.</abstracttext></p><p class='copyright'>© 2016 International Society for Magnetic Resonance in Medicine.</p></div></div>",,MRI; Machine learning; cell therapy; iron oxide,https://www.ncbi.nlm.nih.gov//pubmed/28019017,pubmed,2016,4bcccda4-3499-4ca8-b0c8-d8a965c86142,1
remembered or forgotten?-an eeg-based computational prediction approach,/pubmed/27973531,"Sun X, Qian C, Chen Z, Wu Z, Luo B, Pan G.",PLoS One. 2016 Dec 14;11(12):e0167497. doi: 10.1371/journal.pone.0167497. eCollection 2016.,PLoS One.  2016,PubMed,citation,PMID:27973531 | PMCID:PMC5156350,pubmed,27973531,create date:2016/12/16 | first author:Sun X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Prediction of memory performance (remembered or forgotten) has various potential applications not only for knowledge learning but also for disease diagnosis. Recently, subsequent memory effects (SMEs)-the statistical differences in electroencephalography (EEG) signals before or during learning between subsequently remembered and forgotten events-have been found. This finding indicates that EEG signals convey the information relevant to memory performance. In this paper, based on SMEs we propose a computational approach to predict memory performance of an event from EEG signals. We devise a convolutional neural network for EEG, called ConvEEGNN, to predict subsequently remembered and forgotten events from EEG recorded during memory process. With the ConvEEGNN, prediction of memory performance can be achieved by integrating two main stages: feature extraction and classification. To verify the proposed approach, we employ an auditory memory task to collect EEG signals from scalp electrodes. For ConvEEGNN, the average prediction accuracy was 72.07% by using EEG data from pre-stimulus and during-stimulus periods, outperforming other approaches. It was observed that signals from pre-stimulus period and those from during-stimulus period had comparable contributions to memory performance. Furthermore, the connection weights of ConvEEGNN network can reveal prominent channels, which are consistent with the distribution of SME studied previously.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27973531,pubmed,2016,10b67bd3-b06e-4d1c-b546-34e41dd32da1,1
anatomical location differences between mutated and wild-type isocitrate dehydrogenase 1 in low-grade gliomas,/pubmed/27929688,"Yu J, Shi Z, Ji C, Lian Y, Wang Y, Chen L, Mao Y.",Int J Neurosci. 2017 Oct;127(10):873-880. doi: 10.1080/00207454.2016.1270278. Epub 2017 Jan 6.,Int J Neurosci.  2017,PubMed,citation,PMID:27929688,pubmed,27929688,create date:2016/12/09 | first author:Yu J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Anatomical location of gliomas has been considered as a factor implicating the contributions of a specific precursor cells during the tumor growth. Isocitrate dehydrogenase 1 (IDH1) is a pathognomonic biomarker with a significant impact on the development of gliomas and remarkable prognostic effect. The correlation between anatomical location of tumor and IDH1 states for low-grade gliomas was analyzed quantitatively in this study. Ninety-two patients diagnosed of low-grade glioma pathologically were recruited in this study, including 65 patients with IDH1-mutated glioma and 27 patients with wide-type IDH1. A convolutional neural network was designed to segment the tumor from three-dimensional magnetic resonance imaging images. Voxel-based lesion symptom mapping was then employed to study the tumor location distribution differences between gliomas with mutated and wild-type IDH1. In order to characterize the location differences quantitatively, the Automated Anatomical Labeling Atlas was used to partition the standard brain atlas into 116 anatomical volumes of interests (AVOIs). The percentages of tumors with different IDH1 states in 116 AVOIs were calculated and compared. Support vector machine and AdaBoost algorithms were used to estimate the IDH1 status based on the 116 location features of each patient. Experimental results proved that the quantitative tumor location measurement could be a very important group of imaging features in biomarker estimation based on radiomics analysis of glioma.</abstracttext></p></div></div>",,IDH1; Low-grade glioma; MRI; anatomic location; image analysis,https://www.ncbi.nlm.nih.gov//pubmed/27929688,pubmed,2017,0c0c38bb-aaca-4345-82f6-957bbbe432fa,1
training and validating a deep convolutional neural network for computer-aided detection and classification of abnormalities on frontal chest radiographs,/pubmed/27922974,"Cicero M, Bilbily A, Colak E, Dowdell T, Gray B, Perampaladas K, Barfett J.",Invest Radiol. 2017 May;52(5):281-287. doi: 10.1097/RLI.0000000000000341.,Invest Radiol.  2017,PubMed,citation,PMID:27922974,pubmed,27922974,create date:2016/12/07 | first author:Cicero M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVES: </h4><p><abstracttext label='OBJECTIVES' nlmcategory='OBJECTIVE'>Convolutional neural networks (CNNs) are a subtype of artificial neural network that have shown strong performance in computer vision tasks including image classification. To date, there has been limited application of CNNs to chest radiographs, the most frequently performed medical imaging study. We hypothesize CNNs can learn to classify frontal chest radiographs according to common findings from a sufficiently large data set.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>Our institution's research ethics board approved a single-center retrospective review of 35,038 adult posterior-anterior chest radiographs and final reports performed between 2005 and 2015 (56% men, average age of 56, patient type: 24% inpatient, 39% outpatient, 37% emergency department) with a waiver for informed consent. The GoogLeNet CNN was trained using 3 graphics processing units to automatically classify radiographs as normal (n = 11,702) or into 1 or more of cardiomegaly (n = 9240), consolidation (n = 6788), pleural effusion (n = 7786), pulmonary edema (n = 1286), or pneumothorax (n = 1299). The network's performance was evaluated using receiver operating curve analysis on a test set of 2443 radiographs with the criterion standard being board-certified radiologist interpretation.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Using 256 × 256-pixel images as input, the network achieved an overall sensitivity and specificity of 91% with an area under the curve of 0.964 for classifying a study as normal (n = 1203). For the abnormal categories, the sensitivity, specificity, and area under the curve, respectively, were 91%, 91%, and 0.962 for pleural effusion (n = 782), 82%, 82%, and 0.868 for pulmonary edema (n = 356), 74%, 75%, and 0.850 for consolidation (n = 214), 81%, 80%, and 0.875 for cardiomegaly (n = 482), and 78%, 78%, and 0.861 for pneumothorax (n = 167).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Current deep CNN architectures can be trained with modest-sized medical data sets to achieve clinically useful performance at detecting and excluding common pathology on chest radiographs.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27922974,pubmed,2017,e2b8bd91-8dca-4529-8866-c43030022374,1
mass detection in digital breast tomosynthesis: deep convolutional neural network with transfer learning from mammography,/pubmed/27908154,"Samala RK, Chan HP, Hadjiiski L, Helvie MA, Wei J, Cha K.",Med Phys. 2016 Dec;43(12):6654.,Med Phys.  2016,PubMed,citation,PMID:27908154 | PMCID:PMC5135717,pubmed,27908154,create date:2016/12/03 | first author:Samala RK,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Develop a computer-aided detection (CAD) system for masses in digital breast tomosynthesis (DBT) volume using a deep convolutional neural network (DCNN) with transfer learning from mammograms.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A data set containing 2282 digitized film and digital mammograms and 324 DBT volumes were collected with IRB approval. The mass of interest on the images was marked by an experienced breast radiologist as reference standard. The data set was partitioned into a training set (2282 mammograms with 2461 masses and 230 DBT views with 228 masses) and an independent test set (94 DBT views with 89 masses). For DCNN training, the region of interest (ROI) containing the mass (true positive) was extracted from each image. False positive (FP) ROIs were identified at prescreening by their previously developed CAD systems. After data augmentation, a total of 45 072 mammographic ROIs and 37 450 DBT ROIs were obtained. Data normalization and reduction of non-uniformity in the ROIs across heterogeneous data was achieved using a background correction method applied to each ROI. A DCNN with four convolutional layers and three fully connected (FC) layers was first trained on the mammography data. Jittering and dropout techniques were used to reduce overfitting. After training with the mammographic ROIs, all weights in the first three convolutional layers were frozen, and only the last convolution layer and the FC layers were randomly initialized again and trained using the DBT training ROIs. The authors compared the performances of two CAD systems for mass detection in DBT: one used the DCNN-based approach and the other used their previously developed feature-based approach for FP reduction. The prescreening stage was identical in both systems, passing the same set of mass candidates to the FP reduction stage. For the feature-based CAD system, 3D clustering and active contour method was used for segmentation; morphological, gray level, and texture features were extracted and merged with a linear discriminant classifier to score the detected masses. For the DCNN-based CAD system, ROIs from five consecutive slices centered at each candidate were passed through the trained DCNN and a mass likelihood score was generated. The performances of the CAD systems were evaluated using free-response ROC curves and the performance difference was analyzed using a non-parametric method.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Before transfer learning, the DCNN trained only on mammograms with an AUC of 0.99 classified DBT masses with an AUC of 0.81 in the DBT training set. After transfer learning with DBT, the AUC improved to 0.90. For breast-based CAD detection in the test set, the sensitivity for the feature-based and the DCNN-based CAD systems was 83% and 91%, respectively, at 1 FP/DBT volume. The difference between the performances for the two systems was statistically significant (p-value &lt; 0.05).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The image patterns learned from the mammograms were transferred to the mass detection on DBT slices through the DCNN. This study demonstrated that large data sets collected from mammography are useful for developing new CAD systems for DBT, alleviating the problem and effort of collecting entirely new large data sets for the new modality.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27908154,pubmed,2016,66011fb4-b2af-48b2-a7c0-9e36323a1c3c,1
a novel deep learning approach for classification of eeg motor imagery signals,/pubmed/27900952,"Tabar YR, Halici U.",J Neural Eng. 2017 Feb;14(1):016003. doi: 10.1088/1741-2560/14/1/016003. Epub 2016 Nov 30.,J Neural Eng.  2017,PubMed,citation,PMID:27900952,pubmed,27900952,create date:2016/12/03 | first author:Tabar YR,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Signal classification is an important issue in brain computer interface (BCI) systems. Deep learning approaches have been used successfully in many recent studies to learn features and classify different types of data. However, the number of studies that employ these approaches on BCI applications is very limited. In this study we aim to use deep learning methods to improve classification performance of EEG motor imagery signals.</abstracttext></p><h4>APPROACH: </h4><p><abstracttext label='APPROACH' nlmcategory='METHODS'>In this study we investigate convolutional neural networks (CNN) and stacked autoencoders (SAE) to classify EEG Motor Imagery signals. A new form of input is introduced to combine time, frequency and location information extracted from EEG signal and it is used in CNN having one 1D convolutional and one max-pooling layers. We also proposed a new deep network by combining CNN and SAE. In this network, the features that are extracted in CNN are classified through the deep network SAE.</abstracttext></p><h4>MAIN RESULTS: </h4><p><abstracttext label='MAIN RESULTS' nlmcategory='RESULTS'>The classification performance obtained by the proposed method on BCI competition IV dataset 2b in terms of kappa value is 0.547. Our approach yields 9% improvement over the winner algorithm of the competition.</abstracttext></p><h4>SIGNIFICANCE: </h4><p><abstracttext label='SIGNIFICANCE' nlmcategory='CONCLUSIONS'>Our results show that deep learning methods provide better classification performance compared to other state of art approaches. These methods can be applied successfully to BCI systems where the amount of data is large due to daily recording.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27900952,pubmed,2017,dc3d0dbc-7b7f-4759-9c84-244f133a545d,1
development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs,/pubmed/27898976,"Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, Venugopalan S, Widner K, Madams T, Cuadros J, Kim R, Raman R, Nelson PC, Mega JL, Webster DR.",JAMA. 2016 Dec 13;316(22):2402-2410. doi: 10.1001/jama.2016.17216.,JAMA.  2016,PubMed,citation,PMID:27898976,pubmed,27898976,create date:2016/11/30 | first author:Gulshan V,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Importance: </h4><p><abstracttext label='Importance'>Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.</abstracttext></p><h4>Objective: </h4><p><abstracttext label='Objective'>To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.</abstracttext></p><h4>Design and Setting: </h4><p><abstracttext label='Design and Setting'>A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.</abstracttext></p><h4>Exposure: </h4><p><abstracttext label='Exposure'>Deep learning-trained algorithm.</abstracttext></p><h4>Main Outcomes and Measures: </h4><p><abstracttext label='Main Outcomes and Measures'>The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results'>The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.</abstracttext></p><h4>Conclusions and Relevance: </h4><p><abstracttext label='Conclusions and Relevance'>In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27898976,pubmed,2016,dff6154b-3f27-421d-ac6a-4d94c8600288,1
deep motif dashboard: visualizing and understanding genomic sequences using deep neural networks,/pubmed/27896980,"Lanchantin J, Singh R, Wang B, Qi Y.",Pac Symp Biocomput. 2016;22:254-265.,Pac Symp Biocomput.  2016,PubMed,citation,PMID:27896980,pubmed,27896980,create date:2016/11/30 | first author:Lanchantin J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep neural network (DNN) models have recently obtained state-of-the-art prediction accuracy for the transcription factor binding (TFBS) site classification task. However, it remains unclear how these approaches identify meaningful DNA sequence signals and give insights as to why TFs bind to certain locations. In this paper, we propose a toolkit called the Deep Motif Dashboard (DeMo Dashboard) which provides a suite of visualization strategies to extract motifs, or sequence patterns from deep neural network models for TFBS classification. We demonstrate how to visualize and understand three important DNN models: convolutional, recurrent, and convolutional-recurrent networks. Our first visualization method is finding a test sequence's saliency map which uses first-order derivatives to describe the importance of each nucleotide in making the final prediction. Second, considering recurrent models make predictions in a temporal manner (from one end of a TFBS sequence to the other), we introduce temporal output scores, indicating the prediction score of a model over time for a sequential input. Lastly, a class-specific visualization strategy finds the optimal input sequence for a given TFBS positive class via stochastic gradient optimization. Our experimental results indicate that a convolutional-recurrent architecture performs the best among the three architectures. The visualization techniques indicate that CNN-RNN makes predictions by modeling both motifs as well as dependencies among them.</abstracttext></p></div></div>",jjl5sw@virginia.edu,,https://www.ncbi.nlm.nih.gov//pubmed/27896980,pubmed,2016,99f6e96a-432b-48ac-a2f8-2f3d7b3eeb76,1
transfer learning with convolutional neural networks for classification of abdominal ultrasound images,/pubmed/27896451,"Cheng PM, Malhi HS.",J Digit Imaging. 2017 Apr;30(2):234-243. doi: 10.1007/s10278-016-9929-2.,J Digit Imaging.  2017,PubMed,citation,PMID:27896451 | PMCID:PMC5359213,pubmed,27896451,create date:2016/11/30 | first author:Cheng PM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The purpose of this study is to evaluate transfer learning with deep convolutional neural networks for the classification of abdominal ultrasound images. Grayscale images from 185 consecutive clinical abdominal ultrasound studies were categorized into 11 categories based on the text annotation specified by the technologist for the image. Cropped images were rescaled to 256 × 256 resolution and randomized, with 4094 images from 136 studies constituting the training set, and 1423 images from 49 studies constituting the test set. The fully connected layers of two convolutional neural networks based on CaffeNet and VGGNet, previously trained on the 2012 Large Scale Visual Recognition Challenge data set, were retrained on the training set. Weights in the convolutional layers of each network were frozen to serve as fixed feature extractors. Accuracy on the test set was evaluated for each network. A radiologist experienced in abdominal ultrasound also independently classified the images in the test set into the same 11 categories. The CaffeNet network classified 77.3% of the test set images accurately (1100/1423 images), with a top-2 accuracy of 90.4% (1287/1423 images). The larger VGGNet network classified 77.9% of the test set accurately (1109/1423 images), with a top-2 accuracy of VGGNet was 89.7% (1276/1423 images). The radiologist classified 71.7% of the test set images correctly (1020/1423 images). The differences in classification accuracies between both neural networks and the radiologist were statistically significant (p &lt; 0.001). The results demonstrate that transfer learning with convolutional neural networks may be used to construct effective classifiers for abdominal ultrasound images.</abstracttext></p></div></div>",phillip.cheng@med.usc.edu,Artificial neural networks; Classification; Deep learning; Digital image processing; Machine learning,https://www.ncbi.nlm.nih.gov//pubmed/27896451,pubmed,2017,0dffbac9-2cdf-45f3-9199-dfdeb51b6b2f,1
a convolutional neural network for automatic characterization of plaque composition in carotid ultrasound,/pubmed/27893402,"Lekadir K, Galimzianova A, Betriu A, Del Mar Vila M, Igual L, Rubin DL, Fernandez E, Radeva P, Napel S.",IEEE J Biomed Health Inform. 2017 Jan;21(1):48-55. doi: 10.1109/JBHI.2016.2631401. Epub 2016 Nov 22.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:27893402 | PMCID:PMC5293622,pubmed,27893402,create date:2016/11/29 | first author:Lekadir K,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27893402,pubmed,2017,16e5db34-c1dc-4410-9006-c52ce95544bb,1
classification of teeth in cone-beam ct using deep convolutional neural network,/pubmed/27889430,"Miki Y, Muramatsu C, Hayashi T, Zhou X, Hara T, Katsumata A, Fujita H.",Comput Biol Med. 2017 Jan 1;80:24-29. doi: 10.1016/j.compbiomed.2016.11.003. Epub 2016 Nov 12.,Comput Biol Med.  2017,PubMed,citation,PMID:27889430,pubmed,27889430,create date:2016/11/28 | first author:Miki Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Dental records play an important role in forensic identification. To this end, postmortem dental findings and teeth conditions are recorded in a dental chart and compared with those of antemortem records. However, most dentists are inexperienced at recording the dental chart for corpses, and it is a physically and mentally laborious task, especially in large scale disasters. Our goal is to automate the dental filing process by using dental x-ray images. In this study, we investigated the application of a deep convolutional neural network (DCNN) for classifying tooth types on dental cone-beam computed tomography (CT) images. Regions of interest (ROIs) including single teeth were extracted from CT slices. Fifty two CT volumes were randomly divided into 42 training and 10 test cases, and the ROIs obtained from the training cases were used for training the DCNN. For examining the sampling effect, random sampling was performed 3 times, and training and testing were repeated. We used the AlexNet network architecture provided in the Caffe framework, which consists of 5 convolution layers, 3 pooling layers, and 2 full connection layers. For reducing the overtraining effect, we augmented the data by image rotation and intensity transformation. The test ROIs were classified into 7 tooth types by the trained network. The average classification accuracy using the augmented training data by image rotation and intensity transformation was 88.8%. Compared with the result without data augmentation, data augmentation resulted in an approximately 5% improvement in classification accuracy. This indicates that the further improvement can be expected by expanding the CT dataset. Unlike the conventional methods, the proposed method is advantageous in obtaining high classification accuracy without the need for precise tooth segmentation. The proposed tooth classification method can be useful in automatic filing of dental charts for forensic identification.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",chisa@fjt.info.gifu-u.ac.jp,Deep convolutional neural networks; Dental chart; Dental cone-beam CT; Forensic identification; Tooth classification,https://www.ncbi.nlm.nih.gov//pubmed/27889430,pubmed,2017,5d20cbe9-c3f0-41a5-9084-f623c01a9258,1
classification of ct brain images based on deep learning networks,/pubmed/27886714,"Gao XW, Hui R, Tian Z.",Comput Methods Programs Biomed. 2017 Jan;138:49-56. doi: 10.1016/j.cmpb.2016.10.007. Epub 2016 Oct 20.,Comput Methods Programs Biomed.  2017,PubMed,citation,PMID:27886714,pubmed,27886714,create date:2016/11/26 | first author:Gao XW,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>While computerised tomography (CT) may have been the first imaging tool to study human brain, it has not yet been implemented into clinical decision making process for diagnosis of Alzheimer's disease (AD). On the other hand, with the nature of being prevalent, inexpensive and non-invasive, CT does present diagnostic features of AD to a great extent. This study explores the significance and impact on the application of the burgeoning deep learning techniques to the task of classification of CT brain images, in particular utilising convolutional neural network (CNN), aiming at providing supplementary information for the early diagnosis of Alzheimer's disease. Towards this end, three categories of CT images (N = 285) are clustered into three groups, which are AD, lesion (e.g. tumour) and normal ageing. In addition, considering the characteristics of this collection with larger thickness along the direction of depth (z) (~3-5 mm), an advanced CNN architecture is established integrating both 2D and 3D CNN networks. The fusion of the two CNN networks is subsequently coordinated based on the average of Softmax scores obtained from both networks consolidating 2D images along spatial axial directions and 3D segmented blocks respectively. As a result, the classification accuracy rates rendered by this elaborated CNN architecture are 85.2%, 80% and 95.3% for classes of AD, lesion and normal respectively with an average of 87.6%. Additionally, this improved CNN network appears to outperform the others when in comparison with 2D version only of CNN network as well as a number of state of the art hand-crafted approaches. As a result, these approaches deliver accuracy rates in percentage of 86.3, 85.6 ± 1.10, 86.3 ± 1.04, 85.2 ± 1.60, 83.1 ± 0.35 for 2D CNN, 2D SIFT, 2D KAZE, 3D SIFT and 3D KAZE respectively. The two major contributions of the paper constitute a new 3-D approach while applying deep learning technique to extract signature information rooted in both 2D slices and 3D blocks of CT images and an elaborated hand-crated approach of 3D KAZE.</abstracttext></p><p class='copyright'>Copyright Â© 2016 Elsevier Ireland Ltd. All rights reserved.</p></div></div>",x.gao@mdx.ac.uk,3D CNN; CT brain images; Classification; Convolutional neural network; Deep learning; KAZE,https://www.ncbi.nlm.nih.gov//pubmed/27886714,pubmed,2017,e5ea6071-0000-4d61-9afc-db6351e9c946,1
automatic abdominal multi-organ segmentation using deep convolutional neural network and time-implicit level sets,/pubmed/27885540,"Hu P, Wu F, Peng J, Bao Y, Chen F, Kong D.",Int J Comput Assist Radiol Surg. 2017 Mar;12(3):399-411. doi: 10.1007/s11548-016-1501-5. Epub 2016 Nov 24.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:27885540,pubmed,27885540,create date:2016/11/26 | first author:Hu P,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Multi-organ segmentation from CT images is an essential step for computer-aided diagnosis and surgery planning. However, manual delineation of the organs by radiologists is tedious, time-consuming and poorly reproducible. Therefore, we propose a fully automatic method for the segmentation of multiple organs from three-dimensional abdominal CT images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The proposed method employs deep fully convolutional neural networks (CNNs) for organ detection and segmentation, which is further refined by a time-implicit multi-phase evolution method. Firstly, a 3D CNN is trained to automatically localize and delineate the organs of interest with a probability prediction map. The learned probability map provides both subject-specific spatial priors and initialization for subsequent fine segmentation. Then, for the refinement of the multi-organ segmentation, image intensity models, probability priors as well as a disjoint region constraint are incorporated into an unified energy functional. Finally, a novel time-implicit multi-phase level-set algorithm is utilized to efficiently optimize the proposed energy functional model.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our method has been evaluated on 140 abdominal CT scans for the segmentation of four organs (liver, spleen and both kidneys). With respect to the ground truth, average Dice overlap ratios for the liver, spleen and both kidneys are 96.0, 94.2 and 95.4%, respectively, and average symmetric surface distance is less than 1.3 mm for all the segmented organs. The computation time for a CT volume is 125 s in average. The achieved accuracy compares well to state-of-the-art methods with much higher efficiency.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>A fully automatic method for multi-organ segmentation from abdominal CT images was developed and evaluated. The results demonstrated its potential in clinical usage with high effectiveness, robustness and efficiency.</abstracttext></p></div></div>",dkong@zju.edu.cn,3D CT; Deep CNN; Multi-organ segmentation; Time-implicit multi-phase level sets,https://www.ncbi.nlm.nih.gov//pubmed/27885540,pubmed,2017,6c2acb0a-fe10-4425-b388-792d6ffc0cce,1
automatic 3d liver segmentation based on deep learning and globally optimized surface evolution,/pubmed/27880735,"Hu P, Wu F, Peng J, Liang P, Kong D.",Phys Med Biol. 2016 Dec 21;61(24):8676-8698. Epub 2016 Nov 23.,Phys Med Biol.  2016,PubMed,citation,PMID:27880735,pubmed,27880735,create date:2016/11/24 | first author:Hu P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The detection and delineation of the liver from abdominal 3D computed tomography (CT) images are fundamental tasks in computer-assisted liver surgery planning. However, automatic and accurate segmentation, especially liver detection, remains challenging due to complex backgrounds, ambiguous boundaries, heterogeneous appearances and highly varied shapes of the liver. To address these difficulties, we propose an automatic segmentation framework based on 3D convolutional neural network (CNN) and globally optimized surface evolution. First, a deep 3D CNN is trained to learn a subject-specific probability map of the liver, which gives the initial surface and acts as a shape prior in the following segmentation step. Then, both global and local appearance information from the prior segmentation are adaptively incorporated into a segmentation model, which is globally optimized in a surface evolution way. The proposed method has been validated on 42 CT images from the public Sliver07 database and local hospitals. On the Sliver07 online testing set, the proposed method can achieve an overall score of [Formula: see text], yielding a mean Dice similarity coefficient of [Formula: see text], and an average symmetric surface distance of [Formula: see text] mm. The quantitative validations and comparisons show that the proposed method is accurate and effective for clinical application.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27880735,pubmed,2016,fd69be33-92aa-4c8d-be78-a2305629d1b2,1
microscopic medical image classification framework via deep learning and shearlet transform,/pubmed/27872871,"Rezaeilouyeh H, Mollahosseini A, Mahoor MH.",J Med Imaging (Bellingham). 2016 Oct;3(4):044501. Epub 2016 Nov 3.,J Med Imaging (Bellingham).  2016,PubMed,citation,PMID:27872871 | PMCID:PMC5093219,pubmed,27872871,create date:2016/11/23 | first author:Rezaeilouyeh H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cancer is the second leading cause of death in US after cardiovascular disease. Image-based computer-aided diagnosis can assist physicians to efficiently diagnose cancers in early stages. Existing computer-aided algorithms use hand-crafted features such as wavelet coefficients, co-occurrence matrix features, and recently, histogram of shearlet coefficients for classification of cancerous tissues and cells in images. These hand-crafted features often lack generalizability since every cancerous tissue and cell has a specific texture, structure, and shape. An alternative approach is to use convolutional neural networks (CNNs) to learn the most appropriate feature abstractions directly from the data and handle the limitations of hand-crafted features. A framework for breast cancer detection and prostate Gleason grading using CNN trained on images along with the magnitude and phase of shearlet coefficients is presented. Particularly, we apply shearlet transform on images and extract the magnitude and phase of shearlet coefficients. Then we feed shearlet features along with the original images to our CNN consisting of multiple layers of convolution, max pooling, and fully connected layers. Our experiments show that using the magnitude and phase of shearlet coefficients as extra information to the network can improve the accuracy of detection and generalize better compared to the state-of-the-art methods that rely on hand-crafted features. This study expands the application of deep neural networks into the field of medical image analysis, which is a difficult domain considering the limited medical data available for such analysis.</abstracttext></p></div></div>",,breast cancer; deep neural network; microscopic images; prostate cancer; shearlet transform,https://www.ncbi.nlm.nih.gov//pubmed/27872871,pubmed,2016,4418f80f-76f1-4964-af9c-52c0e5d76663,1
analyzing brain functions by subject classification of functional near-infrared spectroscopy data using convolutional neural networks analysis,/pubmed/27872636,"Hiwa S, Hanawa K, Tamura R, Hachisuka K, Hiroyasu T.",Comput Intell Neurosci. 2016;2016:1841945. Epub 2016 Oct 31.,Comput Intell Neurosci.  2016,PubMed,citation,PMID:27872636 | PMCID:PMC5107881,pubmed,27872636,create date:2016/11/23 | first author:Hiwa S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Functional near-infrared spectroscopy (fNIRS) is suitable for noninvasive mapping of relative changes in regional cortical activity but is limited for quantitative comparisons among cortical sites, subjects, and populations. We have developed a convolutional neural network (CNN) analysis method that learns feature vectors for accurate identification of group differences in fNIRS responses. In this study, subject gender was classified using CNN analysis of fNIRS data. fNIRS data were acquired from male and female subjects during a visual number memory task performed in a white noise environment because previous studies had revealed that the pattern of cortical blood flow during the task differed between males and females. A learned classifier accurately distinguished males from females based on distinct fNIRS signals from regions of interest (ROI) including the inferior frontal gyrus and premotor areas that were identified by the learning algorithm. These cortical regions are associated with memory storage, attention, and task motor response. The accuracy of the classifier suggests stable gender-based differences in cerebral blood flow during this task. The proposed CNN analysis method can objectively identify ROIs using fNIRS time series data for machine learning to distinguish features between groups.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27872636,pubmed,2016,fcbc2834-0c96-4b51-9b03-8bf61be20917,1
efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation,/pubmed/27865153,"Kamnitsas K, Ledig C, Newcombe VF, Simpson JP, Kane AD, Menon DK, Rueckert D, Glocker B.",Med Image Anal. 2017 Feb;36:61-78. doi: 10.1016/j.media.2016.10.004. Epub 2016 Oct 29.,Med Image Anal.  2017,PubMed,citation,PMID:27865153,pubmed,27865153,create date:2016/11/20 | first author:Kamnitsas K,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.</abstracttext></p><p class='copyright'>Copyright © 2016 The Authors. Published by Elsevier B.V. All rights reserved.</p></div></div>",konstantinos.kamnitsas12@imperial.ac.uk,3D convolutional neural network; Brain lesions; Deep learning; Fully connected CRF; Segmentation,https://www.ncbi.nlm.nih.gov//pubmed/27865153,pubmed,2017,e56ec2d9-1a11-426d-a583-d11647509252,1
convolutional deep belief networks for single-cell/object tracking in computational biology and computer vision,/pubmed/27847827,"Zhong B, Pan S, Zhang H, Wang T, Du J, Chen D, Cao L.",Biomed Res Int. 2016;2016:9406259. Epub 2016 Oct 26.,Biomed Res Int.  2016,PubMed,citation,PMID:27847827 | PMCID:PMC5101405,pubmed,27847827,create date:2016/11/17 | first author:Zhong B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we propose deep architecture to dynamically learn the most discriminative features from data for both single-cell and object tracking in computational biology and computer vision. Firstly, the discriminative features are automatically learned via a convolutional deep belief network (CDBN). Secondly, we design a simple yet effective method to transfer features learned from CDBNs on the source tasks for generic purpose to the object tracking tasks using only limited amount of training data. Finally, to alleviate the tracker drifting problem caused by model updating, we jointly consider three different types of positive samples. Extensive experiments validate the robustness and effectiveness of the proposed method.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27847827,pubmed,2016,6972ae73-2086-44c4-a8ba-0c66711de830,1
exploring deep learning and transfer learning for colonic polyp classification,/pubmed/27847543,"Ribeiro E, Uhl A, Wimmer G, Häfner M.",Comput Math Methods Med. 2016;2016:6584725. Epub 2016 Oct 26.,Comput Math Methods Med.  2016,PubMed,citation,PMID:27847543 | PMCID:PMC5101370,pubmed,27847543,create date:2016/11/17 | first author:Ribeiro E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recently, Deep Learning, especially through Convolutional Neural Networks (CNNs) has been widely used to enable the extraction of highly representative features. This is done among the network layers by filtering, selecting, and using these features in the last fully connected layers for pattern classification. However, CNN training for automated endoscopic image classification still provides a challenge due to the lack of large and publicly available annotated databases. In this work we explore Deep Learning for the automated classification of colonic polyps using different configurations for training CNNs from scratch (or full training) and distinct architectures of pretrained CNNs tested on 8-HD-endoscopic image databases acquired using different modalities. We compare our results with some commonly used features for colonic polyp classification and the good results suggest that features learned by CNNs trained from scratch and the 'off-the-shelf' CNNs features can be highly relevant for automated classification of colonic polyps. Moreover, we also show that the combination of classical features and 'off-the-shelf' CNNs features can be a good approach to further improve the results.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27847543,pubmed,2016,79c1099b-05f8-4b08-9afd-8b363d12b356,1
deepcut: object segmentation from bounding box annotations using convolutional neural networks,/pubmed/27845654,"Rajchl M, Lee MC, Oktay O, Kamnitsas K, Passerat-Palmbach J, Bai W, Damodaram M, Rutherford MA, Hajnal JV, Kainz B, Rueckert D.",IEEE Trans Med Imaging. 2017 Feb;36(2):674-683. doi: 10.1109/TMI.2016.2621185. Epub 2016 Nov 9.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:27845654,pubmed,27845654,create date:2016/11/16 | first author:Rajchl M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled weak annotations, in our case bounding boxes. It extends the approach of the well-known GrabCut [1] method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a naïve approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27845654,pubmed,2017,cf47270a-ba51-48aa-b18f-32464dac617b,1
a bottom-up approach for pancreas segmentation using cascaded superpixels and (deep) image patch labeling,/pubmed/27831881,"Farag A, Lu L, Roth HR, Liu J, Turkbey E, Summers RM.",IEEE Trans Image Process. 2016 Nov 1. [Epub ahead of print],IEEE Trans Image Process.  2016,PubMed,citation,PMID:27831881,pubmed,27831881,create date:2016/11/11 | first author:Farag A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Robust organ segmentation is a prerequisite for computer-aided diagnosis (CAD), quantitative imaging analysis, pathology detection and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared to well studied organs, such as the liver or heart.We present an automated bottomup approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Oversegmented 2D CT slices by the Simple Linear Iterative Clustering (SLIC) approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27831881,pubmed,2016,d0609ff7-fcf0-4114-835f-432e8dee46a4,1
machine learning based single-frame super-resolution processing for lensless blood cell counting,/pubmed/27827837,"Huang X, Jiang Y, Liu X, Xu H, Han Z, Rong H, Yang H, Yan M, Yu H.",Sensors (Basel). 2016 Nov 2;16(11). pii: E1836.,Sensors (Basel).  2016,PubMed,citation,PMID:27827837 | PMCID:PMC5134495,pubmed,27827837,create date:2016/11/10 | first author:Huang X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>A lensless blood cell counting system integrating microfluidic channel and a complementary metal oxide semiconductor (CMOS) image sensor is a promising technique to miniaturize the conventional optical lens based imaging system for point-of-care testing (POCT). However, such a system has limited resolution, making it imperative to improve resolution from the system-level using super-resolution (SR) processing. Yet, how to improve resolution towards better cell detection and recognition with low cost of processing resources and without degrading system throughput is still a challenge. In this article, two machine learning based single-frame SR processing types are proposed and compared for lensless blood cell counting, namely the Extreme Learning Machine based SR (ELMSR) and Convolutional Neural Network based SR (CNNSR). Moreover, lensless blood cell counting prototypes using commercial CMOS image sensors and custom designed backside-illuminated CMOS image sensors are demonstrated with ELMSR and CNNSR. When one captured low-resolution lensless cell image is input, an improved high-resolution cell image will be output. The experimental results show that the cell resolution is improved by 4×, and CNNSR has 9.5% improvement over the ELMSR on resolution enhancing performance. The cell counting results also match well with a commercial flow cytometer. Such ELMSR and CNNSR therefore have the potential for efficient resolution improvement in lensless blood cell counting systems towards POCT applications.</abstracttext></p></div></div>",huangxiwei@hdu.edu.cn,CMOS image sensor; convolutional neural network; extreme learning machine; microfluidic cytometer; point-of-care testing; super-resolution,https://www.ncbi.nlm.nih.gov//pubmed/27827837,pubmed,2016,e8140237-b497-47fd-96b5-b7ab66c1879b,1
deep learning for automated skeletal bone age assessment in x-ray images,/pubmed/27816861,"Spampinato C, Palazzo S, Giordano D, Aldinucci M, Leonardi R.",Med Image Anal. 2017 Feb;36:41-51. doi: 10.1016/j.media.2016.10.010. Epub 2016 Oct 29.,Med Image Anal.  2017,PubMed,citation,PMID:27816861,pubmed,27816861,create date:2016/11/07 | first author:Spampinato C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Skeletal bone age assessment is a common clinical practice to investigate endocrinology, genetic and growth disorders in children. It is generally performed by radiological examination of the left hand by using either the Greulich and Pyle (G&amp;P) method or the Tanner-Whitehouse (TW) one. However, both clinical procedures show several limitations, from the examination effort of radiologists to (most importantly) significant intra- and inter-operator variability. To address these problems, several automated approaches (especially relying on the TW method) have been proposed; nevertheless, none of them has been proved able to generalize to different races, age ranges and genders. In this paper, we propose and test several deep learning approaches to assess skeletal bone age automatically; the results showed an average discrepancy between manual and automatic evaluation of about 0.8 years, which is state-of-the-art performance. Furthermore, this is the first automated skeletal bone age assessment work tested on a public dataset and for all age ranges, races and genders, for which the source code is available, thus representing an exhaustive baseline for future research in the field. Beside the specific application scenario, this paper aims at providing answers to more general questions about deep learning on medical images: from the comparison between deep-learned features and manually-crafted ones, to the usage of deep-learning methods trained on general imagery for medical problems, to how to train a CNN with few images.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",cspampin@dieei.unict.it,Convolutional neural networks; Deep learning for medical images; Greulich and Pyle; Tanner–Whitehouse,https://www.ncbi.nlm.nih.gov//pubmed/27816861,pubmed,2017,625f9617-d73e-441a-a453-e007382f14d4,1
sleep quality prediction from wearable data using deep learning,/pubmed/27815231,"Sathyanarayana A, Joty S, Fernandez-Luque L, Ofli F, Srivastava J, Elmagarmid A, Arora T, Taheri S.",JMIR Mhealth Uhealth. 2016 Nov 4;4(4):e125. Erratum in: http://mhealth.jmir.org/2016/4/e130/.  JMIR Mhealth Uhealth. 2016 Nov 25;4(4):e130. ,JMIR Mhealth Uhealth.  2016,PubMed,citation,PMID:27815231 | PMCID:PMC5116102,pubmed,27815231,create date:2016/11/07 | first author:Sathyanarayana A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND'>The importance of sleep is paramount to health. Insufficient sleep can reduce physical, emotional, and mental well-being and can lead to a multitude of health complications among people with chronic conditions. Physical activity and sleep are highly interrelated health behaviors. Our physical activity during the day (ie, awake time) influences our quality of sleep, and vice versa. The current popularity of wearables for tracking physical activity and sleep, including actigraphy devices, can foster the development of new advanced data analytics. This can help to develop new electronic health (eHealth) applications and provide more insights into sleep science.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE'>The objective of this study was to evaluate the feasibility of predicting sleep quality (ie, poor or adequate sleep efficiency) given the physical activity wearable data during awake time. In this study, we focused on predicting good or poor sleep efficiency as an indicator of sleep quality.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS'>Actigraphy sensors are wearable medical devices used to study sleep and physical activity patterns. The dataset used in our experiments contained the complete actigraphy data from a subset of 92 adolescents over 1 full week. Physical activity data during awake time was used to create predictive models for sleep quality, in particular, poor or good sleep efficiency. The physical activity data from sleep time was used for the evaluation. We compared the predictive performance of traditional logistic regression with more advanced deep learning methods: multilayer perceptron (MLP), convolutional neural network (CNN), simple Elman-type recurrent neural network (RNN), long short-term memory (LSTM-RNN), and a time-batched version of LSTM-RNN (TB-LSTM).</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS'>Deep learning models were able to predict the quality of sleep (ie, poor or good sleep efficiency) based on wearable data from awake periods. More specifically, the deep learning methods performed better than traditional logistic regression. “CNN had the highest specificity and sensitivity, and an overall area under the receiver operating characteristic (ROC) curve (AUC) of 0.9449, which was 46% better as compared with traditional logistic regression (0.6463).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS'>Deep learning methods can predict the quality of sleep based on actigraphy data from awake periods. These predictive models can be an important tool for sleep research and to improve eHealth solutions for sleep.</abstracttext></p></div></div>",,accelerometer; actigraphy; body sensor networks; connected health; consumer health informatics; deep learning; mobile health; pervasive health; physical activity; sleep efficiency; sleep quality; wearables,https://www.ncbi.nlm.nih.gov//pubmed/27815231,pubmed,2016,b9562f52-1479-4687-97d9-578f2a5d3361,1
deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments,/pubmed/27814364,"Van Valen DA, Kudo T, Lane KM, Macklin DN, Quach NT, DeFelice MM, Maayan I, Tanouchi Y, Ashley EA, Covert MW.",PLoS Comput Biol. 2016 Nov 4;12(11):e1005177. doi: 10.1371/journal.pcbi.1005177. eCollection 2016 Nov.,PLoS Comput Biol.  2016,PubMed,citation,PMID:27814364 | PMCID:PMC5096676,pubmed,27814364,create date:2016/11/05 | first author:Van Valen DA,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Live-cell imaging has opened an exciting window into the role cellular heterogeneity plays in dynamic, living systems. A major critical challenge for this class of experiments is the problem of image segmentation, or determining which parts of a microscope image correspond to which individual cells. Current approaches require many hours of manual curation and depend on approaches that are difficult to share between labs. They are also unable to robustly segment the cytoplasms of mammalian cells. Here, we show that deep convolutional neural networks, a supervised machine learning method, can solve this challenge for multiple cell types across the domains of life. We demonstrate that this approach can robustly segment fluorescent images of cell nuclei as well as phase images of the cytoplasms of individual bacterial and mammalian cells from phase contrast images without the need for a fluorescent cytoplasmic marker. These networks also enable the simultaneous segmentation and identification of different mammalian cell types grown in co-culture. A quantitative comparison with prior methods demonstrates that convolutional neural networks have improved accuracy and lead to a significant reduction in curation time. We relay our experience in designing and optimizing deep convolutional neural networks for this task and outline several design rules that we found led to robust performance. We conclude that deep convolutional neural networks are an accurate method that require less curation time, are generalizable to a multiplicity of cell types, from bacteria to mammalian cells, and expand live-cell imaging capabilities to include multi-cell type systems.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27814364,pubmed,2016,673929ed-7339-45d2-8565-5d21e1f04020,1
generic feature learning for wireless capsule endoscopy analysis,/pubmed/27810622,"Seguí S, Drozdzal M, Pascual G, Radeva P, Malagelada C, Azpiroz F, Vitrià J.",Comput Biol Med. 2016 Dec 1;79:163-172. doi: 10.1016/j.compbiomed.2016.10.011. Epub 2016 Oct 19.,Comput Biol Med.  2016,PubMed,citation,PMID:27810622,pubmed,27810622,create date:2016/11/05 | first author:Seguí S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The interpretation and analysis of wireless capsule endoscopy (WCE) recordings is a complex task which requires sophisticated computer aided decision (CAD) systems to help physicians with video screening and, finally, with the diagnosis. Most CAD systems used in capsule endoscopy share a common system design, but use very different image and video representations. As a result, each time a new clinical application of WCE appears, a new CAD system has to be designed from the scratch. This makes the design of new CAD systems very time consuming. Therefore, in this paper we introduce a system for small intestine motility characterization, based on Deep Convolutional Neural Networks, which circumvents the laborious step of designing specific features for individual motility events. Experimental results show the superiority of the learned features over alternative classifiers constructed using state-of-the-art handcrafted features. In particular, it reaches a mean classification accuracy of 96% for six intestinal motility events, outperforming the other classifiers by a large margin (a 14% relative performance increase).</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",santi.segui@ub.edu,Deep learning; Feature learning; Motility analysis; Wireless capsule endoscopy,https://www.ncbi.nlm.nih.gov//pubmed/27810622,pubmed,2016,00de577b-26a3-47ce-9a98-27259e3c3c3b,1
fast and robust segmentation of the striatum using deep convolutional neural networks,/pubmed/27777000,"Choi H, Jin KH.",J Neurosci Methods. 2016 Dec 1;274:146-153. doi: 10.1016/j.jneumeth.2016.10.007. Epub 2016 Oct 21.,J Neurosci Methods.  2016,PubMed,citation,PMID:27777000,pubmed,27777000,create date:2016/11/05 | first author:Choi H,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Automated segmentation of brain structures is an important task in structural and functional image analysis. We developed a fast and accurate method for the striatum segmentation using deep convolutional neural networks (CNN).</abstracttext></p><h4>NEW METHOD: </h4><p><abstracttext label='NEW METHOD' nlmcategory='UNASSIGNED'>T1 magnetic resonance (MR) images were used for our CNN-based segmentation, which require neither image feature extraction nor nonlinear transformation. We employed two serial CNN, Global and Local CNN: The Global CNN determined approximate locations of the striatum. It performed a regression of input MR images fitted to smoothed segmentation maps of the striatum. From the output volume of Global CNN, cropped MR volumes which included the striatum were extracted. The cropped MR volumes and the output volumes of Global CNN were used for inputs of Local CNN. Local CNN predicted the accurate label of all voxels. Segmentation results were compared with a widely used segmentation method, FreeSurfer.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our method showed higher Dice Similarity Coefficient (DSC) (0.893±0.017 vs. 0.786±0.015) and precision score (0.905±0.018 vs. 0.690±0.022) than FreeSurfer-based striatum segmentation (p=0.06). Our approach was also tested using another independent dataset, which showed high DSC (0.826±0.038) comparable with that of FreeSurfer. Comparison with existing method Segmentation performance of our proposed method was comparable with that of FreeSurfer. The running time of our approach was approximately three seconds.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>We suggested a fast and accurate deep CNN-based segmentation for small brain structures which can be widely applied to brain image analysis.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",chy1000@gmail.com,Convolutional neural network; Deep learning; MRI; Segmentation; Striatum,https://www.ncbi.nlm.nih.gov//pubmed/27777000,pubmed,2016,3e098602-df7a-4f32-8f57-62c45c1a8488,1
brainnetcnn: convolutional neural networks for brain networks; towards predicting neurodevelopment,/pubmed/27693612,"Kawahara J, Brown CJ, Miller SP, Booth BG, Chau V, Grunau RE, Zwicker JG, Hamarneh G.",Neuroimage. 2017 Feb 1;146:1038-1049. doi: 10.1016/j.neuroimage.2016.09.046. Epub 2016 Sep 28.,Neuroimage.  2017,PubMed,citation,PMID:27693612,pubmed,27693612,create date:2016/11/05 | first author:Kawahara J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose BrainNetCNN, a convolutional neural network (CNN) framework to predict clinical neurodevelopmental outcomes from brain networks. In contrast to the spatially local convolutions done in traditional image-based CNNs, our BrainNetCNN is composed of novel edge-to-edge, edge-to-node and node-to-graph convolutional filters that leverage the topological locality of structural brain networks. We apply the BrainNetCNN framework to predict cognitive and motor developmental outcome scores from structural brain networks of infants born preterm. Diffusion tensor images (DTI) of preterm infants, acquired between 27 and 46 weeks gestational age, were used to construct a dataset of structural brain connectivity networks. We first demonstrate the predictive capabilities of BrainNetCNN on synthetic phantom networks with simulated injury patterns and added noise. BrainNetCNN outperforms a fully connected neural-network with the same number of model parameters on both phantoms with focal and diffuse injury patterns. We then apply our method to the task of joint prediction of Bayley-III cognitive and motor scores, assessed at 18 months of age, adjusted for prematurity. We show that our BrainNetCNN framework outperforms a variety of other methods on the same data. Furthermore, BrainNetCNN is able to identify an infant's postmenstrual age to within about 2 weeks. Finally, we explore the high-level features learned by BrainNetCNN by visualizing the importance of each connection in the brain with respect to predicting the outcome scores. These findings are then discussed in the context of the anatomy and function of the developing preterm infant brain.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Inc. All rights reserved.</p></div></div>",hamarneh@sfu.ca,Brain networks; Connectome; Convolutional neural networks; Deep learning; Diffusion MRI; Neurodevelopment; Prediction; Preterm infants,https://www.ncbi.nlm.nih.gov//pubmed/27693612,pubmed,2017,1539f753-fba9-4fc1-b889-835f9adda80c,1
patch-based convolutional neural network for whole slide tissue image classification,/pubmed/27795661,"Hou L, Samaras D, Kurc TM, Gao Y, Davis JE, Saltz JH.",Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit. 2016 Jun-Jul;2016:2424-2433.,Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit.  2016,PubMed,citation,PMID:27795661 | PMCID:PMC5085270,pubmed,27795661,create date:2016/11/01 | first author:Hou L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27795661,pubmed,2016,dc8439cf-1207-4cd9-b68f-3303b1640887,1
a pre-trained convolutional neural network based method for thyroid nodule diagnosis,/pubmed/27668999,"Ma J, Wu F, Zhu J, Xu D, Kong D.",Ultrasonics. 2017 Jan;73:221-230. doi: 10.1016/j.ultras.2016.09.011. Epub 2016 Sep 12.,Ultrasonics.  2017,PubMed,citation,PMID:27668999,pubmed,27668999,create date:2016/10/21 | first author:Ma J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In ultrasound images, most thyroid nodules are in heterogeneous appearances with various internal components and also have vague boundaries, so it is difficult for physicians to discriminate malignant thyroid nodules from benign ones. In this study, we propose a hybrid method for thyroid nodule diagnosis, which is a fusion of two pre-trained convolutional neural networks (CNNs) with different convolutional layers and fully-connected layers. Firstly, the two networks pre-trained with ImageNet database are separately trained. Secondly, we fuse feature maps learned by trained convolutional filters, pooling and normalization operations of the two CNNs. Finally, with the fused feature maps, a softmax classifier is used to diagnose thyroid nodules. The proposed method is validated on 15,000 ultrasound images collected from two local hospitals. Experiment results show that the proposed CNN based methods can accurately and effectively diagnose thyroid nodules. In addition, the fusion of the two CNN based models lead to significant performance improvement, with an accuracy of 83.02%±0.72%. These demonstrate the potential clinical applications of this method.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",dkong@zju.edu.cn,Classification; Convolutional neural network; Diagnosis; Feature extraction; Thyroid nodule; Ultrasound image,https://www.ncbi.nlm.nih.gov//pubmed/27668999,pubmed,2017,825b2ea7-add8-491c-a6e1-09d3d44cc703,1
high-throughput classification of radiographs using deep convolutional neural networks,/pubmed/27730417,"Rajkomar A, Lingam S, Taylor AG, Blum M, Mongan J.",J Digit Imaging. 2017 Feb;30(1):95-101. doi: 10.1007/s10278-016-9914-9.,J Digit Imaging.  2017,PubMed,citation,PMID:27730417 | PMCID:PMC5267603,pubmed,27730417,create date:2016/10/13 | first author:Rajkomar A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The study aimed to determine if computer vision techniques rooted in deep learning can use a small set of radiographs to perform clinically relevant image classification with high fidelity. One thousand eight hundred eighty-five chest radiographs on 909 patients obtained between January 2013 and July 2015 at our institution were retrieved and anonymized. The source images were manually annotated as frontal or lateral and randomly divided into training, validation, and test sets. Training and validation sets were augmented to over 150,000 images using standard image manipulations. We then pre-trained a series of deep convolutional networks based on the open-source GoogLeNet with various transformations of the open-source ImageNet (non-radiology) images. These trained networks were then fine-tuned using the original and augmented radiology images. The model with highest validation accuracy was applied to our institutional test set and a publicly available set. Accuracy was assessed by using the Youden Index to set a binary cutoff for frontal or lateral classification. This retrospective study was IRB approved prior to initiation. A network pre-trained on 1.2 million greyscale ImageNet images and fine-tuned on augmented radiographs was chosen. The binary classification method correctly classified 100 % (95 % CI 99.73-100 %) of both our test set and the publicly available images. Classification was rapid, at 38 images per second. A deep convolutional neural network created using non-radiological images, and an augmented set of radiographs is effective in highly accurate classification of chest radiograph view type and is a feasible, rapid method for high-throughput annotation.</abstracttext></p></div></div>",Alvin.rajkomar@ucsf.edu,Artificial neural networks; Chest radiographs; Computer vision; Convolutional neural network; Deep learning; Machine learning; Radiography,https://www.ncbi.nlm.nih.gov//pubmed/27730417,pubmed,2017,f7875d17-bb0c-4b93-bb21-e3537f78a8c8,1
antibody-supervised deep learning for quantification of tumor-infiltrating immune cells in hematoxylin and eosin stained breast cancer samples,/pubmed/27688929,"Turkki R, Linder N, Kovanen PE, Pellinen T, Lundin J.",J Pathol Inform. 2016 Sep 1;7:38. doi: 10.4103/2153-3539.189703. eCollection 2016. Erratum in: J Pathol Inform. 2016 Sep 28;7:41. ,J Pathol Inform.  2016,PubMed,citation,PMID:27688929 | PMCID:PMC5027738,pubmed,27688929,create date:2016/10/01 | first author:Turkki R,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Immune cell infiltration in tumor is an emerging prognostic biomarker in breast cancer. The gold standard for quantification of immune cells in tissue sections is visual assessment through a microscope, which is subjective and semi-quantitative. In this study, we propose and evaluate an approach based on antibody-guided annotation and deep learning to quantify immune cell-rich areas in hematoxylin and eosin (H&amp;E) stained samples.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Consecutive sections of formalin-fixed parafin-embedded samples obtained from the primary tumor of twenty breast cancer patients were cut and stained with H&amp;E and the pan-leukocyte CD45 antibody. The stained slides were digitally scanned, and a training set of immune cell-rich and cell-poor tissue regions was annotated in H&amp;E whole-slide images using the CD45-expression as a guide. In analysis, the images were divided into small homogenous regions, superpixels, from which features were extracted using a pretrained convolutional neural network (CNN) and classified with a support of vector machine. The CNN approach was compared to texture-based classification and to visual assessments performed by two pathologists.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In a set of 123,442 labeled superpixels, the CNN approach achieved an F-score of 0.94 (range: 0.92-0.94) in discrimination of immune cell-rich and cell-poor regions, as compared to an F-score of 0.88 (range: 0.87-0.89) obtained with the texture-based classification. When compared to visual assessment of 200 images, an agreement of 90% (κ = 0.79) to quantify immune infiltration with the CNN approach was achieved while the inter-observer agreement between pathologists was 90% (κ = 0.78).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Our findings indicate that deep learning can be applied to quantify immune cell infiltration in breast cancer samples using a basic morphology staining only. A good discrimination of immune cell-rich areas was achieved, well in concordance with both leukocyte antigen expression and pathologists' visual assessment.</abstracttext></p></div></div>",,breast cancer; convolutional neural network; digital pathology; tumor microenvironment; tumor-infiltrating immune cells,https://www.ncbi.nlm.nih.gov//pubmed/27688929,pubmed,2016,c6c79b8b-9da5-4476-8e57-cadb99881758,1
deep learning with convolutional neural networks applied to electromyography data: a resource for the classification of movements for prosthetic hands,/pubmed/27656140,"Atzori M, Cognolato M, Müller H.",Front Neurorobot. 2016 Sep 7;10:9. doi: 10.3389/fnbot.2016.00009. eCollection 2016.,Front Neurorobot.  2016,PubMed,citation,PMID:27656140 | PMCID:PMC5013051,pubmed,27656140,create date:2016/09/23 | first author:Atzori M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Natural control methods based on surface electromyography (sEMG) and pattern recognition are promising for hand prosthetics. However, the control robustness offered by scientific research is still not sufficient for many real life applications, and commercial prostheses are capable of offering natural control for only a few movements. In recent years deep learning revolutionized several fields of machine learning, including computer vision and speech recognition. Our objective is to test its methods for natural control of robotic hands via sEMG using a large number of intact subjects and amputees. We tested convolutional networks for the classification of an average of 50 hand movements in 67 intact subjects and 11 transradial amputees. The simple architecture of the neural network allowed to make several tests in order to evaluate the effect of pre-processing, layer architecture, data augmentation and optimization. The classification results are compared with a set of classical classification methods applied on the same datasets. The classification accuracy obtained with convolutional neural networks using the proposed architecture is higher than the average results obtained with the classical classification methods, but lower than the results obtained with the best reference methods in our tests. The results show that convolutional neural networks with a very simple architecture can produce accurate results comparable to the average classical classification methods. They show that several factors (including pre-processing, the architecture of the net and the optimization parameters) can be fundamental for the analysis of sEMG data. Larger networks can achieve higher accuracy on computer vision and object recognition tasks. This fact suggests that it may be interesting to evaluate if larger networks can increase sEMG classification accuracy too. </abstracttext></p></div></div>",,convolutional neural networks; deep learning; electromyography; machine learning; prosthetics; rehabilitation robotics,https://www.ncbi.nlm.nih.gov//pubmed/27656140,pubmed,2016,763fd3b2-1816-423f-9bf3-03f5d0ceeca8,1
deep neural networks for identifying cough sounds,/pubmed/27654978,"Amoh J, Odame K.",IEEE Trans Biomed Circuits Syst. 2016 Oct;10(5):1003-1011. doi: 10.1109/TBCAS.2016.2598794. Epub 2016 Sep 16.,IEEE Trans Biomed Circuits Syst.  2016,PubMed,citation,PMID:27654978,pubmed,27654978,create date:2016/09/23 | first author:Amoh J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we consider two different approaches of using deep neural networks for cough detection. The cough detection task is cast as a visual recognition problem and as a sequence-to-sequence labeling problem. A convolutional neural network and a recurrent neural network are implemented to address these problems, respectively. We evaluate the performance of the two networks and compare them to other conventional approaches for identifying cough sounds. In addition, we also explore the effect of the network size parameters and the impact of long-term signal dependencies in cough classifier performance. Experimental results show both network architectures outperform traditional methods. Between the two, our convolutional network yields a higher specificity 92.7% whereas the recurrent attains a higher sensitivity of 87.7%.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27654978,pubmed,2016,53fae7a6-4a2c-4790-a8c3-e05902f552a5,1
digital mammographic tumor classification using transfer learning from deep convolutional neural networks,/pubmed/27610399,"Huynh BQ, Li H, Giger ML.",J Med Imaging (Bellingham). 2016 Jul;3(3):034501. doi: 10.1117/1.JMI.3.3.034501. Epub 2016 Aug 22.,J Med Imaging (Bellingham).  2016,PubMed,citation,PMID:27610399 | PMCID:PMC4992049,pubmed,27610399,create date:2016/09/10 | first author:Huynh BQ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Convolutional neural networks (CNNs) show potential for computer-aided diagnosis (CADx) by learning features directly from the image data instead of using analytically extracted features. However, CNNs are difficult to train from scratch for medical images due to small sample sizes and variations in tumor presentations. Instead, transfer learning can be used to extract tumor information from medical images via CNNs originally pretrained for nonmedical tasks, alleviating the need for large datasets. Our database includes 219 breast lesions (607 full-field digital mammographic images). We compared support vector machine classifiers based on the CNN-extracted image features and our prior computer-extracted tumor features in the task of distinguishing between benign and malignant breast lesions. Five-fold cross validation (by lesion) was conducted with the area under the receiver operating characteristic (ROC) curve as the performance metric. Results show that classifiers based on CNN-extracted features (with transfer learning) perform comparably to those using analytically extracted features [area under the ROC curve [Formula: see text]]. Further, the performance of ensemble classifiers based on both types was significantly better than that of either classifier type alone ([Formula: see text] versus 0.81, [Formula: see text]). We conclude that transfer learning can improve current CADx methods while also providing standalone classifiers without large datasets, facilitating machine-learning methods in radiomics and precision medicine. </abstracttext></p></div></div>",,computer-aided diagnosis; convolutional neural networks; deep learning; mammography; precision medicine; radiomics; transfer learning,https://www.ncbi.nlm.nih.gov//pubmed/27610399,pubmed,2016,7336b832-fd05-4ea4-ba3a-b490771e02d4,1
automatic 3d liver location and segmentation via convolutional neural network and graph cut,/pubmed/27604760,"Lu F, Wu F, Hu P, Peng Z, Kong D.",Int J Comput Assist Radiol Surg. 2017 Feb;12(2):171-182. doi: 10.1007/s11548-016-1467-3. Epub 2016 Sep 7.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:27604760,pubmed,27604760,create date:2016/09/09 | first author:Lu F,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural network; (ii) accuracy refinement of the initial segmentation with graph cut and the previously learned probability map.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb1. For the MICCAI-Sliver07 test dataset, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root-mean-square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9, 2.7 %, 0.91, 1.88 and 18.94 mm, respectively. For the 3Dircadb1 dataset, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36, 0.97 %, 1.89, 4.15 and 33.14 mm, respectively.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and nonreproducible manual segmentation method.</abstracttext></p></div></div>",dkong@zju.edu.cn,3D convolution neural network; CT images; Graph cut; Liver segmentation,https://www.ncbi.nlm.nih.gov//pubmed/27604760,pubmed,2017,69ed3643-72cc-45b2-99c8-61a05f0a1462,1
glaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation,/pubmed/27590198,"Zilly J, Buhmann JM, Mahapatra D.",Comput Med Imaging Graph. 2017 Jan;55:28-41. doi: 10.1016/j.compmedimag.2016.07.012. Epub 2016 Aug 23.,Comput Med Imaging Graph.  2017,PubMed,citation,PMID:27590198,pubmed,27590198,create date:2016/09/04 | first author:Zilly J,<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We present a novel method to segment retinal images using ensemble learning based convolutional neural network (CNN) architectures. An entropy sampling technique is used to select informative points thus reducing computational complexity while performing superior to uniform sampling. The sampled points are used to design a novel learning framework for convolutional filters based on boosting. Filters are learned in several layers with the output of previous layers serving as the input to the next layer. A softmax logistic classifier is subsequently trained on the output of all learned filters and applied on test images. The output of the classifier is subject to an unsupervised graph cut algorithm followed by a convex hull transformation to obtain the final segmentation. Our proposed algorithm for optic cup and disc segmentation outperforms existing methods on the public DRISHTI-GS data set on several metrics.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>,dwarikanath.mahapatra@inf.ethz.ch,Boosting; CNN; Ensemble learning; Glaucoma; Optic cup; Optic disc; Segmentation,https://www.ncbi.nlm.nih.gov//pubmed/27590198,pubmed,2017,a6654bed-8dce-4e67-b8c5-bbae1af01175,1
cascade of multi-scale convolutional neural networks for bone suppression of chest radiographs in gradient domain,/pubmed/27589577,"Yang W, Chen Y, Liu Y, Zhong L, Qin G, Lu Z, Feng Q, Chen W.",Med Image Anal. 2017 Jan;35:421-433. doi: 10.1016/j.media.2016.08.004. Epub 2016 Aug 16.,Med Image Anal.  2017,PubMed,citation,PMID:27589577,pubmed,27589577,create date:2016/09/03 | first author:Yang W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Suppression of bony structures in chest radiographs (CXRs) is potentially useful for radiologists and computer-aided diagnostic schemes. In this paper, we present an effective deep learning method for bone suppression in single conventional CXR using deep convolutional neural networks (ConvNets) as basic prediction units. The deep ConvNets were adapted to learn the mapping between the gradients of the CXRs and the corresponding bone images. We propose a cascade architecture of ConvNets (called CamsNet) to refine progressively the predicted bone gradients in which the ConvNets work at successively increased resolutions. The predicted bone gradients at different scales from the CamsNet are fused in a maximum-a-posteriori framework to produce the final estimation of a bone image. This estimation of a bone image is subtracted from the original CXR to produce a soft-tissue image in which the bone components are eliminated. Our method was evaluated on a dataset that consisted of 504 cases of real two-exposure dual-energy subtraction chest radiographs (404 cases for training and 100 cases for test). The results demonstrate that our method can produce high-quality and high-resolution bone and soft-tissue images. The average relative mean absolute error of the produced bone images and peak signal-to-noise ratio of the produced soft-tissue images were 3.83% and 38.7dB, respectively. The average bone suppression ratio of our method was 83.8% for the CXRs with pixel sizes of nearly 0.194mm. Furthermore, we apply the trained CamsNet model on the CXRs acquired by various types of X-ray machines, including scanned films, and our method can also produce visually appealing bone and soft-tissue images.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",qianjinfeng08@gmail.com,Bone suppression; Chest radiography; Convolutional neural network; Dual-energy subtraction,https://www.ncbi.nlm.nih.gov//pubmed/27589577,pubmed,2017,e209803a-6c09-4ebc-9b51-6a0acaf82d8f,1
aucpred: proteome-level protein disorder prediction by auc-maximized deep convolutional neural fields,/pubmed/27587688,"Wang S, Ma J, Xu J.",Bioinformatics. 2016 Sep 1;32(17):i672-i679. doi: 10.1093/bioinformatics/btw446.,Bioinformatics.  2016,PubMed,citation,PMID:27587688 | PMCID:PMC5013916,pubmed,27587688,create date:2016/09/03 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Protein intrinsically disordered regions (IDRs) play an important role in many biological processes. Two key properties of IDRs are (i) the occurrence is proteome-wide and (ii) the ratio of disordered residues is about 6%, which makes it challenging to accurately predict IDRs. Most IDR prediction methods use sequence profile to improve accuracy, which prevents its application to proteome-wide prediction since it is time-consuming to generate sequence profiles. On the other hand, the methods without using sequence profile fare much worse than using sequence profile.</abstracttext></p><h4>METHOD: </h4><p><abstracttext label='METHOD' nlmcategory='METHODS'>This article formulates IDR prediction as a sequence labeling problem and employs a new machine learning method called Deep Convolutional Neural Fields (DeepCNF) to solve it. DeepCNF is an integration of deep convolutional neural networks (DCNN) and conditional random fields (CRF); it can model not only complex sequence-structure relationship in a hierarchical manner, but also correlation among adjacent residues. To deal with highly imbalanced order/disorder ratio, instead of training DeepCNF by widely used maximum-likelihood, we develop a novel approach to train it by maximizing area under the ROC curve (AUC), which is an unbiased measure for class-imbalanced data.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our experimental results show that our IDR prediction method AUCpreD outperforms existing popular disorder predictors. More importantly, AUCpreD works very well even without sequence profile, comparing favorably to or even outperforming many methods using sequence profile. Therefore, our method works for proteome-wide disorder prediction while yielding similar or better accuracy than the others.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>http://raptorx2.uchicago.edu/StructurePropertyPred/predict/</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>wangsheng@uchicago.edu, jinboxu@gmail.com</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27587688,pubmed,2016,5b63a13b-9479-4151-b551-9296f5204375,1
deepchrome: deep-learning for predicting gene expression from histone modifications,/pubmed/27587684,"Singh R, Lanchantin J, Robins G, Qi Y.",Bioinformatics. 2016 Sep 1;32(17):i639-i648. doi: 10.1093/bioinformatics/btw427.,Bioinformatics.  2016,PubMed,citation,PMID:27587684,pubmed,27587684,create date:2016/09/03 | first author:Singh R,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Histone modifications are among the most important factors that control gene regulation. Computational methods that predict gene expression from histone modification signals are highly desirable for understanding their combinatorial effects in gene regulation. This knowledge can help in developing 'epigenetic drugs' for diseases like cancer. Previous studies for quantifying the relationship between histone modifications and gene expression levels either failed to capture combinatorial effects or relied on multiple methods that separate predictions and combinatorial analysis. This paper develops a unified discriminative framework using a deep convolutional neural network to classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among important features. To simultaneously visualize the combinatorial interactions among histone modifications, we propose a novel optimization-based technique that generates feature pattern maps from the learnt deep model. This provides an intuitive description of underlying epigenetic mechanisms that regulate genes.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We show that DeepChrome outperforms state-of-the-art models like Support Vector Machines and Random Forests for gene expression classification task on 56 different cell-types from REMC database. The output of our visualization technique not only validates the previous observations but also allows novel insights about combinatorial interactions among histone modification marks, some of which have recently been observed by experimental studies.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>Codes and results are available at www.deepchrome.org</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>yanjun@virginia.edu</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27587684,pubmed,2016,f5d095c4-1fd7-4d64-9260-d92c0c3180c2,1
large scale deep learning for computer aided detection of mammographic lesions,/pubmed/27497072,"Kooi T, Litjens G, van Ginneken B, Gubern-Mérida A, Sánchez CI, Mann R, den Heeten A, Karssemeijer N.",Med Image Anal. 2017 Jan;35:303-312. doi: 10.1016/j.media.2016.07.007. Epub 2016 Aug 2.,Med Image Anal.  2017,PubMed,citation,PMID:27497072,pubmed,27497072,create date:2016/08/09 | first author:Kooi T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent advances in machine learning yielded new techniques to train deep neural networks, which resulted in highly successful applications in many pattern recognition tasks such as object detection and speech recognition. In this paper we provide a head-to-head comparison between a state-of-the art in mammography CAD system, relying on a manually designed feature set and a Convolutional Neural Network (CNN), aiming for a system that can ultimately read mammograms independently. Both systems are trained on a large data set of around 45,000 images and results show the CNN outperforms the traditional CAD system at low sensitivity and performs comparable at high sensitivity. We subsequently investigate to what extent features such as location and patient information and commonly used manual features can still complement the network and see improvements at high specificity over the CNN especially with location and context features, which contain information not available to the CNN. Additionally, a reader study was performed, where the network was compared to certified screening radiologists on a patch level and we found no significant difference between the network and the readers.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",thijs.kooi@radboudumc.nl,Breast cancer; Computer aided detection; Convolutional neural networks; Deep learning; Machine learning; Mammography,https://www.ncbi.nlm.nih.gov//pubmed/27497072,pubmed,2017,c30b05f2-53ad-47a2-a0e9-629e70a8037b,1
a shortest dependency path based convolutional neural network for protein-protein relation extraction,/pubmed/27493967,"Hua L, Quan C.",Biomed Res Int. 2016;2016:8479587. doi: 10.1155/2016/8479587. Epub 2016 Jul 14.,Biomed Res Int.  2016,PubMed,citation,PMID:27493967 | PMCID:PMC4963603,pubmed,27493967,create date:2016/08/06 | first author:Hua L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The state-of-the-art methods for protein-protein interaction (PPI) extraction are primarily based on kernel methods, and their performances strongly depend on the handcraft features. In this paper, we tackle PPI extraction by using convolutional neural networks (CNN) and propose a shortest dependency path based CNN (sdpCNN) model. The proposed method (1) only takes the sdp and word embedding as input and (2) could avoid bias from feature selection by using CNN. We performed experiments on standard Aimed and BioInfer datasets, and the experimental results demonstrated that our approach outperformed state-of-the-art kernel based methods. In particular, by tracking the sdpCNN model, we find that sdpCNN could extract key features automatically and it is verified that pretrained word embedding is crucial in PPI task. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27493967,pubmed,2016,d89efd0c-197c-4e5b-a5c4-2df7a947cee4,1
deep learning in drug discovery,/pubmed/27491648,"Gawehn E, Hiss JA, Schneider G.",Mol Inform. 2016 Jan;35(1):3-14. doi: 10.1002/minf.201501008. Epub 2015 Dec 30. Review.,Mol Inform.  2016,PubMed,citation,PMID:27491648,pubmed,27491648,create date:2016/08/06 | first author:Gawehn E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Artificial neural networks had their first heyday in molecular informatics and drug discovery approximately two decades ago. Currently, we are witnessing renewed interest in adapting advanced neural network architectures for pharmaceutical research by borrowing from the field of 'deep learning'. Compared with some of the other life sciences, their application in drug discovery is still limited. Here, we provide an overview of this emerging field of molecular informatics, present the basic concepts of prominent deep learning methods and offer motivation to explore these techniques for their usefulness in computer-assisted drug discovery and design. We specifically emphasize deep neural networks, restricted Boltzmann machine networks and convolutional networks. </abstracttext></p><p class='copyright'>© 2016 WILEY-VCH Verlag GmbH &amp; Co. KGaA, Weinheim.</p></div></div>",gisbert.schneider@pharma.ethz.ch,bioinformatics; cheminformatics; drug design; machine-learning; neural network; virtual screening,https://www.ncbi.nlm.nih.gov//pubmed/27491648,pubmed,2016,df643f91-8e88-4497-90b2-4858086c106f,1
enhancing deep convolutional neural network scheme for breast cancer diagnosis with unlabeled data,/pubmed/27475279,"Sun W, Tseng TB, Zhang J, Qian W.",Comput Med Imaging Graph. 2017 Apr;57:4-9. doi: 10.1016/j.compmedimag.2016.07.004. Epub 2016 Jul 19.,Comput Med Imaging Graph.  2017,PubMed,citation,PMID:27475279,pubmed,27475279,create date:2016/08/01 | first author:Sun W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this study we developed a graph based semi-supervised learning (SSL) scheme using deep convolutional neural network (CNN) for breast cancer diagnosis. CNN usually needs a large amount of labeled data for training and fine tuning the parameters, and our proposed scheme only requires a small portion of labeled data in training set. Four modules were included in the diagnosis system: data weighing, feature selection, dividing co-training data labeling, and CNN. 3158 region of interests (ROIs) with each containing a mass extracted from 1874 pairs of mammogram images were used for this study. Among them 100 ROIs were treated as labeled data while the rest were treated as unlabeled. The area under the curve (AUC) observed in our study was 0.8818, and the accuracy of CNN is 0.8243 using the mixed labeled and unlabeled data.</abstracttext></p><p class='copyright'>Copyright © 2016. Published by Elsevier Ltd.</p></div></div>",wqian@utep.edu,Computer aided diagnosis; Convolutional neural network; Deep learning; Semi-supervised learning; Unlabeled data,https://www.ncbi.nlm.nih.gov//pubmed/27475279,pubmed,2017,a61b6282-8a88-4759-a79d-23bb0366093a,1
deep learning in bioinformatics,/pubmed/27473064,"Min S, Lee B, Yoon S.",Brief Bioinform. 2016 Jul 29. pii: bbw068. [Epub ahead of print],Brief Bioinform.  2016,PubMed,citation,PMID:27473064,pubmed,27473064,create date:2016/07/31 | first author:Min S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press. For Permissions, please email: journals.permissions@oup.com.</p></div></div>",,bioinformatics; biomedical imaging; biomedical signal processing.; deep learning; machine learning; neural network; omics,https://www.ncbi.nlm.nih.gov//pubmed/27473064,pubmed,2016,6b845c72-50e0-4c62-bfa7-1c29487cabf9,1
drug drug interaction extraction from biomedical literature using syntax convolutional neural network,/pubmed/27466626,"Zhao Z, Yang Z, Luo L, Lin H, Wang J.",Bioinformatics. 2016 Nov 15;32(22):3444-3453. doi: 10.1093/bioinformatics/btw486. Epub 2016 Jul 27.,Bioinformatics.  2016,PubMed,citation,PMID:27466626 | PMCID:PMC5181565,pubmed,27466626,create date:2016/07/29 | first author:Zhao Z,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Detecting drug-drug interaction (DDI) has become a vital part of public health safety. Therefore, using text mining techniques to extract DDIs from biomedical literature has received great attentions. However, this research is still at an early stage and its performance has much room to improve.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>In this article, we present a syntax convolutional neural network (SCNN) based DDI extraction method. In this method, a novel word embedding, syntax word embedding, is proposed to employ the syntactic information of a sentence. Then the position and part of speech features are introduced to extend the embedding of each word. Later, auto-encoder is introduced to encode the traditional bag-of-words feature (sparse 0-1 vector) as the dense real value vector. Finally, a combination of embedding-based convolutional features and traditional features are fed to the softmax classifier to extract DDIs from biomedical literature. Experimental results on the DDIExtraction 2013 corpus show that SCNN obtains a better performance (an F-score of 0.686) than other state-of-the-art methods.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>The source code is available for academic use at http://202.118.75.18:8080/DDI/SCNN-DDI.zip CONTACT: yangzh@dlut.edu.cnSupplementary information: Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27466626,pubmed,2016,f4a9f59e-c347-4e6a-9b54-da483690427f,1
endonet: a deep architecture for recognition tasks on laparoscopic videos,/pubmed/27455522,"Twinanda AP, Shehata S, Mutter D, Marescaux J, de Mathelin M, Padoy N.",IEEE Trans Med Imaging. 2017 Jan;36(1):86-97. doi: 10.1109/TMI.2016.2593957. Epub 2016 Jul 22.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:27455522,pubmed,27455522,create date:2016/07/28 | first author:Twinanda AP,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, surgical phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the used visual features are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27455522,pubmed,2017,10b22c30-0e6c-4054-a2fb-7629e282d887,1
brain tumor segmentation with deep neural networks,/pubmed/27310171,"Havaei M, Davy A, Warde-Farley D, Biard A, Courville A, Bengio Y, Pal C, Jodoin PM, Larochelle H.",Med Image Anal. 2017 Jan;35:18-31. doi: 10.1016/j.media.2016.05.004. Epub 2016 May 19.,Med Image Anal.  2017,PubMed,citation,PMID:27310171,pubmed,27310171,create date:2016/06/17 | first author:Havaei M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",seyed.mohammad.havaei@usherbrooke.ca,Brain tumor segmentation; Cascaded convolutional neural networks; Convolutional neural networks; Deep neural networks,https://www.ncbi.nlm.nih.gov//pubmed/27310171,pubmed,2017,002d0404-5e87-48c7-b0c8-ffb2939dd24a,1
classifying and segmenting microscopy images with deep multiple instance learning,/pubmed/27307644,"Kraus OZ, Ba JL, Frey BJ.",Bioinformatics. 2016 Jun 15;32(12):i52-i59. doi: 10.1093/bioinformatics/btw252.,Bioinformatics.  2016,PubMed,citation,PMID:27307644 | PMCID:PMC4908336,pubmed,27307644,create date:2016/06/17 | first author:Kraus OZ,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>High-content screening (HCS) technologies have enabled large scale imaging experiments for studying cell biology and for drug screening. These systems produce hundreds of thousands of microscopy images per day and their utility depends on automated image analysis. Recently, deep learning approaches that learn feature representations directly from pixel intensity values have dominated object recognition challenges. These tasks typically have a single centered object per image and existing models are not directly applicable to microscopy datasets. Here we develop an approach that combines deep convolutional neural networks (CNNs) with multiple instance learning (MIL) in order to classify and segment microscopy images using only whole image level annotations.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We introduce a new neural network architecture that uses MIL to simultaneously classify and segment microscopy images with populations of cells. We base our approach on the similarity between the aggregation function used in MIL and pooling layers used in CNNs. To facilitate aggregating across large numbers of instances in CNN feature maps we present the Noisy-AND pooling function, a new MIL operator that is robust to outliers. Combining CNNs with MIL enables training CNNs using whole microscopy images with image level labels. We show that training end-to-end MIL CNNs outperforms several previous methods on both mammalian and yeast datasets without requiring any segmentation steps.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>Torch7 implementation available upon request.</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>oren.kraus@mail.utoronto.ca.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27307644,pubmed,2016,9e25ae32-33cd-460a-8142-d5ff19e258a6,1
convolutional neural network architectures for predicting dna-protein binding,/pubmed/27307608,"Zeng H, Edwards MD, Liu G, Gifford DK.",Bioinformatics. 2016 Jun 15;32(12):i121-i127. doi: 10.1093/bioinformatics/btw255.,Bioinformatics.  2016,PubMed,citation,PMID:27307608 | PMCID:PMC4908339,pubmed,27307608,create date:2016/06/17 | first author:Zeng H,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Convolutional neural networks (CNN) have outperformed conventional methods in modeling the sequence specificity of DNA-protein binding. Yet inappropriate CNN architectures can yield poorer performance than simpler models. Thus an in-depth understanding of how to match CNN architecture to a given task is needed to fully harness the power of CNNs for computational biology applications.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We present a systematic exploration of CNN architectures for predicting DNA sequence binding using a large compendium of transcription factor datasets. We identify the best-performing architectures by varying CNN width, depth and pooling designs. We find that adding convolutional kernels to a network is important for motif-based tasks. We show the benefits of CNNs in learning rich higher-order sequence features, such as secondary motifs and local sequence context, by comparing network performance on multiple modeling tasks ranging in difficulty. We also demonstrate how careful construction of sequence benchmark datasets, using approaches that control potentially confounding effects like positional or motif strength bias, is critical in making fair comparisons between competing methods. We explore how to establish the sufficiency of training data for these learning tasks, and we have created a flexible cloud-based framework that permits the rapid exploration of alternative neural network architectures for problems in computational biology.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>All the models analyzed are available at http://cnn.csail.mit.edu</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>gifford@mit.edu</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27307608,pubmed,2016,6f605dcd-ac54-411e-8a52-73eb0886510e,1
automated detection of pulmonary nodules in pet/ct images: ensemble false-positive reduction using a convolutional neural network technique,/pubmed/27277030,"Teramoto A, Fujita H, Yamamuro O, Tamaki T.",Med Phys. 2016 Jun;43(6):2821-2827. doi: 10.1118/1.4948498.,Med Phys.  2016,PubMed,citation,PMID:27277030,pubmed,27277030,create date:2016/06/10 | first author:Teramoto A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Automated detection of solitary pulmonary nodules using positron emission tomography (PET) and computed tomography (CT) images shows good sensitivity; however, it is difficult to detect nodules in contact with normal organs, and additional efforts are needed so that the number of false positives (FPs) can be further reduced. In this paper, the authors propose an improved FP-reduction method for the detection of pulmonary nodules in PET/CT images by means of convolutional neural networks (CNNs).</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The overall scheme detects pulmonary nodules using both CT and PET images. In the CT images, a massive region is first detected using an active contour filter, which is a type of contrast enhancement filter that has a deformable kernel shape. Subsequently, high-uptake regions detected by the PET images are merged with the regions detected by the CT images. FP candidates are eliminated using an ensemble method; it consists of two feature extractions, one by shape/metabolic feature analysis and the other by a CNN, followed by a two-step classifier, one step being rule based and the other being based on support vector machines.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The authors evaluated the detection performance using 104 PET/CT images collected by a cancer-screening program. The sensitivity in detecting candidates at an initial stage was 97.2%, with 72.8 FPs/case. After performing the proposed FP-reduction method, the sensitivity of detection was 90.1%, with 4.9 FPs/case; the proposed method eliminated approximately half the FPs existing in the previous study.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>An improved FP-reduction scheme using CNN technique has been developed for the detection of pulmonary nodules in PET/CT images. The authors' ensemble FP-reduction method eliminated 93% of the FPs; their proposed method using CNN technique eliminates approximately half the FPs existing in the previous study. These results indicate that their method may be useful in the computer-aided detection of pulmonary nodules using PET/CT images.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27277030,pubmed,2016,e0656273-b1ac-467e-b1bb-928fae956bfe,1
chinese herbal medicine image recognition and retrieval by convolutional neural network,/pubmed/27258404,"Sun X, Qian H.",PLoS One. 2016 Jun 3;11(6):e0156327. doi: 10.1371/journal.pone.0156327. eCollection 2016.,PLoS One.  2016,PubMed,citation,PMID:27258404 | PMCID:PMC4892594,pubmed,27258404,create date:2016/06/04 | first author:Sun X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Chinese herbal medicine image recognition and retrieval have great potential of practical applications. Several previous studies have focused on the recognition with hand-crafted image features, but there are two limitations in them. Firstly, most of these hand-crafted features are low-level image representation, which is easily affected by noise and background. Secondly, the medicine images are very clean without any backgrounds, which makes it difficult to use in practical applications. Therefore, designing high-level image representation for recognition and retrieval in real world medicine images is facing a great challenge. Inspired by the recent progress of deep learning in computer vision, we realize that deep learning methods may provide robust medicine image representation. In this paper, we propose to use the Convolutional Neural Network (CNN) for Chinese herbal medicine image recognition and retrieval. For the recognition problem, we use the softmax loss to optimize the recognition network; then for the retrieval problem, we fine-tune the recognition network by adding a triplet loss to search for the most similar medicine images. To evaluate our method, we construct a public database of herbal medicine images with cluttered backgrounds, which has in total 5523 images with 95 popular Chinese medicine categories. Experimental results show that our method can achieve the average recognition precision of 71% and the average retrieval precision of 53% over all the 95 medicine categories, which are quite promising given the fact that the real world images have multiple pieces of occluded herbal and cluttered backgrounds. Besides, our proposed method achieves the state-of-the-art performance by improving previous studies with a large margin. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27258404,pubmed,2016,be8b8547-c11a-4198-9708-cda94434a627,1
detection of lobular structures in normal breast tissue,/pubmed/27209271,"Apou G, Schaadt NS, Naegel B, Forestier G, Schönmeyer R, Feuerhake F, Wemmert C, Grote A.",Comput Biol Med. 2016 Jul 1;74:91-102. doi: 10.1016/j.compbiomed.2016.05.004. Epub 2016 May 11.,Comput Biol Med.  2016,PubMed,citation,PMID:27209271,pubmed,27209271,create date:2016/05/23 | first author:Apou G,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Ongoing research into inflammatory conditions raises an increasing need to evaluate immune cells in histological sections in biologically relevant regions of interest (ROIs). Herein, we compare different approaches to automatically detect lobular structures in human normal breast tissue in digitized whole slide images (WSIs). This automation is required to perform objective and consistent quantitative studies on large data sets.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>In normal breast tissue from nine healthy patients immunohistochemically stained for different markers, we evaluated and compared three different image analysis methods to automatically detect lobular structures in WSIs: (1) a bottom-up approach using the cell-based data for subsequent tissue level classification, (2) a top-down method starting with texture classification at tissue level analysis of cell densities in specific ROIs, and (3) a direct texture classification using deep learning technology.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>All three methods result in comparable overall quality allowing automated detection of lobular structures with minor advantage in sensitivity (approach 3), specificity (approach 2), or processing time (approach 1). Combining the outputs of the approaches further improved the precision.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Different approaches of automated ROI detection are feasible and should be selected according to the individual needs of biomarker research. Additionally, detected ROIs could be used as a basis for quantification of immune infiltration in lobular structures.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",gapou@unistra.fr,Convolutional neural network; Digital histopathology; Image analysis; Normal breast lobule; Whole slide image,https://www.ncbi.nlm.nih.gov//pubmed/27209271,pubmed,2016,48752b7a-3043-4a6f-ae1c-49f1d134b915,1
a study of the effectiveness of machine learning methods for classification of clinical interview fragments into a large number of categories,/pubmed/27185608,"Hasan M, Kotov A, Carcone A, Dong M, Naar S, Hartlieb KB.",J Biomed Inform. 2016 Aug;62:21-31. doi: 10.1016/j.jbi.2016.05.004. Epub 2016 May 13.,J Biomed Inform.  2016,PubMed,citation,PMID:27185608 | PMCID:PMC4987168,pubmed,27185608,create date:2016/05/18 | first author:Hasan M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This study examines the effectiveness of state-of-the-art supervised machine learning methods in conjunction with different feature types for the task of automatic annotation of fragments of clinical text based on codebooks with a large number of categories. We used a collection of motivational interview transcripts consisting of 11,353 utterances, which were manually annotated by two human coders as the gold standard, and experimented with state-of-art classifiers, including Naïve Bayes, J48 Decision Tree, Support Vector Machine (SVM), Random Forest (RF), AdaBoost, DiscLDA, Conditional Random Fields (CRF) and Convolutional Neural Network (CNN) in conjunction with lexical, contextual (label of the previous utterance) and semantic (distribution of words in the utterance across the Linguistic Inquiry and Word Count dictionaries) features. We found out that, when the number of classes is large, the performance of CNN and CRF is inferior to SVM. When only lexical features were used, interview transcripts were automatically annotated by SVM with the highest classification accuracy among all classifiers of 70.8%, 61% and 53.7% based on the codebooks consisting of 17, 20 and 41 codes, respectively. Using contextual and semantic features, as well as their combination, in addition to lexical ones, improved the accuracy of SVM for annotation of utterances in motivational interview transcripts with a codebook consisting of 17 classes to 71.5%, 74.2%, and 75.1%, respectively. Our results demonstrate the potential of using machine learning methods in conjunction with lexical, semantic and contextual features for automatic annotation of clinical interview transcripts with near-human accuracy. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Inc. All rights reserved.</p></div></div>",,Annotation of clinical text; Deep learning; Machine learning; Motivational interviewing; Text classification,https://www.ncbi.nlm.nih.gov//pubmed/27185608,pubmed,2016,0df48a37-73f9-4775-967a-09cc1947a448,1
empirical comparison of color normalization methods for epithelial-stromal classification in h and e images,/pubmed/27141322,"Sethi A, Sha L, Vahadane AR, Deaton RJ, Kumar N, Macias V, Gann PH.",J Pathol Inform. 2016 Apr 11;7:17. doi: 10.4103/2153-3539.179984. eCollection 2016.,J Pathol Inform.  2016,PubMed,citation,PMID:27141322 | PMCID:PMC4837797,pubmed,27141322,create date:2016/05/04 | first author:Sethi A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>CONTEXT: </h4><p><abstracttext label='CONTEXT' nlmcategory='BACKGROUND'>Color normalization techniques for histology have not been empirically tested for their utility for computational pathology pipelines.</abstracttext></p><h4>AIMS: </h4><p><abstracttext label='AIMS' nlmcategory='OBJECTIVE'>We compared two contemporary techniques for achieving a common intermediate goal - epithelial-stromal classification.</abstracttext></p><h4>SETTINGS AND DESIGN: </h4><p><abstracttext label='SETTINGS AND DESIGN' nlmcategory='METHODS'>Expert-annotated regions of epithelium and stroma were treated as ground truth for comparing classifiers on original and color-normalized images.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>Epithelial and stromal regions were annotated on thirty diverse-appearing H and E stained prostate cancer tissue microarray cores. Corresponding sets of thirty images each were generated using the two color normalization techniques. Color metrics were compared for original and color-normalized images. Separate epithelial-stromal classifiers were trained and compared on test images. Main analyses were conducted using a multiresolution segmentation (MRS) approach; comparative analyses using two other classification approaches (convolutional neural network [CNN], Wndchrm) were also performed.</abstracttext></p><h4>STATISTICAL ANALYSIS: </h4><p><abstracttext label='STATISTICAL ANALYSIS' nlmcategory='METHODS'>For the main MRS method, which relied on classification of super-pixels, the number of variables used was reduced using backward elimination without compromising accuracy, and test - area under the curves (AUCs) were compared for original and normalized images. For CNN and Wndchrm, pixel classification test-AUCs were compared.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Khan method reduced color saturation while Vahadane reduced hue variance. Super-pixel-level test-AUC for MRS was 0.010-0.025 (95% confidence interval limits ± 0.004) higher for the two normalized image sets compared to the original in the 10-80 variable range. Improvement in pixel classification accuracy was also observed for CNN and Wndchrm for color-normalized images.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Color normalization can give a small incremental benefit when a super-pixel-based classification method is used with features that perform implicit color normalization while the gain is higher for patch-based classification methods for classifying epithelium versus stroma.</abstracttext></p></div></div>",,Color normalization; computational pathology; epithelial-stromal classification,https://www.ncbi.nlm.nih.gov//pubmed/27141322,pubmed,2016,6af9794d-3cc8-4201-a501-0a08a68f0dc7,1
automatic coronary artery calcium scoring in cardiac ct angiography using paired convolutional neural networks,/pubmed/27138584,"Wolterink JM, Leiner T, de Vos BD, van Hamersvelt RW, Viergever MA, Išgum I.",Med Image Anal. 2016 Dec;34:123-136. doi: 10.1016/j.media.2016.04.004. Epub 2016 Apr 21.,Med Image Anal.  2016,PubMed,citation,PMID:27138584,pubmed,27138584,create date:2016/05/04 | first author:Wolterink JM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The amount of coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular events. CAC is clinically quantified in cardiac calcium scoring CT (CSCT), but it has been shown that cardiac CT angiography (CCTA) may also be used for this purpose. We present a method for automatic CAC quantification in CCTA. This method uses supervised learning to directly identify and quantify CAC without a need for coronary artery extraction commonly used in existing methods. The study included cardiac CT exams of 250 patients for whom both a CCTA and a CSCT scan were available. To restrict the volume-of-interest for analysis, a bounding box around the heart is automatically determined. The bounding box detection algorithm employs a combination of three ConvNets, where each detects the heart in a different orthogonal plane (axial, sagittal, coronal). These ConvNets were trained using 50 cardiac CT exams. In the remaining 200 exams, a reference standard for CAC was defined in CSCT and CCTA. Out of these, 100 CCTA scans were used for training, and the remaining 100 for evaluation of a voxel classification method for CAC identification. The method uses ConvPairs, pairs of convolutional neural networks (ConvNets). The first ConvNet in a pair identifies voxels likely to be CAC, thereby discarding the majority of non-CAC-like voxels such as lung and fatty tissue. The identified CAC-like voxels are further classified by the second ConvNet in the pair, which distinguishes between CAC and CAC-like negatives. Given the different task of each ConvNet, they share their architecture, but not their weights. Input patches are either 2.5D or 3D. The ConvNets are purely convolutional, i.e. no pooling layers are present and fully connected layers are implemented as convolutions, thereby allowing efficient voxel classification. The performance of individual 2.5D and 3D ConvPairs with input sizes of 15 and 25 voxels, as well as the performance of ensembles of these ConvPairs, were evaluated by a comparison with reference annotations in CCTA and CSCT. In all cases, ensembles of ConvPairs outperformed their individual members. The best performing individual ConvPair detected 72% of lesions in the test set, with on average 0.85 false positive (FP) errors per scan. The best performing ensemble combined all ConvPairs and obtained a sensitivity of 71% at 0.48 FP errors per scan. For this ensemble, agreement with the reference mass score in CSCT was excellent (ICC 0.944 [0.918-0.962]). Aditionally, based on the Agatston score in CCTA, this ensemble assigned 83% of patients to the same cardiovascular risk category as reference CSCT. In conclusion, CAC can be accurately automatically identified and quantified in CCTA using the proposed pattern recognition method. This might obviate the need to acquire a dedicated CSCT scan for CAC scoring, which is regularly acquired prior to a CCTA, and thus reduce the CT radiation dose received by patients.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",j.m.wolterink@umcutrecht.nl,Automatic calcium scoring; Cardiac CT angiography; Convolutional neural network; Coronary artery calcifications,https://www.ncbi.nlm.nih.gov//pubmed/27138584,pubmed,2016,623758ef-ee59-4aac-a7f1-7e903f450a0d,1
raptorx-property: a web server for protein structure property prediction,/pubmed/27112573,"Wang S, Li W, Liu S, Xu J.",Nucleic Acids Res. 2016 Jul 8;44(W1):W430-5. doi: 10.1093/nar/gkw306. Epub 2016 Apr 25.,Nucleic Acids Res.  2016,PubMed,citation,PMID:27112573 | PMCID:PMC4987890,pubmed,27112573,create date:2016/04/27 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>RaptorX Property (http://raptorx2.uchicago.edu/StructurePropertyPred/predict/) is a web server predicting structure property of a protein sequence without using any templates. It outperforms other servers, especially for proteins without close homologs in PDB or with very sparse sequence profile (i.e. carries little evolutionary information). This server employs a powerful in-house deep learning model DeepCNF (Deep Convolutional Neural Fields) to predict secondary structure (SS), solvent accessibility (ACC) and disorder regions (DISO). DeepCNF not only models complex sequence-structure relationship by a deep hierarchical architecture, but also interdependency between adjacent property labels. Our experimental results show that, tested on CASP10, CASP11 and the other benchmarks, this server can obtain ∼84% Q3 accuracy for 3-state SS, ∼72% Q8 accuracy for 8-state SS, ∼66% Q3 accuracy for 3-state solvent accessibility, and ∼0.89 area under the ROC curve (AUC) for disorder prediction. </abstracttext></p><p class='copyright'>© The Author(s) 2016. Published by Oxford University Press on behalf of Nucleic Acids Research.</p></div></div>",wangsheng@uchicago.edu,,https://www.ncbi.nlm.nih.gov//pubmed/27112573,pubmed,2016,b69adb11-0f0b-4915-be8c-e67732dbd699,1
danq: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences,/pubmed/27084946,"Quang D, Xie X.",Nucleic Acids Res. 2016 Jun 20;44(11):e107. doi: 10.1093/nar/gkw226. Epub 2016 Apr 15.,Nucleic Acids Res.  2016,PubMed,citation,PMID:27084946 | PMCID:PMC4914104,pubmed,27084946,create date:2016/04/17 | first author:Quang D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Modeling the properties and functions of DNA sequences is an important, but challenging task in the broad field of genomics. This task is particularly difficult for non-coding DNA, the vast majority of which is still poorly understood in terms of function. A powerful predictive model for the function of non-coding DNA can have enormous benefit for both basic science and translational research because over 98% of the human genome is non-coding and 93% of disease-associated variants lie in these regions. To address this need, we propose DanQ, a novel hybrid convolutional and bi-directional long short-term memory recurrent neural network framework for predicting non-coding function de novo from sequence. In the DanQ model, the convolution layer captures regulatory motifs, while the recurrent layer captures long-term dependencies between the motifs in order to learn a regulatory 'grammar' to improve predictions. DanQ improves considerably upon other models across several metrics. For some regulatory markers, DanQ can achieve over a 50% relative improvement in the area under the precision-recall curve metric compared to related models. We have made the source code available at the github repository http://github.com/uci-cbcl/DanQ. </abstracttext></p><p class='copyright'>© The Author(s) 2016. Published by Oxford University Press on behalf of Nucleic Acids Research.</p></div></div>",xhx@uci.edu,,https://www.ncbi.nlm.nih.gov//pubmed/27084946,pubmed,2016,c923273c-68e7-43d4-9e42-2529b1d46bed,1
multiscale cnns for brain tumor segmentation and diagnosis,/pubmed/27069501,"Zhao L, Jia K.",Comput Math Methods Med. 2016;2016:8356294. doi: 10.1155/2016/8356294. Epub 2016 Mar 16.,Comput Math Methods Med.  2016,PubMed,citation,PMID:27069501 | PMCID:PMC4812495,pubmed,27069501,create date:2016/04/14 | first author:Zhao L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Early brain tumor detection and diagnosis are critical to clinics. Thus segmentation of focused tumor area needs to be accurate, efficient, and robust. In this paper, we propose an automatic brain tumor segmentation method based on Convolutional Neural Networks (CNNs). Traditional CNNs focus only on local features and ignore global region features, which are both important for pixel classification and recognition. Besides, brain tumor can appear in any place of the brain and be any size and shape in patients. We design a three-stream framework named as multiscale CNNs which could automatically detect the optimum top-three scales of the image sizes and combine information from different scales of the regions around that pixel. Datasets provided by Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized by MICCAI 2013 are utilized for both training and testing. The designed multiscale CNNs framework also combines multimodal features from T1, T1-enhanced, T2, and FLAIR MRI images. By comparison with traditional CNNs and the best two methods in BRATS 2012 and 2013, our framework shows advances in brain tumor segmentation accuracy and robustness. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27069501,pubmed,2016,3cf7916a-bbdc-4746-a7f3-d26db165cd6d,1
automatic segmentation of mr brain images with a convolutional neural network,/pubmed/27046893,"Moeskops P, Viergever MA, Mendrik AM, de Vries LS, Benders MJ, Isgum I.",IEEE Trans Med Imaging. 2016 May;35(5):1252-1261. doi: 10.1109/TMI.2016.2548501. Epub 2016 Mar 30.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:27046893,pubmed,27046893,create date:2016/04/06 | first author:Moeskops P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T<sub>2</sub>-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T<sub>2</sub>-weighted images of preterm infants acquired at 40 weeks PMA, axial T<sub>1</sub>-weighted images of ageing adults acquired at an average age of 70 years, and T<sub>1</sub>-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27046893,pubmed,2016,d2d09d32-ea01-4ac7-8b56-060ecd723b7f,1
urinary bladder segmentation in ct urography using deep-learning convolutional neural network and level sets,/pubmed/27036584,"Cha KH, Hadjiiski L, Samala RK, Chan HP, Caoili EM, Cohan RH.",Med Phys. 2016 Apr;43(4):1882. doi: 10.1118/1.4944498.,Med Phys.  2016,PubMed,citation,PMID:27036584 | PMCID:PMC4808067,pubmed,27036584,create date:2016/04/03 | first author:Cha KH,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>The authors are developing a computerized system for bladder segmentation in CT urography (CTU) as a critical component for computer-aided detection of bladder cancer.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A deep-learning convolutional neural network (DL-CNN) was trained to distinguish between the inside and the outside of the bladder using 160 000 regions of interest (ROI) from CTU images. The trained DL-CNN was used to estimate the likelihood of an ROI being inside the bladder for ROIs centered at each voxel in a CTU case, resulting in a likelihood map. Thresholding and hole-filling were applied to the map to generate the initial contour for the bladder, which was then refined by 3D and 2D level sets. The segmentation performance was evaluated using 173 cases: 81 cases in the training set (42 lesions, 21 wall thickenings, and 18 normal bladders) and 92 cases in the test set (43 lesions, 36 wall thickenings, and 13 normal bladders). The computerized segmentation accuracy using the DL likelihood map was compared to that using a likelihood map generated by Haar features and a random forest classifier, and that using our previous conjoint level set analysis and segmentation system (CLASS) without using a likelihood map. All methods were evaluated relative to the 3D hand-segmented reference contours.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>With DL-CNN-based likelihood map and level sets, the average volume intersection ratio, average percent volume error, average absolute volume error, average minimum distance, and the Jaccard index for the test set were 81.9% ± 12.1%, 10.2% ± 16.2%, 14.0% ± 13.0%, 3.6 ± 2.0 mm, and 76.2% ± 11.8%, respectively. With the Haar-feature-based likelihood map and level sets, the corresponding values were 74.3% ± 12.7%, 13.0% ± 22.3%, 20.5% ± 15.7%, 5.7 ± 2.6 mm, and 66.7% ± 12.6%, respectively. With our previous CLASS with local contour refinement (LCR) method, the corresponding values were 78.0% ± 14.7%, 16.5% ± 16.8%, 18.2% ± 15.0%, 3.8 ± 2.3 mm, and 73.9% ± 13.5%, respectively.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The authors demonstrated that the DL-CNN can overcome the strong boundary between two regions that have large difference in gray levels and provides a seamless mask to guide level set segmentation, which has been a problem for many gradient-based segmentation methods. Compared to our previous CLASS with LCR method, which required two user inputs to initialize the segmentation, DL-CNN with level sets achieved better segmentation performance while using a single user input. Compared to the Haar-feature-based likelihood map, the DL-CNN-based likelihood map could guide the level sets to achieve better segmentation. The results demonstrate the feasibility of our new approach of using DL-CNN in combination with level sets for segmentation of the bladder.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27036584,pubmed,2016,c34e8096-858a-49a3-a7b9-efda328dcb07,1
convolutional neural networks for medical image analysis: full training or fine tuning?,/pubmed/26978662,"Tajbakhsh N, Shin JY, Gurudu SR, Hurst RT, Kendall CB, Gotway MB, Jianming Liang.",IEEE Trans Med Imaging. 2016 May;35(5):1299-1312. doi: 10.1109/TMI.2016.2535302. Epub 2016 Mar 7.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26978662,pubmed,26978662,create date:2016/03/16 | first author:Tajbakhsh N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26978662,pubmed,2016,80f4cee4-bef3-4be0-a21b-014d7248bf8d,1
brain tumor segmentation using convolutional neural networks in mri images,/pubmed/26960222,"Pereira S, Pinto A, Alves V, Silva CA.",IEEE Trans Med Imaging. 2016 May;35(5):1240-1251. doi: 10.1109/TMI.2016.2538465. Epub 2016 Mar 4.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26960222,pubmed,26960222,create date:2016/03/10 | first author:Pereira S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 ×3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26960222,pubmed,2016,a784547b-f7f0-4977-8dd7-fd7b822f2203,1
lung pattern classification for interstitial lung diseases using a deep convolutional neural network,/pubmed/26955021,"Anthimopoulos M, Christodoulidis S, Ebner L, Christe A, Mougiakakou S.",IEEE Trans Med Imaging. 2016 May;35(5):1207-1216. doi: 10.1109/TMI.2016.2535865. Epub 2016 Feb 29.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26955021,pubmed,26955021,create date:2016/03/10 | first author:Anthimopoulos M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26955021,pubmed,2016,59422cd9-e570-4658-89ba-ebffbc6432d2,1
single-cell phenotype classification using deep convolutional neural networks,/pubmed/26950929,"Dürr O, Sick B.",J Biomol Screen. 2016 Oct;21(9):998-1003. doi: 10.1177/1087057116631284. Epub 2016 Feb 12.,J Biomol Screen.  2016,PubMed,citation,PMID:26950929,pubmed,26950929,create date:2016/03/08 | first author:Dürr O,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning methods are currently outperforming traditional state-of-the-art computer vision algorithms in diverse applications and recently even surpassed human performance in object recognition. Here we demonstrate the potential of deep learning methods to high-content screening-based phenotype classification. We trained a deep learning classifier in the form of convolutional neural networks with approximately 40,000 publicly available single-cell images from samples treated with compounds from four classes known to lead to different phenotypes. The input data consisted of multichannel images. The construction of appropriate feature definitions was part of the training and carried out by the convolutional network, without the need for expert knowledge or handcrafted features. We compare our results against the recent state-of-the-art pipeline in which predefined features are extracted from each cell using specialized software and then fed into various machine learning algorithms (support vector machine, Fisher linear discriminant, random forest) for classification. The performance of all classification approaches is evaluated on an untouched test image set with known phenotype classes. Compared to the best reference machine learning algorithm, the misclassification rate is reduced from 8.9% to 6.6%. </abstracttext></p><p class='copyright'>© 2016 Society for Laboratory Automation and Screening.</p></div></div>",oliver.duerr@zhaw.ch,cell-based assays; deep learning; high-content screening; single-cell classification,https://www.ncbi.nlm.nih.gov//pubmed/26950929,pubmed,2016,7d6cb79f-2295-42f9-93ba-a85370548134,1
drug-drug interaction extraction via convolutional neural networks,/pubmed/26941831,"Liu S, Tang B, Chen Q, Wang X.",Comput Math Methods Med. 2016;2016:6918381. doi: 10.1155/2016/6918381. Epub 2016 Jan 31.,Comput Math Methods Med.  2016,PubMed,citation,PMID:26941831 | PMCID:PMC4752975,pubmed,26941831,create date:2016/03/05 | first author:Liu S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Drug-drug interaction (DDI) extraction as a typical relation extraction task in natural language processing (NLP) has always attracted great attention. Most state-of-the-art DDI extraction systems are based on support vector machines (SVM) with a large number of manually defined features. Recently, convolutional neural networks (CNN), a robust machine learning method which almost does not need manually defined features, has exhibited great potential for many NLP tasks. It is worth employing CNN for DDI extraction, which has never been investigated. We proposed a CNN-based method for DDI extraction. Experiments conducted on the 2013 DDIExtraction challenge corpus demonstrate that CNN is a good choice for DDI extraction. The CNN-based DDI extraction method achieves an F-score of 69.75%, which outperforms the existing best performing method by 2.75%. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26941831,pubmed,2016,cd5f0b55-3dea-430a-8e7b-ddbb3efcaae0,1
using goal-driven deep learning models to understand sensory cortex,/pubmed/26906502,"Yamins DL, DiCarlo JJ.",Nat Neurosci. 2016 Mar;19(3):356-65. doi: 10.1038/nn.4244. Review.,Nat Neurosci.  2016,PubMed,citation,PMID:26906502,pubmed,26906502,create date:2016/02/26 | first author:Yamins DL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Fueled by innovation in the computer vision and artificial intelligence communities, recent developments in computational neuroscience have used goal-driven hierarchical convolutional neural networks (HCNNs) to make strides in modeling neural single-unit and population responses in higher visual cortical areas. In this Perspective, we review the recent progress in a broader modeling context and describe some of the key technical innovations that have supported it. We then outline how the goal-driven HCNN approach can be used to delve even more deeply into understanding the development and organization of sensory cortical processing. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26906502,pubmed,2016,e7d9a13a-0ea2-40f0-a751-df3aab3ed655,1
aggnet: deep learning from crowds for mitosis detection in breast cancer histology images,/pubmed/26891484,"Albarqouni S, Baur C, Achilles F, Belagiannis V, Demirci S, Navab N.",IEEE Trans Med Imaging. 2016 May;35(5):1313-21. doi: 10.1109/TMI.2016.2528120. Epub 2016 Feb 11.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26891484,pubmed,26891484,create date:2016/02/19 | first author:Albarqouni S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26891484,pubmed,2016,be62f51d-9c7e-4995-8f95-eba3da043802,1
hep-2 cell image classification with deep convolutional neural networks,/pubmed/26887016,"Gao Z, Wang L, Zhou L, Zhang J.",IEEE J Biomed Health Inform. 2017 Mar;21(2):416-428. doi: 10.1109/JBHI.2016.2526603. Epub 2016 Feb 8.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:26887016,pubmed,26887016,create date:2016/02/18 | first author:Gao Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Efficient Human Epithelial-2 cell image classification can facilitate the diagnosis of many autoimmune diseases. This paper proposes an automatic framework for this classification task, by utilizing the deep convolutional neural networks (CNNs) which have recently attracted intensive attention in visual recognition. In addition to describing the proposed classification framework, this paper elaborates several interesting observations and findings obtained by our investigation. They include the important factors that impact network design and training, the role of rotation-based data augmentation for cell images, the effectiveness of cell image masks for classification, and the adaptability of the CNN-based classification system across different datasets. Extensive experimental study is conducted to verify the above findings and compares the proposed framework with the well-established image classification models in the literature. The results on benchmark datasets demonstrate that 1) the proposed framework can effectively outperform existing models by properly applying data augmentation, 2) our CNN-based framework has excellent adaptability across different datasets, which is highly desirable for cell image classification under varying laboratory settings. Our system is ranked high in the cell image classification competition hosted by ICPR 2014.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26887016,pubmed,2017,9fd1072f-ef26-454d-b440-f50a69aff346,1
deep 3d convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation,/pubmed/26886978,"Brosch T, Tang LY, Youngjin Yoo, Li DK, Traboulsee A, Tam R.",IEEE Trans Med Imaging. 2016 May;35(5):1229-1239. doi: 10.1109/TMI.2016.2528821. Epub 2016 Feb 11.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26886978,pubmed,26886978,create date:2016/02/18 | first author:Brosch T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26886978,pubmed,2016,b27bb781-a223-4132-b8f6-841b39121c86,1
"deep convolutional neural networks for computer-aided detection: cnn architectures, dataset characteristics and transfer learning",/pubmed/26886976,"Shin HC, Roth HR, Gao M, Lu L, Xu Z, Nogues I, Yao J, Mollura D, Summers RM.",IEEE Trans Med Imaging. 2016 May;35(5):1285-98. doi: 10.1109/TMI.2016.2528162. Epub 2016 Feb 11.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26886976 | PMCID:PMC4890616,pubmed,26886976,create date:2016/02/18 | first author:Shin HC,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26886976,pubmed,2016,bb14eaf1-4e07-4aef-9b60-5301e0ddbee0,1
automatic detection of cerebral microbleeds from mr images via 3d convolutional neural networks,/pubmed/26886975,"Qi Dou, Hao Chen, Lequan Yu, Lei Zhao, Jing Qin, Defeng Wang, Mok VC, Lin Shi, Pheng-Ann Heng.",IEEE Trans Med Imaging. 2016 May;35(5):1182-1195. doi: 10.1109/TMI.2016.2528129. Epub 2016 Feb 11.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26886975,pubmed,26886975,create date:2016/02/18 | first author:Qi Dou,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26886975,pubmed,2016,d70b6921-40fd-4dc2-91d7-2ce6add7b53b,1
fast convolutional neural network training using selective data sampling: application to hemorrhage detection in color fundus images,/pubmed/26886969,"van Grinsven MJ, van Ginneken B, Hoyng CB, Theelen T, Sanchez CI.",IEEE Trans Med Imaging. 2016 May;35(5):1273-1284. doi: 10.1109/TMI.2016.2526689. Epub 2016 Feb 8.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26886969,pubmed,26886969,create date:2016/02/18 | first author:van Grinsven MJ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26886969,pubmed,2016,9af70ed9-4fcf-44e3-9d80-429e78b3185b,1
combining generative and discriminative representation learning for lung ct analysis with convolutional restricted boltzmann machines,/pubmed/26886968,"van Tulder G, de Bruijne M.",IEEE Trans Med Imaging. 2016 May;35(5):1262-1272. doi: 10.1109/TMI.2016.2526687. Epub 2016 Feb 8.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26886968,pubmed,26886968,create date:2016/02/18 | first author:van Tulder G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26886968,pubmed,2016,8c7c149a-3611-4d13-bbe5-bb0eea97cfab,1
locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images,/pubmed/26863654,"Sirinukunwattana K, Ahmed Raza SE, Yee-Wah Tsang, Snead DR, Cree IA, Rajpoot NM.",IEEE Trans Med Imaging. 2016 May;35(5):1196-1206. doi: 10.1109/TMI.2016.2525803. Epub 2016 Feb 4.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26863654,pubmed,26863654,create date:2016/02/11 | first author:Sirinukunwattana K,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26863654,pubmed,2016,8a08cd39-5c89-4959-a407-5f0a6e0b28b7,1
multi-instance deep learning: discover discriminative local anatomies for bodypart recognition,/pubmed/26863652,"Zhennan Yan, Yiqiang Zhan, Zhigang Peng, Shu Liao, Shinagawa Y, Shaoting Zhang, Metaxas DN, Xiang Sean Zhou.",IEEE Trans Med Imaging. 2016 May;35(5):1332-1343. doi: 10.1109/TMI.2016.2524985. Epub 2016 Feb 3.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26863652,pubmed,26863652,create date:2016/02/11 | first author:Zhennan Yan,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. 'Bodypart identity' of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26863652,pubmed,2016,0ff3cac4-c8b1-4532-88b3-47656d7ec003,1
a cnn regression approach for real-time 2d/3d registration,/pubmed/26829785,"Shun Miao, Wang ZJ, Rui Liao.",IEEE Trans Med Imaging. 2016 May;35(5):1352-1363. doi: 10.1109/TMI.2016.2521800. Epub 2016 Jan 26.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26829785,pubmed,26829785,create date:2016/02/02 | first author:Shun Miao,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26829785,pubmed,2016,dba3fd4b-a9fb-412d-88c0-e70f8f1743aa,1
representation learning for mammography mass lesion classification with convolutional neural networks,/pubmed/26826901,"Arevalo J, González FA, Ramos-Pollán R, Oliveira JL, Guevara Lopez MA.",Comput Methods Programs Biomed. 2016 Apr;127:248-57. doi: 10.1016/j.cmpb.2015.12.014. Epub 2016 Jan 7.,Comput Methods Programs Biomed.  2016,PubMed,citation,PMID:26826901,pubmed,26826901,create date:2016/02/02 | first author:Arevalo J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND AND OBJECTIVE: </h4><p><abstracttext label='BACKGROUND AND OBJECTIVE' nlmcategory='OBJECTIVE'>The automatic classification of breast imaging lesions is currently an unsolved problem. This paper describes an innovative representation learning framework for breast cancer diagnosis in mammography that integrates deep learning techniques to automatically learn discriminative features avoiding the design of specific hand-crafted image-based feature detectors.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A new biopsy proven benchmarking dataset was built from 344 breast cancer patients' cases containing a total of 736 film mammography (mediolateral oblique and craniocaudal) views, representative of manually segmented lesions associated with masses: 426 benign lesions and 310 malignant lesions. The developed method comprises two main stages: (i) preprocessing to enhance image details and (ii) supervised training for learning both the features and the breast imaging lesions classifier. In contrast to previous works, we adopt a hybrid approach where convolutional neural networks are used to learn the representation in a supervised way instead of designing particular descriptors to explain the content of mammography images.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Experimental results using the developed benchmarking breast cancer dataset demonstrated that our method exhibits significant improved performance when compared to state-of-the-art image descriptors, such as histogram of oriented gradients (HOG) and histogram of the gradient divergence (HGD), increasing the performance from 0.787 to 0.822 in terms of the area under the ROC curve (AUC). Interestingly, this model also outperforms a set of hand-crafted features that take advantage of additional information from segmentation by the radiologist. Finally, the combination of both representations, learned and hand-crafted, resulted in the best descriptor for mass lesion classification, obtaining 0.826 in the AUC score.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>A novel deep learning based framework to automatically address classification of breast mass lesions in mammography was developed.</abstracttext></p><p class='copyright'>Copyright © 2015 Elsevier Ireland Ltd. All rights reserved.</p></div></div>",jearevaloo@unal.edu.co,Breast cancer; Computer-aided diagnosis; Convolutional neural networks; Feature learning; Mammography,https://www.ncbi.nlm.nih.gov//pubmed/26826901,pubmed,2016,64fd830f-c188-4ee1-bcdb-3e37f308c7f1,1
kinetic energy of hydrocarbons as a function of electron density and convolutional neural networks,/pubmed/26812530,"Yao K, Parkhill J.",J Chem Theory Comput. 2016 Mar 8;12(3):1139-47. doi: 10.1021/acs.jctc.5b01011. Epub 2016 Feb 8.,J Chem Theory Comput.  2016,PubMed,citation,PMID:26812530,pubmed,26812530,create date:2016/01/27 | first author:Yao K,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We demonstrate a convolutional neural network trained to reproduce the Kohn-Sham kinetic energy of hydrocarbons from an input electron density. The output of the network is used as a nonlocal correction to conventional local and semilocal kinetic functionals. We show that this approximation qualitatively reproduces Kohn-Sham potential energy surfaces when used with conventional exchange correlation functionals. The density which minimizes the total energy given by the functional is examined in detail. We identify several avenues to improve on this exploratory work, by reducing numerical noise and changing the structure of our functional. Finally we examine the features in the density learned by the neural network to anticipate the prospects of generalizing these models. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26812530,pubmed,2016,b9a8493e-32aa-43de-b3f9-77aeba1b747b,1
deep mri brain extraction: a 3d convolutional neural network for skull stripping,/pubmed/26808333,"Kleesiek J, Urban G, Hubert A, Schwarz D, Maier-Hein K, Bendszus M, Biller A.",Neuroimage. 2016 Apr 1;129:460-469. doi: 10.1016/j.neuroimage.2016.01.024. Epub 2016 Jan 22.,Neuroimage.  2016,PubMed,citation,PMID:26808333,pubmed,26808333,create date:2016/01/26 | first author:Kleesiek J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Brain extraction from magnetic resonance imaging (MRI) is crucial for many neuroimaging workflows. Current methods demonstrate good results on non-enhanced T1-weighted images, but struggle when confronted with other modalities and pathologically altered tissue. In this paper we present a 3D convolutional deep learning architecture to address these shortcomings. In contrast to existing methods, we are not limited to non-enhanced T1w images. When trained appropriately, our approach handles an arbitrary number of modalities including contrast-enhanced scans. Its applicability to MRI data, comprising four channels: non-enhanced and contrast-enhanced T1w, T2w and FLAIR contrasts, is demonstrated on a challenging clinical data set containing brain tumors (N=53), where our approach significantly outperforms six commonly used tools with a mean Dice score of 95.19. Further, the proposed method at least matches state-of-the-art performance as demonstrated on three publicly available data sets: IBSR, LPBA40 and OASIS, totaling N=135 volumes. For the IBSR (96.32) and LPBA40 (96.96) data set the convolutional neuronal network (CNN) obtains the highest average Dice scores, albeit not being significantly different from the second best performing method. For the OASIS data the second best Dice (95.02) results are achieved, with no statistical difference in comparison to the best performing tool. For all data sets the highest average specificity measures are evaluated, whereas the sensitivity displays about average results. Adjusting the cut-off threshold for generating the binary masks from the CNN's probability output can be used to increase the sensitivity of the method. Of course, this comes at the cost of a decreased specificity and has to be decided application specific. Using an optimized GPU implementation predictions can be achieved in less than one minute. The proposed method may prove useful for large-scale studies and clinical trials. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Inc. All rights reserved.</p></div></div>",kleesiek@uni-heidelberg.de,Brain extraction; Brain mask; Convolutional networks; Deep learning; MRI; Skull stripping,https://www.ncbi.nlm.nih.gov//pubmed/26808333,pubmed,2016,8d3741ae-aa9e-4764-acfa-c673c4de945e,1
protein secondary structure prediction using deep convolutional neural fields,/pubmed/26752681,"Wang S, Peng J, Ma J, Xu J.",Sci Rep. 2016 Jan 11;6:18962. doi: 10.1038/srep18962.,Sci Rep.  2016,PubMed,citation,PMID:26752681 | PMCID:PMC4707437,pubmed,26752681,create date:2016/01/12 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Protein secondary structure (SS) prediction is important for studying protein structure and function. When only the sequence (profile) information is used as input feature, currently the best predictors can obtain ~80% Q3 accuracy, which has not been improved in the past decade. Here we present DeepCNF (Deep Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep Learning extension of Conditional Neural Fields (CNF), which is an integration of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can model not only complex sequence-structure relationship by a deep hierarchical architecture, but also interdependency between adjacent SS labels, so it is much more powerful than CNF. Experimental results show that DeepCNF can obtain ~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the CASP and CAMEO test proteins, greatly outperforming currently popular predictors. As a general framework, DeepCNF can be used to predict other protein structure properties such as contact number, disorder regions, and solvent accessibility. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26752681,pubmed,2016,95edffad-fce8-4f30-9740-80f495d72da5,1
bacterial colony counting by convolutional neural networks,/pubmed/26738016,"Ferrari A, Lombardi S, Signoroni A.",Conf Proc IEEE Eng Med Biol Soc. 2015;2015:7458-61. doi: 10.1109/EMBC.2015.7320116.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26738016,pubmed,26738016,create date:2016/01/07 | first author:Ferrari A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Counting bacterial colonies on microbiological culture plates is a time-consuming, error-prone, nevertheless fundamental task in microbiology. Computer vision based approaches can increase the efficiency and the reliability of the process, but accurate counting is challenging, due to the high degree of variability of agglomerated colonies. In this paper, we propose a solution which adopts Convolutional Neural Networks (CNN) for counting the number of colonies contained in confluent agglomerates, that scored an overall accuracy of the 92.8% on a large challenging dataset. The proposed CNN-based technique for estimating the cardinality of colony aggregates outperforms traditional image processing approaches, becoming a promising approach to many related applications. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26738016,pubmed,2015,d077a6b8-a563-41c7-aa51-72f3d58ee930,1
on the use of convolutional neural networks and augmented csp features for multi-class motor imagery of eeg signals classification,/pubmed/26736829,"Yang H, Sakhavi S, Ang KK, Guan C.",Conf Proc IEEE Eng Med Biol Soc. 2015;2015:2620-3. doi: 10.1109/EMBC.2015.7318929.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736829,pubmed,26736829,create date:2016/01/07 | first author:Yang H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Learning the deep structures and unknown correlations is important for the detection of motor imagery of EEG signals (MI-EEG). This study investigates the use of convolutional neural networks (CNNs) for the classification of multi-class MI-EEG signals. Augmented common spatial pattern (ACSP) features are generated based on pair-wise projection matrices, which covers various frequency ranges. We propose a frequency complementary feature map selection (FCMS) scheme by constraining the dependency among frequency bands. Experiments are conducted on BCI competition IV dataset IIa with 9 subjects. Averaged cross-validation accuracy of 68.45% and 69.27% is achieved for FCMS and all feature maps, respectively, which is significantly higher (4.53% and 5.34%) than random map selection and higher (1.44% and 2.26%) than filter-bank CSP (FBCSP). The results demonstrate that the CNNs are capable of learning discriminant, deep structure features for EEG classification without relying on the handcrafted features. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736829,pubmed,2015,17e23771-a1f3-4e33-ae23-d9d76bdb2c3c,1
convolutional neural networks for patient-specific ecg classification,/pubmed/26736826,"Kiranyaz S, Ince T, Hamila R, Gabbouj M.",Conf Proc IEEE Eng Med Biol Soc. 2015;2015:2608-11. doi: 10.1109/EMBC.2015.7318926.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736826,pubmed,26736826,create date:2016/01/07 | first author:Kiranyaz S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system using an adaptive implementation of 1D Convolutional Neural Networks (CNNs) that can fuse feature extraction and classification into a unified learner. In this way, a dedicated CNN will be trained for each patient by using relatively small common and patient-specific training data and thus it can also be used to classify long ECG records such as Holter registers in a fast and accurate manner. Alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. The experimental results demonstrate that the proposed system achieves a superior classification performance for the detection of ventricular ectopic beats (VEB) and supraventricular ectopic beats (SVEB). </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736826,pubmed,2015,24a30bd4-310e-423c-96e3-2e2ea964e4e6,1
a unified framework for automatic wound segmentation and analysis with deep convolutional neural networks,/pubmed/26736781,"Wang C, Yan X, Smith M, Kochhar K, Rubin M, Warren SM, Wrobel J, Lee H.",Conf Proc IEEE Eng Med Biol Soc. 2015;2015:2415-8. doi: 10.1109/EMBC.2015.7318881.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736781,pubmed,26736781,create date:2016/01/07 | first author:Wang C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736781,pubmed,2015,fe17a37c-1680-4eb4-8416-f54dda7f0932,1
automatic detection of cell divisions (mitosis) in live-imaging microscopy images using convolutional neural networks,/pubmed/26736369,"Shkolyar A, Gefen A, Benayahu D, Greenspan H.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:743-6. doi: 10.1109/EMBC.2015.7318469.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736369,pubmed,26736369,create date:2016/01/07 | first author:Shkolyar A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We propose a semi-automated pipeline for the detection of possible cell divisions in live-imaging microscopy and the classification of these mitosis candidates using a Convolutional Neural Network (CNN). We use time-lapse images of NIH3T3 scratch assay cultures, extract patches around bright candidate regions that then undergo segmentation and binarization, followed by a classification of the binary patches into either containing or not containing cell division. The classification is performed by training a Convolutional Neural Network on a specially constructed database. We show strong results of AUC = 0.91 and F-score = 0.89, competitive with state-of-the-art methods in this field. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736369,pubmed,2015,cdc35476-0bec-404d-9d9f-7033deef8497,1
glaucoma detection based on deep convolutional neural network,/pubmed/26736362,"Xiangyu Chen, Yanwu Xu, Damon Wing Kee Wong, Tien Yin Wong, Jiang Liu.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:715-8. doi: 10.1109/EMBC.2015.7318462.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736362,pubmed,26736362,create date:2016/01/07 | first author:Xiangyu Chen,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736362,pubmed,2015,9ac9730f-0724-41f8-93cc-7bc7173b9ea4,1
automatic localization of the left ventricle in cardiac mri images using deep learning,/pubmed/26736354,"Emad O, Yassine IA, Fahmy AS.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:683-6. doi: 10.1109/EMBC.2015.7318454.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736354,pubmed,26736354,create date:2016/01/07 | first author:Emad O,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic localization of the left ventricle (LV) in cardiac MRI images is an essential step for automatic segmentation, functional analysis, and content based retrieval of cardiac images. In this paper, we introduce a new approach based on deep Convolutional Neural Network (CNN) to localize the LV in cardiac MRI in short axis views. A six-layer CNN with different kernel sizes was employed for feature extraction, followed by Softmax fully connected layer for classification. The pyramids of scales analysis was introduced in order to take account of the different sizes of the heart. A publically-available database of 33 patients was used for learning and testing. The proposed method was able it localize the LV with 98.66%, 83.91% and 99.07% for accuracy, sensitivity and specificity respectively. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736354,pubmed,2015,f2625c37-8398-465e-a635-30efc12e9a86,1
convolutional neural network for multi-category rapid serial visual presentation bci,/pubmed/26696875,"Manor R, Geva AB.",Front Comput Neurosci. 2015 Dec 2;9:146. doi: 10.3389/fncom.2015.00146. eCollection 2015.,Front Comput Neurosci.  2015,PubMed,citation,PMID:26696875 | PMCID:PMC4667102,pubmed,26696875,create date:2015/12/24 | first author:Manor R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Brain computer interfaces rely on machine learning (ML) algorithms to decode the brain's electrical activity into decisions. For example, in rapid serial visual presentation (RSVP) tasks, the subject is presented with a continuous stream of images containing rare target images among standard images, while the algorithm has to detect brain activity associated with target images. Here, we continue our previous work, presenting a deep neural network model for the use of single trial EEG classification in RSVP tasks. Deep neural networks have shown state of the art performance in computer vision and speech recognition and thus have great promise for other learning tasks, like classification of EEG samples. In our model, we introduce a novel spatio-temporal regularization for EEG data to reduce overfitting. We show improved classification performance compared to our earlier work on a five categories RSVP experiment. In addition, we compare performance on data from different sessions and validate the model on a public benchmark data set of a P300 speller task. Finally, we discuss the advantages of using neural network models compared to manually designing feature extraction algorithms. </abstracttext></p></div></div>",,Brain computer interface (BCI); Electroencephalography (EEG); P300; convolutional neural networks; deep learning; rapid serial visual presentation (RSVP),https://www.ncbi.nlm.nih.gov//pubmed/26696875,pubmed,2015,88a3d362-5cc3-421b-8e13-b6f438a01cc9,1
classifiers for ischemic stroke lesion segmentation: a comparison study,/pubmed/26672989,"Maier O, Schröder C, Forkert ND, Martinetz T, Handels H.",PLoS One. 2015 Dec 16;10(12):e0145118. doi: 10.1371/journal.pone.0145118. eCollection 2015. Erratum in: PLoS One. 2016;11(2):e0149828. ,PLoS One.  2015,PubMed,citation,PMID:26672989 | PMCID:PMC4687679,pubmed,26672989,create date:2015/12/18 | first author:Maier O,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION'>Ischemic stroke, triggered by an obstruction in the cerebral blood supply, leads to infarction of the affected brain tissue. An accurate and reproducible automatic segmentation is of high interest, since the lesion volume is an important end-point for clinical trials. However, various factors, such as the high variance in lesion shape, location and appearance, render it a difficult task.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS'>In this article, nine classification methods (e.g. Generalized Linear Models, Random Decision Forests and Convolutional Neural Networks) are evaluated and compared with each other using 37 multiparametric MRI datasets of ischemic stroke patients in the sub-acute phase in terms of their accuracy and reliability for ischemic stroke lesion segmentation. Within this context, a multi-spectral classification approach is compared against mono-spectral classification performance using only FLAIR MRI datasets and two sets of expert segmentations are used for inter-observer agreement evaluation.</abstracttext></p><h4>RESULTS AND CONCLUSION: </h4><p><abstracttext label='RESULTS AND CONCLUSION'>The results of this study reveal that high-level machine learning methods lead to significantly better segmentation results compared to the rather simple classification methods, pointing towards a difficult non-linear problem. The overall best segmentation results were achieved by a Random Decision Forest and a Convolutional Neural Networks classification approach, even outperforming all previously published results. However, none of the methods tested in this work are capable of achieving results in the range of the human observer agreement and the automatic ischemic stroke lesion segmentation remains a complicated problem that needs to be explored in more detail to improve the segmentation results.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26672989,pubmed,2015,514d001c-217e-4431-b61e-f4a9f2bb0cae,1
crowdsourcing the creation of image segmentation algorithms for connectomics,/pubmed/26594156,"Arganda-Carreras I, Turaga SC, Berger DR, Cireşan D, Giusti A, Gambardella LM, Schmidhuber J, Laptev D, Dwivedi S, Buhmann JM, Liu T, Seyedhosseini M, Tasdizen T, Kamentsky L, Burget R, Uher V, Tan X, Sun C, Pham TD, Bas E, Uzunbas MG, Cardona A, et al.",Front Neuroanat. 2015 Nov 5;9:142. doi: 10.3389/fnana.2015.00142. eCollection 2015.,Front Neuroanat.  2015,PubMed,citation,PMID:26594156 | PMCID:PMC4633678,pubmed,26594156,create date:2015/11/26 | first author:Arganda-Carreras I,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic (EM) images of the brain. Participants submitted boundary maps predicted for a test set of images, and were scored based on their agreement with a consensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This 'deep learning' approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from cooperation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test dataset. Retrospective evaluation of the challenge scoring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge. </abstracttext></p></div></div>",,connectomics; electron microscopy; image segmentation; machine learning; reconstruction,https://www.ncbi.nlm.nih.gov//pubmed/26594156,pubmed,2015,5ece2eee-86ad-4248-9d25-44a1a615c90a,1
deep neural networks: a new framework for modeling biological vision and brain information processing,/pubmed/28532370,Kriegeskorte N.,Annu Rev Vis Sci. 2015 Nov 24;1:417-446. doi: 10.1146/annurev-vision-082114-035447.,Annu Rev Vis Sci.  2015,PubMed,citation,PMID:28532370,pubmed,28532370,create date:2015/11/24 | first author:Kriegeskorte N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent advances in neural network modeling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.</abstracttext></p></div></div>",nikolaus.kriegeskorte@mrc-cbu.cam.ac.uk,artificial intelligence; biological vision; computational neuroscience; computer vision; deep learning; neural network; object recognition,https://www.ncbi.nlm.nih.gov//pubmed/28532370,pubmed,2015,42de714a-9f3b-4608-b2c4-dbf431ae9289,1
automatic classification of pulmonary peri-fissural nodules in computed tomography using an ensemble of 2d views and a convolutional neural network out-of-the-box,/pubmed/26458112,"Ciompi F, de Hoop B, van Riel SJ, Chung K, Scholten ET, Oudkerk M, de Jong PA, Prokop M, van Ginneken B.",Med Image Anal. 2015 Dec;26(1):195-202. doi: 10.1016/j.media.2015.08.001. Epub 2015 Sep 8.,Med Image Anal.  2015,PubMed,citation,PMID:26458112,pubmed,26458112,create date:2015/10/13 | first author:Ciompi F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we tackle the problem of automatic classification of pulmonary peri-fissural nodules (PFNs). The classification problem is formulated as a machine learning approach, where detected nodule candidates are classified as PFNs or non-PFNs. Supervised learning is used, where a classifier is trained to label the detected nodule. The classification of the nodule in 3D is formulated as an ensemble of classifiers trained to recognize PFNs based on 2D views of the nodule. In order to describe nodule morphology in 2D views, we use the output of a pre-trained convolutional neural network known as OverFeat. We compare our approach with a recently presented descriptor of pulmonary nodule morphology, namely Bag of Frequencies, and illustrate the advantages offered by the two strategies, achieving performance of AUC = 0.868, which is close to the one of human experts. </abstracttext></p><p class='copyright'>Copyright © 2015 Elsevier B.V. All rights reserved.</p></div></div>",francesco.ciompi@radboudumc.nl,Chest CT; Convolutional neural networks; Deep learning; Lung cancer screening; OverFeat; Peri-fissural nodules,https://www.ncbi.nlm.nih.gov//pubmed/26458112,pubmed,2015,9771827c-2f5a-482e-b128-481c590e6f0f,1
improving computer-aided detection using convolutional neural networks and random view aggregation,/pubmed/26441412,"Roth HR, Lu L, Liu J, Yao J, Seff A, Cherry K, Kim L, Summers RM.",IEEE Trans Med Imaging. 2016 May;35(5):1170-81. doi: 10.1109/TMI.2015.2482920. Epub 2015 Sep 28.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26441412,pubmed,26441412,create date:2015/10/07 | first author:Roth HR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities  ∼ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26441412,pubmed,2016,b5cdb989-bf80-4b37-a480-a7ff61e9cc53,1
beyond classification: structured regression for robust cell detection using convolutional neural network,/pubmed/28090601,"Xie Y, Xing F, Kong X, Su H, Yang L.",Med Image Comput Comput Assist Interv. 2015 Oct;9351:358-365. doi: 10.1007/978-3-319-24574-4_43. Epub 2015 Nov 18.,Med Image Comput Comput Assist Interv.  2015,PubMed,citation,PMID:28090601 | PMCID:PMC5226438,pubmed,28090601,create date:2015/10/01 | first author:Xie Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Robust cell detection serves as a critical prerequisite for many biomedical image analysis applications. In this paper, we present a novel convolutional neural network (CNN) based structured regression model, which is shown to be able to handle touching cells, inhomogeneous background noises, and large variations in sizes and shapes. The proposed method only requires a few training images with weak annotations (just one click near the center of the object). Given an input image patch, instead of providing a single class label like many traditional methods, our algorithm will generate the structured outputs (referred to as proximity patches). These proximity patches, which exhibit higher values for pixels near cell centers, will then be gathered from all testing image patches and fused to obtain the final proximity map, where the maximum positions indicate the cell centroids. The algorithm is tested using three data sets representing different image stains and modalities. The comparative experiments demonstrate the superior performance of this novel method over existing state-of-the-art.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28090601,pubmed,2015,89bd629f-86c6-4dda-8709-d9c441e7d984,1
deep voting: a robust approach toward nucleus localization in microscopy images,/pubmed/28083567,"Xie Y, Kong X, Xing F, Liu F, Su H, Yang L.",Med Image Comput Comput Assist Interv. 2015 Oct;9351:374-382. doi: 10.1007/978-3-319-24574-4_45. Epub 2015 Nov 18.,Med Image Comput Comput Assist Interv.  2015,PubMed,citation,PMID:28083567 | PMCID:PMC5224767,pubmed,28083567,create date:2015/10/01 | first author:Xie Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Robust and accurate nuclei localization in microscopy image can provide crucial clues for accurate computer-aid diagnosis. In this paper, we propose a convolutional neural network (CNN) based hough voting method to localize nucleus centroids with heavy cluttering and morphologic variations in microscopy images. Our method, which we name as deep voting, mainly consists of two steps. (1) Given an input image, our method assigns each local patch several pairs of voting <i>offset</i> vectors which indicate the positions it votes to, and the corresponding voting <i>confidence</i> (used to weight each votes), our model can be viewed as an implicit hough-voting codebook. (2) We collect the weighted votes from all the testing patches and compute the final voting density map in a way similar to Parzen-window estimation. The final nucleus positions are identified by searching the local maxima of the density map. Our method only requires a few annotation efforts (just one click near the nucleus center). Experiment results on Neuroendocrine Tumor (NET) microscopy images proves the proposed method to be state-of-the-art.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28083567,pubmed,2015,28fa97ee-f439-495a-beb6-bae389ce3b7e,1
an automatic learning-based framework for robust nucleus segmentation,/pubmed/26415167,"Xing F, Xie Y, Yang L.",IEEE Trans Med Imaging. 2016 Feb;35(2):550-66. doi: 10.1109/TMI.2015.2481436. Epub 2015 Sep 23.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26415167,pubmed,26415167,create date:2015/09/29 | first author:Xing F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26415167,pubmed,2016,0b9531f0-80db-48fd-80dc-14c40ba28bb3,1
predicting response to neoadjuvant chemotherapy with pet imaging using convolutional neural networks,/pubmed/26355298,"Ypsilantis PP, Siddique M, Sohn HM, Davies A, Cook G, Goh V, Montana G.",PLoS One. 2015 Sep 10;10(9):e0137036. doi: 10.1371/journal.pone.0137036. eCollection 2015.,PLoS One.  2015,PubMed,citation,PMID:26355298 | PMCID:PMC4565716,pubmed,26355298,create date:2015/09/12 | first author:Ypsilantis PP,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Imaging of cancer with 18F-fluorodeoxyglucose positron emission tomography (18F-FDG PET) has become a standard component of diagnosis and staging in oncology, and is becoming more important as a quantitative monitor of individual response to therapy. In this article we investigate the challenging problem of predicting a patient's response to neoadjuvant chemotherapy from a single 18F-FDG PET scan taken prior to treatment. We take a 'radiomics' approach whereby a large amount of quantitative features is automatically extracted from pretherapy PET images in order to build a comprehensive quantification of the tumor phenotype. While the dominant methodology relies on hand-crafted texture features, we explore the potential of automatically learning low- to high-level features directly from PET scans. We report on a study that compares the performance of two competing radiomics strategies: an approach based on state-of-the-art statistical classifiers using over 100 quantitative imaging descriptors, including texture features as well as standardized uptake values, and a convolutional neural network, 3S-CNN, trained directly from PET scans by taking sets of adjacent intra-tumor slices. Our experimental results, based on a sample of 107 patients with esophageal cancer, provide initial evidence that convolutional neural networks have the potential to extract PET imaging representations that are highly predictive of response to therapy. On this dataset, 3S-CNN achieves an average 80.7% sensitivity and 81.6% specificity in predicting non-responders, and outperforms other competing predictive models. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26355298,pubmed,2015,aa191f74-5689-4256-9484-8b6a4c5ff987,1
computer-aided classification of lung nodules on computed tomography images via deep learning technique,/pubmed/26346558,"Hua KL, Hsu CH, Hidayati SC, Cheng WH, Chen YJ.",Onco Targets Ther. 2015 Aug 4;8:2015-22. doi: 10.2147/OTT.S80733. eCollection 2015.,Onco Targets Ther.  2015,PubMed,citation,PMID:26346558 | PMCID:PMC4531007,pubmed,26346558,create date:2015/09/09 | first author:Hua KL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Lung cancer has a poor prognosis when not diagnosed early and unresectable lesions are present. The management of small lung nodules noted on computed tomography scan is controversial due to uncertain tumor characteristics. A conventional computer-aided diagnosis (CAD) scheme requires several image processing and pattern recognition steps to accomplish a quantitative tumor differentiation result. In such an ad hoc image analysis pipeline, every step depends heavily on the performance of the previous step. Accordingly, tuning of classification performance in a conventional CAD scheme is very complicated and arduous. Deep learning techniques, on the other hand, have the intrinsic advantage of an automatic exploitation feature and tuning of performance in a seamless fashion. In this study, we attempted to simplify the image analysis pipeline of conventional CAD with deep learning techniques. Specifically, we introduced models of a deep belief network and a convolutional neural network in the context of nodule classification in computed tomography images. Two baseline methods with feature computing steps were implemented for comparison. The experimental results suggest that deep learning methods could achieve better discriminative results and hold promise in the CAD application domain. </abstracttext></p></div></div>",,convolutional neural network; deep belief network; deep learning; nodule classification,https://www.ncbi.nlm.nih.gov//pubmed/26346558,pubmed,2015,1f67260b-cb82-41a1-802e-8c69829d5e12,1
convolutional neural networks for biomedical text classification: application in indexing biomedical articles,/pubmed/28736769,"Rios A, Kavuluru R.",ACM BCB. 2015 Sep;2015:258-267. doi: 10.1145/2808719.2808746.,ACM BCB.  2015,PubMed,citation,PMID:28736769 | PMCID:PMC5521984,pubmed,28736769,create date:2015/09/01 | first author:Rios A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Building high accuracy text classifiers is an important task in biomedicine given the wealth of information hidden in unstructured narratives such as research articles and clinical documents. Due to large feature spaces, traditionally, discriminative approaches such as logistic regression and support vector machines with n-gram and semantic features (e.g., named entities) have been used for text classification where additional performance gains are typically made through feature selection and ensemble approaches. In this paper, we demonstrate that a more direct approach using convolutional neural networks (CNNs) outperforms several traditional approaches in biomedical text classification with the specific use-case of assigning medical subject headings (or MeSH terms) to biomedical articles. Trained annotators at the national library of medicine (NLM) assign on an average 13 codes to each biomedical article, thus semantically indexing scientific literature to support NLM's PubMed search system. Recent evidence suggests that effective automated efforts for MeSH term assignment start with binary classifiers for each term. In this paper, we use CNNs to build binary text classifiers and achieve an absolute improvement of over 3% in macro F-score over a set of selected hard-to-classify MeSH terms when compared with the best prior results on a public dataset. Additional experiments on 50 high frequency terms in the dataset also show improvements with CNNs. Our results indicate the strong potential of CNNs in biomedical text classification tasks.</abstracttext></p></div></div>",,convolutional neural networks; medical subject headings; text classification,https://www.ncbi.nlm.nih.gov//pubmed/28736769,pubmed,2015,c1f08bd5-43f0-4adb-a5b2-a49936b48aa9,1
real-time patient-specific ecg classification by 1-d convolutional neural networks,/pubmed/26285054,"Kiranyaz S, Ince T, Gabbouj M.",IEEE Trans Biomed Eng. 2016 Mar;63(3):664-75. doi: 10.1109/TBME.2015.2468589. Epub 2015 Aug 14.,IEEE Trans Biomed Eng.  2016,PubMed,citation,PMID:26285054,pubmed,26285054,create date:2015/08/19 | first author:Kiranyaz S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>GOAL: </h4><p><abstracttext label='GOAL' nlmcategory='OBJECTIVE'>This paper presents a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>An adaptive implementation of 1-D convolutional neural networks (CNNs) is inherently used to fuse the two major blocks of the ECG classification into a single learning body: feature extraction and classification. Therefore, for each patient, an individual and simple CNN will be trained by using relatively small common and patient-specific training data, and thus, such patient-specific feature extraction ability can further improve the classification performance. Since this also negates the necessity to extract hand-crafted manual features, once a dedicated CNN is trained for a particular patient, it can solely be used to classify possibly long ECG data stream in a fast and accurate manner or alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The results over the MIT-BIH arrhythmia benchmark database demonstrate that the proposed solution achieves a superior classification performance than most of the state-of-the-art methods for the detection of ventricular ectopic beats and supraventricular ectopic beats.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Besides the speed and computational efficiency achieved, once a dedicated CNN is trained for an individual patient, it can solely be used to classify his/her long ECG records such as Holter registers in a fast and accurate manner.</abstracttext></p><h4>SIGNIFICANCE: </h4><p><abstracttext label='SIGNIFICANCE' nlmcategory='CONCLUSIONS'>Due to its simple and parameter invariant nature, the proposed system is highly generic, and, thus, applicable to any ECG dataset.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26285054,pubmed,2016,acbce314-1dc5-499c-b1f7-c0b468f467a7,1
deepcnf-d: predicting protein order/disorder regions by weighted deep convolutional neural fields,/pubmed/26230689,"Wang S, Weng S, Ma J, Tang Q.",Int J Mol Sci. 2015 Jul 29;16(8):17315-30. doi: 10.3390/ijms160817315.,Int J Mol Sci.  2015,PubMed,citation,PMID:26230689 | PMCID:PMC4581195,pubmed,26230689,create date:2015/08/01 | first author:Wang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Intrinsically disordered proteins or protein regions are involved in key biological processes including regulation of transcription, signal transduction, and alternative splicing. Accurately predicting order/disorder regions ab initio from the protein sequence is a prerequisite step for further analysis of functions and mechanisms for these disordered regions. This work presents a learning method, weighted DeepCNF (Deep Convolutional Neural Fields), to improve the accuracy of order/disorder prediction by exploiting the long-range sequential information and the interdependency between adjacent order/disorder labels and by assigning different weights for each label during training and prediction to solve the label imbalance issue. Evaluated by the CASP9 and CASP10 targets, our method obtains 0.855 and 0.898 AUC values, which are higher than the state-of-the-art single ab initio predictors. </abstracttext></p></div></div>",wangsheng@uchicago.edu,conditional neural field; deep convolutional neural network; deep learning; intrinsically disordered proteins; machine learning; prediction of disordered regions,https://www.ncbi.nlm.nih.gov//pubmed/26230689,pubmed,2015,37993baa-0432-471e-b634-172a5fa22ce8,1
multi-scale convolutional neural networks for lung nodule classification,/pubmed/26221705,"Shen W, Zhou M, Yang F, Yang C, Tian J.",Inf Process Med Imaging. 2015;24:588-99.,Inf Process Med Imaging.  2015,PubMed,citation,PMID:26221705,pubmed,26221705,create date:2015/07/30 | first author:Shen W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We investigate the problem of diagnostic lung nodule classification using thoracic Computed Tomography (CT) screening. Unlike traditional studies primarily relying on nodule segmentation for regional analysis, we tackle a more challenging problem on directly modelling raw nodule patches without any prior definition of nodule morphology. We propose a hierarchical learning framework--Multi-scale Convolutional Neural Networks (MCNN)--to capture nodule heterogeneity by extracting discriminative features from alternatingly stacked layers. In particular, to sufficiently quantify nodule characteristics, our framework utilizes multi-scale nodule patches to learn a set of class-specific features simultaneously by concatenating response neuron activations obtained at the last layer from each input scale. We evaluate the proposed method on CT images from Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), where both lung nodule screening and nodule annotations are provided. Experimental results demonstrate the effectiveness of our method on classifying malignant and benign nodules without nodule segmentation.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26221705,pubmed,2015,2d9723d8-ee0d-4678-8b6c-e843ff0667de,1
bodypart recognition using multi-stage deep learning,/pubmed/26221694,"Yan Z, Zhan Y, Peng Z, Liao S, Shinagawa Y, Metaxas DN, Zhou XS.",Inf Process Med Imaging. 2015;24:449-61.,Inf Process Med Imaging.  2015,PubMed,citation,PMID:26221694,pubmed,26221694,create date:2015/07/30 | first author:Yan Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic medical image analysis systems often start from identifying the human body part contained in the image; Specifically, given a transversal slice, it is important to know which body part it comes from, namely 'slice-based bodypart recognition'. This problem has its unique characteristic--the body part of a slice is usually identified by local discriminative regions instead of global image context, e.g., a cardiac slice is differentiated from an aorta arch slice by the mediastinum region. To leverage this characteristic, we design a multi-stage deep learning framework that aims at: (1) discover the local regions that are discriminative to the bodypart recognition, and (2) learn a bodypart identifier based on these local regions. These two tasks are achieved by the two stages of our learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative local patches from the training slices. In the boosting stage, the learned CNN is further boosted by these local patches for bodypart recognition. By exploiting the discriminative local appearances, the learned CNN becomes more accurate than global image context-based approaches. As a key hallmark, our method does not require manual annotations of the discriminative local patches. Instead, it automatically discovers them through multi-instance deep learning. We validate our method on a synthetic dataset and a large scale CT dataset (7000+ slices from wholebody CT scans). Our method achieves better performances than state-of-the-art approaches, including the standard CNN.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26221694,pubmed,2015,3e27281f-41c4-49ef-af71-684644cf71f3,1
predicting semantic descriptions from medical images with convolutional neural networks,/pubmed/26221693,"Schlegl T, Waldstein SM, Vogl WD, Schmidt-Erfurth U, Langs G.",Inf Process Med Imaging. 2015;24:437-48.,Inf Process Med Imaging.  2015,PubMed,citation,PMID:26221693,pubmed,26221693,create date:2015/07/30 | first author:Schlegl T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Learning representative computational models from medical imaging data requires large training data sets. Often, voxel-level annotation is unfeasible for sufficient amounts of data. An alternative to manual annotation, is to use the enormous amount of knowledge encoded in imaging data and corresponding reports generated during clinical routine. Weakly supervised learning approaches can link volume-level labels to image content but suffer from the typical label distributions in medical imaging data where only a small part consists of clinically relevant abnormal structures. In this paper we propose to use a semantic representation of clinical reports as a learning target that is predicted from imaging data by a convolutional neural network. We demonstrate how we can learn accurate voxel-level classifiers based on weak volume-level semantic descriptions on a set of 157 optical coherence tomography (OCT) volumes. We specifically show how semantic information increases classification accuracy for intraretinal cystoid fluid (IRC), subretinal fluid (SRF) and normal retinal tissue, and how the learning algorithm links semantic concepts to image content and geometry.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26221693,pubmed,2015,e1cae1ef-8a53-4a72-9959-5040bec60d36,1
a comprehensive computer-aided polyp detection system for colonoscopy videos,/pubmed/26221684,"Tajbakhsh N, Gurudu SR, Liang J.",Inf Process Med Imaging. 2015;24:327-38.,Inf Process Med Imaging.  2015,PubMed,citation,PMID:26221684,pubmed,26221684,create date:2015/07/30 | first author:Tajbakhsh N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer-aided detection (CAD) can help colonoscopists reduce their polyp miss-rate, but existing CAD systems are handicapped by using either shape, texture, or temporal information for detecting polyps, achieving limited sensitivity and specificity. To overcome this limitation, our key contribution of this paper is to fuse all possible polyp features by exploiting the strengths of each feature while minimizing its weaknesses. Our new CAD system has two stages, where the first stage builds on the robustness of shape features to reliably generate a set of candidates with a high sensitivity, while the second stage utilizes the high discriminative power of the computationally expensive features to effectively reduce false positives. Specifically, we employ a unique edge classifier and an original voting scheme to capture geometric features of polyps in context and then harness the power of convolutional neural networks in a novel score fusion approach to extract and combine shape, color, texture, and temporal information of the candidates. Our experimental results based on FROC curves and a new analysis of polyp detection latency demonstrate a superiority over the state-of-the-art where our system yields a lower polyp detection latency and achieves a significantly higher sensitivity while generating dramatically fewer false positives. This performance improvement is attributed to our reliable candidate generation and effective false positive reduction methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26221684,pubmed,2015,0bebdb5d-5ef7-4454-9123-464cd98639b8,1
mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features,/pubmed/26158062,"Wang H, Cruz-Roa A, Basavanhally A, Gilmore H, Shih N, Feldman M, Tomaszewski J, Gonzalez F, Madabhushi A.",J Med Imaging (Bellingham). 2014 Oct;1(3):034003. doi: 10.1117/1.JMI.1.3.034003. Epub 2014 Oct 10.,J Med Imaging (Bellingham).  2014,PubMed,citation,PMID:26158062 | PMCID:PMC4479031,pubmed,26158062,create date:2015/07/15 | first author:Wang H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Breast cancer (BCa) grading plays an important role in predicting disease aggressiveness and patient outcome. A key component of BCa grade is the mitotic count, which involves quantifying the number of cells in the process of dividing (i.e., undergoing mitosis) at a specific point in time. Currently, mitosis counting is done manually by a pathologist looking at multiple high power fields (HPFs) on a glass slide under a microscope, an extremely laborious and time consuming process. The development of computerized systems for automated detection of mitotic nuclei, while highly desirable, is confounded by the highly variable shape and appearance of mitoses. Existing methods use either handcrafted features that capture certain morphological, statistical, or textural attributes of mitoses or features learned with convolutional neural networks (CNN). Although handcrafted features are inspired by the domain and the particular application, the data-driven CNN models tend to be domain agnostic and attempt to learn additional feature bases that cannot be represented through any of the handcrafted features. On the other hand, CNN is computationally more complex and needs a large number of labeled training instances. Since handcrafted features attempt to model domain pertinent attributes and CNN approaches are largely supervised feature generation methods, there is an appeal in attempting to combine these two distinct classes of feature generation strategies to create an integrated set of attributes that can potentially outperform either class of feature extraction strategies individually. We present a cascaded approach for mitosis detection that intelligently combines a CNN model and handcrafted features (morphology, color, and texture features). By employing a light CNN model, the proposed approach is far less demanding computationally, and the cascaded strategy of combining handcrafted features and CNN-derived features enables the possibility of maximizing the performance by leveraging the disconnected feature sets. Evaluation on the public ICPR12 mitosis dataset that has 226 mitoses annotated on 35 HPFs ([Formula: see text] magnification) by several pathologists and 15 testing HPFs yielded an [Formula: see text]-measure of 0.7345. Our approach is accurate, fast, and requires fewer computing resources compared to existent methods, making this feasible for clinical use. </abstracttext></p></div></div>",,breast cancer; cascaded ensemble; convolutional neural networks; digital pathology; handcrafted feature; mitosis,https://www.ncbi.nlm.nih.gov//pubmed/26158062,pubmed,2014,4cdb282d-cac7-4bc4-aa32-4b8415db181d,1
deep adaptive log-demons: diffeomorphic image registration with very large deformations,/pubmed/26120356,"Zhao L, Jia K.",Comput Math Methods Med. 2015;2015:836202. doi: 10.1155/2015/836202. Epub 2015 May 18.,Comput Math Methods Med.  2015,PubMed,citation,PMID:26120356 | PMCID:PMC4450337,pubmed,26120356,create date:2015/06/30 | first author:Zhao L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper proposes a new framework for capturing large and complex deformation in image registration. Traditionally, this challenging problem relies firstly on a preregistration, usually an affine matrix containing rotation, scale, and translation and afterwards on a nonrigid transformation. According to preregistration, the directly calculated affine matrix, which is obtained by limited pixel information, may misregistrate when large biases exist, thus misleading following registration subversively. To address this problem, for two-dimensional (2D) images, the two-layer deep adaptive registration framework proposed in this paper firstly accurately classifies the rotation parameter through multilayer convolutional neural networks (CNNs) and then identifies scale and translation parameters separately. For three-dimensional (3D) images, affine matrix is located through feature correspondences by a triplanar 2D CNNs. Then deformation removal is done iteratively through preregistration and demons registration. By comparison with the state-of-the-art registration framework, our method gains more accurate registration results on both synthetic and real datasets. Besides, principal component analysis (PCA) is combined with correlation like Pearson and Spearman to form new similarity standards in 2D and 3D registration. Experiment results also show faster convergence speed. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26120356,pubmed,2015,b8f3ecbf-ec05-48bc-8b25-f5d01dc61144,1
automatic feature learning to grade nuclear cataracts based on deep learning,/pubmed/26080373,"Gao X, Lin S, Wong TY.",IEEE Trans Biomed Eng. 2015 Nov;62(11):2693-701. doi: 10.1109/TBME.2015.2444389. Epub 2015 Jun 11.,IEEE Trans Biomed Eng.  2015,PubMed,citation,PMID:26080373,pubmed,26080373,create date:2015/06/17 | first author:Gao X,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>GOAL: </h4><p><abstracttext label='GOAL' nlmcategory='OBJECTIVE'>Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed system is validated on a large population-based dataset of [Formula: see text] images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error ( ε) of 0.304, a 70.7% exact integral agreement ratio ( R0), an 88.4% decimal grading error ≤ 0.5 ( Re0.5 ), and a 99.0% decimal grading error ≤ 1.0 ( Re1.0 ).</abstracttext></p><h4>SIGNIFICANCE: </h4><p><abstracttext label='SIGNIFICANCE' nlmcategory='CONCLUSIONS'>The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26080373,pubmed,2015,3927f8c8-a0b9-4bd5-92a2-96f1524778e0,1
an unsupervised feature learning framework for basal cell carcinoma image analysis,/pubmed/25976208,"Arevalo J, Cruz-Roa A, Arias V, Romero E, González FA.",Artif Intell Med. 2015 Jun;64(2):131-45. doi: 10.1016/j.artmed.2015.04.004. Epub 2015 Apr 23.,Artif Intell Med.  2015,PubMed,citation,PMID:25976208,pubmed,25976208,create date:2015/05/16 | first author:Arevalo J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>The paper addresses the problem of automatic detection of basal cell carcinoma (BCC) in histopathology images. In particular, it proposes a framework to both, learn the image representation in an unsupervised way and visualize discriminative features supported by the learned model.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>This paper presents an integrated unsupervised feature learning (UFL) framework for histopathology image analysis that comprises three main stages: (1) local (patch) representation learning using different strategies (sparse autoencoders, reconstruct independent component analysis and topographic independent component analysis (TICA), (2) global (image) representation learning using a bag-of-features representation or a convolutional neural network, and (3) a visual interpretation layer to highlight the most discriminant regions detected by the model. The integrated unsupervised feature learning framework was exhaustively evaluated in a histopathology image dataset for BCC diagnosis.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The experimental evaluation produced a classification performance of 98.1%, in terms of the area under receiver-operating-characteristic curve, for the proposed framework outperforming by 7% the state-of-the-art discrete cosine transform patch-based representation.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The proposed UFL-representation-based approach outperforms state-of-the-art methods for BCC detection. Thanks to its visual interpretation layer, the method is able to highlight discriminative tissue regions providing a better diagnosis support. Among the different UFL strategies tested, TICA-learned features exhibited the best performance thanks to its ability to capture low-level invariances, which are inherent to the nature of the problem.</abstracttext></p><p class='copyright'>Copyright © 2015 Elsevier B.V. All rights reserved.</p></div></div>",jearevaloo@unal.edu.co,Basal cell carcinoma; Digital pathology; Representation learning; Unsupervised feature learning,https://www.ncbi.nlm.nih.gov//pubmed/25976208,pubmed,2015,7811fc02-097c-443d-bb26-bd45af98a5ed,1
deep convolutional neural networks for annotating gene expression patterns in the mouse brain,/pubmed/25948335,"Zeng T, Li R, Mukkamala R, Ye J, Ji S.",BMC Bioinformatics. 2015 May 7;16:147. doi: 10.1186/s12859-015-0553-9.,BMC Bioinformatics.  2015,PubMed,citation,PMID:25948335 | PMCID:PMC4432953,pubmed,25948335,create date:2015/05/08 | first author:Zeng T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Profiling gene expression in brain structures at various spatial and temporal scales is essential to understanding how genes regulate the development of brain structures. The Allen Developing Mouse Brain Atlas provides high-resolution 3-D in situ hybridization (ISH) gene expression patterns in multiple developing stages of the mouse brain. Currently, the ISH images are annotated with anatomical terms manually. In this paper, we propose a computational approach to annotate gene expression pattern images in the mouse brain at various structural levels over the course of development.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We applied deep convolutional neural network that was trained on a large set of natural images to extract features from the ISH images of developing mouse brain. As a baseline representation, we applied invariant image feature descriptors to capture local statistics from ISH images and used the bag-of-words approach to build image-level representations. Both types of features from multiple ISH image sections of the entire brain were then combined to build 3-D, brain-wide gene expression representations. We employed regularized learning methods for discriminating gene expression patterns in different brain structures. Results show that our approach of using convolutional model as feature extractors achieved superior performance in annotating gene expression patterns at multiple levels of brain structures throughout four developing ages. Overall, we achieved average AUC of 0.894 ± 0.014, as compared with 0.820 ± 0.046 yielded by the bag-of-words approach.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Deep convolutional neural network model trained on natural image sets and applied to gene expression pattern annotation tasks yielded superior performance, demonstrating its transfer learning property is applicable to such biological image sets.</abstracttext></p></div></div>",tzeng@cs.odu.edu,,https://www.ncbi.nlm.nih.gov//pubmed/25948335,pubmed,2015,37668165-a59c-40f5-b025-a22390053496,1
standard plane localization in fetal ultrasound via domain transferred deep neural networks,/pubmed/25910262,"Chen H, Ni D, Qin J, Li S, Yang X, Wang T, Heng PA.",IEEE J Biomed Health Inform. 2015 Sep;19(5):1627-36. doi: 10.1109/JBHI.2015.2425041. Epub 2015 Apr 21.,IEEE J Biomed Health Inform.  2015,PubMed,citation,PMID:25910262,pubmed,25910262,create date:2015/04/25 | first author:Chen H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25910262,pubmed,2015,3be0ed1b-1d01-4033-a755-1745203677de,1
automatic muscle perimysium annotation using deep convolutional neural network,/pubmed/28435514,"Sapkota M, Xing F, Su H, Yang L.",Proc IEEE Int Symp Biomed Imaging. 2015 Apr;2015:205-208. doi: 10.1109/ISBI.2015.7163850. Epub 2015 Jul 23.,Proc IEEE Int Symp Biomed Imaging.  2015,PubMed,citation,PMID:28435514 | PMCID:PMC5397117,pubmed,28435514,create date:2015/04/01 | first author:Sapkota M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Diseased skeletal muscle expresses mononuclear cell infiltration in the regions of perimysium. Accurate annotation or segmentation of perimysium can help biologists and clinicians to determine individualized patient treatment and allow for reasonable prognostication. However, manual perimysium annotation is time consuming and prone to inter-observer variations. Meanwhile, the presence of ambiguous patterns in muscle images significantly challenge many traditional automatic annotation algorithms. In this paper, we propose an automatic perimysium annotation algorithm based on deep convolutional neural network (CNN). We formulate the automatic annotation of perimysium in muscle images as a pixel-wise classification problem, and the CNN is trained to label each image pixel with raw RGB values of the patch centered at the pixel. The algorithm is applied to 82 diseased skeletal muscle images. We have achieved an average precision of 94% on the test dataset.</abstracttext></p></div></div>",,Perimysium annotation; convolutional neural network; muscle,https://www.ncbi.nlm.nih.gov//pubmed/28435514,pubmed,2015,05d89950-91c3-41c2-a3cb-da3d97729c8f,1
nuclei segmentation via sparsity constrained convolutional regression,/pubmed/28101301,"Zhou Y, Chang H, Barner KE, Parvin B.",Proc IEEE Int Symp Biomed Imaging. 2015 Apr;2015:1284-1287. doi: 10.1109/ISBI.2015.7164109. Epub 2015 Jul 23.,Proc IEEE Int Symp Biomed Imaging.  2015,PubMed,citation,PMID:28101301 | PMCID:PMC5239217,pubmed,28101301,create date:2015/04/01 | first author:Zhou Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated profiling of nuclear architecture, in histology sections, can potentially help predict the clinical outcomes. However, the task is challenging as a result of nuclear pleomorphism and cellular states (e.g., cell fate, cell cycle), which are compounded by the batch effect (e.g., variations in fixation and staining). Present methods, for nuclear segmentation, are based on human-designed features that may not effectively capture intrinsic nuclear architecture. In this paper, we propose a novel approach, called sparsity constrained convolutional regression (SCCR), for nuclei segmentation. Specifically, given raw image patches and the corresponding annotated binary masks, our algorithm jointly learns a bank of convolutional filters and a sparse linear regressor, where the former is used for feature extraction, and the latter aims to produce a likelihood for each pixel being nuclear region or background. During classification, the pixel label is simply determined by a thresholding operation applied on the likelihood map. The method has been evaluated using the benchmark dataset collected from The Cancer Genome Atlas (TCGA). Experimental results demonstrate that our method outperforms traditional nuclei segmentation algorithms and is able to achieve competitive performance compared to the state-of-the-art algorithm built upon human-designed features with biological prior knowledge.</abstracttext></p></div></div>",,H&E tissue section; Nuclear/Background classification; convolutional neural network; sparse coding,https://www.ncbi.nlm.nih.gov//pubmed/28101301,pubmed,2015,d34a642e-7157-46f3-a8b7-dad05b3b55da,1
deep convolutional neural networks for multi-modality isointense infant brain image segmentation,/pubmed/25562829,"Zhang W, Li R, Deng H, Wang L, Lin W, Ji S, Shen D.",Neuroimage. 2015 Mar;108:214-24. doi: 10.1016/j.neuroimage.2014.12.061. Epub 2015 Jan 3.,Neuroimage.  2015,PubMed,citation,PMID:25562829 | PMCID:PMC4323729,pubmed,25562829,create date:2015/01/07 | first author:Zhang W,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) plays an important role in studying early brain development in health and disease. In the isointense stage (approximately 6-8 months of age), WM and GM exhibit similar levels of intensity in both T1 and T2 MR images, making the tissue segmentation very challenging. Only a small number of existing methods have been designed for tissue segmentation in this isointense stage; however, they only used a single T1 or T2 images, or the combination of T1 and T2 images. In this paper, we propose to use deep convolutional neural networks (CNNs) for segmenting isointense stage brain tissues using multi-modality MR images. CNNs are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. Specifically, we used multi-modality information from T1, T2, and fractional anisotropy (FA) images as inputs and then generated the segmentation maps as outputs. The multiple intermediate layers applied convolution, pooling, normalization, and other operations to capture the highly nonlinear mappings between inputs and outputs. We compared the performance of our approach with that of the commonly used segmentation methods on a set of manually segmented isointense stage brain images. Results showed that our proposed model significantly outperformed prior methods on infant brain tissue segmentation. In addition, our results indicated that integration of multi-modality images led to significant performance improvement. </abstracttext></p><p class='copyright'>Copyright © 2014 Elsevier Inc. All rights reserved.</p></div></div>",sji@cs.odu.edu,Convolutional neural networks; Deep learning; Image segmentation; Infant brain image; Multi-modality data,https://www.ncbi.nlm.nih.gov//pubmed/25562829,pubmed,2015,1d403c1a-8d83-4459-bbf1-42350ee4a4f5,1
efficient training of convolutional deep belief networks in the frequency domain for application to high-resolution 2d and 3d images,/pubmed/25380341,"Brosch T, Tam R.",Neural Comput. 2015 Jan;27(1):211-27. doi: 10.1162/NECO_a_00682.,Neural Comput.  2015,PubMed,citation,PMID:25380341,pubmed,25380341,create date:2014/11/08 | first author:Brosch T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8 times on 2D images and up to 200 times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128×128×128 voxels practical, which opens new directions for using deep learning for medical image analysis.</abstracttext></p></div></div>",brosch.tom@gmail.com,,https://www.ncbi.nlm.nih.gov//pubmed/25380341,pubmed,2015,34f76b08-0e1b-47d8-b766-f393fa639ad2,1
a new 25d representation for lymph node detection using random sets of deep convolutional neural network observations,/pubmed/25333158,"Roth HR, Lu L, Seff A, Cherry KM, Hoffman J, Wang S, Liu J, Turkbey E, Summers RM.",Med Image Comput Comput Assist Interv. 2014;17(Pt 1):520-7.,Med Image Comput Comput Assist Interv.  2014,PubMed,citation,PMID:25333158 | PMCID:PMC4295635,pubmed,25333158,create date:2014/10/22 | first author:Roth HR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards -100% sensitivity at the cost of high FP levels (-40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25333158,pubmed,2014,b5ed1084-7dbf-448a-b112-9ae32a24ed88,1
single-trial classification of event-related potentials in rapid serial visual presentation tasks using supervised spatial filtering,/pubmed/25330426,"Cecotti H, Eckstein MP, Giesbrecht B.",IEEE Trans Neural Netw Learn Syst. 2014 Nov;25(11):2030-42. doi: 10.1109/TNNLS.2014.2302898.,IEEE Trans Neural Netw Learn Syst.  2014,PubMed,citation,PMID:25330426,pubmed,25330426,create date:2014/10/21 | first author:Cecotti H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate detection of single-trial event-related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques. Supervised spatial filtering methods that enhance the discriminative information in EEG data are commonly used to improve single-trial ERP detection. We propose a convolutional neural network (CNN) with a layer dedicated to spatial filtering for the detection of ERPs and with training based on the maximization of the area under the receiver operating characteristic curve (AUC). The CNN is compared with three common classifiers: 1) Bayesian linear discriminant analysis; 2) multilayer perceptron (MLP); and 3) support vector machines. Prior to classification, the data were spatially filtered with xDAWN (for the maximization of the signal-to-signal-plus-noise ratio), common spatial pattern, or not spatially filtered. The 12 analytical techniques were tested on EEG data recorded in three rapid serial visual presentation experiments that required the observer to discriminate rare target stimuli from frequent nontarget stimuli. Classification performance discriminating targets from nontargets depended on both the spatial filtering method and the classifier. In addition, the nonlinear classifier MLP outperformed the linear methods. Finally, training based AUC maximization provided better performance than training based on the minimization of the mean square error. The results support the conclusion that the choice of the systems architecture is critical and both spatial filtering and classification must be considered together. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25330426,pubmed,2014,fb19825a-8d59-488f-81ef-d8e520b77ed6,1
deep learning based imaging data completion for improved brain disease diagnosis,/pubmed/25320813,"Li R, Zhang W, Suk HI, Wang L, Li J, Shen D, Ji S.",Med Image Comput Comput Assist Interv. 2014;17(Pt 3):305-12.,Med Image Comput Comput Assist Interv.  2014,PubMed,citation,PMID:25320813 | PMCID:PMC4464771,pubmed,25320813,create date:2014/10/17 | first author:Li R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Combining multi-modality brain data for disease diagnosis commonly leads to improved performance. A challenge in using multimodality data is that the data are commonly incomplete; namely, some modality might be missing for some subjects. In this work, we proposed a deep learning based framework for estimating multi-modality imaging data. Our method takes the form of convolutional neural networks, where the input and output are two volumetric modalities. The network contains a large number of trainable parameters that capture the relationship between input and output modalities. When trained on subjects with all modalities, the network can estimate the output modality given the input modality. We evaluated our method on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, where the input and output modalities are MRI and PET images, respectively. Results showed that our method significantly outperformed prior methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25320813,pubmed,2014,efbcaf43-23a2-471f-8e7c-b4dadd0b103f,1
representation of semantic similarity in the left intraparietal sulcus: functional magnetic resonance imaging evidence,/pubmed/28824405,"Neyens V, Bruffaerts R, Liuzzi AG, Kalfas I, Peeters R, Keuleers E, Vogels R, De Deyne S, Storms G, Dupont P, Vandenberghe R.",Front Hum Neurosci. 2017 Aug 4;11:402. doi: 10.3389/fnhum.2017.00402. eCollection 2017.,Front Hum Neurosci.  2017,PubMed,citation,PMID:28824405 | PMCID:PMC5543089,pubmed,28824405,create date:2017/08/22 | first author:Neyens V,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>According to a recent study, semantic similarity between concrete entities correlates with the similarity of activity patterns in left middle IPS during category naming. We examined the replicability of this effect under passive viewing conditions, the potential role of visuoperceptual similarity, where the effect is situated compared to regions that have been previously implicated in visuospatial attention, and how it compares to effects of object identity and location. Forty-six subjects participated. Subjects passively viewed pictures from two categories, musical instruments and vehicles. Semantic similarity between entities was estimated based on a concept-feature matrix obtained in more than 1,000 subjects. Visuoperceptual similarity was modeled based on the HMAX model, the AlexNet deep convolutional learning model, and thirdly, based on subjective visuoperceptual similarity ratings. Among the IPS regions examined, only left middle IPS showed a semantic similarity effect. The effect was significant in hIP1, hIP2, and hIP3. Visuoperceptual similarity did not correlate with similarity of activity patterns in left middle IPS. The semantic similarity effect in left middle IPS was significantly stronger than in the right middle IPS and also stronger than in the left or right posterior IPS. The semantic similarity effect was similar to that seen in the angular gyrus. Object identity effects were much more widespread across nearly all parietal areas examined. Location effects were relatively specific for posterior IPS and area 7 bilaterally. To conclude, the current findings replicate the semantic similarity effect in left middle IPS under passive viewing conditions, and demonstrate its anatomical specificity within a cytoarchitectonic reference frame. We propose that the semantic similarity effect in left middle IPS reflects the transient uploading of semantic representations in working memory.</abstracttext></p></div></div>",,geon; intraparietal sulcus; multi-voxel pattern analysis; object identity; representational similarity analysis; semantic processing,https://www.ncbi.nlm.nih.gov//pubmed/28824405,pubmed,2017,38802e98-fc46-4fac-b434-2eb77c98daa1,1
bladder cancer treatment response assessment in ct using radiomics with deep-learning,/pubmed/28821822,"Cha KH, Hadjiiski L, Chan HP, Weizer AZ, Alva A, Cohan RH, Caoili EM, Paramagul C, Samala RK.",Sci Rep. 2017 Aug 18;7(1):8738. doi: 10.1038/s41598-017-09315-w.,Sci Rep.  2017,PubMed,citation,PMID:28821822 | PMCID:PMC5562694,pubmed,28821822,create date:2017/08/20 | first author:Cha KH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cross-sectional X-ray imaging has become the standard for staging most solid organ malignancies. However, for some malignancies such as urinary bladder cancer, the ability to accurately assess local extent of the disease and understand response to systemic chemotherapy is limited with current imaging approaches. In this study, we explored the feasibility that radiomics-based predictive models using pre- and post-treatment computed tomography (CT) images might be able to distinguish between bladder cancers with and without complete chemotherapy responses. We assessed three unique radiomics-based predictive models, each of which employed different fundamental design principles ranging from a pattern recognition method via deep-learning convolution neural network (DL-CNN), to a more deterministic radiomics feature-based approach and then a bridging method between the two, utilizing a system which extracts radiomics features from the image patterns. Our study indicates that the computerized assessment using radiomics information from the pre- and post-treatment CT of bladder cancer patients has the potential to assist in assessment of treatment response.</abstracttext></p></div></div>",heekon@med.umich.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28821822,pubmed,2017,63f96e91-99ea-4c65-bfe0-ddd8b3693b4a,1
predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker,/pubmed/28765056,"Cole JH, Poudel RPK, Tsagkrasoulis D, Caan MWA, Steves C, Spector TD, Montana G.",Neuroimage. 2017 Jul 29. pii: S1053-8119(17)30640-7. doi: 10.1016/j.neuroimage.2017.07.059. [Epub ahead of print],Neuroimage.  2017,PubMed,citation,PMID:28765056,pubmed,28765056,create date:2017/08/03 | first author:Cole JH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people. Deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of 'brain-predicted age' as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. CNN accurately predicted chronological age using GM (correlation between brain-predicted age and chronological age r = 0.96, mean absolute error [MAE] = 4.16 years) and raw (r = 0.94, MAE = 4.65 years) data. This was comparable to GPR brain-predicted age using GM data (r = 0.95, MAE = 4.66 years). Brain-predicted age was a heritable phenotype for all models and input data (h<sup>2</sup> ≥ 0.5). Brain-predicted age showed high test-retest reliability (intraclass correlation coefficient [ICC] = 0.90-0.99). Multi-centre reliability was more variable within high ICCs for GM (0.83-0.96) and poor-moderate levels for WM and raw data (0.51-0.77). Brain-predicted age represents an accurate, highly reliable and genetically-influenced phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Inc.</p></div></div>",giovanni.montana@kcl.ac.uk,Biomarker; Brain ageing; Convolutional neural networks; Deep learning; Gaussian processes; Heritability; Neuroimaging; Reliability,https://www.ncbi.nlm.nih.gov//pubmed/28765056,pubmed,2017,6514c275-0b4e-4bef-881e-01ea51fe0d2d,1
"validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge",/pubmed/28732268,"Setio AAA, Traverso A, de Bel T, Berens MSN, Bogaard CVD, Cerello P, Chen H, Dou Q, Fantacci ME, Geurts B, Gugten RV, Heng PA, Jansen B, de Kaste MMJ, Kotov V, Lin JY, Manders JTMC, Sóñora-Mengana A, García-Naranjo JC, Papavasileiou E, Prokop M, Saletta M, et al.",Med Image Anal. 2017 Jul 13;42:1-13. doi: 10.1016/j.media.2017.06.015. [Epub ahead of print],Med Image Anal.  2017,PubMed,citation,PMID:28732268,pubmed,28732268,create date:2017/07/22 | first author:Setio AAA,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",arnaud.setio@gmail.com,Computed tomography; Computer-aided detection; Convolutional networks; Deep learning; Medical image challenges; Pulmonary nodules,https://www.ncbi.nlm.nih.gov//pubmed/28732268,pubmed,2017,68f2903d-21b4-4db6-943d-a672d689c191,1
artificial intelligence for analyzing orthopedic trauma radiographs,/pubmed/28681679,"Olczak J, Fahlberg N, Maki A, Razavian AS, Jilert A, Stark A, Sköldenberg O, Gordon M.",Acta Orthop. 2017 Jul 6:1-6. doi: 10.1080/17453674.2017.1344459. [Epub ahead of print],Acta Orthop.  2017,PubMed,citation,PMID:28681679,pubmed,28681679,create date:2017/07/07 | first author:Olczak J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Background and purpose - Recent advances in artificial intelligence (deep learning) have shown remarkable performance in classifying non-medical images, and the technology is believed to be the next technological revolution. So far it has never been applied in an orthopedic setting, and in this study we sought to determine the feasibility of using deep learning for skeletal radiographs. Methods - We extracted 256,000 wrist, hand, and ankle radiographs from Danderyd's Hospital and identified 4 classes: fracture, laterality, body part, and exam view. We then selected 5 openly available deep learning networks that were adapted for these images. The most accurate network was benchmarked against a gold standard for fractures. We furthermore compared the network's performance with 2 senior orthopedic surgeons who reviewed images at the same resolution as the network. Results - All networks exhibited an accuracy of at least 90% when identifying laterality, body part, and exam view. The final accuracy for fractures was estimated at 83% for the best performing network. The network performed similarly to senior orthopedic surgeons when presented with images at the same resolution as the network. The 2 reviewer Cohen's kappa under these conditions was 0.76. Interpretation - This study supports the use for orthopedic radiographs of artificial intelligence, which can perform at a human level. While current implementation lacks important features that surgeons require, e.g. risk of dislocation, classifications, measurements, and combining multiple exam views, these problems have technical solutions that are waiting to be implemented for orthopedics.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28681679,pubmed,2017,b9d5152e-0ebf-4764-8839-2758d35fd9c1,1
deepinfer: open-source deep learning deployment toolkit for image-guided therapy,/pubmed/28615794,"Mehrtash A, Pesteie M, Hetherington J, Behringer PA, Kapur T, Wells WM 3rd, Rohling R, Fedorov A, Abolmaesumi P.",Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10135. pii: 101351K. doi: 10.1117/12.2256011. Epub 2017 Mar 3.,Proc SPIE Int Soc Opt Eng.  2017,PubMed,citation,PMID:28615794 | PMCID:PMC5467894,pubmed,28615794,create date:2017/06/16 | first author:Mehrtash A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning models have outperformed some of the previous state-of-the-art approaches in medical image analysis. Instead of using hand-engineered features, deep models attempt to automatically extract hierarchical representations at multiple levels of abstraction from the data. Therefore, deep models are usually considered to be more flexible and robust solutions for image analysis problems compared to conventional computer vision models. They have demonstrated significant improvements in computer-aided diagnosis and automatic medical image analysis applied to such tasks as image segmentation, classification and registration. However, deploying deep learning models often has a steep learning curve and requires detailed knowledge of various software packages. Thus, many deep models have not been integrated into the clinical research workflows causing a gap between the state-of-the-art machine learning in medical applications and evaluation in clinical research procedures. In this paper, we propose 'DeepInfer' - an open-source toolkit for developing and deploying deep learning models within the 3D Slicer medical image analysis platform. Utilizing a repository of task-specific models, DeepInfer allows clinical researchers and biomedical engineers to deploy a trained model selected from the public registry, and apply it to new data without the need for software development or configuration. As two practical use cases, we demonstrate the application of DeepInfer in prostate segmentation for targeted MRI-guided biopsy and identification of the target plane in 3D ultrasound for spinal injections.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28615794,pubmed,2017,170ee446-9eee-455b-b85a-b7b697948c26,1
automated diagnosis of prostate cancer in multi-parametric mri based on multimodal convolutional neural networks,/pubmed/28582269,"Le MH, Chen J, Wang L, Wang Z, Liu W, Cheng KT, Yang X.",Phys Med Biol. 2017 Jul 24;62(16):6497-6514. doi: 10.1088/1361-6560/aa7731.,Phys Med Biol.  2017,PubMed,citation,PMID:28582269,pubmed,28582269,create date:2017/06/06 | first author:Le MH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated methods for prostate cancer (PCa) diagnosis in multi-parametric magnetic resonance imaging (MP-MRIs) are critical for alleviating requirements for interpretation of radiographs while helping to improve diagnostic accuracy (Artan et al 2010 IEEE Trans. Image Process. 19 2444-55, Litjens et al 2014 IEEE Trans. Med. Imaging 33 1083-92, Liu et al 2013 SPIE Medical Imaging (International Society for Optics and Photonics) p 86701G, Moradi et al 2012 J. Magn. Reson. Imaging 35 1403-13, Niaf et al 2014 IEEE Trans. Image Process. 23 979-91, Niaf et al 2012 Phys. Med. Biol. 57 3833, Peng et al 2013a SPIE Medical Imaging (International Society for Optics and Photonics) p 86701H, Peng et al 2013b Radiology 267 787-96, Wang et al 2014 BioMed. Res. Int. 2014). This paper presents an automated method based on multimodal convolutional neural networks (CNNs) for two PCa diagnostic tasks: (1) distinguishing between cancerous and noncancerous tissues and (2) distinguishing between clinically significant (CS) and indolent PCa. Specifically, our multimodal CNNs effectively fuse apparent diffusion coefficients (ADCs) and T2-weighted MP-MRI images (T2WIs). To effectively fuse ADCs and T2WIs we design a new similarity loss function to enforce consistent features being extracted from both ADCs and T2WIs. The similarity loss is combined with the conventional classification loss functions and integrated into the back-propagation procedure of CNN training. The similarity loss enables better fusion results than existing methods as the feature learning processes of both modalities are mutually guided, jointly facilitating CNN to 'see' the true visual patterns of PCa. The classification results of multimodal CNNs are further combined with the results based on handcrafted features using a support vector machine classifier. To achieve a satisfactory accuracy for clinical use, we comprehensively investigate three critical factors which could greatly affect the performance of our multimodal CNNs but have not been carefully studied previously. (1) Given limited training data, how can these be augmented in sufficient numbers and variety for fine-tuning deep CNN networks for PCa diagnosis? (2) How can multimodal MP-MRI information be effectively combined in CNNs? (3) What is the impact of different CNN architectures on the accuracy of PCa diagnosis? Experimental results on extensive clinical data from 364 patients with a total of 463 PCa lesions and 450 identified noncancerous image patches demonstrate that our system can achieve a sensitivity of 89.85% and a specificity of 95.83% for distinguishing cancer from noncancerous tissues and a sensitivity of 100% and a specificity of 76.92% for distinguishing indolent PCa from CS PCa. This result is significantly superior to the state-of-the-art method relying on handcrafted features.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28582269,pubmed,2017,246801b1-62c2-4b93-8fd0-9b0dea810591,1
performance of an artificial multi-observer deep neural network for fully automated segmentation of polycystic kidneys,/pubmed/28550374,"Kline TL, Korfiatis P, Edwards ME, Blais JD, Czerwiec FS, Harris PC, King BF, Torres VE, Erickson BJ.",J Digit Imaging. 2017 Aug;30(4):442-448. doi: 10.1007/s10278-017-9978-1.,J Digit Imaging.  2017,PubMed,citation,PMID:28550374 | PMCID:PMC5537093,pubmed,28550374,create date:2017/05/28 | first author:Kline TL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning techniques are being rapidly applied to medical imaging tasks-from organ and lesion segmentation to tissue and tumor classification. These techniques are becoming the leading algorithmic approaches to solve inherently difficult image processing tasks. Currently, the most critical requirement for successful implementation lies in the need for relatively large datasets that can be used for training the deep learning networks. Based on our initial studies of MR imaging examinations of the kidneys of patients affected by polycystic kidney disease (PKD), we have generated a unique database of imaging data and corresponding reference standard segmentations of polycystic kidneys. In the study of PKD, segmentation of the kidneys is needed in order to measure total kidney volume (TKV). Automated methods to segment the kidneys and measure TKV are needed to increase measurement throughput and alleviate the inherent variability of human-derived measurements. We hypothesize that deep learning techniques can be leveraged to perform fast, accurate, reproducible, and fully automated segmentation of polycystic kidneys. Here, we describe a fully automated approach for segmenting PKD kidneys within MR images that simulates a multi-observer approach in order to create an accurate and robust method for the task of segmentation and computation of TKV for PKD patients. A total of 2000 cases were used for training and validation, and 400 cases were used for testing. The multi-observer ensemble method had mean ± SD percent volume difference of 0.68 ± 2.2% compared with the reference standard segmentations. The complete framework performs fully automated segmentation at a level comparable with interobserver variability and could be considered as a replacement for the task of segmentation of PKD kidneys by a human.</abstracttext></p></div></div>",kline.timothy@mayo.edu,Autosomal dominant polycystic kidney disease; Deep learning; Magnetic resonance imaging; Planimetry; Segmentation; Total kidney volume,https://www.ncbi.nlm.nih.gov//pubmed/28550374,pubmed,2017,a1a2a2af-b656-44bb-ac7b-263e28ec4e0d,1
deep learning in medical image analysis,/pubmed/28301734,"Shen D, Wu G, Suk HI.",Annu Rev Biomed Eng. 2017 Jun 21;19:221-248. doi: 10.1146/annurev-bioeng-071516-044442. Epub 2017 Mar 9.,Annu Rev Biomed Eng.  2017,PubMed,citation,PMID:28301734 | PMCID:PMC5479722,pubmed,28301734,create date:2017/03/17 | first author:Shen D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.</abstracttext></p></div></div>",dgshen@med.unc.edu,deep learning; medical image analysis; unsupervised feature learning,https://www.ncbi.nlm.nih.gov//pubmed/28301734,pubmed,2017,a9f97567-3201-450e-b2e4-0c8ecac23841,1
deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography,/pubmed/28268573,"Lahiri A, Roy AG, Sheet D, Biswas PK.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:1340-1343. doi: 10.1109/EMBC.2016.7590955.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268573,pubmed,28268573,create date:2017/03/09 | first author:Lahiri A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268573,pubmed,2016,d169973a-13da-4d15-9ad1-5075194276cf,1
deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer,/pubmed/28212138,"Becker AS, Marcon M, Ghafoor S, Wurnig MC, Frauenfelder T, Boss A.",Invest Radiol. 2017 Jul;52(7):434-440. doi: 10.1097/RLI.0000000000000358.,Invest Radiol.  2017,PubMed,citation,PMID:28212138,pubmed,28212138,create date:2017/02/18 | first author:Becker AS,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVES: </h4><p><abstracttext label='OBJECTIVES' nlmcategory='OBJECTIVE'>The aim of this study was to evaluate the diagnostic accuracy of a multipurpose image analysis software based on deep learning with artificial neural networks for the detection of breast cancer in an independent, dual-center mammography data set.</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>In this retrospective, Health Insurance Portability and Accountability Act-compliant study, all patients undergoing mammography in 2012 at our institution were reviewed (n = 3228). All of their prior and follow-up mammographies from a time span of 7 years (2008-2015) were considered as a reference for clinical diagnosis. After applying exclusion criteria (missing reference standard, prior procedures or therapies), patients with the first diagnosis of a malignoma or borderline lesion were selected (n = 143). Histology or clinical long-term follow-up served as reference standard. In a first step, a breast density-and age-matched control cohort was selected (n = 143) from the remaining patients with more than 2 years follow-up (n = 1003). The neural network was trained with this data set. From the publicly available Breast Cancer Digital Repository data set, patients with cancer and a matched control cohort were selected (n = 35 × 2). The performance of the trained neural network was also tested with this external data set. Three radiologists (3, 5, and 10 years of experience) evaluated the test data set. In a second step, the neural network was trained with all cases from January to September and tested with cases from October to December 2012 (screening-like cohort). The radiologists also evaluated this second test data set. The areas under the receiver operating characteristic curve between readers and the neural network were compared. A Bonferroni-corrected P value of less than 0.016 was considered statistically significant.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Mean age of patients with lesion was 59.6 years (range, 35-88 years) and in controls, 59.1 years (35-83 years). Breast density distribution (A/B/C/D) was 21/59/42/21 and 22/60/41/20, respectively. Histologic diagnoses were invasive ductal carcinoma in 90, ductal in situ carcinoma in 13, invasive lobular carcinoma in 13, mucinous carcinoma in 3, and borderline lesion in 12 patients. In the first step, the area under the receiver operating characteristic curve of the trained neural network was 0.81 and comparable on the test cases 0.79 (P = 0.63). One of the radiologists showed almost equal performance (0.83, P = 0.17), whereas 2 were significantly better (0.91 and 0.94, P &lt; 0.016). In the second step, performance of the neural network (0.82) was not significantly different from the human performance (0.77-0.87, P &gt; 0.016); however, radiologists were consistently less sensitive and more specific than the neural network.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Current state-of-the-art artificial neural networks for general image analysis are able to detect cancer in mammographies with similar accuracy to radiologists, even in a screening-like cohort with low breast cancer prevalence.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28212138,pubmed,2017,43653cd1-fe31-443c-9ce1-6e8614599889,1
machine learning for medical imaging,/pubmed/28212054,"Erickson BJ, Korfiatis P, Akkus Z, Kline TL.",Radiographics. 2017 Mar-Apr;37(2):505-515. doi: 10.1148/rg.2017160130. Epub 2017 Feb 17.,Radiographics.  2017,PubMed,citation,PMID:28212054 | PMCID:PMC5375621,pubmed,28212054,create date:2017/02/18 | first author:Erickson BJ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works. <sup>©</sup>RSNA, 2017.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28212054,pubmed,2017,2875da8e-2916-4101-9baf-9611d3a96f9e,1
"fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning",/pubmed/28211015,van Ginneken B.,Radiol Phys Technol. 2017 Mar;10(1):23-32. doi: 10.1007/s12194-017-0394-5. Epub 2017 Feb 16. Review.,Radiol Phys Technol.  2017,PubMed,citation,PMID:28211015 | PMCID:PMC5337239,pubmed,28211015,create date:2017/02/18 | first author:van Ginneken B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Half a century ago, the term 'computer-aided diagnosis' (CAD) was introduced in the scientific literature. Pulmonary imaging, with chest radiography and computed tomography, has always been one of the focus areas in this field. In this study, I describe how machine learning became the dominant technology for tackling CAD in the lungs, generally producing better results than do classical rule-based approaches, and how the field is now rapidly changing: in the last few years, we have seen how even better results can be obtained with deep learning. The key differences among rule-based processing, machine learning, and deep learning are summarized and illustrated for various applications of CAD in the chest.</abstracttext></p></div></div>",b.vanginneken@radboudumc.nl,Computer-aided detection; Computer-aided diagnosis; Deep learning; Image processing; Machine learning; Pulmonary image analysis,https://www.ncbi.nlm.nih.gov//pubmed/28211015,pubmed,2017,100915b1-e7ab-4813-94d0-215b06210678,1
bladder cancer segmentation in ct for treatment response assessment: application of deep-learning convolution neural network-a pilot study,/pubmed/28105470,"Cha KH, Hadjiiski LM, Samala RK, Chan HP, Cohan RH, Caoili EM, Paramagul C, Alva A, Weizer AZ.",Tomography. 2016 Dec;2(4):421-429. doi: 10.18383/j.tom.2016.00184.,Tomography.  2016,PubMed,citation,PMID:28105470 | PMCID:PMC5241049,pubmed,28105470,create date:2017/01/21 | first author:Cha KH,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Assessing the response of bladder cancer to neoadjuvant chemotherapy is crucial for reducing morbidity and increasing quality of life of patients. Changes in tumor volume during treatment is generally used to predict treatment outcome. We are developing a method for bladder cancer segmentation in CT using a pilot data set of 62 cases. 65 000 regions of interests were extracted from pre-treatment CT images to train a deep-learning convolution neural network (DL-CNN) for tumor boundary detection using leave-one-case-out cross-validation. The results were compared to our previous AI-CALS method. For all lesions in the data set, the longest diameter and its perpendicular were measured by two radiologists, and 3D manual segmentation was obtained from one radiologist. The World Health Organization (WHO) criteria and the Response Evaluation Criteria In Solid Tumors (RECIST) were calculated, and the prediction accuracy of complete response to chemotherapy was estimated by the area under the receiver operating characteristic curve (AUC). The AUCs were 0.73 ± 0.06, 0.70 ± 0.07, and 0.70 ± 0.06, respectively, for the volume change calculated using DL-CNN segmentation, the AI-CALS and the manual contours. The differences did not achieve statistical significance. The AUCs using the WHO criteria were 0.63 ± 0.07 and 0.61 ± 0.06, while the AUCs using RECIST were 0.65 ± 007 and 0.63 ± 0.06 for the two radiologists, respectively. Our results indicate that DL-CNN can produce accurate bladder cancer segmentation for calculation of tumor size change in response to treatment. The volume change performed better than the estimations from the WHO criteria and RECIST for the prediction of complete response.</abstracttext></p></div></div>",,CT; bladder cancer; computer-aided diagnosis; deep-learning; level set; segmentation; treatment response,https://www.ncbi.nlm.nih.gov//pubmed/28105470,pubmed,2016,f9f7053b-2fff-46c0-ac4d-9dcbfefa7647,1
detection and labeling of vertebrae in mr images using deep learning with clinical annotations as training data,/pubmed/28083827,"Forsberg D, Sjöblom E, Sunshine JL.",J Digit Imaging. 2017 Aug;30(4):406-412. doi: 10.1007/s10278-017-9945-x.,J Digit Imaging.  2017,PubMed,citation,PMID:28083827 | PMCID:PMC5537089,pubmed,28083827,create date:2017/01/14 | first author:Forsberg D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The purpose of this study was to investigate the potential of using clinically provided spine label annotations stored in a single institution image archive as training data for deep learning-based vertebral detection and labeling pipelines. Lumbar and cervical magnetic resonance imaging cases with annotated spine labels were identified and exported from an image archive. Two separate pipelines were configured and trained for lumbar and cervical cases respectively, using the same setup with convolutional neural networks for detection and parts-based graphical models to label the vertebrae. The detection sensitivity, precision and accuracy rates ranged between 99.1-99.8, 99.6-100, and 98.8-99.8% respectively, the average localization error ranges were 1.18-1.24 and 2.38-2.60 mm for cervical and lumbar cases respectively, and with a labeling accuracy of 96.0-97.0%. Failed labeling results typically involved failed S1 detections or missed vertebrae that were not fully visible on the image. These results show that clinically annotated image data from one image archive is sufficient to train a deep learning-based pipeline for accurate detection and labeling of MR images depicting the spine. Further, these results support using deep learning to assist radiologists in their work by providing highly accurate labels that only require rapid confirmation.</abstracttext></p></div></div>",daniel.forsberg@sectra.com,Archive; Artificial neural networks (ANNs); Machine learning; Magnetic resonance imaging,https://www.ncbi.nlm.nih.gov//pubmed/28083827,pubmed,2017,75b0dd66-30be-4822-9c9f-6162f46493e7,1
a comprehensive non-invasive framework for diagnosing prostate cancer,/pubmed/28063376,"Reda I, Shalaby A, Elmogy M, Elfotouh AA, Khalifa F, El-Ghar MA, Hosseini-Asl E, Gimel'farb G, Werghi N, El-Baz A.",Comput Biol Med. 2017 Feb 1;81:148-158. doi: 10.1016/j.compbiomed.2016.12.010. Epub 2016 Dec 23.,Comput Biol Med.  2017,PubMed,citation,PMID:28063376,pubmed,28063376,create date:2017/01/08 | first author:Reda I,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Early detection of prostate cancer increases chances of patients' survival. Our automated non-invasive system for computer-aided diagnosis (CAD) of prostate cancer segments the prostate on diffusion-weighted magnetic resonance images (DW-MRI) acquired at different b-values, estimates its apparent diffusion coefficients (ADC), and classifies their descriptors - empirical cumulative distribution functions (CDF) - with a trained deep learning network. To segment the prostate, an evolving geometric (level-set-based) deformable model is guided by a speed function depending on intensity attributes extracted from the DW-MRI with nonnegative matrix factorization (NMF). For a more robust evolution, the attributes are fused with a probabilistic shape prior and estimated spatial dependencies between prostate voxels. To preserve continuity, the ADCs of the segmented prostate volume at different b-values are normalized and refined using a generalized Gauss-Markov random field image model. The CDFs of the refined ADCs at different b-values are considered global water diffusion features and used to distinguish between benign and malignant prostates. A deep learning network of stacked non-negativity-constrained auto-encoders (SNCAE) is trained to classify the benign or malignant prostates on the basis of the constructed CDFs. Our experiments on 53 clinical DW-MRI data sets resulted in 92.3% accuracy, 83.3% sensitivity, and 100% specificity, indicating that the proposed CAD system could be used as a reliable non-invasive diagnostic tool.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Ltd. All rights reserved.</p></div></div>",aselba01@louisville.edu,CAD; DW-MRI; MGRF; NMF; Prostate cancer,https://www.ncbi.nlm.nih.gov//pubmed/28063376,pubmed,2017,7fcd720b-fbed-4952-b67d-85796ec5eee4,1
using deep learning to segment breast and fibroglandular tissue in mri volumes,/pubmed/28035663,"Dalmış MU, Litjens G, Holland K, Setio A, Mann R, Karssemeijer N, Gubern-Mérida A.",Med Phys. 2017 Feb;44(2):533-546. doi: 10.1002/mp.12079.,Med Phys.  2017,PubMed,citation,PMID:28035663,pubmed,28035663,create date:2016/12/31 | first author:Dalmış MU,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Automated segmentation of breast and fibroglandular tissue (FGT) is required for various computer-aided applications of breast MRI. Traditional image analysis and computer vision techniques, such atlas, template matching, or, edge and surface detection, have been applied to solve this task. However, applicability of these methods is usually limited by the characteristics of the images used in the study datasets, while breast MRI varies with respect to the different MRI protocols used, in addition to the variability in breast shapes. All this variability, in addition to various MRI artifacts, makes it a challenging task to develop a robust breast and FGT segmentation method using traditional approaches. Therefore, in this study, we investigated the use of a deep-learning approach known as 'U-net.'</abstracttext></p><h4>MATERIALS AND METHODS: </h4><p><abstracttext label='MATERIALS AND METHODS' nlmcategory='METHODS'>We used a dataset of 66 breast MRI's randomly selected from our scientific archive, which includes five different MRI acquisition protocols and breasts from four breast density categories in a balanced distribution. To prepare reference segmentations, we manually segmented breast and FGT for all images using an in-house developed workstation. We experimented with the application of U-net in two different ways for breast and FGT segmentation. In the first method, following the same pipeline used in traditional approaches, we trained two consecutive (2C) U-nets: first for segmenting the breast in the whole MRI volume and the second for segmenting FGT inside the segmented breast. In the second method, we used a single 3-class (3C) U-net, which performs both tasks simultaneously by segmenting the volume into three regions: nonbreast, fat inside the breast, and FGT inside the breast. For comparison, we applied two existing and published methods to our dataset: an atlas-based method and a sheetness-based method. We used Dice Similarity Coefficient (DSC) to measure the performances of the automated methods, with respect to the manual segmentations. Additionally, we computed Pearson's correlation between the breast density values computed based on manual and automated segmentations.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The average DSC values for breast segmentation were 0.933, 0.944, 0.863, and 0.848 obtained from 3C U-net, 2C U-nets, atlas-based method, and sheetness-based method, respectively. The average DSC values for FGT segmentation obtained from 3C U-net, 2C U-nets, and atlas-based methods were 0.850, 0.811, and 0.671, respectively. The correlation between breast density values based on 3C U-net and manual segmentations was 0.974. This value was significantly higher than 0.957 as obtained from 2C U-nets (P &lt; 0.0001, Steiger's Z-test with Bonferoni correction) and 0.938 as obtained from atlas-based method (P = 0.0016).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>In conclusion, we applied a deep-learning method, U-net, for segmenting breast and FGT in MRI in a dataset that includes a variety of MRI protocols and breast densities. Our results showed that U-net-based methods significantly outperformed the existing algorithms and resulted in significantly more accurate breast density computation.</abstracttext></p><p class='copyright'>© 2016 American Association of Physicists in Medicine.</p></div></div>",,MRI; breast segmentation; deep learning,https://www.ncbi.nlm.nih.gov//pubmed/28035663,pubmed,2017,86fb33c9-eea9-41de-a29a-2925d17070a8,1
lung nodule classification using deep feature fusion in chest radiography,/pubmed/27986379,"Wang C, Elazab A, Wu J, Hu Q.",Comput Med Imaging Graph. 2017 Apr;57:10-18. doi: 10.1016/j.compmedimag.2016.11.004. Epub 2016 Nov 12.,Comput Med Imaging Graph.  2017,PubMed,citation,PMID:27986379,pubmed,27986379,create date:2016/12/18 | first author:Wang C,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Lung nodules are small, round, or oval-shaped masses of tissue in the lung region. Early diagnosis and treatment of lung nodules can significantly improve the quality of patients' lives. Because of their small size and the interlaced nature of chest anatomy, detection of lung nodules using different medical imaging techniques becomes challenging. Recently, several methods for computer aided diagnosis (CAD) were proposed to improve the detection of lung nodules with good performances. However, the current methods are unable to achieve high sensitivity and high specificity. In this paper, we propose using deep feature fusion from the non-medical training and hand-crafted features to reduce the false positive results. Based on our experimentation of the public dataset, our results show that, the deep fusion feature can achieve promising results in terms of sensitivity and specificity (69.3% and 96.2%) at 1.19 false positive per image, which is better than the single hand-crafted features (62% and 95.4%) at 1.45 false positive per image. As it stands, fusion features that were used to classify our candidate nodules have resulted in a more promising outcome as compared to the single features from deep learning features and the hand-crafted features. This will improve the current CAD method based on the use of deep feature fusion to more effectively diagnose the presence of lung nodules.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",qm.hu@siat.ac.cn,Computer aided diagnosis; Deep learning; Feature fusion; Lung nodule,https://www.ncbi.nlm.nih.gov//pubmed/27986379,pubmed,2017,173b45cb-2612-48a1-8f49-5c431ac50646,1
discrimination of breast cancer with microcalcifications on mammography by deep learning,/pubmed/27273294,"Wang J, Yang X, Cai H, Tan W, Jin C, Li L.",Sci Rep. 2016 Jun 7;6:27327. doi: 10.1038/srep27327.,Sci Rep.  2016,PubMed,citation,PMID:27273294 | PMCID:PMC4895132,pubmed,27273294,create date:2016/06/09 | first author:Wang J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Microcalcification is an effective indicator of early breast cancer. To improve the diagnostic accuracy of microcalcifications, this study evaluates the performance of deep learning-based models on large datasets for its discrimination. A semi-automated segmentation method was used to characterize all microcalcifications. A discrimination classifier model was constructed to assess the accuracies of microcalcifications and breast masses, either in isolation or combination, for classifying breast lesions. Performances were compared to benchmark models. Our deep learning model achieved a discriminative accuracy of 87.3% if microcalcifications were characterized alone, compared to 85.8% with a support vector machine. The accuracies were 61.3% for both methods with masses alone and improved to 89.7% and 85.8% after the combined analysis with microcalcifications. Image segmentation with our deep learning model yielded 15, 26 and 41 features for the three scenarios, respectively. Overall, deep learning based on large datasets was superior to standard methods for the discrimination of microcalcifications. Accuracy was increased by adopting a combinatorial approach to detect microcalcifications and masses simultaneously. This may have clinical value for early detection and treatment of breast cancer. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27273294,pubmed,2016,702a7fd6-4dde-426d-ac92-52941a6b437a,1
deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis,/pubmed/27212078,"Litjens G, Sánchez CI, Timofeeva N, Hermsen M, Nagtegaal I, Kovacs I, Hulsbergen-van de Kaa C, Bult P, van Ginneken B, van der Laak J.",Sci Rep. 2016 May 23;6:26286. doi: 10.1038/srep26286.,Sci Rep.  2016,PubMed,citation,PMID:27212078 | PMCID:PMC4876324,pubmed,27212078,create date:2016/05/24 | first author:Litjens G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Pathologists face a substantial increase in workload and complexity of histopathologic cancer diagnosis due to the advent of personalized medicine. Therefore, diagnostic protocols have to focus equally on efficiency and accuracy. In this paper we introduce 'deep learning' as a technique to improve the objectivity and efficiency of histopathologic slide analysis. Through two examples, prostate cancer identification in biopsy specimens and breast cancer metastasis detection in sentinel lymph nodes, we show the potential of this new methodology to reduce the workload for pathologists, while at the same time increasing objectivity of diagnoses. We found that all slides containing prostate cancer and micro- and macro-metastases of breast cancer could be identified automatically while 30-40% of the slides containing benign and normal tissue could be excluded without the use of any additional immunohistochemical markers or human intervention. We conclude that 'deep learning' holds great promise to improve the efficacy of prostate cancer diagnosis and breast cancer staging. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27212078,pubmed,2016,3e8cf38d-4935-45dd-847a-c0a606eae0af,1
computer-aided diagnosis with deep learning architecture: applications to breast lesions in us images and pulmonary nodules in ct scans,/pubmed/27079888,"Cheng JZ, Ni D, Chou YH, Qin J, Tiu CM, Chang YC, Huang CS, Shen D, Chen CM.",Sci Rep. 2016 Apr 15;6:24454. doi: 10.1038/srep24454.,Sci Rep.  2016,PubMed,citation,PMID:27079888 | PMCID:PMC4832199,pubmed,27079888,create date:2016/04/16 | first author:Cheng JZ,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper performs a comprehensive study on the deep-learning-based computer-aided diagnosis (CADx) for the differential diagnosis of benign and malignant nodules/lesions by avoiding the potential errors caused by inaccurate image processing results (e.g., boundary segmentation), as well as the classification bias resulting from a less robust feature set, as involved in most conventional CADx algorithms. Specifically, the stacked denoising auto-encoder (SDAE) is exploited on the two CADx applications for the differentiation of breast ultrasound lesions and lung CT nodules. The SDAE architecture is well equipped with the automatic feature exploration mechanism and noise tolerance advantage, and hence may be suitable to deal with the intrinsically noisy property of medical image data from various imaging modalities. To show the outperformance of SDAE-based CADx over the conventional scheme, two latest conventional CADx algorithms are implemented for comparison. 10 times of 10-fold cross-validations are conducted to illustrate the efficacy of the SDAE-based CADx algorithm. The experimental results show the significant performance boost by the SDAE-based CADx algorithm over the two conventional methods, suggesting that deep learning techniques can potentially change the design paradigm of the CADx systems without the need of explicit design and selection of problem-oriented features. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27079888,pubmed,2016,9e75c15c-4212-42b3-8569-03bf81f1ef30,1
unsupervised deep learning applied to breast density segmentation and mammographic risk scoring,/pubmed/26915120,"Kallenberg M, Petersen K, Nielsen M, Ng AY, Pengfei Diao, Igel C, Vachon CM, Holland K, Winkel RR, Karssemeijer N, Lillholm M.",IEEE Trans Med Imaging. 2016 May;35(5):1322-1331. doi: 10.1109/TMI.2016.2532122. Epub 2016 Feb 18.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26915120,pubmed,26915120,create date:2016/02/26 | first author:Kallenberg M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26915120,pubmed,2016,8596ef62-1eda-4c0a-a351-d0c465fd786e,1
state-space model with deep learning for functional dynamics estimation in resting-state fmri,/pubmed/26774612,"Suk HI, Wee CY, Lee SW, Shen D.",Neuroimage. 2016 Apr 1;129:292-307. doi: 10.1016/j.neuroimage.2016.01.005. Epub 2016 Jan 14.,Neuroimage.  2016,PubMed,citation,PMID:26774612 | PMCID:PMC5437848,pubmed,26774612,create date:2016/01/18 | first author:Suk HI,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Studies on resting-state functional Magnetic Resonance Imaging (rs-fMRI) have shown that different brain regions still actively interact with each other while a subject is at rest, and such functional interaction is not stationary but changes over time. In terms of a large-scale brain network, in this paper, we focus on time-varying patterns of functional networks, i.e., functional dynamics, inherent in rs-fMRI, which is one of the emerging issues along with the network modelling. Specifically, we propose a novel methodological architecture that combines deep learning and state-space modelling, and apply it to rs-fMRI based Mild Cognitive Impairment (MCI) diagnosis. We first devise a Deep Auto-Encoder (DAE) to discover hierarchical non-linear functional relations among regions, by which we transform the regional features into an embedding space, whose bases are complex functional networks. Given the embedded functional features, we then use a Hidden Markov Model (HMM) to estimate dynamic characteristics of functional networks inherent in rs-fMRI via internal states, which are unobservable but can be inferred from observations statistically. By building a generative model with an HMM, we estimate the likelihood of the input features of rs-fMRI as belonging to the corresponding status, i.e., MCI or normal healthy control, based on which we identify the clinical label of a testing subject. In order to validate the effectiveness of the proposed method, we performed experiments on two different datasets and compared with state-of-the-art methods in the literature. We also analyzed the functional networks learned by DAE, estimated the functional connectivities by decoding hidden states in HMM, and investigated the estimated functional connectivities by means of a graph-theoretic approach. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Inc. All rights reserved.</p></div></div>",hisuk@korea.ac.kr,Deep learning; Dynamic functional connectivity; Hidden Markov model; Mild cognitive impairment; Resting-state functional magnetic resonance imaging,https://www.ncbi.nlm.nih.gov//pubmed/26774612,pubmed,2016,3384c67e-080b-4509-8f4a-9b57e22a24c7,1
automatic tissue characterization of air trapping in chest radiographs using deep neural networks,/pubmed/28324924,"Mansoor A, Perez G, Nino G, Linguraru MG.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:97-100. doi: 10.1109/EMBC.2016.7590649.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28324924 | PMCID:PMC5459489,pubmed,28324924,create date:2016/01/01 | first author:Mansoor A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Significant progress has been made in recent years for computer-aided diagnosis of abnormal pulmonary textures from computed tomography (CT) images. Similar initiatives in chest radiographs (CXR), the common modality for pulmonary diagnosis, are much less developed. CXR are fast, cost effective and low-radiation solution to diagnosis over CT. However, the subtlety of textures in CXR makes them hard to discern even by trained eye. We explore the performance of deep learning abnormal tissue characterization from CXR. Prior studies have used CT imaging to characterize air trapping in subjects with pulmonary disease; however, the use of CT in children is not recommended mainly due to concerns pertaining to radiation dosage. In this work, we present a stacked autoencoder (SAE) deep learning architecture for automated tissue characterization of air-trapping from CXR. To our best knowledge this is the first study applying deep learning framework for the specific problem on 51 CXRs, an F-score of ≈ 76.5% and a strong correlation with the expert visual scoring (R=0.93, p =&lt;; 0.01) demonstrate the potential of the proposed method to characterization of air trapping.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28324924,pubmed,2016,d7d6f5d3-6396-4f3b-adb2-75f4c9b667dc,1
deep sparse multi-task learning for feature selection in alzheimer's disease diagnosis,/pubmed/25993900,"Suk HI, Lee SW, Shen D; Alzheimer’s Disease Neuroimaging Initiative..",Brain Struct Funct. 2016 Jun;221(5):2569-87. doi: 10.1007/s00429-015-1059-y. Epub 2015 May 21.,Brain Struct Funct.  2016,PubMed,citation,PMID:25993900 | PMCID:PMC4714963,pubmed,25993900,create date:2015/05/23 | first author:Suk HI,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recently, neuroimaging-based Alzheimer's disease (AD) or mild cognitive impairment (MCI) diagnosis has attracted researchers in the field, due to the increasing prevalence of the diseases. Unfortunately, the unfavorable high-dimensional nature of neuroimaging data, but a limited small number of samples available, makes it challenging to build a robust computer-aided diagnosis system. Machine learning techniques have been considered as a useful tool in this respect and, among various methods, sparse regression has shown its validity in the literature. However, to our best knowledge, the existing sparse regression methods mostly try to select features based on the optimal regression coefficients in one step. We argue that since the training feature vectors are composed of both informative and uninformative or less informative features, the resulting optimal regression coefficients are inevidently affected by the uninformative or less informative features. To this end, we first propose a novel deep architecture to recursively discard uninformative features by performing sparse multi-task learning in a hierarchical fashion. We further hypothesize that the optimal regression coefficients reflect the relative importance of features in representing the target response variables. In this regard, we use the optimal regression coefficients learned in one hierarchy as feature weighting factors in the following hierarchy, and formulate a weighted sparse multi-task learning method. Lastly, we also take into account the distributional characteristics of samples per class and use clustering-induced subclass label vectors as target response values in our sparse regression model. In our experiments on the ADNI cohort, we performed both binary and multi-class classification tasks in AD/MCI diagnosis and showed the superiority of the proposed method by comparing with the state-of-the-art methods. </abstracttext></p></div></div>",hisuk@korea.ac.kr,Alzheimer’s disease (AD); Deep architecture; Feature selection; Magnetic resonance imaging (MRI); Mild cognitive impairment (MCI); Multi-task learning; Positron emission topography (PET); Sparse least squared regression,https://www.ncbi.nlm.nih.gov//pubmed/25993900,pubmed,2016,0dfb90b1-5c16-47b8-80b6-302e4c62a5ae,1
automated grading of gliomas using deep learning in digital pathology images: a modular approach with ensemble of convolutional neural networks,/pubmed/26958289,"Ertosun MG, Rubin DL.",AMIA Annu Symp Proc. 2015 Nov 5;2015:1899-908. eCollection 2015.,AMIA Annu Symp Proc.  2015,PubMed,citation,PMID:26958289 | PMCID:PMC4765616,pubmed,26958289,create date:2015/01/01 | first author:Ertosun MG,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Brain glioma is the most common primary malignant brain tumors in adults with different pathologic subtypes: Lower Grade Glioma (LGG) Grade II, Lower Grade Glioma (LGG) Grade III, and Glioblastoma Multiforme (GBM) Grade IV. The survival and treatment options are highly dependent of this glioma grade. We propose a deep learning-based, modular classification pipeline for automated grading of gliomas using digital pathology images. Whole tissue digitized images of pathology slides obtained from The Cancer Genome Atlas (TCGA) were used to train our deep learning modules. Our modular pipeline provides diagnostic quality statistics, such as precision, sensitivity and specificity, of the individual deep learning modules, and (1) facilitates training given the limited data in this domain, (2) enables exploration of different deep learning structures for each module, (3) leads to developing less complex modules that are simpler to analyze, and (4) provides flexibility, permitting use of single modules within the framework or use of other modeling or machine learning applications, such as probabilistic graphical models or support vector machines. Our modular approach helps us meet the requirements of minimum accuracy levels that are demanded by the context of different decision points within a multi-class classification scheme. Convolutional Neural Networks are trained for each module for each sub-task with more than 90% classification accuracies on validation data set, and achieved classification accuracy of 96% for the task of GBM vs LGG classification, 71% for further identifying the grade of LGG into Grade II or Grade III on independent data set coming from new patients from the multi-institutional repository. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26958289,pubmed,2015,6b5ac46a-5b91-473e-a636-687ece609c57,1
hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis,/pubmed/25042445,"Suk HI, Lee SW, Shen D; Alzheimer's Disease Neuroimaging Initiative..",Neuroimage. 2014 Nov 1;101:569-82. doi: 10.1016/j.neuroimage.2014.06.077. Epub 2014 Jul 18.,Neuroimage.  2014,PubMed,citation,PMID:25042445 | PMCID:PMC4165842,pubmed,25042445,create date:2014/07/22 | first author:Suk HI,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM)(2), a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35%, 85.67%, and 74.58%, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET. </abstracttext></p><p class='copyright'>Copyright © 2014 Elsevier Inc. All rights reserved.</p></div></div>",dgshen@med.unc.edu,Alzheimer's Disease; Deep Boltzmann Machine; Mild Cognitive Impairment; Multimodal data fusion; Shared feature representation,https://www.ncbi.nlm.nih.gov//pubmed/25042445,pubmed,2014,53e270e4-1afc-4c02-9109-c023966ea77a,1
latent feature representation with stacked auto-encoder for ad/mci diagnosis,/pubmed/24363140,"Suk HI, Lee SW, Shen D; Alzheimer’s Disease Neuroimaging Initiative..",Brain Struct Funct. 2015 Mar;220(2):841-59. doi: 10.1007/s00429-013-0687-3. Epub 2013 Dec 22.,Brain Struct Funct.  2015,PubMed,citation,PMID:24363140 | PMCID:PMC4065852,pubmed,24363140,create date:2013/12/24 | first author:Suk HI,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recently, there have been great interests for computer-aided diagnosis of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI). Unlike the previous methods that considered simple low-level features such as gray matter tissue volumes from MRI, and mean signal intensities from PET, in this paper, we propose a deep learning-based latent feature representation with a stacked auto-encoder (SAE). We believe that there exist latent non-linear complicated patterns inherent in the low-level features such as relations among features. Combining the latent information with the original features helps build a robust model in AD/MCI classification, with high diagnostic accuracy. Furthermore, thanks to the unsupervised characteristic of the pre-training in deep learning, we can benefit from the target-unrelated samples to initialize parameters of SAE, thus finding optimal parameters in fine-tuning with the target-related samples, and further enhancing the classification performances across four binary classification problems: AD vs. healthy normal control (HC), MCI vs. HC, AD vs. MCI, and MCI converter (MCI-C) vs. MCI non-converter (MCI-NC). In our experiments on ADNI dataset, we validated the effectiveness of the proposed method, showing the accuracies of 98.8, 90.7, 83.7, and 83.3 % for AD/HC, MCI/HC, AD/MCI, and MCI-C/MCI-NC classification, respectively. We believe that deep learning can shed new light on the neuroimaging data analysis, and our work presented the applicability of this method to brain disease diagnosis. </abstracttext></p></div></div>",hsuk@med.unc.edu,,https://www.ncbi.nlm.nih.gov//pubmed/24363140,pubmed,2015,a8ac3d31-0818-4aa4-a15f-fce72ec950f9,1
predicting clinical outcomes from large scale cancer genomic profiles with deep survival models,/pubmed/28916782,"Yousefi S, Amrollahi F, Amgad M, Dong C, Lewis JE, Song C, Gutman DA, Halani SH, Vega JEV, Brat DJ, Cooper LAD.",Sci Rep. 2017 Sep 15;7(1):11707. doi: 10.1038/s41598-017-11817-6.,Sci Rep.  2017,PubMed,citation,PMID:28916782,pubmed,28916782,create date:2017/09/17 | first author:Yousefi S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Translating the vast data generated by genomic platforms into accurate predictions of clinical outcomes is a fundamental challenge in genomic medicine. Many prediction methods face limitations in learning from the high-dimensional profiles generated by these platforms, and rely on experts to hand-select a small number of features for training prediction models. In this paper, we demonstrate how deep learning and Bayesian optimization methods that have been remarkably successful in general high-dimensional prediction tasks can be adapted to the problem of predicting cancer outcomes. We perform an extensive comparison of Bayesian optimized deep survival models and other state of the art machine learning methods for survival analysis, and describe a framework for interpreting deep survival models using a risk backpropagation technique. Finally, we illustrate that deep survival models can successfully transfer information across diseases to improve prognostic accuracy. We provide an open-source software implementation of this framework called SurvivalNet that enables automatic training, evaluation and interpretation of deep survival models.</abstracttext></p></div></div>",lee.cooper@emory.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28916782,pubmed,2017,29b28c5a-e126-434e-8c64-c9275e9c237a,1
deep learning for classification of colorectal polyps on whole-slide images,/pubmed/28828201,"Korbar B, Olofson AM, Miraflor AP, Nicka CM, Suriawinata MA, Torresani L, Suriawinata AA, Hassanpour S.",J Pathol Inform. 2017 Jul 25;8:30. doi: 10.4103/jpi.jpi_34_17. eCollection 2017.,J Pathol Inform.  2017,PubMed,citation,PMID:28828201 | PMCID:PMC5545773,pubmed,28828201,create date:2017/08/23 | first author:Korbar B,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>CONTEXT: </h4><p><abstracttext label='CONTEXT' nlmcategory='BACKGROUND'>Histopathological characterization of colorectal polyps is critical for determining the risk of colorectal cancer and future rates of surveillance for patients. However, this characterization is a challenging task and suffers from significant inter- and intra-observer variability.</abstracttext></p><h4>AIMS: </h4><p><abstracttext label='AIMS' nlmcategory='OBJECTIVE'>We built an automatic image analysis method that can accurately classify different types of colorectal polyps on whole-slide images to help pathologists with this characterization and diagnosis.</abstracttext></p><h4>SETTING AND DESIGN: </h4><p><abstracttext label='SETTING AND DESIGN' nlmcategory='METHODS'>Our method is based on deep-learning techniques, which rely on numerous levels of abstraction for data representation and have shown state-of-the-art results for various image analysis tasks.</abstracttext></p><h4>SUBJECTS AND METHODS: </h4><p><abstracttext label='SUBJECTS AND METHODS' nlmcategory='METHODS'>Our method covers five common types of polyps (i.e., hyperplastic, sessile serrated, traditional serrated, tubular, and tubulovillous/villous) that are included in the US Multisociety Task Force guidelines for colorectal cancer risk assessment and surveillance. We developed multiple deep-learning approaches by leveraging a dataset of 2074 crop images, which were annotated by multiple domain expert pathologists as reference standards.</abstracttext></p><h4>STATISTICAL ANALYSIS: </h4><p><abstracttext label='STATISTICAL ANALYSIS' nlmcategory='METHODS'>We evaluated our method on an independent test set of 239 whole-slide images and measured standard machine-learning evaluation metrics of accuracy, precision, recall, and F1 score and their 95% confidence intervals.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our evaluation shows that our method with residual network architecture achieves the best performance for classification of colorectal polyps on whole-slide images (overall accuracy: 93.0%, 95% confidence interval: 89.0%-95.9%).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Our method can reduce the cognitive burden on pathologists and improve their efficacy in histopathological characterization of colorectal polyps and in subsequent risk assessment and follow-up recommendations.</abstracttext></p></div></div>",,Colorectal polyps; deep learning; digital pathology; histopathological characterization,https://www.ncbi.nlm.nih.gov//pubmed/28828201,pubmed,2017,19398cc6-b72e-4f45-aa98-df350e2a4188,1
precision diagnosis of melanoma and other skin lesions from digital images,/pubmed/28815132,"Bhattacharya A, Young A, Wong A, Stalling S, Wei M, Hadley D.",AMIA Jt Summits Transl Sci Proc. 2017 Jul 26;2017:220-226. eCollection 2017.,AMIA Jt Summits Transl Sci Proc.  2017,PubMed,citation,PMID:28815132 | PMCID:PMC5543387,pubmed,28815132,create date:2017/08/18 | first author:Bhattacharya A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Melanoma will affect an estimated 73,000 new cases this year and result in 9,000 deaths, yet precise diagnosis remains a serious problem. Without early detection and preventative care, melanoma can quickly spread to become fatal (Stage IV 5-year survival rate is 20-10%) from a once localized skin lesion (Stage IA 5- year survival rate is 97%). There is no biomarker for melanoma in clinical use, and the current diagnostic criteria for skin lesions remains subjective and imprecise. Accurate diagnosis of melanoma relies on a histopathologic gold standard; thus, aggressive excision of melanocytic skin lesions has been the mainstay of treatment. It is estimated that 36 biopsies are performed for every melanoma confirmed by pathology among excised lesions. There is significant morbidity in misdiagnosing melanoma such as progression of the disease for a false negative prediction vs the risks of unnecessary surgery for a false positive prediction. Every year, poor diagnostic precision adds an estimated $673 million in overall cost to manage the disease. Currently, manual dermatoscopic imaging is the standard of care in selecting atypical skin lesions for biopsy, and at best it achieves 90% sensitivity but only 59% specificity when performed by an expert dermatologist. Many computer vision (CV) algorithms perform better than dermatologists in classifying skin lesions although not significantly so in clinical practice. Meanwhile, open source deep learning (DL) techniques in CV have been gaining dominance since 2012 for image classification, and today DL can outperform humans in classifying millions of digital images with less than 5% error rates. Moreover, DL algorithms are readily run on commoditized hardware and have a strong online community of developers supporting their rapid adoption. In this work, we performed a successful pilot study to show proof of concept to DL skin pathology from images. However, DL algorithms must be trained on very large labelled datasets of images to achieve high accuracy. Here, we begin to assemble a large imageset of skin lesions from the UCSF and the San Francisco Veterans Affairs Medical Center (VAMC) dermatology clinics that are well characterized by their underlying pathology, on which to train DL algorithms. If trained on sufficient data, we hypothesize that our approach will significantly outperform general dermatologists in predicting skin lesion pathology. We posit that our work will allow for precision diagnosis of melanoma from widely available digital photography, which may optimize the management of the disease by decreasing unnecessary office visits and the significant morbidity and cost of melanoma misdiagnosis.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28815132,pubmed,2017,d078c189-14d6-4f36-8568-99f283e64dbb,1
a survey on deep learning in medical image analysis,/pubmed/28778026,"Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, van der Laak JAWM, van Ginneken B, Sánchez CI.",Med Image Anal. 2017 Jul 26;42:60-88. doi: 10.1016/j.media.2017.07.005. [Epub ahead of print] Review.,Med Image Anal.  2017,PubMed,citation,PMID:28778026,pubmed,28778026,create date:2017/08/05 | first author:Litjens G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",geert.litjens@radboudumc.nl,Convolutional neural networks; Deep learning; Medical imaging; Survey,https://www.ncbi.nlm.nih.gov//pubmed/28778026,pubmed,2017,bb627114-4fd7-4179-a40f-25e7db16d2b4,1
applying deep neural networks to unstructured text notes in electronic medical records for phenotyping youth depression,/pubmed/28739578,"Geraci J, Wilansky P, de Luca V, Roy A, Kennedy JL, Strauss J.",Evid Based Ment Health. 2017 Jul 24. pii: ebmental-2017-102688. doi: 10.1136/eb-2017-102688. [Epub ahead of print],Evid Based Ment Health.  2017,PubMed,citation,PMID:28739578 | PMCID:PMC5566092,pubmed,28739578,create date:2017/07/26 | first author:Geraci J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>We report a study of machine learning applied to the phenotyping of psychiatric diagnosis for research recruitment in youth depression, conducted with 861 labelled electronic medical records (EMRs) documents. A model was built that could accurately identify individuals who were suitable candidates for a study on youth depression.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Our objective was a model to identify individuals who meet inclusion criteria as well as unsuitable patients who would require exclusion.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Our methods included applying a system that coded the EMR documents by removing personally identifying information, using two psychiatrists who labelled a set of EMR documents (from which the 861 came), using a brute force search and training a deep neural network for this task.</abstracttext></p><h4>FINDINGS: </h4><p><abstracttext label='FINDINGS' nlmcategory='RESULTS'>According to a cross-validation evaluation, we describe a model that had a specificity of 97% and a sensitivity of 45% and a second model with a specificity of 53% and a sensitivity of 89%. We combined these two models into a third one (sensitivity 93.5%; specificity 68%; positive predictive value (precision) 77%) to generate a list of most suitable candidates in support of research recruitment.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Our efforts are meant to demonstrate the potential for this type of approach for patient recruitment purposes but it should be noted that a larger sample size is required to build a truly reliable recommendation system.</abstracttext></p><h4>CLINICAL IMPLICATIONS: </h4><p><abstracttext label='CLINICAL IMPLICATIONS' nlmcategory='CONCLUSIONS'>Future efforts will employ alternate neural network algorithms available and other machine learning methods.</abstracttext></p><p class='copyright'>© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017. All rights reserved. No commercial use is permitted unless otherwise expressly granted.</p></div></div>",,deep learning; depression; neural network; phenotyping; youth,https://www.ncbi.nlm.nih.gov//pubmed/28739578,pubmed,2017,2261ddb8-7dbb-489c-9a7f-87755509dd1b,1
a study of the suitability of autoencoders for preprocessing data in breast cancer experimentation,/pubmed/28663073,"Macías-García L, Luna-Romera JM, García-Gutiérrez J, Martínez-Ballesteros M, Riquelme-Santos JC, González-Cámpora R.",J Biomed Inform. 2017 Aug;72:33-44. doi: 10.1016/j.jbi.2017.06.020. Epub 2017 Jun 27.,J Biomed Inform.  2017,PubMed,citation,PMID:28663073,pubmed,28663073,create date:2017/07/01 | first author:Macías-García L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Breast cancer is the most common cause of cancer death in women. Today, post-transcriptional protein products of the genes involved in breast cancer can be identified by immunohistochemistry. However, this method has problems arising from the intra-observer and inter-observer variability in the assessment of pathologic variables, which may result in misleading conclusions. Using an optimal selection of preprocessing techniques may help to reduce observer variability. Deep learning has emerged as a powerful technique for any tasks related to machine learning such as classification and regression. The aim of this work is to use autoencoders (neural networks commonly used to feed deep learning architectures) to improve the quality of the data for developing immunohistochemistry signatures with prognostic value in breast cancer. Our testing on data from 222 patients with invasive non-special type breast carcinoma shows that an automatic binarization of experimental data after autoencoding could outperform other classical preprocessing techniques (such as human-dependent or automatic binarization only) when applied to the prognosis of breast cancer by immunohistochemical signatures.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",jmluna@us.es,Autoencoder; Biomedical data; Breast cancer; Deep learning; Preprocessing,https://www.ncbi.nlm.nih.gov//pubmed/28663073,pubmed,2017,fdca90f1-681d-46db-873e-4ebd4eedcc9e,1
applying artificial intelligence to disease staging: deep learning for improved staging of diabetic retinopathy,/pubmed/28640840,"Takahashi H, Tampo H, Arai Y, Inoue Y, Kawashima H.",PLoS One. 2017 Jun 22;12(6):e0179790. doi: 10.1371/journal.pone.0179790. eCollection 2017.,PLoS One.  2017,PubMed,citation,PMID:28640840 | PMCID:PMC5480986,pubmed,28640840,create date:2017/06/24 | first author:Takahashi H,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Disease staging involves the assessment of disease severity or progression and is used for treatment selection. In diabetic retinopathy, disease staging using a wide area is more desirable than that using a limited area. We investigated if deep learning artificial intelligence (AI) could be used to grade diabetic retinopathy and determine treatment and prognosis.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>The retrospective study analyzed 9,939 posterior pole photographs of 2,740 patients with diabetes. Nonmydriatic 45° field color fundus photographs were taken of four fields in each eye annually at Jichi Medical University between May 2011 and June 2015. A modified fully randomly initialized GoogLeNet deep learning neural network was trained on 95% of the photographs using manual modified Davis grading of three additional adjacent photographs. We graded 4,709 of the 9,939 posterior pole fundus photographs using real prognoses. In addition, 95% of the photographs were learned by the modified GoogLeNet. Main outcome measures were prevalence and bias-adjusted Fleiss' kappa (PABAK) of AI staging of the remaining 5% of the photographs.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The PABAK to modified Davis grading was 0.64 (accuracy, 81%; correct answer in 402 of 496 photographs). The PABAK to real prognosis grading was 0.37 (accuracy, 96%).</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>We propose a novel AI disease-staging system for grading diabetic retinopathy that involves a retinal area not typically visualized on fundoscopy and another AI that directly suggests treatments and determines prognoses.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28640840,pubmed,2017,16314db0-c472-4489-98c2-6934bf140023,1
dynamiceuticals: the next stage in personalized medicine,/pubmed/28638319,Perez Velazquez JL.,Front Neurosci. 2017 Jun 7;11:329. doi: 10.3389/fnins.2017.00329. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28638319 | PMCID:PMC5461286,pubmed,28638319,create date:2017/06/24 | first author:Perez Velazquez JL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The surge in the interest in personalized medicine necessitates a corresponding rational approach for implementing such individualized therapies. Dynamiceuticals represents a natural extension of the Pharmaceutical and Electroceutical fields, where the precise determination of the dynamical regimes of the pathophysiology will guide to devise therapies that ameliorate the pathology in a well-controlled manner, thus being precisely tailored toward the implementation of individualized medicine. This approach foretells to lessen side-effects and achieve superior efficacy as compared with current trial-and-error or open-loop strategies. But does the current state of knowledge and technology allow this scheme to offer what it claims?</abstracttext></p></div></div>",,deep brain stimulation; dynamical systems; epilepsy; machine learning; neuromodulation; personalized medicine; translational medicine,https://www.ncbi.nlm.nih.gov//pubmed/28638319,pubmed,2017,2e849572-8597-443d-b590-7d96b39cc1e0,1
detection and grading of prostate cancer using temporal enhanced ultrasound: combining deep neural networks and tissue mimicking simulations,/pubmed/28634789,"Azizi S, Bayat S, Yan P, Tahmasebi A, Nir G, Kwak JT, Xu S, Wilson S, Iczkowski KA, Lucia MS, Goldenberg L, Salcudean SE, Pinto PA, Wood B, Abolmaesumi P, Mousavi P.",Int J Comput Assist Radiol Surg. 2017 Aug;12(8):1293-1305. doi: 10.1007/s11548-017-1627-0. Epub 2017 Jun 20.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28634789,pubmed,28634789,create date:2017/06/22 | first author:Azizi S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>PURPOSE  : Temporal Enhanced Ultrasound (TeUS) has been proposed as a new paradigm for tissue characterization based on a sequence of ultrasound radio frequency (RF) data. We previously used TeUS to successfully address the problem of prostate cancer detection in the fusion biopsies. METHODS  : In this paper, we use TeUS to address the problem of grading prostate cancer in a clinical study of 197 biopsy cores from 132 patients. Our method involves capturing high-level latent features of TeUS with a deep learning approach followed by distribution learning to cluster aggressive cancer in a biopsy core. In this hypothesis-generating study, we utilize deep learning based feature visualization as a means to obtain insight into the physical phenomenon governing the interaction of temporal ultrasound with tissue. RESULTS  : Based on the evidence derived from our feature visualization, and the structure of tissue from digital pathology, we build a simulation framework for studying the physical phenomenon underlying TeUS-based tissue characterization. CONCLUSION  : Results from simulation and feature visualization corroborated with the hypothesis that micro-vibrations of tissue microstructure, captured by low-frequency spectral features of TeUS, can be used for detection of prostate cancer.</abstracttext></p></div></div>",shazizi@ece.ubc.ca,Cancer grading; Deep belief network; Deep learning; Prostate cancer; Temporal enhanced ultrasound,https://www.ncbi.nlm.nih.gov//pubmed/28634789,pubmed,2017,fd5c994c-960a-415d-9be2-087e5a3340b4,1
deep image mining for diabetic retinopathy screening,/pubmed/28511066,"Quellec G, Charrière K, Boudi Y, Cochener B, Lamard M.",Med Image Anal. 2017 Jul;39:178-193. doi: 10.1016/j.media.2017.04.012. Epub 2017 Apr 28.,Med Image Anal.  2017,PubMed,citation,PMID:28511066,pubmed,28511066,create date:2017/05/17 | first author:Quellec G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition and a private dataset of almost 110,000 photographs (e-ophtha). For the task of detecting referable DR, very good detection performance was achieved: A<sub>z</sub>=0.954 in Kaggle's dataset and A<sub>z</sub>=0.949 in e-ophtha. Performance was also evaluated at the image level and at the lesion level in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For the task of detecting images containing these four lesion types, the proposed detector, which was trained to detect referable DR, outperforms recent algorithms trained to detect those lesions specifically, with pixel-level supervision. At the lesion level, the proposed detector outperforms heatmap generation algorithms for ConvNets. This detector is part of the Messidor® system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution is a promising image mining tool, which has the potential to discover new biomarkers in images.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier B.V. All rights reserved.</p></div></div>",gwenole.quellec@inserm.fr,Deep learning; Diabetic retinopathy screening; Image mining; Lesion detection,https://www.ncbi.nlm.nih.gov//pubmed/28511066,pubmed,2017,b29e2805-09a6-4a21-a319-c75273af500c,1
towards automatic pulmonary nodule management in lung cancer screening with deep learning,/pubmed/28422152,"Ciompi F, Chung K, van Riel SJ, Setio AAA, Gerke PK, Jacobs C, Scholten ET, Schaefer-Prokop C, Wille MMW, Marchianò A, Pastorino U, Prokop M, van Ginneken B.",Sci Rep. 2017 Apr 19;7:46479. doi: 10.1038/srep46479. Erratum in: Sci Rep. 2017 Sep 07;7:46878. ,Sci Rep.  2017,PubMed,citation,PMID:28422152 | PMCID:PMC5395959,pubmed,28422152,create date:2017/04/20 | first author:Ciompi F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type that surpasses the one of classical machine learning approaches and is within the inter-observer variability among four experienced human observers.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28422152,pubmed,2017,71a35644-f382-4000-856d-361d0f6493a1,1
relevance of deep learning to facilitate the diagnosis of her2 status in breast cancer,/pubmed/28378829,"Vandenberghe ME, Scott ML, Scorer PW, Söderberg M, Balcerzak D, Barker C.",Sci Rep. 2017 Apr 5;7:45938. doi: 10.1038/srep45938.,Sci Rep.  2017,PubMed,citation,PMID:28378829 | PMCID:PMC5380996,pubmed,28378829,create date:2017/04/06 | first author:Vandenberghe ME,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Tissue biomarker scoring by pathologists is central to defining the appropriate therapy for patients with cancer. Yet, inter-pathologist variability in the interpretation of ambiguous cases can affect diagnostic accuracy. Modern artificial intelligence methods such as deep learning have the potential to supplement pathologist expertise to ensure constant diagnostic accuracy. We developed a computational approach based on deep learning that automatically scores HER2, a biomarker that defines patient eligibility for anti-HER2 targeted therapies in breast cancer. In a cohort of 71 breast tumour resection samples, automated scoring showed a concordance of 83% with a pathologist. The twelve discordant cases were then independently reviewed, leading to a modification of diagnosis from initial pathologist assessment for eight cases. Diagnostic discordance was found to be largely caused by perceptual differences in assessing HER2 expression due to high HER2 staining heterogeneity. This study provides evidence that deep learning aided diagnosis can facilitate clinical decision making in breast cancer by identifying cases at high risk of misdiagnosis.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28378829,pubmed,2017,beefdcf0-f50e-4997-ae2a-c62fcb11f1e5,1
a deep bag-of-features model for the classification of melanomas in dermoscopy images,/pubmed/28268580,"Sabbaghi S, Aldeen M, Garnavi R.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:1369-1372. doi: 10.1109/EMBC.2016.7590962.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268580,pubmed,28268580,create date:2017/03/09 | first author:Sabbaghi S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning and unsupervised feature learning have received great attention in past years for their ability to transform input data into high level representations using machine learning techniques. Such interest has been growing steadily in the field of medical image diagnosis, particularly in melanoma classification. In this paper, a novel application of deep learning (stacked sparse auto-encoders) is presented for skin lesion classification task. The stacked sparse auto-encoder discovers latent information features in input images (pixel intensities). These high-level features are subsequently fed into a classifier for classifying dermoscopy images. In addition, we proposed a new deep neural network architecture based on bag-of-features (BoF) model, which learns high-level image representation and maps images into BoF space. Then, we examine how using this deep representation of BoF, compared with pixel intensities of images, can improve the classification accuracy. The proposed method is evaluated on a test set of 244 skin images. To test the performance of the proposed method, the area under the receiver operating characteristics curve (AUC) is utilized. The proposed method is found to achieve 95% accuracy.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268580,pubmed,2016,a20e17dd-4643-4b56-97bd-a8fbc1604a32,1
a deep learning based strategy for identifying and associating mitotic activity with gene expression derived risk categories in estrogen receptor positive breast cancers,/pubmed/28192639,"Romo-Bucheli D, Janowczyk A, Gilmore H, Romero E, Madabhushi A.",Cytometry A. 2017 Jun;91(6):566-573. doi: 10.1002/cyto.a.23065. Epub 2017 Feb 13.,Cytometry A.  2017,PubMed,citation,PMID:28192639,pubmed,28192639,create date:2017/02/14 | first author:Romo-Bucheli D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The treatment and management of early stage estrogen receptor positive (ER+) breast cancer is hindered by the difficulty in identifying patients who require adjuvant chemotherapy in contrast to those that will respond to hormonal therapy. To distinguish between the more and less aggressive breast tumors, which is a fundamental criterion for the selection of an appropriate treatment plan, Oncotype DX (ODX) and other gene expression tests are typically employed. While informative, these gene expression tests are expensive, tissue destructive, and require specialized facilities. Bloom-Richardson (BR) grade, the common scheme employed in breast cancer grading, has been shown to be correlated with the Oncotype DX risk score. Unfortunately, studies have also shown that the BR grade determined experiences notable inter-observer variability. One of the constituent categories in BR grading is the mitotic index. The goal of this study was to develop a deep learning (DL) classifier to identify mitotic figures from whole slides images of ER+ breast cancer, the hypothesis being that the number of mitoses identified by the DL classifier would correlate with the corresponding Oncotype DX risk categories. The mitosis detector yielded an average F-score of 0.556 in the AMIDA mitosis dataset using a 6-fold validation setup. For a cohort of 174 whole slide images with early stage ER+ breast cancer for which the corresponding Oncotype DX score was available, the distributions of the number of mitoses identified by the DL classifier was found to be significantly different between the high vs low Oncotype DX risk groups (P &lt; 0.01). Comparisons of other risk groups, using both ODX score and histological grade, were also found to present significantly different automated mitoses distributions. Additionally, a support vector machine classifier trained to separate low/high Oncotype DX risk categories using the mitotic count determined by the DL classifier yielded a 83.19% classification accuracy. © 2017 International Society for Advancement of Cytometry.</abstracttext></p><p class='copyright'>© 2017 International Society for Advancement of Cytometry.</p></div></div>",,breast cancer risk; digital pathology; mitosis detection; whole slide images,https://www.ncbi.nlm.nih.gov//pubmed/28192639,pubmed,2017,6c47f6fa-2cac-4d73-9e8f-7a7c964b76a9,1
deep learning to predict the formation of quinone species in drug metabolism,/pubmed/28099803,"Hughes TB, Swamidass SJ.",Chem Res Toxicol. 2017 Feb 20;30(2):642-656. doi: 10.1021/acs.chemrestox.6b00385. Epub 2017 Feb 2.,Chem Res Toxicol.  2017,PubMed,citation,PMID:28099803,pubmed,28099803,create date:2017/01/19 | first author:Hughes TB,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Many adverse drug reactions are thought to be caused by electrophilically reactive drug metabolites that conjugate to nucleophilic sites within DNA and proteins, causing cancer or toxic immune responses. Quinone species, including quinone-imines, quinone-methides, and imine-methides, are electrophilic Michael acceptors that are often highly reactive and comprise over 40% of all known reactive metabolites. Quinone metabolites are created by cytochromes P450 and peroxidases. For example, cytochromes P450 oxidize acetaminophen to N-acetyl-p-benzoquinone imine, which is electrophilically reactive and covalently binds to nucleophilic sites within proteins. This reactive quinone metabolite elicits a toxic immune response when acetaminophen exceeds a safe dose. Using a deep learning approach, this study reports the first published method for predicting quinone formation: the formation of a quinone species by metabolic oxidation. We model both one- and two-step quinone formation, enabling accurate quinone formation predictions in nonobvious cases. We predict atom pairs that form quinones with an AUC accuracy of 97.6%, and we identify molecules that form quinones with 88.2% AUC. By modeling the formation of quinones, one of the most common types of reactive metabolites, our method provides a rapid screening tool for a key drug toxicity risk. The XenoSite quinone formation model is available at http://swami.wustl.edu/xenosite/p/quinone .</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28099803,pubmed,2017,1bf484d7-3169-491a-bb8c-31e054eff82c,1
deep learning for digital pathology image analysis: a comprehensive tutorial with selected use cases,/pubmed/27563488,"Janowczyk A, Madabhushi A.",J Pathol Inform. 2016 Jul 26;7:29. doi: 10.4103/2153-3539.186902. eCollection 2016.,J Pathol Inform.  2016,PubMed,citation,PMID:27563488 | PMCID:PMC4977982,pubmed,27563488,create date:2016/08/27 | first author:Janowczyk A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Deep learning (DL) is a representation learning approach ideally suited for image analysis challenges in digital pathology (DP). The variety of image analysis tasks in the context of DP includes detection and counting (e.g., mitotic events), segmentation (e.g., nuclei), and tissue classification (e.g., cancerous vs. non-cancerous). Unfortunately, issues with slide preparation, variations in staining and scanning across sites, and vendor platforms, as well as biological variance, such as the presentation of different grades of disease, make these image analysis tasks particularly challenging. Traditional approaches, wherein domain-specific cues are manually identified and developed into task-specific 'handcrafted' features, can require extensive tuning to accommodate these variances. However, DL takes a more domain agnostic approach combining both feature discovery and implementation to maximally discriminate between the classes of interest. While DL approaches have performed well in a few DP related image analysis tasks, such as detection and tissue classification, the currently available open source tools and tutorials do not provide guidance on challenges such as (a) selecting appropriate magnification, (b) managing errors in annotations in the training (or learning) dataset, and (c) identifying a suitable training set containing information rich exemplars. These foundational concepts, which are needed to successfully translate the DL paradigm to DP tasks, are non-trivial for (i) DL experts with minimal digital histology experience, and (ii) DP and image processing experts with minimal DL experience, to derive on their own, thus meriting a dedicated tutorial.</abstracttext></p><h4>AIMS: </h4><p><abstracttext label='AIMS' nlmcategory='OBJECTIVE'>This paper investigates these concepts through seven unique DP tasks as use cases to elucidate techniques needed to produce comparable, and in many cases, superior to results from the state-of-the-art hand-crafted feature-based classification approaches.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Specifically, in this tutorial on DL for DP image analysis, we show how an open source framework (Caffe), with a singular network architecture, can be used to address: (a) nuclei segmentation (F-score of 0.83 across 12,000 nuclei), (b) epithelium segmentation (F-score of 0.84 across 1735 regions), (c) tubule segmentation (F-score of 0.83 from 795 tubules), (d) lymphocyte detection (F-score of 0.90 across 3064 lymphocytes), (e) mitosis detection (F-score of 0.53 across 550 mitotic events), (f) invasive ductal carcinoma detection (F-score of 0.7648 on 50 k testing patches), and (g) lymphoma classification (classification accuracy of 0.97 across 374 images).</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>This paper represents the largest comprehensive study of DL approaches in DP to date, with over 1200 DP images used during evaluation. The supplemental online material that accompanies this paper consists of step-by-step instructions for the usage of the supplied source code, trained models, and input data.</abstracttext></p></div></div>",,Classification; deep learning; detection; digital histology; machine learning; segmentation,https://www.ncbi.nlm.nih.gov//pubmed/27563488,pubmed,2016,b122ee87-c524-4fb5-9308-d43e58e94abd,1
stain normalization using sparse autoencoders (stanosa): application to digital pathology,/pubmed/27373749,"Janowczyk A, Basavanhally A, Madabhushi A.",Comput Med Imaging Graph. 2017 Apr;57:50-61. doi: 10.1016/j.compmedimag.2016.05.003. Epub 2016 May 16.,Comput Med Imaging Graph.  2017,PubMed,citation,PMID:27373749 | PMCID:PMC5112159,pubmed,27373749,create date:2016/07/05 | first author:Janowczyk A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Digital histopathology slides have many sources of variance, and while pathologists typically do not struggle with them, computer aided diagnostic algorithms can perform erratically. This manuscript presents Stain Normalization using Sparse AutoEncoders (StaNoSA) for use in standardizing the color distributions of a test image to that of a single template image. We show how sparse autoencoders can be leveraged to partition images into tissue sub-types, so that color standardization for each can be performed independently. StaNoSA was validated on three experiments and compared against five other color standardization approaches and shown to have either comparable or superior results.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",andrew.janowczyk@case.edu,Deep learning; Digital histopathology; Image processing; Stain Normalization,https://www.ncbi.nlm.nih.gov//pubmed/27373749,pubmed,2017,76ad8b4d-1dbd-40b6-a4db-a7731970ffd2,1
modeling epoxidation of drug-like molecules with a deep machine learning network,/pubmed/27162970,"Hughes TB, Miller GP, Swamidass SJ.",ACS Cent Sci. 2015 Jul 22;1(4):168-80. doi: 10.1021/acscentsci.5b00131. Epub 2015 Jun 9.,ACS Cent Sci.  2015,PubMed,citation,PMID:27162970 | PMCID:PMC4827534,pubmed,27162970,create date:2016/05/11 | first author:Hughes TB,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Drug toxicity is frequently caused by electrophilic reactive metabolites that covalently bind to proteins. Epoxides comprise a large class of three-membered cyclic ethers. These molecules are electrophilic and typically highly reactive due to ring tension and polarized carbon-oxygen bonds. Epoxides are metabolites often formed by cytochromes P450 acting on aromatic or double bonds. The specific location on a molecule that undergoes epoxidation is its site of epoxidation (SOE). Identifying a molecule's SOE can aid in interpreting adverse events related to reactive metabolites and direct modification to prevent epoxidation for safer drugs. This study utilized a database of 702 epoxidation reactions to build a model that accurately predicted sites of epoxidation. The foundation for this model was an algorithm originally designed to model sites of cytochromes P450 metabolism (called XenoSite) that was recently applied to model the intrinsic reactivity of diverse molecules with glutathione. This modeling algorithm systematically and quantitatively summarizes the knowledge from hundreds of epoxidation reactions with a deep convolution network. This network makes predictions at both an atom and molecule level. The final epoxidation model constructed with this approach identified SOEs with 94.9% area under the curve (AUC) performance and separated epoxidized and non-epoxidized molecules with 79.3% AUC. Moreover, within epoxidized molecules, the model separated aromatic or double bond SOEs from all other aromatic or double bonds with AUCs of 92.5% and 95.1%, respectively. Finally, the model separated SOEs from sites of sp(2) hydroxylation with 83.2% AUC. Our model is the first of its kind and may be useful for the development of safer drugs. The epoxidation model is available at http://swami.wustl.edu/xenosite. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27162970,pubmed,2015,995c070e-11d7-4bab-ae8d-f3461a0fd24b,1
detection of prostate cancer using temporal sequences of ultrasound data: a large clinical feasibility study,/pubmed/27059021,"Azizi S, Imani F, Ghavidel S, Tahmasebi A, Kwak JT, Xu S, Turkbey B, Choyke P, Pinto P, Wood B, Mousavi P, Abolmaesumi P.",Int J Comput Assist Radiol Surg. 2016 Jun;11(6):947-56. doi: 10.1007/s11548-016-1395-2. Epub 2016 Apr 8.,Int J Comput Assist Radiol Surg.  2016,PubMed,citation,PMID:27059021,pubmed,27059021,create date:2016/04/10 | first author:Azizi S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>This paper presents the results of a large study involving fusion prostate biopsies to demonstrate that temporal ultrasound can be used to accurately classify tissue labels identified in multi-parametric magnetic resonance imaging (mp-MRI) as suspicious for cancer.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We use deep learning to analyze temporal ultrasound data obtained from 255 cancer foci identified in mp-MRI. Each target is sampled in axial and sagittal planes. A deep belief network is trained to automatically learn the high-level latent features of temporal ultrasound data. A support vector machine classifier is then applied to differentiate cancerous versus benign tissue, verified by histopathology. Data from 32 targets are used for the training, while the remaining 223 targets are used for testing.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our results indicate that the distance between the biopsy target and the prostate boundary, and the agreement between axial and sagittal histopathology of each target impact the classification accuracy. In 84 test cores that are 5 mm or farther to the prostate boundary, and have consistent pathology outcomes in axial and sagittal biopsy planes, we achieve an area under the curve of 0.80. In contrast, all of these targets were labeled as moderately suspicious in mp-MR.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>Using temporal ultrasound data in a fusion prostate biopsy study, we achieved a high classification accuracy specifically for moderately scored mp-MRI targets. These targets are clinically common and contribute to the high false-positive rates associated with mp-MRI for prostate cancer detection. Temporal ultrasound data combined with mp-MRI have the potential to reduce the number of unnecessary biopsies in fusion biopsy settings.</abstracttext></p></div></div>",shazizi@ece.ubc.ca,Cancer diagnosis; Deep belief network; Deep learning; Prostate cancer; Temporal ultrasound data,https://www.ncbi.nlm.nih.gov//pubmed/27059021,pubmed,2016,fbc8d9ab-6d80-4fc8-92e8-b42c188fbbaf,1
novel approaches for diagnosing melanoma skin lesions through supervised and deep learning algorithms,/pubmed/26872778,"Premaladha J, Ravichandran KS.",J Med Syst. 2016 Apr;40(4):96. doi: 10.1007/s10916-016-0460-2. Epub 2016 Feb 12.,J Med Syst.  2016,PubMed,citation,PMID:26872778,pubmed,26872778,create date:2016/02/15 | first author:Premaladha J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Dermoscopy is a technique used to capture the images of skin, and these images are useful to analyze the different types of skin diseases. Malignant melanoma is a kind of skin cancer whose severity even leads to death. Earlier detection of melanoma prevents death and the clinicians can treat the patients to increase the chances of survival. Only few machine learning algorithms are developed to detect the melanoma using its features. This paper proposes a Computer Aided Diagnosis (CAD) system which equips efficient algorithms to classify and predict the melanoma. Enhancement of the images are done using Contrast Limited Adaptive Histogram Equalization technique (CLAHE) and median filter. A new segmentation algorithm called Normalized Otsu's Segmentation (NOS) is implemented to segment the affected skin lesion from the normal skin, which overcomes the problem of variable illumination. Fifteen features are derived and extracted from the segmented images are fed into the proposed classification techniques like Deep Learning based Neural Networks and Hybrid Adaboost-Support Vector Machine (SVM) algorithms. The proposed system is tested and validated with nearly 992 images (malignant &amp; benign lesions) and it provides a high classification accuracy of 93 %. The proposed CAD system can assist the dermatologists to confirm the decision of the diagnosis and to avoid excisional biopsies.</abstracttext></p></div></div>",premaladha@sastra.edu,Adaboost; Artificial neural networks; Classification; Deep learning; Preprocessing; Segmentation; Support vector machine,https://www.ncbi.nlm.nih.gov//pubmed/26872778,pubmed,2016,54f959da-aa65-4e5a-ab80-4c5a785bdb73,1
a comparative study for chest radiograph image retrieval using binary texture and deep learning classification,/pubmed/26736908,"Anavi Y, Kogan I, Gelbart E, Geva O, Greenspan H.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:2940-3. doi: 10.1109/EMBC.2015.7319008.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736908,pubmed,26736908,create date:2016/01/07 | first author:Anavi Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the 'descriptor'-based approach); the second approach ('classification'-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736908,pubmed,2015,867c91c0-4a70-4c36-a1ee-56423b9dba9c,1
cell recognition based on topological sparse coding for microscopy imaging of focused ultrasound treatment,/pubmed/26498225,"Wang Z, Zhu J, Xue Y, Song C, Bi N.",BMC Med Imaging. 2015 Oct 24;15:46. doi: 10.1186/s12880-015-0087-7.,BMC Med Imaging.  2015,PubMed,citation,PMID:26498225 | PMCID:PMC4620025,pubmed,26498225,create date:2015/10/27 | first author:Wang Z,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Ultrasound is considered a reliable, widely available, non-invasive, and inexpensive imaging technique for assessing and detecting the development phases of cancer; both in vivo and ex vivo, and for understanding the effects on cell cycle and viability after ultrasound treatment.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Based on the topological continuity characteristics, and that adjacent points or areas represent similar features, we propose a topological penalized convex objective function of sparse coding, to recognize similar cell phases.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>This method introduces new features using a deep learning method of sparse coding with topological continuity characteristics. Large-scale comparison tests demonstrate that the RAW can outperform SIFT GIST and HoG as the input features with this method, achieving higher sensitivity, specificity, F1 score, and accuracy.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Experimental results show that the proposed topological sparse coding technique is valid and effective for extracting new features, and the proposed system was effective for cell recognition of microscopy images of theMDA-MB-231 cell line. This method allows features from sparse coding learning methods to have topological continuity characteristics, and the RAW features are more applicable for the deep learning of the topological sparse coding method than SIFT GIST and HoG.</abstracttext></p></div></div>",zhenyouw@gdut.edu.cn,,https://www.ncbi.nlm.nih.gov//pubmed/26498225,pubmed,2015,9532bfce-7ef9-4018-83b1-3379d4c5650d,1
stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images,/pubmed/26208307,"Xu J, Xiang L, Liu Q, Gilmore H, Wu J, Tang J, Madabhushi A.",IEEE Trans Med Imaging. 2016 Jan;35(1):119-30. doi: 10.1109/TMI.2015.2458702. Epub 2015 Jul 20.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26208307 | PMCID:PMC4729702,pubmed,26208307,create date:2015/07/25 | first author:Xu J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of 'Deep Learning' strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26208307,pubmed,2016,3906ee50-bda8-441f-998c-8a14be28abfd,1
high-order neural networks and kernel methods for peptide-mhc binding prediction,/pubmed/26206306,"Kuksa PP, Min MR, Dugar R, Gerstein M.",Bioinformatics. 2015 Nov 15;31(22):3600-7. doi: 10.1093/bioinformatics/btv371. Epub 2015 Jul 23.,Bioinformatics.  2015,PubMed,citation,PMID:26206306,pubmed,26206306,create date:2015/07/25 | first author:Kuksa PP,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Effective computational methods for peptide-protein binding prediction can greatly help clinical peptide vaccine search and design. However, previous computational methods fail to capture key nonlinear high-order dependencies between different amino acid positions. As a result, they often produce low-quality rankings of strong binding peptides. To solve this problem, we propose nonlinear high-order machine learning methods including high-order neural networks (HONNs) with possible deep extensions and high-order kernel support vector machines to predict major histocompatibility complex-peptide binding.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The proposed high-order methods improve quality of binding predictions over other prediction methods. With the proposed methods, a significant gain of up to 25-40% is observed on the benchmark and reference peptide datasets and tasks. In addition, for the first time, our experiments show that pre-training with high-order semi-restricted Boltzmann machines significantly improves the performance of feed-forward HONNs. Moreover, our experiments show that the proposed shallow HONN outperform the popular pre-trained deep neural network on most tasks, which demonstrates the effectiveness of modelling high-order feature interactions for predicting major histocompatibility complex-peptide binding.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>There is no associated distributable software.</abstracttext></p><h4>CONTACT: </h4><p><abstracttext label='CONTACT' nlmcategory='BACKGROUND'>renqiang@nec-labs.com or mark.gerstein@yale.edu</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26206306,pubmed,2015,3418f11a-4983-4cd1-a3cd-556ab1f9784e,1
imaging-based enrichment criteria using deep learning algorithms for efficient clinical trials in mild cognitive impairment,/pubmed/26093156,"Ithapu VK, Singh V, Okonkwo OC, Chappell RJ, Dowling NM, Johnson SC; Alzheimer's Disease Neuroimaging Initiative..",Alzheimers Dement. 2015 Dec;11(12):1489-1499. doi: 10.1016/j.jalz.2015.01.010. Epub 2015 Jun 18.,Alzheimers Dement.  2015,PubMed,citation,PMID:26093156 | PMCID:PMC4684492,pubmed,26093156,create date:2015/06/21 | first author:Ithapu VK,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The mild cognitive impairment (MCI) stage of Alzheimer's disease (AD) may be optimal for clinical trials to test potential treatments for preventing or delaying decline to dementia. However, MCI is heterogeneous in that not all cases progress to dementia within the time frame of a trial and some may not have underlying AD pathology. Identifying those MCIs who are most likely to decline during a trial and thus most likely to benefit from treatment will improve trial efficiency and power to detect treatment effects. To this end, using multimodal, imaging-derived, inclusion criteria may be especially beneficial. Here, we present a novel multimodal imaging marker that predicts future cognitive and neural decline from [F-18]fluorodeoxyglucose positron emission tomography (PET), amyloid florbetapir PET, and structural magnetic resonance imaging, based on a new deep learning algorithm (randomized denoising autoencoder marker, rDAm). Using ADNI2 MCI data, we show that using rDAm as a trial enrichment criterion reduces the required sample estimates by at least five times compared with the no-enrichment regime and leads to smaller trials with high statistical power, compared with existing methods.</abstracttext></p><p class='copyright'>Copyright © 2015 The Alzheimer's Association. Published by Elsevier Inc. All rights reserved.</p></div></div>",ithapu@wisc.edu,Alzheimer's disease; Clinical trials; Deep learning; Sample enrichment,https://www.ncbi.nlm.nih.gov//pubmed/26093156,pubmed,2015,a5edae6b-0de7-40a6-b3c9-57df1c046ebe,1
accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning,/pubmed/25966470,"Song Y, Zhang L, Chen S, Ni D, Lei B, Wang T.",IEEE Trans Biomed Eng. 2015 Oct;62(10):2421-33. doi: 10.1109/TBME.2015.2430895. Epub 2015 May 7.,IEEE Trans Biomed Eng.  2015,PubMed,citation,PMID:25966470,pubmed,25966470,create date:2015/05/13 | first author:Song Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25966470,pubmed,2015,9184e99b-d3a4-4195-b0da-428302fb29d4,1
multi-level gene/mirna feature selection using deep belief nets and active learning,/pubmed/25570858,"Ibrahim R, Yousri NA, Ismail MA, El-Makky NM.",Conf Proc IEEE Eng Med Biol Soc. 2014;2014:3957-60. doi: 10.1109/EMBC.2014.6944490.,Conf Proc IEEE Eng Med Biol Soc.  2014,PubMed,citation,PMID:25570858,pubmed,25570858,create date:2015/01/09 | first author:Ibrahim R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Selecting the most discriminative genes/miRNAs has been raised as an important task in bioinformatics to enhance disease classifiers and to mitigate the dimensionality curse problem. Original feature selection methods choose genes/miRNAs based on their individual features regardless of how they perform together. Considering group features instead of individual ones provides a better view for selecting the most informative genes/miRNAs. Recently, deep learning has proven its ability in representing the data in multiple levels of abstraction, allowing for better discrimination between different classes. However, the idea of using deep learning for feature selection is not widely used in the bioinformatics field yet. In this paper, a novel multi-level feature selection approach named MLFS is proposed for selecting genes/miRNAs based on expression profiles. The approach is based on both deep and active learning. Moreover, an extension to use the technique for miRNAs is presented by considering the biological relation between miRNAs and genes. Experimental results show that the approach was able to outperform classical feature selection methods in hepatocellular carcinoma (HCC) by 9%, lung cancer by 6% and breast cancer by around 10% in F1-measure. Results also show the enhancement in F1-measure of our approach over recently related work in [1] and [2]. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25570858,pubmed,2014,b458acbd-96bc-4b2a-8ebe-cca38e0e9753,1
a deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei,/pubmed/25570598,"Song Y, Zhang L, Chen S, Ni D, Li B, Zhou Y, Lei B, Wang T.",Conf Proc IEEE Eng Med Biol Soc. 2014;2014:2903-6. doi: 10.1109/EMBC.2014.6944230.,Conf Proc IEEE Eng Med Biol Soc.  2014,PubMed,citation,PMID:25570598,pubmed,25570598,create date:2015/01/09 | first author:Song Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, a superpixel and convolution neural network (CNN) based segmentation method is proposed for cervical cancer cell segmentation. Since the background and cytoplasm contrast is not relatively obvious, cytoplasm segmentation is first performed. Deep learning based on CNN is explored for region of interest detection. A coarse-to-fine nucleus segmentation for cervical cancer cell segmentation and further refinement is also developed. Experimental results show that an accuracy of 94.50% is achieved for nucleus region detection and a precision of 0.9143±0.0202 and a recall of 0.8726±0.0008 are achieved for nucleus cell segmentation. Furthermore, our comparative analysis also shows that the proposed method outperforms the related methods. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25570598,pubmed,2014,576f87fc-6b11-427b-a632-c93aeeb263af,1
randomized denoising autoencoders for smaller and efficient imaging based ad clinical trials,/pubmed/25485413,"Ithapul VK, Singh V, Okonkwo O, Johnson SC.",Med Image Comput Comput Assist Interv. 2014;17(Pt 2):470-8.,Med Image Comput Comput Assist Interv.  2014,PubMed,citation,PMID:25485413 | PMCID:PMC4390084,pubmed,25485413,create date:2014/12/09 | first author:Ithapul VK,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>There is growing body of research devoted to designing imaging-based biomarkers that identify Alzheimer's disease (AD) in its prodromal stage using statistical machine learning methods. Recently several authors investigated how clinical trials for AD can be made more efficient (i.e., smaller sample size) using predictive measures from such classification methods. In this paper, we explain why predictive measures given by such SVM type objectives may be less than ideal for use in the setting described above. We give a solution based on a novel deep learning model, randomized denoising autoencoders (rDA), which regresses on training labels y while also accounting for the variance, a property which is very useful for clinical trial design. Our results give strong improvements in sample size estimates over strategies based on multi-kernel learning. Also, rDA predictions appear to more accurately correlate to stages of disease. Separately, our formulation empirically shows how deep architectures can be applied in the large d, small n regime--the default situation in medical imaging. This result is of independent interest.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25485413,pubmed,2014,6f11c1ad-54b4-4e7c-8f08-a6bef6025429,1
modeling the variability in brain morphology and lesion distribution in multiple sclerosis by deep learning,/pubmed/25485412,"Brosch T, Yoo Y, Li DK, Traboulsee A, Tam R.",Med Image Comput Comput Assist Interv. 2014;17(Pt 2):462-9.,Med Image Comput Comput Assist Interv.  2014,PubMed,citation,PMID:25485412,pubmed,25485412,create date:2014/12/09 | first author:Brosch T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Changes in brain morphology and white matter lesions are two hallmarks of multiple sclerosis (MS) pathology, but their variability beyond volumetrics is poorly characterized. To further our understanding of complex MS pathology, we aim to build a statistical model of brain images that can automatically discover spatial patterns of variability in brain morphology and lesion distribution. We propose building such a model using a deep belief network (DBN), a layered network whose parameters can be learned from training images. In contrast to other manifold learning algorithms, the DBN approach does not require a prebuilt proximity graph, which is particularly advantageous for modeling lesions, because their sparse and random nature makes defining a suitable distance measure between lesion images challenging. Our model consists of a morphology DBN, a lesion DBN, and a joint DBN that models concurring morphological and lesion patterns. Our results show that this model can automatically discover the classic patterns of MS pathology, as well as more subtle ones, and that the parameters computed have strong relationships to MS clinical scores.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25485412,pubmed,2014,30cf274a-0235-4aed-a669-e9692730c37f,1
segmenting hippocampus from infant brains by sparse patch matching with deep-learned features,/pubmed/25485393,"Guo Y, Wu G, Commander LA, Szary S, Jewells V, Lin W, Shent D.",Med Image Comput Comput Assist Interv. 2014;17(Pt 2):308-15.,Med Image Comput Comput Assist Interv.  2014,PubMed,citation,PMID:25485393 | PMCID:PMC4445142,pubmed,25485393,create date:2014/12/09 | first author:Guo Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1- and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the interpatch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25485393,pubmed,2014,f51158c4-db40-478b-9a9b-e0ff75d53ac6,1
multimodal neuroimaging feature learning for multiclass diagnosis of alzheimer's disease,/pubmed/25423647,"Liu S, Liu S, Cai W, Che H, Pujol S, Kikinis R, Feng D, Fulham MJ; ADNI..",IEEE Trans Biomed Eng. 2015 Apr;62(4):1132-40. doi: 10.1109/TBME.2014.2372011. Epub 2014 Nov 20.,IEEE Trans Biomed Eng.  2015,PubMed,citation,PMID:25423647 | PMCID:PMC4394860,pubmed,25423647,create date:2014/11/26 | first author:Liu S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25423647,pubmed,2015,22c9d874-4712-4e29-93e2-5b2c3e121d98,1
anatomical and diffusion mri of deep gray matter in pediatric spina bifida,/pubmed/25057465,"Ware AL, Juranek J, Williams VJ, Cirino PT, Dennis M, Fletcher JM.",Neuroimage Clin. 2014 Jun 2;5:120-7. doi: 10.1016/j.nicl.2014.05.012. eCollection 2014.,Neuroimage Clin.  2014,PubMed,citation,PMID:25057465 | PMCID:PMC4097001,pubmed,25057465,create date:2014/07/25 | first author:Ware AL,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Individuals with spina bifida myelomeningocele (SBM) exhibit brain abnormalities in cortical thickness, white matter integrity, and cerebellar structure. Little is known about deep gray matter macro- and microstructure in this population. The current study utilized volumetric and diffusion-weighted MRI techniques to examine gray matter volume and microstructure in several subcortical structures: basal ganglia nuclei, thalamus, hippocampus, and amygdala. Sixty-six children and adolescents (ages 8-18; M = 12.0, SD = 2.73) with SBM and typically developing (TD) controls underwent T1- and diffusion-weighted neuroimaging. Microstructural results indicated that hippocampal volume was disproportionately reduced, whereas the putamen volume was enlarged in the group with SBM. Microstructural analyses indicated increased mean diffusivity (MD) and fractional anisotropy (FA) in the gray matter of most examined structures (i.e., thalamus, caudate, hippocampus), with the putamen exhibiting a unique pattern of decreased MD and increased FA. These results provide further support that SBM differentially disrupts brain regions whereby some structures are volumetrically normal whereas others are reduced or enlarged. In the hippocampus, volumetric reduction coupled with increased MD may imply reduced cellular density and aberrant organization. Alternatively, the enlarged volume and significantly reduced MD in the putamen suggest increased density. </abstracttext></p></div></div>",,DTI; Hydrocephalus; Myelomeningocele; Subcortical gray matter,https://www.ncbi.nlm.nih.gov//pubmed/25057465,pubmed,2014,f69f3f62-032e-4d7b-9910-904d8f1b735f,1
diagnosing autism spectrum disorder from brain resting-state functional connectivity patterns using a deep neural network with a novel feature selection method,/pubmed/28871217,"Guo X, Dominick KC, Minai AA, Li H, Erickson CA, Lu LJ.",Front Neurosci. 2017 Aug 21;11:460. doi: 10.3389/fnins.2017.00460. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28871217 | PMCID:PMC5566619,pubmed,28871217,create date:2017/09/06 | first author:Guo X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The whole-brain functional connectivity (FC) pattern obtained from resting-state functional magnetic resonance imaging data are commonly applied to study neuropsychiatric conditions such as autism spectrum disorder (ASD) by using different machine learning models. Recent studies indicate that both hyper- and hypo- aberrant ASD-associated FCs were widely distributed throughout the entire brain rather than only in some specific brain regions. Deep neural networks (DNN) with multiple hidden layers have shown the ability to systematically extract lower-to-higher level information from high dimensional data across a series of neural hidden layers, significantly improving classification accuracy for such data. In this study, a DNN with a novel feature selection method (DNN-FS) is developed for the high dimensional whole-brain resting-state FC pattern classification of ASD patients vs. typical development (TD) controls. The feature selection method is able to help the DNN generate low dimensional high-quality representations of the whole-brain FC patterns by selecting features with high discriminating power from multiple trained sparse auto-encoders. For the comparison, a DNN without the feature selection method (DNN-woFS) is developed, and both of them are tested with different architectures (i.e., with different numbers of hidden layers/nodes). Results show that the best classification accuracy of <b>86.36%</b> is generated by the DNN-FS approach with 3 hidden layers and 150 hidden nodes (3/150). Remarkably, DNN-FS outperforms DNN-woFS for all architectures studied. The most significant accuracy improvement was <b>9.09%</b> with the 3/150 architecture. The method also outperforms other feature selection methods, e.g., two sample <i>t</i>-test and elastic net. In addition to improving the classification accuracy, a Fisher's score-based biomarker identification method based on the DNN is also developed, and used to identify 32 FCs related to ASD. These FCs come from or cross different pre-defined brain networks including the default-mode, cingulo-opercular, frontal-parietal, and cerebellum. Thirteen of them are statically significant between ASD and TD groups (two sample <i>t</i>-test <i>p</i> &lt; 0.05) while 19 of them are not. The relationship between the statically significant FCs and the corresponding ASD behavior symptoms is discussed based on the literature and clinician's expert knowledge. Meanwhile, the potential reason of obtaining 19 FCs which are not statistically significant is also provided.</abstracttext></p></div></div>",,autism spectrum disorder; deep neural network; feature selection; resting-state fMRI; sparse auto-encoder,https://www.ncbi.nlm.nih.gov//pubmed/28871217,pubmed,2017,8b9bbbcd-5a72-4ec1-8d92-189820db10a2,1
a deep learning-based radiomics model for prediction of survival in glioblastoma multiforme,/pubmed/28871110,"Lao J, Chen Y, Li ZC, Li Q, Zhang J, Liu J, Zhai G.",Sci Rep. 2017 Sep 4;7(1):10353. doi: 10.1038/s41598-017-10649-8.,Sci Rep.  2017,PubMed,citation,PMID:28871110,pubmed,28871110,create date:2017/09/06 | first author:Lao J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Traditional radiomics models mainly rely on explicitly-designed handcrafted features from medical images. This paper aimed to investigate if deep features extracted via transfer learning can generate radiomics signatures for prediction of overall survival (OS) in patients with Glioblastoma Multiforme (GBM). This study comprised a discovery data set of 75 patients and an independent validation data set of 37 patients. A total of 1403 handcrafted features and 98304 deep features were extracted from preoperative multi-modality MR images. After feature selection, a six-deep-feature signature was constructed by using the least absolute shrinkage and selection operator (LASSO) Cox regression model. A radiomics nomogram was further presented by combining the signature and clinical risk factors such as age and Karnofsky Performance Score. Compared with traditional risk factors, the proposed signature achieved better performance for prediction of OS (C-index = 0.710, 95% CI: 0.588, 0.932) and significant stratification of patients into prognostically distinct groups (P &lt; 0.001, HR = 5.128, 95% CI: 2.029, 12.960). The combined model achieved improved predictive performance (C-index = 0.739). Our study demonstrates that transfer learning-based deep features are able to generate prognostic imaging signature for OS prediction and patient stratification for GBM, indicating the potential of deep imaging feature-based biomarker in preoperative care of GBM patients.</abstracttext></p></div></div>",zc.li@siat.ac.cn,,https://www.ncbi.nlm.nih.gov//pubmed/28871110,pubmed,2017,2eba70fb-4e55-4028-ad22-3e7114004e2a,1
relaynet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks,/pubmed/28856040,"Roy AG, Conjeti S, Karri SPK, Sheet D, Katouzian A, Wachinger C, Navab N.",Biomed Opt Express. 2017 Jul 13;8(8):3627-3642. doi: 10.1364/BOE.8.003627. eCollection 2017 Aug 1.,Biomed Opt Express.  2017,PubMed,citation,PMID:28856040 | PMCID:PMC5560830,pubmed,28856040,create date:2017/09/01 | first author:Roy AG,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Optical coherence tomography (OCT) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers. In this paper, we propose a new fully convolutional deep architecture, termed ReLayNet, for end-to-end segmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses a contracting path of convolutional blocks (encoders) to learn a hierarchy of contextual features, followed by an expansive path of convolutional blocks (decoders) for semantic segmentation. ReLayNet is trained to optimize a joint loss function comprising of weighted logistic regression and Dice overlap loss. The framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness.</abstracttext></p></div></div>",abhijit.guha-roy@tum.de,(070.5010) Pattern recognition; (110.4500) Optical coherence tomography; (170.5755) Retina scanning,https://www.ncbi.nlm.nih.gov//pubmed/28856040,pubmed,2017,cc974150-f9f2-4e32-9145-f0a38402b426,1
point-of-care mobile digital microscopy and deep learning for the detection of soil-transmitted helminths and schistosoma haematobium,/pubmed/28838305,"Holmström O, Linder N, Ngasala B, Mårtensson A, Linder E, Lundin M, Moilanen H, Suutala A, Diwan V, Lundin J.",Glob Health Action. 2017 Jun;10(sup3):1337325. doi: 10.1080/16549716.2017.1337325.,Glob Health Action.  2017,PubMed,citation,PMID:28838305,pubmed,28838305,create date:2017/08/26 | first author:Holmström O,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Microscopy remains the gold standard in the diagnosis of neglected tropical diseases. As resource limited, rural areas often lack laboratory equipment and trained personnel, new diagnostic techniques are needed. Low-cost, point-of-care imaging devices show potential in the diagnosis of these diseases. Novel, digital image analysis algorithms can be utilized to automate sample analysis.</abstracttext></p><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Evaluation of the imaging performance of a miniature digital microscopy scanner for the diagnosis of soil-transmitted helminths and Schistosoma haematobium, and training of a deep learning-based image analysis algorithm for automated detection of soil-transmitted helminths in the captured images.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A total of 13 iodine-stained stool samples containing Ascaris lumbricoides, Trichuris trichiura and hookworm eggs and 4 urine samples containing Schistosoma haematobium were digitized using a reference whole slide-scanner and the mobile microscopy scanner. Parasites in the images were identified by visual examination and by analysis with a deep learning-based image analysis algorithm in the stool samples. Results were compared between the digital and visual analysis of the images showing helminth eggs.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Parasite identification by visual analysis of digital slides captured with the mobile microscope was feasible for all analyzed parasites. Although the spatial resolution of the reference slide-scanner is higher, the resolution of the mobile microscope is sufficient for reliable identification and classification of all parasites studied. Digital image analysis of stool sample images captured with the mobile microscope showed high sensitivity for detection of all helminths studied (range of sensitivity = 83.3-100%) in the test set (n = 217) of manually labeled helminth eggs.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>In this proof-of-concept study, the imaging performance of a mobile, digital microscope was sufficient for visual detection of soil-transmitted helminths and Schistosoma haematobium. Furthermore, we show that deep learning-based image analysis can be utilized for the automated detection and classification of helminths in the captured images.</abstracttext></p></div></div>",,Neglected tropical diseases; computer vision; global health; helminth; mHealth for Improved Access and Equity in Health Care; point-of-care,https://www.ncbi.nlm.nih.gov//pubmed/28838305,pubmed,2017,a083e5a4-8a19-4a79-b7fc-14faa7d1f6e1,1
automatic detection of hemorrhagic pericardial effusion on pmct using deep learning - a feasibility study,/pubmed/28819715,"Ebert LC, Heimer J, Schweitzer W, Sieberth T, Leipner A, Thali M, Ampanozi G.",Forensic Sci Med Pathol. 2017 Aug 18. doi: 10.1007/s12024-017-9906-1. [Epub ahead of print],Forensic Sci Med Pathol.  2017,PubMed,citation,PMID:28819715,pubmed,28819715,create date:2017/08/19 | first author:Ebert LC,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Post mortem computed tomography (PMCT) can be used as a triage tool to better identify cases with a possibly non-natural cause of death, especially when high caseloads make it impossible to perform autopsies on all cases. Substantial data can be generated by modern medical scanners, especially in a forensic setting where the entire body is documented at high resolution. A solution for the resulting issues could be the use of deep learning techniques for automatic analysis of radiological images. In this article, we wanted to test the feasibility of such methods for forensic imaging by hypothesizing that deep learning methods can detect and segment a hemopericardium in PMCT. For deep learning image analysis software, we used the ViDi Suite 2.0. We retrospectively selected 28 cases with, and 24 cases without, hemopericardium. Based on these data, we trained two separate deep learning networks. The first one classified images into hemopericardium/not hemopericardium, and the second one segmented the blood content. We randomly selected 50% of the data for training and 50% for validation. This process was repeated 20 times. The best performing classification network classified all cases of hemopericardium from the validation images correctly with only a few false positives. The best performing segmentation network would tend to underestimate the amount of blood in the pericardium, which is the case for most networks. This is the first study that shows that deep learning has potential for automated image analysis of radiological images in forensic medicine.</abstracttext></p></div></div>",Lars.ebert@virtopsy.com,Deep learning; Forensic imaging; Hemopericardium; Neural networks; PMCT,https://www.ncbi.nlm.nih.gov//pubmed/28819715,pubmed,2017,90264942-a70e-41c4-9495-babe4baff46e,1
deep learning poised to revolutionise diagnostic imaging,/pubmed/28734821,Furlow B.,Lancet Respir Med. 2017 Jul 19. pii: S2213-2600(17)30292-8. doi: 10.1016/S2213-2600(17)30292-8. [Epub ahead of print] No abstract available. ,Lancet Respir Med.  2017,PubMed,citation,PMID:28734821,pubmed,28734821,create date:2017/07/25 | first author:Furlow B,,,,https://www.ncbi.nlm.nih.gov//pubmed/28734821,pubmed,2017,6ea51b22-6990-4b4b-b496-7abe6448ad04,1
constrained deep weak supervision for histopathology image segmentation,/pubmed/28692971,"Jia Z, Huang X, Chang EI, Xu Y.",IEEE Trans Med Imaging. 2017 Jul 7. doi: 10.1109/TMI.2017.2724070. [Epub ahead of print],IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28692971,pubmed,28692971,create date:2017/07/12 | first author:Jia Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-toimage weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28692971,pubmed,2017,bfffca90-88eb-4036-91e5-45058f85c96e,1
breast cancer multi-classification from histopathological images with structured deep learning model,/pubmed/28646155,"Han Z, Wei B, Zheng Y, Yin Y, Li K, Li S.",Sci Rep. 2017 Jun 23;7(1):4172. doi: 10.1038/s41598-017-04075-z.,Sci Rep.  2017,PubMed,citation,PMID:28646155 | PMCID:PMC5482871,pubmed,28646155,create date:2017/06/25 | first author:Han Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated breast cancer multi-classification from histopathological images plays a key role in computer-aided breast cancer diagnosis or prognosis. Breast cancer multi-classification is to identify subordinate classes of breast cancer (Ductal carcinoma, Fibroadenoma, Lobular carcinoma, etc.). However, breast cancer multi-classification from histopathological images faces two main challenges from: (1) the great difficulties in breast cancer multi-classification methods contrasting with the classification of binary classes (benign and malignant), and (2) the subtle differences in multiple classes due to the broad variability of high-resolution image appearances, high coherency of cancerous cells, and extensive inhomogeneity of color distribution. Therefore, automated breast cancer multi-classification from histopathological images is of great clinical significance yet has never been explored. Existing works in literature only focus on the binary classification but do not support further breast cancer quantitative assessment. In this study, we propose a breast cancer multi-classification method using a newly proposed deep learning model. The structured deep learning model has achieved remarkable performance (average 93.2% accuracy) on a large-scale dataset, which demonstrates the strength of our method in providing an efficient tool for breast cancer multi-classification in clinical settings.</abstracttext></p></div></div>",wbz99@sina.com,,https://www.ncbi.nlm.nih.gov//pubmed/28646155,pubmed,2017,445e091e-d8a7-4e31-a501-26eaf91c882c,1
marginal shape deep learning: applications to pediatric lung field segmentation,/pubmed/28592911,"Mansoor A, Cerrolaza JJ, Perez G, Biggs E, Nino G, Linguraru MG.",Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10133. pii: 1013304. doi: 10.1117/12.2254412. Epub 2017 Feb 24.,Proc SPIE Int Soc Opt Eng.  2017,PubMed,citation,PMID:28592911 | PMCID:PMC5459493,pubmed,28592911,create date:2017/06/09 | first author:Mansoor A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Representation learning through deep learning (DL) architecture has shown tremendous potential for identification, localization, and texture classification in various medical imaging modalities. However, DL applications to segmentation of objects especially to deformable objects are rather limited and mostly restricted to pixel classification. In this work, we propose marginal shape deep learning (MaShDL), a framework that extends the application of DL to deformable shape segmentation by using deep classifiers to estimate the shape parameters. MaShDL combines the strength of statistical shape models with the automated feature learning architecture of DL. Unlike the iterative shape parameters estimation approach of classical shape models that often leads to a local minima, the proposed framework is robust to local minima optimization and illumination changes. Furthermore, since the direct application of DL framework to a multi-parameter estimation problem results in a very high complexity, our framework provides an excellent run-time performance solution by independently learning shape parameter classifiers in marginal eigenspaces in the decreasing order of variation. We evaluated MaShDL for segmenting the lung field from 314 normal and abnormal pediatric chest radiographs and obtained a mean Dice similarity coefficient of 0.927 using only the four highest modes of variation (compared to 0.888 with classical ASM<sup>1</sup> (p-value=0.01) using same configuration). To the best of our knowledge this is the first demonstration of using DL framework for parametrized shape learning for the delineation of deformable objects.</abstracttext></p></div></div>",,chest radiograph; deep learning; lung field; shape learning; statistical shape models,https://www.ncbi.nlm.nih.gov//pubmed/28592911,pubmed,2017,427dc5db-caa5-43b9-8be4-f5a40e40529c,1
"a neural network approach for fast, automated quantification of dir performance",/pubmed/28477340,"Neylon J, Min Y, Low DA, Santhanam A.",Med Phys. 2017 Aug;44(8):4126-4138. doi: 10.1002/mp.12321. Epub 2017 Jul 17.,Med Phys.  2017,PubMed,citation,PMID:28477340,pubmed,28477340,create date:2017/05/10 | first author:Neylon J,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>A critical step in adaptive radiotherapy (ART) workflow is deformably registering the simulation CT with the daily or weekly volumetric imaging. Quantifying the deformable image registration accuracy under these circumstances is a complex task due to the lack of known ground-truth landmark correspondences between the source data and target data. Generating landmarks manually (using experts) is time-consuming, and limited by image quality and observer variability. While image similarity metrics (ISM) may be used as an alternative approach to quantify the registration error, there is a need to characterize the ISM values by developing a nonlinear cost function and translate them to physical distance measures in order to enable fast, quantitative comparison of registration performance.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>In this paper, we present a proof-of-concept methodology for automated quantification of DIR performance. A nonlinear cost function was developed as a combination of ISM values and governed by the following two expectations for an accurate registration: (a) the deformed data obtained from transforming the simulation CT data with the deformation vector field (DVF) should match the target image data with near perfect similarity, and (b) the similarity between the simulation CT and deformed data should match the similarity between the simulation CT and the target image data. A deep neural network (DNN) was developed that translated the cost function values to actual physical distance measure. To train the neural network, patient-specific biomechanical models of the head-and-neck anatomy were employed. The biomechanical model anatomy was systematically deformed to represent changes in patient posture and physiological regression. Volumetric source and target images with known ground-truth deformations vector fields were then generated, representing the daily or weekly imaging data. Annotated data was then fed through a supervised machine learning process, iteratively optimizing a nonlinear model able to predict the target registration error (TRE) for given ISM values. The cost function for sub-volumes enclosing critical radiotherapy structures in the head-and-neck region were computed and compared with the ground truth TRE values.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>When examining different combinations of registration parameters for a single DIR, the neural network was able to quantify DIR error to within a single voxel for 95% of the sub-volumes examined. In addition, correlations between the neural network predicted error and the ground-truth TRE for the Planning Target Volume and the parotid contours were consistently observed to be &gt; 0.9. For variations in posture and tumor regression for 10 different patients, patient-specific neural networks predicted the TRE to within a single voxel &gt; 90% on average.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The formulation presented in this paper demonstrates the ability for fast, accurate quantification of registration performance. DNN provided the necessary level of abstraction to estimate a quantified TRE from the ISM expectations described above, when sufficiently trained on annotated data. In addition, biomechanical models facilitated the DNN with the required variations in the patient posture and physiological regression. With further development and validation on clinical patient data, such networks have potential impact in patient and site-specific optimization, and stream-lining clinical registration validation.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,DIR; accuracy; machine learning; neural network; radiotherapy,https://www.ncbi.nlm.nih.gov//pubmed/28477340,pubmed,2017,5f9565d4-9dfe-472f-89ed-59a8d747bca7,1
automatic skin lesion segmentation using deep fully convolutional networks with jaccard distance,/pubmed/28436853,"Yuan Y, Chao M, Lo YC.",IEEE Trans Med Imaging. 2017 Sep;36(9):1876-1886. doi: 10.1109/TMI.2017.2695227. Epub 2017 Apr 18.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28436853,pubmed,28436853,create date:2017/04/25 | first author:Yuan Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 skin lesion analysis towards melanoma detection challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28436853,pubmed,2017,1b208097-8a70-43c9-b2e0-4acba990ed69,1
a de-identification pipeline for ultrasound medical images in dicom format,/pubmed/28405948,"Monteiro E, Costa C, Oliveira JL.",J Med Syst. 2017 May;41(5):89. doi: 10.1007/s10916-017-0736-1. Epub 2017 Apr 13.,J Med Syst.  2017,PubMed,citation,PMID:28405948,pubmed,28405948,create date:2017/04/14 | first author:Monteiro E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Clinical data sharing between healthcare institutions, and between practitioners is often hindered by privacy protection requirements. This problem is critical in collaborative scenarios where data sharing is fundamental for establishing a workflow among parties. The anonymization of patient information burned in DICOM images requires elaborate processes somewhat more complex than simple de-identification of textual information. Usually, before sharing, there is a need for manual removal of specific areas containing sensitive information in the images. In this paper, we present a pipeline for ultrasound medical image de-identification, provided as a free anonymization REST service for medical image applications, and a Software-as-a-Service to streamline automatic de-identification of medical images, which is freely available for end-users. The proposed approach applies image processing functions and machine-learning models to bring about an automatic system to anonymize medical images. To perform character recognition, we evaluated several machine-learning models, being Convolutional Neural Networks (CNN) selected as the best approach. For accessing the system quality, 500 processed images were manually inspected showing an anonymization rate of 89.2%. The tool can be accessed at https://bioinformatics.ua.pt/dicom/anonymizer and it is available with the most recent version of Google Chrome, Mozilla Firefox and Safari. A Docker image containing the proposed service is also publicly available for the community.</abstracttext></p></div></div>",eriksson.monteiro@ua.pt,De-identification; Deep-learning; Medical imaging; Neural networks; OCR,https://www.ncbi.nlm.nih.gov//pubmed/28405948,pubmed,2017,f894a3ae-a19c-49b6-b3b0-762e8c9cbd72,1
automated identification of diabetic retinopathy using deep learning,/pubmed/28359545,"Gargeya R, Leng T.",Ophthalmology. 2017 Jul;124(7):962-969. doi: 10.1016/j.ophtha.2017.02.008. Epub 2017 Mar 27.,Ophthalmology.  2017,PubMed,citation,PMID:28359545,pubmed,28359545,create date:2017/04/01 | first author:Gargeya R,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Diabetic retinopathy (DR) is one of the leading causes of preventable blindness globally. Performing retinal screening examinations on all diabetic patients is an unmet need, and there are many undiagnosed and untreated cases of DR. The objective of this study was to develop robust diagnostic technology to automate DR screening. Referral of eyes with DR to an ophthalmologist for further evaluation and treatment would aid in reducing the rate of vision loss, enabling timely and accurate diagnoses.</abstracttext></p><h4>DESIGN: </h4><p><abstracttext label='DESIGN' nlmcategory='METHODS'>We developed and evaluated a data-driven deep learning algorithm as a novel diagnostic tool for automated DR detection. The algorithm processed color fundus images and classified them as healthy (no retinopathy) or having DR, identifying relevant cases for medical referral.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A total of 75 137 publicly available fundus images from diabetic patients were used to train and test an artificial intelligence model to differentiate healthy fundi from those with DR. A panel of retinal specialists determined the ground truth for our data set before experimentation. We also tested our model using the public MESSIDOR 2 and E-Ophtha databases for external validation. Information learned in our automated method was visualized readily through an automatically generated abnormality heatmap, highlighting subregions within each input fundus image for further clinical review.</abstracttext></p><h4>MAIN OUTCOME MEASURES: </h4><p><abstracttext label='MAIN OUTCOME MEASURES' nlmcategory='METHODS'>We used area under the receiver operating characteristic curve (AUC) as a metric to measure the precision-recall trade-off of our algorithm, reporting associated sensitivity and specificity metrics on the receiver operating characteristic curve.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>Our model achieved a 0.97 AUC with a 94% and 98% sensitivity and specificity, respectively, on 5-fold cross-validation using our local data set. Testing against the independent MESSIDOR 2 and E-Ophtha databases achieved a 0.94 and 0.95 AUC score, respectively.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>A fully data-driven artificial intelligence-based grading algorithm can be used to screen fundus photographs obtained from diabetic patients and to identify, with high reliability, which cases should be referred to an ophthalmologist for further evaluation and treatment. The implementation of such an algorithm on a global basis could reduce drastically the rate of vision loss attributed to DR.</abstracttext></p><p class='copyright'>Copyright © 2017 American Academy of Ophthalmology. Published by Elsevier Inc. All rights reserved.</p></div></div>",tedleng@stanford.edu,,https://www.ncbi.nlm.nih.gov//pubmed/28359545,pubmed,2017,48f658dd-a3b2-4f45-b23a-17d2e66f8891,1
integrating holistic and local deep features for glaucoma classification,/pubmed/28268570,"Annan Li, Jun Cheng, Damon Wing Kee Wong, Jiang Liu.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:1328-1331. doi: 10.1109/EMBC.2016.7590952.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268570,pubmed,28268570,create date:2017/03/09 | first author:Annan Li,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automated glaucoma detection is an important application of retinal image analysis. Compared with segmentation based approaches, image classification based approaches have a potential of better performance. However, it still remains a challenging problem for two reasons. Firstly, due to insufficient sample size, learning effective features is difficult. Secondly, the shape variations of optic disc introduce misalignment. To address these problem, a new classification based approach for glaucoma detection is proposed, in which deep convolutional networks derived from large-scale generic dataset is used to representing the visual appearance and holistic and local features are combined to mitigate the influence of misalignment. The proposed method achieves an area under the receiver operating characteristic curve of 0.8384 on the Origa dataset, which clearly demonstrates its effectiveness.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268570,pubmed,2016,a5087ad3-0d17-4e1a-b396-70726ddbd12b,1
prospective identification of hematopoietic lineage choice by deep learning,/pubmed/28218899,"Buggenthin F, Buettner F, Hoppe PS, Endele M, Kroiss M, Strasser M, Schwarzfischer M, Loeffler D, Kokkaliaris KD, Hilsenbeck O, Schroeder T, Theis FJ, Marr C.",Nat Methods. 2017 Apr;14(4):403-406. doi: 10.1038/nmeth.4182. Epub 2017 Feb 20.,Nat Methods.  2017,PubMed,citation,PMID:28218899 | PMCID:PMC5376497,pubmed,28218899,create date:2017/02/22 | first author:Buggenthin F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Differentiation alters molecular properties of stem and progenitor cells, leading to changes in their shape and movement characteristics. We present a deep neural network that prospectively predicts lineage choice in differentiating primary hematopoietic progenitors using image patches from brightfield microscopy and cellular movement. Surprisingly, lineage choice can be detected up to three generations before conventional molecular markers are observable. Our approach allows identification of cells with differentially expressed lineage-specifying genes without molecular labeling.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28218899,pubmed,2017,ec423c54-133b-4088-a500-65558f6d9185,1
deep learning for health informatics,/pubmed/28055930,"Ravi D, Wong C, Deligianni F, Berthelot M, Andreu-Perez J, Lo B, Yang GZ.",IEEE J Biomed Health Inform. 2017 Jan;21(1):4-21. doi: 10.1109/JBHI.2016.2636665. Epub 2016 Dec 29.,IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28055930,pubmed,28055930,create date:2017/01/06 | first author:Ravi D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28055930,pubmed,2017,a0cf38b6-e868-4d29-a867-12265c91d728,1
improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning,/pubmed/27701631,"Abràmoff MD, Lou Y, Erginay A, Clarida W, Amelon R, Folk JC, Niemeijer M.",Invest Ophthalmol Vis Sci. 2016 Oct 1;57(13):5200-5206. doi: 10.1167/iovs.16-19964.,Invest Ophthalmol Vis Sci.  2016,PubMed,citation,PMID:27701631,pubmed,27701631,create date:2016/10/05 | first author:Abràmoff MD,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Purpose: </h4><p><abstracttext label='Purpose' nlmcategory='UNASSIGNED'>To compare performance of a deep-learning enhanced algorithm for automated detection of diabetic retinopathy (DR), to the previously published performance of that algorithm, the Iowa Detection Program (IDP)-without deep learning components-on the same publicly available set of fundus images and previously reported consensus reference standard set, by three US Board certified retinal specialists.</abstracttext></p><h4>Methods: </h4><p><abstracttext label='Methods' nlmcategory='UNASSIGNED'>We used the previously reported consensus reference standard of referable DR (rDR), defined as International Clinical Classification of Diabetic Retinopathy moderate, severe nonproliferative (NPDR), proliferative DR, and/or macular edema (ME). Neither Messidor-2 images, nor the three retinal specialists setting the Messidor-2 reference standard were used for training IDx-DR version X2.1. Sensitivity, specificity, negative predictive value, area under the curve (AUC), and their confidence intervals (CIs) were calculated.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>Sensitivity was 96.8% (95% CI: 93.3%-98.8%), specificity was 87.0% (95% CI: 84.2%-89.4%), with 6/874 false negatives, resulting in a negative predictive value of 99.0% (95% CI: 97.8%-99.6%). No cases of severe NPDR, PDR, or ME were missed. The AUC was 0.980 (95% CI: 0.968-0.992). Sensitivity was not statistically different from published IDP sensitivity, which had a CI of 94.4% to 99.3%, but specificity was significantly better than the published IDP specificity CI of 55.7% to 63.0%.</abstracttext></p><h4>Conclusions: </h4><p><abstracttext label='Conclusions' nlmcategory='UNASSIGNED'>A deep-learning enhanced algorithm for the automated detection of DR, achieves significantly better performance than a previously reported, otherwise essentially identical, algorithm that does not employ deep learning. Deep learning enhanced algorithms have the potential to improve the efficiency of DR screening, and thereby to prevent visual loss and blindness from this devastating disease.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27701631,pubmed,2016,dcf1fc83-2c2a-4674-8ef0-efae74c47591,1
ensembles of deep learning architectures for the early diagnosis of the alzheimer's disease,/pubmed/27478060,"Ortiz A, Munilla J, Górriz JM, Ramírez J.",Int J Neural Syst. 2016 Nov;26(7):1650025. doi: 10.1142/S0129065716500258. Epub 2016 Apr 4.,Int J Neural Syst.  2016,PubMed,citation,PMID:27478060,pubmed,27478060,create date:2016/08/02 | first author:Ortiz A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Computer Aided Diagnosis (CAD) constitutes an important tool for the early diagnosis of Alzheimer's Disease (AD), which, in turn, allows the application of treatments that can be simpler and more likely to be effective. This paper explores the construction of classification methods based on deep learning architectures applied on brain regions defined by the Automated Anatomical Labeling (AAL). Gray Matter (GM) images from each brain area have been split into 3D patches according to the regions defined by the AAL atlas and these patches are used to train different deep belief networks. An ensemble of deep belief networks is then composed where the final prediction is determined by a voting scheme. Two deep learning based structures and four different voting schemes are implemented and compared, giving as a result a potent classification architecture where discriminative features are computed in an unsupervised fashion. The resulting method has been evaluated using a large dataset from the Alzheimer's disease Neuroimaging Initiative (ADNI). Classification results assessed by cross-validation prove that the proposed method is not only valid for differentiate between controls (NC) and AD images, but it also provides good performances when tested for the more challenging case of classifying Mild Cognitive Impairment (MCI) Subjects. In particular, the classification architecture provides accuracy values up to 0.90 and AUC of 0.95 for NC/AD classification, 0.84 and AUC of 0.91 for stable MCI/AD classification and 0.83 and AUC of 0.95 for NC/MCI converters classification. </abstracttext></p></div></div>",,Alzheimer’s disease classification; Deep learning; ensemble,https://www.ncbi.nlm.nih.gov//pubmed/27478060,pubmed,2016,d65234d4-e274-4651-839a-795cb3ce39b1,1
detecting preperimetric glaucoma with standard automated perimetry using a deep learning classifier,/pubmed/27395766,"Asaoka R, Murata H, Iwase A, Araie M.",Ophthalmology. 2016 Sep;123(9):1974-80. doi: 10.1016/j.ophtha.2016.05.029. Epub 2016 Jul 7.,Ophthalmology.  2016,PubMed,citation,PMID:27395766,pubmed,27395766,create date:2016/07/11 | first author:Asaoka R,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>To differentiate the visual fields (VFs) of preperimetric open-angle glaucoma (OAG) patients from the VFs of healthy eyes using a deep learning (DL) method.</abstracttext></p><h4>DESIGN: </h4><p><abstracttext label='DESIGN' nlmcategory='METHODS'>Cohort study.</abstracttext></p><h4>PARTICIPANTS: </h4><p><abstracttext label='PARTICIPANTS' nlmcategory='METHODS'>One hundred seventy-one preperimetric glaucoma VFs (PPGVFs) from 53 eyes in 51 OAG patients and 108 healthy eyes of 87 healthy participants.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Preperimetric glaucoma VFs were defined as all VFs before a first diagnosis of manifest glaucoma (Anderson-Patella's criteria). In total, 171 PPGVFs from 53 eyes in 51 OAG patients and 108 VFs from 108 healthy eyes in 87 healthy participants were analyzed (all VFs were tested using the Humphrey Field Analyzer 30-2 program; Carl Zeiss Meditec, Dublin, CA). The 52 total deviation, mean deviation, and pattern standard deviation values were used as predictors in the DL classifier: a deep feed-forward neural network (FNN), along with other machine learning (ML) methods, including random forests (RF), gradient boosting, support vector machine, and neural network (NN). The area under the receiver operating characteristic curve (AUC) was used to evaluate the accuracy of discrimination for each method.</abstracttext></p><h4>MAIN OUTCOME MEASURES: </h4><p><abstracttext label='MAIN OUTCOME MEASURES' nlmcategory='METHODS'>The AUCs obtained with each classifier method.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>A significantly larger AUC of 92.6% (95% confidence interval [CI], 89.8%-95.4%) was obtained using the deep FNN classifier compared with all other ML methods: 79.0% (95% CI, 73.5%-84.5%) with RF, 77.6% (95% CI, 71.7%-83.5%) with gradient boosting, 71.2% (95% CI, 65.0%-77.5%), and 66.7% (95% CI, 60.1%-73.3%) with NN.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Preperimetric glaucoma VFs can be distinguished from healthy VFs with very high accuracy using a deep FNN classifier.</abstracttext></p><p class='copyright'>Copyright © 2016 American Academy of Ophthalmology. Published by Elsevier Inc. All rights reserved.</p></div></div>",rasaoka-tky@umin.ac.jp,,https://www.ncbi.nlm.nih.gov//pubmed/27395766,pubmed,2016,50f6e5b5-edf7-4549-ac91-f40e7cde73f0,1
"learning clinically useful information from images: past, present and future",/pubmed/27344105,"Rueckert D, Glocker B, Kainz B.",Med Image Anal. 2016 Oct;33:13-18. doi: 10.1016/j.media.2016.06.009. Epub 2016 Jun 15.,Med Image Anal.  2016,PubMed,citation,PMID:27344105,pubmed,27344105,create date:2016/06/28 | first author:Rueckert D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Over the last decade, research in medical imaging has made significant progress in addressing challenging tasks such as image registration and image segmentation. In particular, the use of model-based approaches has been key in numerous, successful advances in methodology. The advantage of model-based approaches is that they allow the incorporation of prior knowledge acting as a regularisation that favours plausible solutions over implausible ones. More recently, medical imaging has moved away from hand-crafted, and often explicitly designed models towards data-driven, implicit models that are constructed using machine learning techniques. This has led to major improvements in all stages of the medical imaging pipeline, from acquisition and reconstruction to analysis and interpretation. As more and more imaging data is becoming available, e.g., from large population studies, this trend is likely to continue and accelerate. At the same time new developments in machine learning, e.g., deep learning, as well as significant improvements in computing power, e.g., parallelisation on graphics hardware, offer new potential for data-driven, semantic and intelligent medical imaging. This article outlines the work of the BioMedIA group in this area and highlights some of the challenges and opportunities for future work. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",d.rueckert@imperial.ac.uk,Intelligent imaging; Machine learning; Semantic imaging,https://www.ncbi.nlm.nih.gov//pubmed/27344105,pubmed,2016,a9ece095-fe66-497a-a0b8-140cc049ba58,1
multi-modal vertebrae recognition using transformed deep convolution network,/pubmed/27104497,"Cai Y, Landis M, Laidley DT, Kornecki A, Lum A, Li S.",Comput Med Imaging Graph. 2016 Jul;51:11-9. doi: 10.1016/j.compmedimag.2016.02.002. Epub 2016 Apr 8.,Comput Med Imaging Graph.  2016,PubMed,citation,PMID:27104497,pubmed,27104497,create date:2016/04/23 | first author:Cai Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic vertebra recognition, including the identification of vertebra locations and naming in multiple image modalities, are highly demanded in spinal clinical diagnoses where large amount of imaging data from various of modalities are frequently and interchangeably used. However, the recognition is challenging due to the variations of MR/CT appearances or shape/pose of the vertebrae. In this paper, we propose a method for multi-modal vertebra recognition using a novel deep learning architecture called Transformed Deep Convolution Network (TDCN). This new architecture can unsupervisely fuse image features from different modalities and automatically rectify the pose of vertebra. The fusion of MR and CT image features improves the discriminativity of feature representation and enhances the invariance of the vertebra pattern, which allows us to automatically process images from different contrast, resolution, protocols, even with different sizes and orientations. The feature fusion and pose rectification are naturally incorporated in a multi-layer deep learning network. Experiment results show that our method outperforms existing detection methods and provides a fully automatic location+naming+pose recognition for routine clinical practice. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Ltd. All rights reserved.</p></div></div>",slishuo@gmail.com,Convolution network; Deep learning; Vertebra detection; Vertebra recognition,https://www.ncbi.nlm.nih.gov//pubmed/27104497,pubmed,2016,db8dcc59-daf8-47ec-84a6-8472e12ae67d,1
q-space deep learning: twelve-fold shorter and model-free diffusion mri scans,/pubmed/27071165,"Golkov V, Dosovitskiy A, Sperl JI, Menzel MI, Czisch M, Samann P, Brox T, Cremers D.",IEEE Trans Med Imaging. 2016 May;35(5):1344-1351. doi: 10.1109/TMI.2016.2551324. Epub 2016 Apr 6.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:27071165,pubmed,27071165,create date:2016/04/14 | first author:Golkov V,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27071165,pubmed,2016,b36956b4-0a88-473e-ac35-56e0813d81cf,1
segmenting brain tissues from chinese visible human dataset by deep-learned features with stacked autoencoder,/pubmed/27057543,"Zhao G, Wang X, Niu Y, Tan L, Zhang SX.",Biomed Res Int. 2016;2016:5284586. doi: 10.1155/2016/5284586. Epub 2016 Jan 26.,Biomed Res Int.  2016,PubMed,citation,PMID:27057543 | PMCID:PMC4807075,pubmed,27057543,create date:2016/04/09 | first author:Zhao G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cryosection brain images in Chinese Visible Human (CVH) dataset contain rich anatomical structure information of tissues because of its high resolution (e.g., 0.167 mm per pixel). Fast and accurate segmentation of these images into white matter, gray matter, and cerebrospinal fluid plays a critical role in analyzing and measuring the anatomical structures of human brain. However, most existing automated segmentation methods are designed for computed tomography or magnetic resonance imaging data, and they may not be applicable for cryosection images due to the imaging difference. In this paper, we propose a supervised learning-based CVH brain tissues segmentation method that uses stacked autoencoder (SAE) to automatically learn the deep feature representations. Specifically, our model includes two successive parts where two three-layer SAEs take image patches as input to learn the complex anatomical feature representation, and then these features are sent to Softmax classifier for inferring the labels. Experimental results validated the effectiveness of our method and showed that it outperformed four other classical brain tissue detection strategies. Furthermore, we reconstructed three-dimensional surfaces of these tissues, which show their potential in exploring the high-resolution anatomical structures of human brain.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27057543,pubmed,2016,3db7be2a-2406-4582-97b1-f61946492cfd,1
segmenting retinal blood vessels with deep neural networks,/pubmed/27046869,"Liskowski P, Krawiec K.",IEEE Trans Med Imaging. 2016 Nov;35(11):2369-2380. Epub 2016 Mar 24.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:27046869,pubmed,27046869,create date:2016/04/06 | first author:Liskowski P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400[Formula: see text]000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to &gt; 0.99) and accuracy of classification (up to &gt; 0.97 ). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity &gt; 0.87 ), and fares well on pathological cases.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27046869,pubmed,2016,d13ef99d-bf6b-45de-b094-420da3a2e7ab,1
marginal space deep learning: efficient architecture for volumetric image parsing,/pubmed/27046846,"Ghesu FC, Krubasik E, Georgescu B, Singh V, Yefeng Zheng, Hornegger J, Comaniciu D.",IEEE Trans Med Imaging. 2016 May;35(5):1217-1228. doi: 10.1109/TMI.2016.2538802. Epub 2016 Mar 7.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:27046846,pubmed,27046846,create date:2016/04/06 | first author:Ghesu FC,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27046846,pubmed,2016,6aee8114-51ec-4a4f-aa23-c9a58807002a,1
multi-scale deep networks and regression forests for direct bi-ventricular volume estimation,/pubmed/26919699,"Zhen X, Wang Z, Islam A, Bhaduri M, Chan I, Li S.",Med Image Anal. 2016 May;30:120-129. doi: 10.1016/j.media.2015.07.003. Epub 2015 Jul 26.,Med Image Anal.  2016,PubMed,citation,PMID:26919699,pubmed,26919699,create date:2016/02/27 | first author:Zhen X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Direct estimation of cardiac ventricular volumes has become increasingly popular and important in cardiac function analysis due to its effectiveness and efficiency by avoiding an intermediate segmentation step. However, existing methods rely on either intensive user inputs or problematic assumptions. To realize the full capacities of direct estimation, this paper presents a general, fully learning-based framework for direct bi-ventricular volume estimation, which removes user inputs and unreliable assumptions. We formulate bi-ventricular volume estimation as a general regression framework which consists of two main full learning stages: unsupervised cardiac image representation learning by multi-scale deep networks and direct bi-ventricular volume estimation by random forests. By leveraging strengths of generative and discriminant learning, the proposed method produces high correlations of around 0.92 with ground truth by human experts for both the left and right ventricles using a leave-one-subject-out cross validation, and largely outperforms existing direct methods on a larger dataset of 100 subjects including both healthy and diseased cases with twice the number of subjects used in previous methods. More importantly, the proposed method can not only be practically used in clinical cardiac function analysis but also be easily extended to other organ volume estimation tasks. </abstracttext></p><p class='copyright'>Copyright © 2015 Elsevier B.V. All rights reserved.</p></div></div>",shuo.li@ge.com,Direct volume estimation; Multi-scale deep networks; Random forests; Regression,https://www.ncbi.nlm.nih.gov//pubmed/26919699,pubmed,2016,82d25b55-815c-4018-b3b6-dcb082e90661,1
a combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac mri,/pubmed/26917105,"Avendi MR, Kheradvar A, Jafarkhani H.",Med Image Anal. 2016 May;30:108-119. doi: 10.1016/j.media.2016.01.005. Epub 2016 Feb 6.,Med Image Anal.  2016,PubMed,citation,PMID:26917105,pubmed,26917105,create date:2016/02/27 | first author:Avendi MR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Segmentation of the left ventricle (LV) from cardiac magnetic resonance imaging (MRI) datasets is an essential step for calculation of clinical indices such as ventricular volume and ejection fraction. In this work, we employ deep learning algorithms combined with deformable models to develop and evaluate a fully automatic LV segmentation tool from short-axis cardiac MRI datasets. The method employs deep learning algorithms to learn the segmentation task from the ground true data. Convolutional networks are employed to automatically detect the LV chamber in MRI dataset. Stacked autoencoders are used to infer the LV shape. The inferred shape is incorporated into deformable models to improve the accuracy and robustness of the segmentation. We validated our method using 45 cardiac MR datasets from the MICCAI 2009 LV segmentation challenge and showed that it outperforms the state-of-the art methods. Excellent agreement with the ground truth was achieved. Validation metrics, percentage of good contours, Dice metric, average perpendicular distance and conformity, were computed as 96.69%, 0.94, 1.81 mm and 0.86, versus those of 79.2-95.62%, 0.87-0.9, 1.76-2.97 mm and 0.67-0.78, obtained by other methods, respectively.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",m.avendi@uci.edu,Caridac MRI; Deep learning; Deformable models; LV segmentation; Machine learning,https://www.ncbi.nlm.nih.gov//pubmed/26917105,pubmed,2016,3660a423-bf7e-44fc-b82e-bdb77efecfa5,1
segmentation of human brain using structural mri,/pubmed/26739264,Helms G.,MAGMA. 2016 Apr;29(2):111-24. doi: 10.1007/s10334-015-0518-z. Epub 2016 Jan 6. Review.,MAGMA.  2016,PubMed,citation,PMID:26739264,pubmed,26739264,create date:2016/01/08 | first author:Helms G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Segmentation of human brain using structural MRI is a key step of processing in imaging neuroscience. The methods have undergone a rapid development in the past two decades and are now widely available. This non-technical review aims at providing an overview and basic understanding of the most common software. Starting with the basis of structural MRI contrast in brain and imaging protocols, the concepts of voxel-based and surface-based segmentation are discussed. Special emphasis is given to the typical contrast features and morphological constraints of cortical and sub-cortical grey matter. In addition to the use for voxel-based morphometry, basic applications in quantitative MRI, cortical thickness estimations, and atrophy measurements as well as assignment of cortical regions and deep brain nuclei are briefly discussed. Finally, some fields for clinical applications are given. </abstracttext></p></div></div>",gunther.helms@med.lu.se,Brain; Cortical thickness; MRI; Morphometry; Segmentation,https://www.ncbi.nlm.nih.gov//pubmed/26739264,pubmed,2016,c152c786-6ebf-40c2-8fa0-90c237fb88ae,1
deformable mr prostate segmentation via deep feature learning and sparse patch matching,/pubmed/26685226,"Guo Y, Gao Y, Shen D.",IEEE Trans Med Imaging. 2016 Apr;35(4):1077-89. doi: 10.1109/TMI.2015.2508280. Epub 2015 Dec 11.,IEEE Trans Med Imaging.  2016,PubMed,citation,PMID:26685226 | PMCID:PMC5002995,pubmed,26685226,create date:2015/12/20 | first author:Guo Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26685226,pubmed,2016,34681073-c3ce-4344-9ca4-3d693147fa17,1
scalable high-performance image registration framework by unsupervised deep feature representations learning,/pubmed/26552069,"Wu G, Kim M, Wang Q, Munsell BC, Shen D.",IEEE Trans Biomed Eng. 2016 Jul;63(7):1505-16. doi: 10.1109/TBME.2015.2496253. Epub 2015 Nov 2.,IEEE Trans Biomed Eng.  2016,PubMed,citation,PMID:26552069 | PMCID:PMC4853306,pubmed,26552069,create date:2015/11/10 | first author:Wu G,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26552069,pubmed,2016,0dc0dade-b3ab-416e-ac28-4ad0114e4e79,1
a robust deep model for improved classification of ad/mci patients,/pubmed/25955998,"Li F, Tran L, Thung KH, Ji S, Shen D, Li J.",IEEE J Biomed Health Inform. 2015 Sep;19(5):1610-6. doi: 10.1109/JBHI.2015.2429556. Epub 2015 May 4.,IEEE J Biomed Health Inform.  2015,PubMed,citation,PMID:25955998 | PMCID:PMC4573581,pubmed,25955998,create date:2015/05/09 | first author:Li F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25955998,pubmed,2015,3e39d355-a18a-4574-8c8b-3f9d359183a1,1
deep learning for neuroimaging: a validation study,/pubmed/25191215,"Plis SM, Hjelm DR, Salakhutdinov R, Allen EA, Bockholt HJ, Long JD, Johnson HJ, Paulsen JS, Turner JA, Calhoun VD.",Front Neurosci. 2014 Aug 20;8:229. doi: 10.3389/fnins.2014.00229. eCollection 2014.,Front Neurosci.  2014,PubMed,citation,PMID:25191215 | PMCID:PMC4138493,pubmed,25191215,create date:2014/09/06 | first author:Plis SM,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. These methods include deep belief networks and their building block the restricted Boltzmann machine. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data. </abstracttext></p></div></div>",,MRI; classification; fMRI; intrinsic networks; unsupervised learning,https://www.ncbi.nlm.nih.gov//pubmed/25191215,pubmed,2014,ddd821a8-571b-47a8-a87f-1bd020a33f0f,1
medical image retrieval: a multimodal approach,/pubmed/26309389,"Cao Y, Steffey S, He J, Xiao D, Tao C, Chen P, Müller H.",Cancer Inform. 2015 Jul 22;13(Suppl 3):125-36. doi: 10.4137/CIN.S14053. eCollection 2014.,Cancer Inform.  2014,PubMed,citation,PMID:26309389 | PMCID:PMC4533857,pubmed,26309389,create date:2014/01/01 | first author:Cao Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Medical imaging is becoming a vital component of war on cancer. Tremendous amounts of medical image data are captured and recorded in a digital format during cancer care and cancer research. Facing such an unprecedented volume of image data with heterogeneous image modalities, it is necessary to develop effective and efficient content-based medical image retrieval systems for cancer clinical practice and research. While substantial progress has been made in different areas of content-based image retrieval (CBIR) research, direct applications of existing CBIR techniques to the medical images produced unsatisfactory results, because of the unique characteristics of medical images. In this paper, we develop a new multimodal medical image retrieval approach based on the recent advances in the statistical graphic model and deep learning. Specifically, we first investigate a new extended probabilistic Latent Semantic Analysis model to integrate the visual and textual information from medical images to bridge the semantic gap. We then develop a new deep Boltzmann machine-based multimodal learning model to learn the joint density model from multimodal information in order to derive the missing modality. Experimental results with large volume of real-world medical images have shown that our new approach is a promising solution for the next-generation medical imaging indexing and retrieval system. </abstracttext></p></div></div>",,content-based image retrieval; deep boltzmann machine; deep learning; extended probabilistic latent semantic analysis; multi-modal and content-based medical image retrieval,https://www.ncbi.nlm.nih.gov//pubmed/26309389,pubmed,2014,c07ffdbe-1bf8-46b2-baa3-28ab513dcd94,1
titer: predicting translation initiation sites by deep learning,/pubmed/28881981,"Zhang S, Hu H, Jiang T, Zhang L, Zeng J.",Bioinformatics. 2017 Jul 15;33(14):i234-i242. doi: 10.1093/bioinformatics/btx247.,Bioinformatics.  2017,PubMed,citation,PMID:28881981,pubmed,28881981,create date:2017/09/09 | first author:Zhang S,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Translation initiation is a key step in the regulation of gene expression. In addition to the annotated translation initiation sites (TISs), the translation process may also start at multiple alternative TISs (including both AUG and non-AUG codons), which makes it challenging to predict TISs and study the underlying regulatory mechanisms. Meanwhile, the advent of several high-throughput sequencing techniques for profiling initiating ribosomes at single-nucleotide resolution, e.g. GTI-seq and QTI-seq, provides abundant data for systematically studying the general principles of translation initiation and the development of computational method for TIS identification.</abstracttext></p><h4>Methods: </h4><p><abstracttext label='Methods' nlmcategory='UNASSIGNED'>We have developed a deep learning-based framework, named TITER, for accurately predicting TISs on a genome-wide scale based on QTI-seq data. TITER extracts the sequence features of translation initiation from the surrounding sequence contexts of TISs using a hybrid neural network and further integrates the prior preference of TIS codon composition into a unified prediction framework.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>Extensive tests demonstrated that TITER can greatly outperform the state-of-the-art prediction methods in identifying TISs. In addition, TITER was able to identify important sequence signatures for individual types of TIS codons, including a Kozak-sequence-like motif for AUG start codon. Furthermore, the TITER prediction score can be related to the strength of translation initiation in various biological scenarios, including the repressive effect of the upstream open reading frames on gene expression and the mutational effects influencing translation initiation efficiency.</abstracttext></p><h4>Availability and Implementation: </h4><p><abstracttext label='Availability and Implementation' nlmcategory='UNASSIGNED'>TITER is available as an open-source software and can be downloaded from https://github.com/zhangsaithu/titer .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>lzhang20@mail.tsinghua.edu.cn or zengjy321@tsinghua.edu.cn.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28881981,pubmed,2017,18a15639-415e-4e9a-94a2-b6750acc9ef4,1
deep learning with word embeddings improves biomedical named entity recognition,/pubmed/28881963,"Habibi M, Weber L, Neves M, Wiegandt DL, Leser U.",Bioinformatics. 2017 Jul 15;33(14):i37-i48. doi: 10.1093/bioinformatics/btx228.,Bioinformatics.  2017,PubMed,citation,PMID:28881963,pubmed,28881963,create date:2017/09/09 | first author:Habibi M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Text mining has become an important tool for biomedical research. The most fundamental text-mining task is the recognition of biomedical named entities (NER), such as genes, chemicals and diseases. Current NER methods rely on pre-defined features which try to capture the specific surface properties of entity types, properties of the typical local context, background knowledge, and linguistic information. State-of-the-art tools are entity-specific, as dictionaries and empirically optimal feature sets differ between entity types, which makes their development costly. Furthermore, features are often optimized for a specific gold standard corpus, which makes extrapolation of quality measures difficult.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>We show that a completely generic method based on deep learning and statistical word embeddings [called long short-term memory network-conditional random field (LSTM-CRF)] outperforms state-of-the-art entity-specific NER tools, and often by a large margin. To this end, we compared the performance of LSTM-CRF on 33 data sets covering five different entity classes with that of best-of-class NER tools and an entity-agnostic CRF implementation. On average, F1-score of LSTM-CRF is 5% above that of the baselines, mostly due to a sharp increase in recall.</abstracttext></p><h4>Availability and implementation: </h4><p><abstracttext label='Availability and implementation' nlmcategory='UNASSIGNED'>The source code for LSTM-CRF is available at https://github.com/glample/tagger and the links to the corpora are available at https://corposaurus.github.io/corpora/ .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>habibima@informatik.hu-berlin.de.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28881963,pubmed,2017,b8f0c2ae-bd98-4e21-8559-c7b7ba567a78,1
rectified factor networks for biclustering of omics data,/pubmed/28881961,"Clevert DA, Unterthiner T, Povysil G, Hochreiter S.",Bioinformatics. 2017 Jul 15;33(14):i59-i66. doi: 10.1093/bioinformatics/btx226.,Bioinformatics.  2017,PubMed,citation,PMID:28881961,pubmed,28881961,create date:2017/09/09 | first author:Clevert DA,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Biclustering has become a major tool for analyzing large datasets given as matrix of samples times features and has been successfully applied in life sciences and e-commerce for drug design and recommender systems, respectively. actor nalysis for cluster cquisition (FABIA), one of the most successful biclustering methods, is a generative model that represents each bicluster by two sparse membership vectors: one for the samples and one for the features. However, FABIA is restricted to about 20 code units because of the high computational complexity of computing the posterior. Furthermore, code units are sometimes insufficiently decorrelated and sample membership is difficult to determine. We propose to use the recently introduced unsupervised Deep Learning approach Rectified Factor Networks (RFNs) to overcome the drawbacks of existing biclustering methods. RFNs efficiently construct very sparse, non-linear, high-dimensional representations of the input via their posterior means. RFN learning is a generalized alternating minimization algorithm based on the posterior regularization method which enforces non-negative and normalized posterior means. Each code unit represents a bicluster, where samples for which the code unit is active belong to the bicluster and features that have activating weights to the code unit belong to the bicluster.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>On 400 benchmark datasets and on three gene expression datasets with known clusters, RFN outperformed 13 other biclustering methods including FABIA. On data of the 1000 Genomes Project, RFN could identify DNA segments which indicate, that interbreeding with other hominins starting already before ancestors of modern humans left Africa.</abstracttext></p><h4>Availability and implementation: </h4><p><abstracttext label='Availability and implementation' nlmcategory='UNASSIGNED'>https://github.com/bioinf-jku/librfn.</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>djork-arne.clevert@bayer.com or hochreit@bioinf.jku.at.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28881961,pubmed,2017,cfcf9626-99b8-4e53-bd52-664c8b1fdbe0,1
nrc: non-coding rna classifier based on structural features,/pubmed/28785313,"Fiannaca A, La Rosa M, La Paglia L, Rizzo R, Urso A.",BioData Min. 2017 Aug 1;10:27. doi: 10.1186/s13040-017-0148-2. eCollection 2017.,BioData Min.  2017,PubMed,citation,PMID:28785313 | PMCID:PMC5540506,pubmed,28785313,create date:2017/08/09 | first author:Fiannaca A,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Non-coding RNA (ncRNA) are small non-coding sequences involved in gene expression regulation of many biological processes and diseases. The recent discovery of a large set of different ncRNAs with biologically relevant roles has opened the way to develop methods able to discriminate between the different ncRNA classes. Moreover, the lack of knowledge about the complete mechanisms in regulative processes, together with the development of high-throughput technologies, has required the help of bioinformatics tools in addressing biologists and clinicians with a deeper comprehension of the functional roles of ncRNAs. In this work, we introduce a new ncRNA classification tool, nRC (non-coding RNA Classifier). Our approach is based on features extraction from the ncRNA secondary structure together with a supervised classification algorithm implementing a deep learning architecture based on convolutional neural networks.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We tested our approach for the classification of 13 different ncRNA classes. We obtained classification scores, using the most common statistical measures. In particular, we reach an accuracy and sensitivity score of about 74%.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The proposed method outperforms other similar classification methods based on secondary structure features and machine learning algorithms, including the RNAcon tool that, to date, is the reference classifier. nRC tool is freely available as a docker image at https://hub.docker.com/r/tblab/nrc/. The source code of nRC tool is also available at https://github.com/IcarPA-TBlab/nrc.</abstracttext></p></div></div>",,Classification; Deep learning; Structural features; ncRNA,https://www.ncbi.nlm.nih.gov//pubmed/28785313,pubmed,2017,7908a299-73df-4907-a977-c50672d48015,1
near perfect protein multi-label classification with deep neural networks,/pubmed/28684341,"Szalkai B, Grolmusz V.",Methods. 2017 Jul 3. pii: S1046-2023(17)30035-X. doi: 10.1016/j.ymeth.2017.06.034. [Epub ahead of print],Methods.  2017,PubMed,citation,PMID:28684341,pubmed,28684341,create date:2017/07/08 | first author:Szalkai B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Biological sequences can be considered as data items of high-, non-fixed dimensions, corresponding to the length of those sequences. The comparison and the classification of biological sequences in their relations to large databases are important areas of research today. Artificial neural networks (ANNs) have gained a well-deserved popularity among machine learning tools upon their recent successful applications in image- and sound processing and classification problems. ANNs have also been applied for predicting the family or function of a protein, knowing its residue sequence. Here we present two new ANNs with multi-label classification ability, showing impressive accuracy when classifying protein sequences into 698 UniProt families (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%).</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",szalkai@pitgroup.org,,https://www.ncbi.nlm.nih.gov//pubmed/28684341,pubmed,2017,d0c87e39-8b77-4c66-877b-6f363945d6be,1
partitioned learning of deep boltzmann machines for snp data,/pubmed/28655145,"Hess M, Lenz S, Blätte TJ, Bullinger L, Binder H.",Bioinformatics. 2017 Jun 26. doi: 10.1093/bioinformatics/btx408. [Epub ahead of print],Bioinformatics.  2017,PubMed,citation,PMID:28655145,pubmed,28655145,create date:2017/06/29 | first author:Hess M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Learning the joint distributions of measurements, and in particular identification of an appropriate low-dimensional manifold, has been found to be a powerful ingredient of deep leaning approaches. Yet, such approaches have hardly been applied to single nucleotide polymorphism (SNP) data, probably due to the high number of features typically exceeding the number of studied individuals.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>After a brief overview of how deep Boltzmann machines (DBMs), a deep learning approach, can be adapted to SNP data in principle, we specifically present a way to alleviate the dimensionality problem by partitioned learning. We propose a sparse regression approach to coarsely screen the joint distribution of SNPs, followed by training several DBMs on SNP partitions that were identified by the screening. Aggregate features representing SNP patterns and the corresponding SNPs are extracted from the DBMs by a combination of statistical tests and sparse regression. In simulated case-control data, we show how this can uncover complex SNP patterns and augment results from univariate approaches, while maintaining type 1 error control. Time-to-event endpoints are considered in an application with acute myeloid leukemia patients, where SNP patterns are modeled after a pre-screening based on gene expression data. The proposed approach identified three SNPs that seem to jointly influence survival in a validation data set. This indicates the added value of jointly investigating SNPs compared to standard univariate analyses and makes partitioned learning of DBMs an interesting complementary approach when analyzing SNP data.</abstracttext></p><h4>Availability: </h4><p><abstracttext label='Availability' nlmcategory='UNASSIGNED'>A Julia package is provided at ' http://github.com/binderh/BoltzmannMachines.jl '.</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>binderh@uni-mainz.de.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28655145,pubmed,2017,378b2de9-dd12-458c-971e-ab2e6b35c107,1
a deep boosting based approach for capturing the sequence binding preferences of rna-binding proteins from high-throughput clip-seq data,/pubmed/28575488,"Li S, Dong F, Wu Y, Zhang S, Zhang C, Liu X, Jiang T, Zeng J.",Nucleic Acids Res. 2017 Aug 21;45(14):e129. doi: 10.1093/nar/gkx492.,Nucleic Acids Res.  2017,PubMed,citation,PMID:28575488,pubmed,28575488,create date:2017/06/03 | first author:Li S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Characterizing the binding behaviors of RNA-binding proteins (RBPs) is important for understanding their functional roles in gene expression regulation. However, current high-throughput experimental methods for identifying RBP targets, such as CLIP-seq and RNAcompete, usually suffer from the false negative issue. Here, we develop a deep boosting based machine learning approach, called DeBooster, to accurately model the binding sequence preferences and identify the corresponding binding targets of RBPs from CLIP-seq data. Comprehensive validation tests have shown that DeBooster can outperform other state-of-the-art approaches in RBP target prediction. In addition, we have demonstrated that DeBooster may provide new insights into understanding the regulatory functions of RBPs, including the binding effects of the RNA helicase MOV10 on mRNA degradation, the potentially different ADAR1 binding behaviors related to its editing activity, as well as the antagonizing effect of RBP binding on miRNA repression. Moreover, DeBooster may provide an effective index to investigate the effect of pathogenic mutations in RBP binding sites, especially those related to splicing events. We expect that DeBooster will be widely applied to analyze large-scale CLIP-seq experimental data and can provide a practically useful tool for novel biological discoveries in understanding the regulatory mechanisms of RBPs. The source code of DeBooster can be downloaded from http://github.com/dongfanghong/deepboost.</abstracttext></p><p class='copyright'>© The Author(s) 2017. Published by Oxford University Press on behalf of Nucleic Acids Research.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28575488,pubmed,2017,8fa0e4be-ae87-49ab-a16a-bde8050188d2,1
deepppi: boosting prediction of protein-protein interactions with deep neural networks,/pubmed/28514151,"Du X, Sun S, Hu C, Yao Y, Yan Y, Zhang Y.",J Chem Inf Model. 2017 Jun 26;57(6):1499-1510. doi: 10.1021/acs.jcim.7b00028. Epub 2017 May 26.,J Chem Inf Model.  2017,PubMed,citation,PMID:28514151,pubmed,28514151,create date:2017/05/18 | first author:Du X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The complex language of eukaryotic gene expression remains incompletely understood. Despite the importance suggested by many proteins variants statistically associated with human disease, nearly all such variants have unknown mechanisms, for example, protein-protein interactions (PPIs). In this study, we address this challenge using a recent machine learning advance-deep neural networks (DNNs). We aim at improving the performance of PPIs prediction and propose a method called DeepPPI (Deep neural networks for Protein-Protein Interactions prediction), which employs deep neural networks to learn effectively the representations of proteins from common protein descriptors. The experimental results indicate that DeepPPI achieves superior performance on the test data set with an Accuracy of 92.50%, Precision of 94.38%, Recall of 90.56%, Specificity of 94.49%, Matthews Correlation Coefficient of 85.08% and Area Under the Curve of 97.43%, respectively. Extensive experiments show that DeepPPI can learn useful features of proteins pairs by a layer-wise abstraction, and thus achieves better prediction performance than existing methods. The source code of our approach can be available via http://ailab.ahu.edu.cn:8087/DeepPPI/index.html .</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28514151,pubmed,2017,5758ee62-a6b9-4434-ba01-cf1f7dff865f,1
biren: predicting enhancers with a deep-learning-based model using the dna sequence alone,/pubmed/28334114,"Yang B, Liu F, Ren C, Ouyang Z, Xie Z, Bo X, Shu W.",Bioinformatics. 2017 Jul 1;33(13):1930-1936. doi: 10.1093/bioinformatics/btx105.,Bioinformatics.  2017,PubMed,citation,PMID:28334114,pubmed,28334114,create date:2017/03/24 | first author:Yang B,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Enhancer elements are noncoding stretches of DNA that play key roles in controlling gene expression programmes. Despite major efforts to develop accurate enhancer prediction methods, identifying enhancer sequences continues to be a challenge in the annotation of mammalian genomes. One of the major issues is the lack of large, sufficiently comprehensive and experimentally validated enhancers for humans or other species. Thus, the development of computational methods based on limited experimentally validated enhancers and deciphering the transcriptional regulatory code encoded in the enhancer sequences is urgent.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>We present a deep-learning-based hybrid architecture, BiRen, which predicts enhancers using the DNA sequence alone. Our results demonstrate that BiRen can learn common enhancer patterns directly from the DNA sequence and exhibits superior accuracy, robustness and generalizability in enhancer prediction relative to other state-of-the-art enhancer predictors based on sequence characteristics. Our BiRen will enable researchers to acquire a deeper understanding of the regulatory code of enhancer sequences.</abstracttext></p><h4>Availability and Implementation: </h4><p><abstracttext label='Availability and Implementation' nlmcategory='UNASSIGNED'>Our BiRen method can be freely accessed at https://github.com/wenjiegroup/BiRen .</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>shuwj@bmi.ac.cn or boxc@bmi.ac.cn.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28334114,pubmed,2017,9166e72c-a5d3-42bb-89aa-10a13b49feec,1
imputation for transcription factor binding predictions based on deep learning,/pubmed/28234893,"Qin Q, Feng J.",PLoS Comput Biol. 2017 Feb 24;13(2):e1005403. doi: 10.1371/journal.pcbi.1005403. eCollection 2017 Feb.,PLoS Comput Biol.  2017,PubMed,citation,PMID:28234893 | PMCID:PMC5345877,pubmed,28234893,create date:2017/02/25 | first author:Qin Q,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Understanding the cell-specific binding patterns of transcription factors (TFs) is fundamental to studying gene regulatory networks in biological systems, for which ChIP-seq not only provides valuable data but is also considered as the gold standard. Despite tremendous efforts from the scientific community to conduct TF ChIP-seq experiments, the available data represent only a limited percentage of ChIP-seq experiments, considering all possible combinations of TFs and cell lines. In this study, we demonstrate a method for accurately predicting cell-specific TF binding for TF-cell line combinations based on only a small fraction (4%) of the combinations using available ChIP-seq data. The proposed model, termed TFImpute, is based on a deep neural network with a multi-task learning setting to borrow information across transcription factors and cell lines. Compared with existing methods, TFImpute achieves comparable accuracy on TF-cell line combinations with ChIP-seq data; moreover, TFImpute achieves better accuracy on TF-cell line combinations without ChIP-seq data. This approach can predict cell line specific enhancer activities in K562 and HepG2 cell lines, as measured by massively parallel reporter assays, and predicts the impact of SNPs on TF binding.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28234893,pubmed,2017,b03d3a06-da71-4e57-9885-7b0a354aae78,1
deepgene: an advanced cancer type classifier based on deep learning and somatic point mutations,/pubmed/28155641,"Yuan Y, Shi Y, Li C, Kim J, Cai W, Han Z, Feng DD.",BMC Bioinformatics. 2016 Dec 23;17(Suppl 17):476. doi: 10.1186/s12859-016-1334-9.,BMC Bioinformatics.  2016,PubMed,citation,PMID:28155641 | PMCID:PMC5259816,pubmed,28155641,create date:2017/02/06 | first author:Yuan Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>With the developments of DNA sequencing technology, large amounts of sequencing data have become available in recent years and provide unprecedented opportunities for advanced association studies between somatic point mutations and cancer types/subtypes, which may contribute to more accurate somatic point mutation based cancer classification (SMCC). However in existing SMCC methods, issues like high data sparsity, small volume of sample size, and the application of simple linear classifiers, are major obstacles in improving the classification performance.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>To address the obstacles in existing SMCC studies, we propose DeepGene, an advanced deep neural network (DNN) based classifier, that consists of three steps: firstly, the clustered gene filtering (CGF) concentrates the gene data by mutation occurrence frequency, filtering out the majority of irrelevant genes; secondly, the indexed sparsity reduction (ISR) converts the gene data into indexes of its non-zero elements, thereby significantly suppressing the impact of data sparsity; finally, the data after CGF and ISR is fed into a DNN classifier, which extracts high-level features for accurate classification. Experimental results on our curated TCGA-DeepGene dataset, which is a reformulated subset of the TCGA dataset containing 12 selected types of cancer, show that CGF, ISR and DNN all contribute in improving the overall classification performance. We further compare DeepGene with three widely adopted classifiers and demonstrate that DeepGene has at least 24% performance improvement in terms of testing accuracy.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Based on deep learning and somatic point mutation data, we devise DeepGene, an advanced cancer type classifier, which addresses the obstacles in existing SMCC studies. Experiments indicate that DeepGene outperforms three widely adopted existing classifiers, which is mainly attributed to its deep learning module that is able to extract the high level features between combinatorial somatic point mutations and cancer types.</abstracttext></p></div></div>",yishi@sjtu.edu.cn,,https://www.ncbi.nlm.nih.gov//pubmed/28155641,pubmed,2016,02255bf4-a0ff-4366-8b25-c707fb774a31,1
a deep learning approach for cancer detection and relevant gene identification,/pubmed/27896977,"Danaee P, Ghaeini R, Hendrix DA.",Pac Symp Biocomput. 2016;22:219-229.,Pac Symp Biocomput.  2016,PubMed,citation,PMID:27896977 | PMCID:PMC5177447,pubmed,27896977,create date:2016/11/30 | first author:Danaee P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Cancer detection from gene expression data continues to pose a challenge due to the high dimensionality and complexity of these data. After decades of research there is still uncertainty in the clinical diagnosis of cancer and the identification of tumor-specific markers. Here we present a deep learning approach to cancer detection, and to the identification of genes critical for the diagnosis of breast cancer. First, we used Stacked Denoising Autoencoder (SDAE) to deeply extract functional features from high dimensional gene expression profiles. Next, we evaluated the performance of the extracted representation through supervised classification models to verify the usefulness of the new features in cancer detection. Lastly, we identified a set of highly interactive genes by analyzing the SDAE connectivity matrices. Our results and analysis illustrate that these highly interactive genes could be useful cancer biomarkers for the detection of breast cancer that deserve further studies.</abstracttext></p></div></div>",danaeep@oregonstate.edu,,https://www.ncbi.nlm.nih.gov//pubmed/27896977,pubmed,2016,36e1aa56-b455-4a8d-92b7-90b75a32ffdd,1
opening up the blackbox: an interpretable deep neural network-based classifier for cell-type specific enhancer predictions,/pubmed/27490187,"Kim SG, Theera-Ampornpunt N, Fang CH, Harwani M, Grama A, Chaterji S.",BMC Syst Biol. 2016 Aug 1;10 Suppl 2:54. doi: 10.1186/s12918-016-0302-3.,BMC Syst Biol.  2016,PubMed,citation,PMID:27490187 | PMCID:PMC4977478,pubmed,27490187,create date:2016/08/05 | first author:Kim SG,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Gene expression is mediated by specialized cis-regulatory modules (CRMs), the most prominent of which are called enhancers. Early experiments indicated that enhancers located far from the gene promoters are often responsible for mediating gene transcription. Knowing their properties, regulatory activity, and genomic targets is crucial to the functional understanding of cellular events, ranging from cellular homeostasis to differentiation. Recent genome-wide investigation of epigenomic marks has indicated that enhancer elements could be enriched for certain epigenomic marks, such as, combinatorial patterns of histone modifications.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>Our efforts in this paper are motivated by these recent advances in epigenomic profiling methods, which have uncovered enhancer-associated chromatin features in different cell types and organisms. Specifically, in this paper, we use recent state-of-the-art Deep Learning methods and develop a deep neural network (DNN)-based architecture, called EP-DNN, to predict the presence and types of enhancers in the human genome. It uses as features, the expression levels of the histone modifications at the peaks of the functional sites as well as in its adjacent regions. We apply EP-DNN to four different cell types: H1, IMR90, HepG2, and HeLa S3. We train EP-DNN using p300 binding sites as enhancers, and TSS and random non-DHS sites as non-enhancers. We perform EP-DNN predictions to quantify the validation rate for different levels of confidence in the predictions and also perform comparisons against two state-of-the-art computational models for enhancer predictions, DEEP-ENCODE and RFECS.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We find that EP-DNN has superior accuracy and takes less time to make predictions. Next, we develop methods to make EP-DNN interpretable by computing the importance of each input feature in the classification task. This analysis indicates that the important histone modifications were distinct for different cell types, with some overlaps, e.g., H3K27ac was important in cell type H1 but less so in HeLa S3, while H3K4me1 was relatively important in all four cell types. We finally use the feature importance analysis to reduce the number of input features needed to train the DNN, thus reducing training time, which is often the computational bottleneck in the use of a DNN.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>In this paper, we developed EP-DNN, which has high accuracy of prediction, with validation rates above 90 % for the operational region of enhancer prediction for all four cell lines that we studied, outperforming DEEP-ENCODE and RFECS. Then, we developed a method to analyze a trained DNN and determine which histone modifications are important, and within that, which features proximal or distal to the enhancer site, are important.</abstracttext></p></div></div>",schaterj@purdue.edu,ChIP-seq; Cis-regulatory modules (CRMs); Deep neural networks (DNNs); Enhancer prediction; Genomic enhancers; Histone modifications; Interpretability of blackbox models,https://www.ncbi.nlm.nih.gov//pubmed/27490187,pubmed,2016,4c6419a9-85a1-4955-a4a4-ecba8e4de091,1
pedla: predicting enhancers with a deep learning-based algorithmic framework,/pubmed/27329130,"Liu F, Li H, Ren C, Bo X, Shu W.",Sci Rep. 2016 Jun 22;6:28517. doi: 10.1038/srep28517.,Sci Rep.  2016,PubMed,citation,PMID:27329130 | PMCID:PMC4916453,pubmed,27329130,create date:2016/06/23 | first author:Liu F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Transcriptional enhancers are non-coding segments of DNA that play a central role in the spatiotemporal regulation of gene expression programs. However, systematically and precisely predicting enhancers remain a major challenge. Although existing methods have achieved some success in enhancer prediction, they still suffer from many issues. We developed a deep learning-based algorithmic framework named PEDLA (https://github.com/wenjiegroup/PEDLA), which can directly learn an enhancer predictor from massively heterogeneous data and generalize in ways that are mostly consistent across various cell types/tissues. We first trained PEDLA with 1,114-dimensional heterogeneous features in H1 cells, and demonstrated that PEDLA framework integrates diverse heterogeneous features and gives state-of-the-art performance relative to five existing methods for enhancer prediction. We further extended PEDLA to iteratively learn from 22 training cell types/tissues. Our results showed that PEDLA manifested superior performance consistency in both training and independent test sets. On average, PEDLA achieved 95.0% accuracy and a 96.8% geometric mean (GM) of sensitivity and specificity across 22 training cell types/tissues, as well as 95.7% accuracy and a 96.8% GM across 20 independent test cell types/tissues. Together, our work illustrates the power of harnessing state-of-the-art deep learning techniques to consistently identify regulatory elements at a genome-wide scale from massively heterogeneous data across diverse cell types/tissues. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27329130,pubmed,2016,c6c579c7-df0c-4afd-a487-d33023a9a10e,1
deep learning applications for predicting pharmacological properties of drugs and drug repurposing using transcriptomic data,/pubmed/27200455,"Aliper A, Plis S, Artemov A, Ulloa A, Mamoshina P, Zhavoronkov A.",Mol Pharm. 2016 Jul 5;13(7):2524-30. doi: 10.1021/acs.molpharmaceut.6b00248. Epub 2016 Jun 8.,Mol Pharm.  2016,PubMed,citation,PMID:27200455 | PMCID:PMC4965264,pubmed,27200455,create date:2016/05/21 | first author:Aliper A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning is rapidly advancing many areas of science and technology with multiple success stories in image, text, voice and video recognition, robotics, and autonomous driving. In this paper we demonstrate how deep neural networks (DNN) trained on large transcriptional response data sets can classify various drugs to therapeutic categories solely based on their transcriptional profiles. We used the perturbation samples of 678 drugs across A549, MCF-7, and PC-3 cell lines from the LINCS Project and linked those to 12 therapeutic use categories derived from MeSH. To train the DNN, we utilized both gene level transcriptomic data and transcriptomic data processed using a pathway activation scoring algorithm, for a pooled data set of samples perturbed with different concentrations of the drug for 6 and 24 hours. In both pathway and gene level classification, DNN achieved high classification accuracy and convincingly outperformed the support vector machine (SVM) model on every multiclass classification problem, however, models based on pathway level data performed significantly better. For the first time we demonstrate a deep learning neural net trained on transcriptomic data to recognize pharmacological properties of multiple drugs across different biological systems and conditions. We also propose using deep neural net confusion matrices for drug repositioning. This work is a proof of principle for applying deep learning to drug discovery and development. </abstracttext></p></div></div>",,DNN; confusion matrix; deep learning; deep neural networks; drug discovery; drug repurposing; predictor,https://www.ncbi.nlm.nih.gov//pubmed/27200455,pubmed,2016,aacfcecc-bb5e-43c6-9262-fd3fe9adf75d,1
gene expression inference with deep learning,/pubmed/26873929,"Chen Y, Li Y, Narayan R, Subramanian A, Xie X.",Bioinformatics. 2016 Jun 15;32(12):1832-9. doi: 10.1093/bioinformatics/btw074. Epub 2016 Feb 11.,Bioinformatics.  2016,PubMed,citation,PMID:26873929 | PMCID:PMC4908320,pubmed,26873929,create date:2016/02/14 | first author:Chen Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>MOTIVATION: </h4><p><abstracttext label='MOTIVATION' nlmcategory='BACKGROUND'>Large-scale gene expression profiling has been widely used to characterize cellular states in response to various disease conditions, genetic perturbations, etc. Although the cost of whole-genome expression profiles has been dropping steadily, generating a compendium of expression profiling over thousands of samples is still very expensive. Recognizing that gene expressions are often highly correlated, researchers from the NIH LINCS program have developed a cost-effective strategy of profiling only ∼1000 carefully selected landmark genes and relying on computational methods to infer the expression of remaining target genes. However, the computational approach adopted by the LINCS program is currently based on linear regression (LR), limiting its accuracy since it does not capture complex nonlinear relationship between expressions of genes.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We present a deep learning method (abbreviated as D-GEX) to infer the expression of target genes from the expression of landmark genes. We used the microarray-based Gene Expression Omnibus dataset, consisting of 111K expression profiles, to train our model and compare its performance to those from other methods. In terms of mean absolute error averaged across all genes, deep learning significantly outperforms LR with 15.33% relative improvement. A gene-wise comparative analysis shows that deep learning achieves lower error than LR in 99.97% of the target genes. We also tested the performance of our learned model on an independent RNA-Seq-based GTEx dataset, which consists of 2921 expression profiles. Deep learning still outperforms LR with 6.57% relative improvement, and achieves lower error in 81.31% of the target genes.</abstracttext></p><h4>AVAILABILITY AND IMPLEMENTATION: </h4><p><abstracttext label='AVAILABILITY AND IMPLEMENTATION' nlmcategory='METHODS'>D-GEX is available at https://github.com/uci-cbcl/D-GEX CONTACT: xhx@ics.uci.edu</abstracttext></p><h4>SUPPLEMENTARY INFORMATION: </h4><p><abstracttext label='SUPPLEMENTARY INFORMATION' nlmcategory='BACKGROUND'>Supplementary data are available at Bioinformatics online.</abstracttext></p><p class='copyright'>© The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com.</p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26873929,pubmed,2016,fd7a4bbf-38b4-41d9-8151-5fe7f0e9c704,1
learning a hierarchical representation of the yeast transcriptomic machinery using an autoencoder model,/pubmed/26818848,"Chen L, Cai C, Chen V, Lu X.",BMC Bioinformatics. 2016 Jan 11;17 Suppl 1:9. doi: 10.1186/s12859-015-0852-1.,BMC Bioinformatics.  2016,PubMed,citation,PMID:26818848 | PMCID:PMC4895523,pubmed,26818848,create date:2016/01/29 | first author:Chen L,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>A living cell has a complex, hierarchically organized signaling system that encodes and assimilates diverse environmental and intracellular signals, and it further transmits signals that control cellular responses, including a tightly controlled transcriptional program. An important and yet challenging task in systems biology is to reconstruct cellular signaling system in a data-driven manner. In this study, we investigate the utility of deep hierarchical neural networks in learning and representing the hierarchical organization of yeast transcriptomic machinery.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We have designed a sparse autoencoder model consisting of a layer of observed variables and four layers of hidden variables. We applied the model to over a thousand of yeast microarrays to learn the encoding system of yeast transcriptomic machinery. After model selection, we evaluated whether the trained models captured biologically sensible information. We show that the latent variables in the first hidden layer correctly captured the signals of yeast transcription factors (TFs), obtaining a close to one-to-one mapping between latent variables and TFs. We further show that genes regulated by latent variables at higher hidden layers are often involved in a common biological process, and the hierarchical relationships between latent variables conform to existing knowledge. Finally, we show that information captured by the latent variables provide more abstract and concise representations of each microarray, enabling the identification of better separated clusters in comparison to gene-based representation.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>Contemporary deep hierarchical latent variable models, such as the autoencoder, can be used to partially recover the organization of transcriptomic machinery.</abstracttext></p></div></div>",luc17@pitt.edu,,https://www.ncbi.nlm.nih.gov//pubmed/26818848,pubmed,2016,433f27f7-9438-4735-b41f-5eae375d082e,1
deep feature selection: theory and application to identify enhancers and promoters,/pubmed/26799292,"Li Y, Chen CY, Wasserman WW.",J Comput Biol. 2016 May;23(5):322-36. doi: 10.1089/cmb.2015.0189. Epub 2016 Jan 22.,J Comput Biol.  2016,PubMed,citation,PMID:26799292,pubmed,26799292,create date:2016/01/23 | first author:Li Y,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Sparse linear models approximate target variable(s) by a sparse linear combination of input variables. Since they are simple, fast, and able to select features, they are widely used in classification and regression. Essentially they are shallow feed-forward neural networks that have three limitations: (1) incompatibility to model nonlinearity of features, (2) inability to learn high-level features, and (3) unnatural extensions to select features in a multiclass case. Deep neural networks are models structured by multiple hidden layers with nonlinear activation functions. Compared with linear models, they have two distinctive strengths: the capability to (1) model complex systems with nonlinear structures and (2) learn high-level representation of features. Deep learning has been applied in many large and complex systems where deep models significantly outperform shallow ones. However, feature selection at the input level, which is very helpful to understand the nature of a complex system, is still not well studied. In genome research, the cis-regulatory elements in noncoding DNA sequences play a key role in the expression of genes. Since the activity of regulatory elements involves highly interactive factors, a deep tool is strongly needed to discover informative features. In order to address the above limitations of shallow and deep models for selecting features of a complex system, we propose a deep feature selection (DFS) model that (1) takes advantages of deep structures to model nonlinearity and (2) conveniently selects a subset of features right at the input level for multiclass data. Simulation experiments convince us that this model is able to correctly identify both linear and nonlinear features. We applied this model to the identification of active enhancers and promoters by integrating multiple sources of genomic information. Results show that our model outperforms elastic net in terms of size of discriminative feature subset and classification accuracy.</abstracttext></p></div></div>",,deep feature selection; deep learning; enhancer; promoter,https://www.ncbi.nlm.nih.gov//pubmed/26799292,pubmed,2016,e2e6f91f-9cbf-4e00-8dc4-f0ea35d5b02d,1
a deep learning framework for modeling structural features of rna-binding protein targets,/pubmed/26467480,"Zhang S, Zhou J, Hu H, Gong H, Chen L, Cheng C, Zeng J.",Nucleic Acids Res. 2016 Feb 29;44(4):e32. doi: 10.1093/nar/gkv1025. Epub 2015 Oct 13.,Nucleic Acids Res.  2016,PubMed,citation,PMID:26467480 | PMCID:PMC4770198,pubmed,26467480,create date:2015/10/16 | first author:Zhang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>RNA-binding proteins (RBPs) play important roles in the post-transcriptional control of RNAs. Identifying RBP binding sites and characterizing RBP binding preferences are key steps toward understanding the basic mechanisms of the post-transcriptional gene regulation. Though numerous computational methods have been developed for modeling RBP binding preferences, discovering a complete structural representation of the RBP targets by integrating their available structural features in all three dimensions is still a challenging task. In this paper, we develop a general and flexible deep learning framework for modeling structural binding preferences and predicting binding sites of RBPs, which takes (predicted) RNA tertiary structural information into account for the first time. Our framework constructs a unified representation that characterizes the structural specificities of RBP targets in all three dimensions, which can be further used to predict novel candidate binding sites and discover potential binding motifs. Through testing on the real CLIP-seq datasets, we have demonstrated that our deep learning framework can automatically extract effective hidden structural features from the encoded raw sequence and structural profiles, and predict accurate RBP binding sites. In addition, we have conducted the first study to show that integrating the additional RNA tertiary structural features can improve the model performance in predicting RBP binding sites, especially for the polypyrimidine tract-binding protein (PTB), which also provides a new evidence to support the view that RBPs may own specific tertiary structural binding preferences. In particular, the tests on the internal ribosome entry site (IRES) segments yield satisfiable results with experimental support from the literature and further demonstrate the necessity of incorporating RNA tertiary structural information into the prediction model. The source code of our approach can be found in https://github.com/thucombio/deepnet-rbp. </abstracttext></p><p class='copyright'>© The Author(s) 2015. Published by Oxford University Press on behalf of Nucleic Acids Research.</p></div></div>",chao.cheng@dartmouth.edu,,https://www.ncbi.nlm.nih.gov//pubmed/26467480,pubmed,2016,6be99719-48cf-46ca-8ba5-208c84bc767d,1
integrative data analysis of multi-platform cancer data with a multimodal deep learning approach,/pubmed/26357333,"Liang M, Li Z, Chen T, Zeng J.",IEEE/ACM Trans Comput Biol Bioinform. 2015 Jul-Aug;12(4):928-37. doi: 10.1109/TCBB.2014.2377729.,IEEE/ACM Trans Comput Biol Bioinform.  2015,PubMed,citation,PMID:26357333,pubmed,26357333,create date:2015/09/12 | first author:Liang M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26357333,pubmed,2015,02e1ef67-6060-4a32-bcff-54b9d3dd8e21,1
lncrna-mfdl: identification of human long non-coding rnas by fusing multiple features and using deep learning,/pubmed/25588719,"Fan XN, Zhang SW.",Mol Biosyst. 2015 Mar;11(3):892-7. doi: 10.1039/c4mb00650j. Epub 2015 Jan 15.,Mol Biosyst.  2015,PubMed,citation,PMID:25588719,pubmed,25588719,create date:2015/01/16 | first author:Fan XN,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Long noncoding RNAs (lncRNAs) are emerging as a novel class of noncoding RNAs and potent gene regulators, which play an important and varied role in cellular functions. lncRNAs are closely related with the occurrence and development of some diseases. High-throughput RNA-sequencing techniques combined with de novo assembly have identified a large number of novel transcripts. The discovery of large and 'hidden' transcriptomes urgently requires the development of effective computational methods that can rapidly distinguish between coding and long noncoding RNAs. In this study, we developed a powerful predictor (named as lncRNA-MFDL) to identify lncRNAs by fusing multiple features of the open reading frame, k-mer, the secondary structure and the most-like coding domain sequence and using deep learning classification algorithms. Using the same human training dataset and a 10-fold cross validation test, lncRNA-MFDL can achieve 97.1% prediction accuracy which is 5.7, 3.7, and 3.4% higher than that of CPC, CNCI and lncRNA-FMFSVM predictors, respectively. Compared with CPC and CNCI predictors in other species (e.g., anole lizard, zebrafish, chicken, gorilla, macaque, mouse, lamprey, orangutan, xenopus and C. elegans) testing datasets, the new lncRNA-MFDL predictor is also much more effective and robust. These results show that lncRNA-MFDL is a powerful tool for identifying lncRNAs. The lncRNA-MFDL software package is freely available at for academic users. </abstracttext></p></div></div>",zhangsw@nwpu.edu.cn,,https://www.ncbi.nlm.nih.gov//pubmed/25588719,pubmed,2015,7b8a1b37-76eb-4874-b204-d74a122285e8,1
a regularized deep learning approach for clinical risk prediction of acute coronary syndrome using electronic health records,/pubmed/28742027,"Huang Z, Dong W, Duan H, Liu J.",IEEE Trans Biomed Eng. 2017 Jul 24. doi: 10.1109/TBME.2017.2731158. [Epub ahead of print],IEEE Trans Biomed Eng.  2017,PubMed,citation,PMID:28742027,pubmed,28742027,create date:2017/07/26 | first author:Huang Z,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>OBJECTIVE: </h4><p><abstracttext label='OBJECTIVE' nlmcategory='OBJECTIVE'>Acute coronary syndrome (ACS), as a common and severe cardiovascular disease, is a leading cause of death and the principal cause of serious long-term disability globally. Clinical risk prediction of ACS is important for early intervention and treatment. Existing ACS risk scoring models are based mainly on a small set of hand-picked risk factors and often dichotomize predictive variables to simplify the score calculation [1-3].</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>This study develops a regularized stacked denoising auto-encoder (SDAE) model to stratify clinical risks of ACS patients from a large volume of electronic health records (EHR). To capture characteristics of patients at similar risk levels, and preserve the discriminating information across different risk levels, two constraints are added on SDAE to make the reconstructed feature representations contain more risk information of patients, which contribute to a better clinical risk prediction result.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>We validate our approach on a real clinical dataset consisting of 3,464 ACS patient samples. The performance of our approach for predicting ACS risk remains robust and reaches 0.868 and 0.73 in terms of both AUC and Accuracy, respectively.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The obtained results show that the proposed approach achieves a competitive performance compared to state-of-the-art models in dealing with the clinical risk prediction problem. In addition, our approach can extract informative risk factors of ACS via a reconstructive learning strategy. Some of these extracted risk factors are not only consistent with existing medical domain knowledge, but also contain suggestive hypotheses that could be validated by further investigations in the medical domain.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28742027,pubmed,2017,d720857c-a261-4163-a2c5-3864156a122e,1
interpretable deep models for icu outcome prediction,/pubmed/28269832,"Che Z, Purushotham S, Khemani R, Liu Y.",AMIA Annu Symp Proc. 2017 Feb 10;2016:371-380. eCollection 2016.,AMIA Annu Symp Proc.  2016,PubMed,citation,PMID:28269832 | PMCID:PMC5333206,pubmed,28269832,create date:2017/03/09 | first author:Che Z,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Exponential surge in health care data, such as longitudinal data from electronic health records (EHR), sensor data from intensive care unit (ICU), etc., is providing new opportunities to discover meaningful data-driven characteristics and patterns ofdiseases. Recently, deep learning models have been employedfor many computational phenotyping and healthcare prediction tasks to achieve state-of-the-art performance. However, deep models lack interpretability which is crucial for wide adoption in medical research and clinical decision-making. In this paper, we introduce a simple yet powerful knowledge-distillation approach called interpretable mimic learning, which uses gradient boosting trees to learn interpretable models and at the same time achieves strong prediction performance as deep learning models. Experiment results on Pediatric ICU dataset for acute lung injury (ALI) show that our proposed method not only outperforms state-of-the-art approaches for morality and ventilator free days prediction tasks but can also provide interpretable models to clinicians.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28269832,pubmed,2016,b4ec0885-fd50-45d9-a4d0-3f1dae865083,1
using recurrent neural network models for early detection of heart failure onset,/pubmed/27521897,"Choi E, Schuetz A, Stewart WF, Sun J.",J Am Med Inform Assoc. 2017 Mar 1;24(2):361-370. doi: 10.1093/jamia/ocw112.,J Am Med Inform Assoc.  2017,PubMed,citation,PMID:27521897 | PMCID:PMC5391725,pubmed,27521897,create date:2016/08/16 | first author:Choi E,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Objective: </h4><p><abstracttext label='Objective'>We explored whether use of deep learning to model temporal relations among events in electronic health records (EHRs) would improve model performance in predicting initial diagnosis of heart failure (HF) compared to conventional methods that ignore temporality.</abstracttext></p><h4>Materials and Methods: </h4><p><abstracttext label='Materials and Methods'>Data were from a health system's EHR on 3884 incident HF cases and 28 903 controls, identified as primary care patients, between May 16, 2000, and May 23, 2013. Recurrent neural network (RNN) models using gated recurrent units (GRUs) were adapted to detect relations among time-stamped events (eg, disease diagnosis, medication orders, procedure orders, etc.) with a 12- to 18-month observation window of cases and controls. Model performance metrics were compared to regularized logistic regression, neural network, support vector machine, and K-nearest neighbor classifier approaches.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results'>Using a 12-month observation window, the area under the curve (AUC) for the RNN model was 0.777, compared to AUCs for logistic regression (0.747), multilayer perceptron (MLP) with 1 hidden layer (0.765), support vector machine (SVM) (0.743), and K-nearest neighbor (KNN) (0.730). When using an 18-month observation window, the AUC for the RNN model increased to 0.883 and was significantly higher than the 0.834 AUC for the best of the baseline methods (MLP).</abstracttext></p><h4>Conclusion: </h4><p><abstracttext label='Conclusion'>Deep learning models adapted to leverage temporal relations appear to improve performance of models for detection of incident heart failure with a short observation window of 12-18 months.</abstracttext></p></div></div>",,deep learning; electronic health records; heart failure prediction; patient progression model; recurrent neural network,https://www.ncbi.nlm.nih.gov//pubmed/27521897,pubmed,2017,54f75e7a-ed34-4637-ad57-55f4d0a113e8,1
accurate classification of protein subcellular localization from high-throughput microscopy images using deep learning,/pubmed/28391243,"Pärnamaa T, Parts L.",G3 (Bethesda). 2017 May 5;7(5):1385-1392. doi: 10.1534/g3.116.033654.,G3 (Bethesda).  2017,PubMed,citation,PMID:28391243 | PMCID:PMC5427497,pubmed,28391243,create date:2017/04/10 | first author:Pärnamaa T,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>High-throughput microscopy of many single cells generates high-dimensional data that are far from straightforward to analyze. One important problem is automatically detecting the cellular compartment where a fluorescently-tagged protein resides, a task relatively simple for an experienced human, but difficult to automate on a computer. Here, we train an 11-layer neural network on data from mapping thousands of yeast proteins, achieving per cell localization classification accuracy of 91%, and per protein accuracy of 99% on held-out images. We confirm that low-level network features correspond to basic image characteristics, while deeper layers separate localization classes. Using this network as a feature calculator, we train standard classifiers that assign proteins to previously unseen compartments after observing only a small number of training examples. Our results are the most accurate subcellular localization classifications to date, and demonstrate the usefulness of deep learning for high-throughput microscopy.</abstracttext></p><p class='copyright'>Copyright © 2017 Parnamaa and Parts.</p></div></div>",leopold.parts@sanger.ac.uk,deep learning; high-content screening; machine learning; microscopy; yeast,https://www.ncbi.nlm.nih.gov//pubmed/28391243,pubmed,2017,d50796da-87f2-4493-846e-6eeb498acc63,1
deepem3d: approaching human-level performance on 3d anisotropic em image segmentation,/pubmed/28379412,"Zeng T, Wu B, Ji S.",Bioinformatics. 2017 Aug 15;33(16):2555-2562. doi: 10.1093/bioinformatics/btx188.,Bioinformatics.  2017,PubMed,citation,PMID:28379412,pubmed,28379412,create date:2017/04/06 | first author:Zeng T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>Motivation: </h4><p><abstracttext label='Motivation' nlmcategory='UNASSIGNED'>Progress in 3D electron microscopy (EM) imaging has greatly facilitated neuroscience research in high-throughput data acquisition. Correspondingly, high-throughput automated image analysis methods are necessary to work on par with the speed of data being produced. One such example is the need for automated EM image segmentation for neurite reconstruction. However, the efficiency and reliability of current methods are still lagging far behind human performance.</abstracttext></p><h4>Results: </h4><p><abstracttext label='Results' nlmcategory='UNASSIGNED'>Here, we propose DeepEM3D, a deep learning method for segmenting 3D anisotropic brain electron microscopy images. In this method, the deep learning model can efficiently build feature representation and incorporate sufficient multi-scale contextual information. We propose employing a combination of novel boundary map generation methods with optimized model ensembles to address the inherent challenges of segmenting anisotropic images. We evaluated our method by participating in the 3D segmentation of neurites in EM images (SNEMI3D) challenge. Our submission is ranked #1 on the current leaderboard as of Oct 15, 2016. More importantly, our result was very close to human-level performance in terms of the challenge evaluation metric: namely, a Rand error of 0.06015 versus the human value of 0.05998.</abstracttext></p><h4>Availability and Implementation: </h4><p><abstracttext label='Availability and Implementation' nlmcategory='UNASSIGNED'>The code is available at https://github.com/divelab/deepem3d/.</abstracttext></p><h4>Contact: </h4><p><abstracttext label='Contact' nlmcategory='UNASSIGNED'>sji@eecs.wsu.edu.</abstracttext></p><h4>Supplementary information: </h4><p><abstracttext label='Supplementary information' nlmcategory='UNASSIGNED'>Supplementary data are available at Bioinformatics online.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28379412,pubmed,2017,a0f317d8-a596-4a3f-b867-846499ba1b97,1
deep learning segmentation of optical microscopy images improves 3-d neuron reconstruction,/pubmed/28287966,"Li R, Zeng T, Peng H, Ji S.",IEEE Trans Med Imaging. 2017 Jul;36(7):1533-1541. doi: 10.1109/TMI.2017.2679713. Epub 2017 Mar 8.,IEEE Trans Med Imaging.  2017,PubMed,citation,PMID:28287966,pubmed,28287966,create date:2017/03/14 | first author:Li R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28287966,pubmed,2017,d932ab7c-0ece-4f62-be87-204bf6f3982a,1
"analysis of live cell images: methods, tools and opportunities",/pubmed/28242295,"Nketia TA, Sailem H, Rohde G, Machiraju R, Rittscher J.",Methods. 2017 Feb 15;115:65-79. doi: 10.1016/j.ymeth.2017.02.007. Epub 2017 Feb 27. Review.,Methods.  2017,PubMed,citation,PMID:28242295,pubmed,28242295,create date:2017/03/01 | first author:Nketia TA,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Advances in optical microscopy, biosensors and cell culturing technologies have transformed live cell imaging. Thanks to these advances live cell imaging plays an increasingly important role in basic biology research as well as at all stages of drug development. Image analysis methods are needed to extract quantitative information from these vast and complex data sets. The aim of this review is to provide an overview of available image analysis methods for live cell imaging, in particular required preprocessing image segmentation, cell tracking and data visualisation methods. The potential opportunities recent advances in machine learning, especially deep learning, and computer vision provide are being discussed. This review includes overview of the different available software packages and toolkits.</abstracttext></p><p class='copyright'>Copyright © 2017. Published by Elsevier Inc.</p></div></div>",jens.rittscher@eng.ox.ac.uk,Biological image analysis; Cell segmentation; Cell tracking; Live cell imaging; Machine learning; Quantitative biological imaging,https://www.ncbi.nlm.nih.gov//pubmed/28242295,pubmed,2017,280fda25-0460-4da8-a36c-d70b28da0afa,1
application of the pamono-sensor for quantiﬁcation of microvesicles and determination of nano-particle size distribution,/pubmed/28134825,"Shpacovitch V, Sidorenko I, Lenssen JE, Temchura V, Weichert F, Müller H, Überla K, Zybin A, Schramm A, Hergenröder R.",Sensors (Basel). 2017 Jan 27;17(2). pii: E244. doi: 10.3390/s17020244.,Sensors (Basel).  2017,PubMed,citation,PMID:28134825 | PMCID:PMC5336007,pubmed,28134825,create date:2017/01/31 | first author:Shpacovitch V,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The PAMONO-sensor (plasmon assisted microscopy of nano-objects) demonstrated an ability to detect and quantify individual viruses and virus-like particles. However, another group of biological vesicles-microvesicles (100-1000 nm)-also attracts growing interest as biomarkers of different pathologies and needs development of novel techniques for characterization. This work shows the applicability of a PAMONO-sensor for selective detection of microvesicles in aquatic samples. The sensor permits comparison of relative concentrations of microvesicles between samples. We also study a possibility of repeated use of a sensor chip after elution of the microvesicle capturing layer. Moreover, we improve the detection features of the PAMONO-sensor. The detection process utilizes novel machine learning techniques on the sensor image data to estimate particle size distributions of nano-particles in polydisperse samples. Altogether, our ﬁndings expand analytical features and the application ﬁeld of the PAMONO-sensor. They can also serve for a maturation of diagnostic tools based on the PAMONO-sensor platform.</abstracttext></p></div></div>",viktoria.shpacovitch@isas.de,deep learning; extracellular vesicles; machine learning; microvesicles; plasmonic sensors; surface plasmon resonance,https://www.ncbi.nlm.nih.gov//pubmed/28134825,pubmed,2017,6ae10bc1-80d3-4cea-a3d4-536d0f43bcb5,1
an overview of data science uses in bioimage informatics,/pubmed/28057585,Chessel A.,Methods. 2017 Feb 15;115:110-118. doi: 10.1016/j.ymeth.2016.12.014. Epub 2017 Jan 3. Review.,Methods.  2017,PubMed,citation,PMID:28057585,pubmed,28057585,create date:2017/01/07 | first author:Chessel A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This review aims at providing a practical overview of the use of statistical features and associated data science methods in bioimage informatics. To achieve a quantitative link between images and biological concepts, one typically replaces an object coming from an image (a segmented cell or intracellular object, a pattern of expression or localisation, even a whole image) by a vector of numbers. They range from carefully crafted biologically relevant measurements to features learnt through deep neural networks. This replacement allows for the use of practical algorithms for visualisation, comparison and inference, such as the ones from machine learning or multivariate statistics. While originating mainly, for biology, in high content screening, those methods are integral to the use of data science for the quantitative analysis of microscopy images to gain biological insight, and they are sure to gather more interest as the need to make sense of the increasing amount of acquired imaging data grows more pressing.</abstracttext></p><p class='copyright'>Copyright © 2017 Elsevier Inc. All rights reserved.</p></div></div>",anatole.chessel@polytechnique.edu,Bioimage informatics; Data science; High content screening; Image analysis,https://www.ncbi.nlm.nih.gov//pubmed/28057585,pubmed,2017,ca6677b6-e770-430f-a8db-ab50a4b8641f,1
"mining textural knowledge in biological images: applications, methods and trends",/pubmed/27994798,"Di Cataldo S, Ficarra E.",Comput Struct Biotechnol J. 2016 Nov 24;15:56-67. eCollection 2017. Review.,Comput Struct Biotechnol J.  2017,PubMed,citation,PMID:27994798 | PMCID:PMC5155047,pubmed,27994798,create date:2016/12/21 | first author:Di Cataldo S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Texture analysis is a major task in many areas of computer vision and pattern recognition, including biological imaging. Indeed, visual textures can be exploited to distinguish specific tissues or cells in a biological sample, to highlight chemical reactions between molecules, as well as to detect subcellular patterns that can be evidence of certain pathologies. This makes automated texture analysis fundamental in many applications of biomedicine, such as the accurate detection and grading of multiple types of cancer, the differential diagnosis of autoimmune diseases, or the study of physiological processes. Due to their specific characteristics and challenges, the design of texture analysis systems for biological images has attracted ever-growing attention in the last few years. In this paper, we perform a critical review of this important topic. First, we provide a general definition of texture analysis and discuss its role in the context of bioimaging, with examples of applications from the recent literature. Then, we review the main approaches to automated texture analysis, with special attention to the methods of feature extraction and encoding that can be successfully applied to microscopy images of cells or tissues. Our aim is to provide an overview of the state of the art, as well as a glimpse into the latest and future trends of research in this area.</abstracttext></p></div></div>",,Bioimaging; Deep learning; Feature encoding; Textural analysis; Textural features extraction; Texture classification,https://www.ncbi.nlm.nih.gov//pubmed/27994798,pubmed,2017,f244b66b-1c5f-4e5d-84a4-25f2ebfd6181,1
deep data mining in a real space: separation of intertwined electronic responses in a lightly doped bafe(2)as(2),/pubmed/27780159,"Ziatdinov M, Maksov A, Li L, Sefat AS, Maksymovych P, Kalinin SV.",Nanotechnology. 2016 Nov 25;27(47):475706. Epub 2016 Oct 25.,Nanotechnology.  2016,PubMed,citation,PMID:27780159,pubmed,27780159,create date:2016/10/27 | first author:Ziatdinov M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Electronic interactions present in material compositions close to the superconducting dome play a key role in the manifestation of high-T <sub>c</sub> superconductivity. In many correlated electron systems, however, the parent or underdoped states exhibit strongly inhomogeneous electronic landscape at the nanoscale that may be associated with competing, coexisting, or intertwined chemical disorder, strain, magnetic, and structural order parameters. Here we demonstrate an approach based on a combination of scanning tunneling microscopy/spectroscopy and advanced statistical learning for an automatic separation and extraction of statistically significant electronic behaviors in the spin density wave regime of a lightly (∼1%) gold-doped BaFe<sub>2</sub>As<sub>2</sub>. We show that the decomposed STS spectral features have a direct relevance to fundamental physical properties of the system, such as SDW-induced gap, pseudogap-like state, and impurity resonance states.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27780159,pubmed,2016,6221314a-7d9d-4e19-80f2-575a8a544983,1
deeppicker: a deep learning approach for fully automated particle picking in cryo-em,/pubmed/27424268,"Wang F, Gong H, Liu G, Li M, Yan C, Xia T, Li X, Zeng J.",J Struct Biol. 2016 Sep;195(3):325-36. doi: 10.1016/j.jsb.2016.07.006. Epub 2016 Jul 14.,J Struct Biol.  2016,PubMed,citation,PMID:27424268,pubmed,27424268,create date:2016/07/18 | first author:Wang F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Particle picking is a time-consuming step in single-particle analysis and often requires significant interventions from users, which has become a bottleneck for future automated electron cryo-microscopy (cryo-EM). Here we report a deep learning framework, called DeepPicker, to address this problem and fill the current gaps toward a fully automated cryo-EM pipeline. DeepPicker employs a novel cross-molecule training strategy to capture common features of particles from previously-analyzed micrographs, and thus does not require any human intervention during particle picking. Tests on the recently-published cryo-EM data of three complexes have demonstrated that our deep learning based scheme can successfully accomplish the human-level particle picking process and identify a sufficient number of particles that are comparable to those picked manually by human experts. These results indicate that DeepPicker can provide a practically useful tool to significantly reduce the time and manual effort spent in single-particle analysis and thus greatly facilitate high-resolution cryo-EM structure determination. DeepPicker is released as an open-source program, which can be downloaded from https://github.com/nejyeah/DeepPicker-python. </abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier Inc. All rights reserved.</p></div></div>",isutian@gmail.com,Automation; Cryo-EM; Deep learning; Particle picking,https://www.ncbi.nlm.nih.gov//pubmed/27424268,pubmed,2016,2693d188-f3a0-477b-bdda-589a9c3ecc96,1
active deep learning-based annotation of electroencephalography reports for cohort identification,/pubmed/28815135,"Maldonado R, Goodwin TR, Harabagiu SM.",AMIA Jt Summits Transl Sci Proc. 2017 Jul 26;2017:229-238. eCollection 2017.,AMIA Jt Summits Transl Sci Proc.  2017,PubMed,citation,PMID:28815135 | PMCID:PMC5543351,pubmed,28815135,create date:2017/08/18 | first author:Maldonado R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The annotation of a large corpus of Electroencephalography (EEG) reports is a crucial step in the development of an EEG-specific patient cohort retrieval system. The annotation of multiple types of EEG-specific medical concepts, along with their polarity and modality, is challenging, especially when automatically performed on Big Data. To address this challenge, we present a novel framework which combines the advantages of active and deep learning while producing annotations that capture a variety of attributes of medical concepts. Results obtained through our novel framework show great promise.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28815135,pubmed,2017,de5a3d75-757d-41a8-a009-ab07e091f180,1
deep learning from eeg reports for inferring underspecified information,/pubmed/28815118,"Goodwin TR, Harabagiu SM.",AMIA Jt Summits Transl Sci Proc. 2017 Jul 26;2017:112-121. eCollection 2017.,AMIA Jt Summits Transl Sci Proc.  2017,PubMed,citation,PMID:28815118 | PMCID:PMC5543361,pubmed,28815118,create date:2017/08/18 | first author:Goodwin TR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Secondary use<sup>1</sup>of electronic health records (EHRs) often relies on the ability to automatically identify and extract information from EHRs. Unfortunately, EHRs are known to suffer from a variety of idiosyncrasies - most prevalently, they have been shown to often omit or underspecify information. Adapting traditional machine learning methods for inferring underspecified information relies on manually specifying features characterizing the specific information to recover (e.g. particular findings, test results, or physician's impressions). By contrast, in this paper, we present a method for jointly (1) automatically extracting word- and report-level features and (2) inferring underspecified information from EHRs. Our approach accomplishes these two tasks jointly by combining recent advances in deep neural learning with access to textual data in electroencephalogram (EEG) reports. We evaluate the performance of our model on the problem of inferring the neurologist's over-all impression (normal or abnormal) from electroencephalogram (EEG) reports and report an accuracy of 91.4% precision of 94.4% recall of 91.2% and <i>F<sub>1</sub></i> measure of 92.8% (a 40% improvement over the performance obtained using Doc2Vec). These promising results demonstrate the power of our approach, while error analysis reveals remaining obstacles as well as areas for future improvement.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28815118,pubmed,2017,03515c24-c222-43eb-bef3-46bfe16de701,1
deep belief networks for electroencephalography: a review of recent contributions and future outlooks,/pubmed/28715343,"Movahedi F, Coyle JL, Sejdic E.",IEEE J Biomed Health Inform. 2017 Jul 14. doi: 10.1109/JBHI.2017.2727218. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28715343,pubmed,28715343,create date:2017/07/18 | first author:Movahedi F,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep learning, a relatively new branch of machine learning, has been investigated for use in a variety of biomedical applications. Deep learning algorithms have been used to analyze different physiological signals and gain a better understanding of human physiology for automated diagnosis of abnormal conditions. In this manuscript, we provide an overview of deep learning approaches with a focus on deep belief networks in electroencephalography applications. We investigate the state of- the-art algorithms for deep belief networks and then cover the application of these algorithms and their performances in electroencephalographic applications. We covered various applications of electroencephalography in medicine, including emotion recognition, sleep stage classification, and seizure detection, in order to understand how deep learning algorithms could be modified to better suit the tasks desired. This review is intended to provide researchers with a broad overview of the currently existing deep belief network methodology for electroencephalography signals, as well as to highlight potential challenges for future research.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28715343,pubmed,2017,bac9a970-6947-4fa3-9aa0-48c5ce42a871,1
deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg,/pubmed/28678710,"Supratak A, Dong H, Wu C, Guo Y.",IEEE Trans Neural Syst Rehabil Eng. 2017 Jun 28. doi: 10.1109/TNSRE.2017.2721116. [Epub ahead of print],IEEE Trans Neural Syst Rehabil Eng.  2017,PubMed,citation,PMID:28678710,pubmed,28678710,create date:2017/07/06 | first author:Supratak A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>The present study proposes a deep learning model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channel EEG. Most of the existing methods rely on hand-engineered features which require prior knowledge of sleep analysis. Only a few of them encode the temporal information such as transition rules, which is important for identifying the next sleep stages, into the extracted features. In the proposed model, we utilize Convolutional Neural Networks to extract timeinvariant features, and bidirectional-Long Short-Term Memory to learn transition rules among sleep stages automatically from EEG epochs.We implement a two-step training algorithm to train our model efficiently. We evaluated our model using different single-channel EEGs (F4-EOG(Left), Fpz-Cz and Pz-Oz) from two public sleep datasets, that have different properties (e.g., sampling rate) and scoring standards (AASM and R&amp;K). The results showed that our model achieved similar overall accuracy and macro F1-score (MASS: 86.2%-81.7, Sleep-EDF: 82.0%-76.9) compared to the state-of-the-art methods (MASS: 85.9%-80.5, Sleep-EDF: 78.9%-73.7) on both datasets. This demonstrated that, without changing the model architecture and the training algorithm, our model could automatically learn features for sleep stage scoring from different raw single-channel EEGs from different datasets without utilizing any hand-engineered features.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28678710,pubmed,2017,f29ede40-367e-4a15-abf4-1fce2f16c373,1
semi-automated annotation of signal events in clinical eeg data,/pubmed/28649599,"Yang S, López S, Golmohammadi M, Obeid I, Picone J.",IEEE Signal Process Med Biol Symp. 2016 Dec;2016. doi: 10.1109/SPMB.2016.7846855. Epub 2017 Feb 9.,IEEE Signal Process Med Biol Symp.  2016,PubMed,citation,PMID:28649599 | PMCID:PMC5480208,pubmed,28649599,create date:2017/06/27 | first author:Yang S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28649599,pubmed,2016,8295e2d3-68eb-4be1-a5c3-c20b63d12211,1
stacked autoencoders for the p300 component detection,/pubmed/28611579,"Vařeka L, Mautner P.",Front Neurosci. 2017 May 30;11:302. doi: 10.3389/fnins.2017.00302. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28611579 | PMCID:PMC5447744,pubmed,28611579,create date:2017/06/15 | first author:Vařeka L,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Novel neural network training methods (commonly referred to as deep learning) have emerged in recent years. Using a combination of unsupervised pre-training and subsequent fine-tuning, deep neural networks have become one of the most reliable classification methods. Since deep neural networks are especially powerful for high-dimensional and non-linear feature vectors, electroencephalography (EEG) and event-related potentials (ERPs) are one of the promising applications. Furthermore, to the authors' best knowledge, there are very few papers that study deep neural networks for EEG/ERP data. The aim of the experiments subsequently presented was to verify if deep learning-based models can also perform well for single trial P300 classification with possible application to P300-based brain-computer interfaces. The P300 data used were recorded in the EEG/ERP laboratory at the Department of Computer Science and Engineering, University of West Bohemia, and are publicly available. Stacked autoencoders (SAEs) were implemented and compared with some of the currently most reliable state-of-the-art methods, such as LDA and multi-layer perceptron (MLP). The parameters of stacked autoencoders were optimized empirically. The layers were inserted one by one and at the end, the last layer was replaced by a supervised softmax classifier. Subsequently, fine-tuning using backpropagation was performed. The architecture of the neural network was 209-130-100-50-20-2. The classifiers were trained on a dataset merged from four subjects and subsequently tested on different 11 subjects without further training. The trained SAE achieved 69.2% accuracy that was higher (<i>p</i> &lt; 0.01) than the accuracy of MLP (64.9%) and LDA (65.9%). The recall of 58.8% was slightly higher when compared with MLP (56.2%) and LDA (58.4%). Therefore, SAEs could be preferable to other state-of-the-art classifiers for high-dimensional event-related potential feature vectors.</abstracttext></p></div></div>",,P300; brain-computer interfaces; deep learning; event-related potentials; machine learning; stacked autoencoders,https://www.ncbi.nlm.nih.gov//pubmed/28611579,pubmed,2017,23abfb08-33ec-4891-a731-469c064a60d3,1
improving eeg-based driver fatigue classification using sparse-deep belief networks,/pubmed/28326009,"Chai R, Ling SH, San PP, Naik GR, Nguyen TN, Tran Y, Craig A, Nguyen HT.",Front Neurosci. 2017 Mar 7;11:103. doi: 10.3389/fnins.2017.00103. eCollection 2017.,Front Neurosci.  2017,PubMed,citation,PMID:28326009 | PMCID:PMC5339284,pubmed,28326009,create date:2017/03/23 | first author:Chai R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>This paper presents an improvement of classification performance for electroencephalography (EEG)-based driver fatigue classification between fatigue and alert states with the data collected from 43 participants. The system employs autoregressive (AR) modeling as the features extraction algorithm, and sparse-deep belief networks (sparse-DBN) as the classification algorithm. Compared to other classifiers, sparse-DBN is a semi supervised learning method which combines unsupervised learning for modeling features in the pre-training layer and supervised learning for classification in the following layer. The sparsity in sparse-DBN is achieved with a regularization term that penalizes a deviation of the expected activation of hidden units from a fixed low-level prevents the network from overfitting and is able to learn low-level structures as well as high-level structures. For comparison, the artificial neural networks (ANN), Bayesian neural networks (BNN), and original deep belief networks (DBN) classifiers are used. The classification results show that using AR feature extractor and DBN classifiers, the classification performance achieves an improved classification performance with a of sensitivity of 90.8%, a specificity of 90.4%, an accuracy of 90.6%, and an area under the receiver operating curve (AUROC) of 0.94 compared to ANN (sensitivity at 80.8%, specificity at 77.8%, accuracy at 79.3% with AUC-ROC of 0.83) and BNN classifiers (sensitivity at 84.3%, specificity at 83%, accuracy at 83.6% with AUROC of 0.87). Using the sparse-DBN classifier, the classification performance improved further with sensitivity of 93.9%, a specificity of 92.3%, and an accuracy of 93.1% with AUROC of 0.96. Overall, the sparse-DBN classifier improved accuracy by 13.8, 9.5, and 2.5% over ANN, BNN, and DBN classifiers, respectively.</abstracttext></p></div></div>",,autoregressive model; deep belief networks; driver fatigue; electroencephalography; sparse-deep belief networks,https://www.ncbi.nlm.nih.gov//pubmed/28326009,pubmed,2017,1eef2d67-8395-45f9-b05d-b1bedb14f7d3,1
multi-modal patient cohort identification from eeg report and signal data,/pubmed/28269938,"Goodwin TR, Harabagiu SM.",AMIA Annu Symp Proc. 2017 Feb 10;2016:1794-1803. eCollection 2016.,AMIA Annu Symp Proc.  2016,PubMed,citation,PMID:28269938 | PMCID:PMC5333290,pubmed,28269938,create date:2017/03/09 | first author:Goodwin TR,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Clinical electroencephalography (EEG) is the most important investigation in the diagnosis and management of epilepsies. An EEG records the electrical activity along the scalp and measures spontaneous electrical activity of the brain. Because the EEG signal is complex, its interpretation is known to produce moderate inter-observer agreement among neurologists. This problem can be addressed by providing clinical experts with the ability to automatically retrieve similar EEG signals and EEG reports through a <b>patient cohort retrieval system</b> operating on a vast archive of EEG data. In this paper, we present a multi-modal EEG patient cohort retrieval system called MERCuRY which leverages the heterogeneous nature of EEG data by processing both the clinical narratives from EEG reports as well as the raw electrode potentials derived from the recorded EEG signal data. At the core of MERCuRY is a novel multimodal clinical indexing scheme which relies on EEG data representations obtained through deep learning. The index is used by two clinical relevance models that we have generated for identifying patient cohorts satisfying the inclusion and exclusion criteria expressed in natural language queries. Evaluations of the MERCuRY system measured the relevance of the patient cohorts, obtaining MAP scores of 69.87% and a NDCG of 83.21%.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28269938,pubmed,2016,a01b2f1d-201c-47da-9ceb-7afde5d27855,1
eeg-based driver fatigue detection using hybrid deep generic model,/pubmed/28268447,"Phyo Phyo San, Sai Ho Ling, Rifai Chai, Tran Y, Craig A, Hung Nguyen.",Conf Proc IEEE Eng Med Biol Soc. 2016 Aug;2016:800-803. doi: 10.1109/EMBC.2016.7590822.,Conf Proc IEEE Eng Med Biol Soc.  2016,PubMed,citation,PMID:28268447,pubmed,28268447,create date:2017/03/09 | first author:Phyo Phyo San,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Classification of electroencephalography (EEG)-based application is one of the important process for biomedical engineering. Driver fatigue is a major case of traffic accidents worldwide and considered as a significant problem in recent decades. In this paper, a hybrid deep generic model (DGM)-based support vector machine is proposed for accurate detection of driver fatigue. Traditionally, a probabilistic DGM with deep architecture is quite good at learning invariant features, but it is not always optimal for classification due to its trainable parameters are in the middle layer. Alternatively, Support Vector Machine (SVM) itself is unable to learn complicated invariance, but produces good decision surface when applied to well-behaved features. Consolidating unsupervised high-level feature extraction techniques, DGM and SVM classification makes the integrated framework stronger and enhance mutually in feature extraction and classification. The experimental results showed that the proposed DBN-based driver fatigue monitoring system achieves better testing accuracy of 73.29 % with 91.10 % sensitivity and 55.48 % specificity. In short, the proposed hybrid DGM-based SVM is an effective method for the detection of driver fatigue in EEG.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28268447,pubmed,2016,e3df9146-fdf4-4d23-94eb-fd7a155d73ad,1
deep learning and insomnia: assisting clinicians with their diagnosis,/pubmed/28092583,"Shahin M, Ahmed B, Tmar-Ben Hamida S, Mulaffer F, Glos M, Penzel T.",IEEE J Biomed Health Inform. 2017 Jan 9. doi: 10.1109/JBHI.2017.2650199. [Epub ahead of print],IEEE J Biomed Health Inform.  2017,PubMed,citation,PMID:28092583,pubmed,28092583,create date:2017/01/17 | first author:Shahin M,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Effective sleep analysis is hampered by the lack of automated tools catering for disordered sleep patterns and cumbersome monitoring hardware. In this paper, we apply deep learning on a set of 57 EEG features extracted from a maximum of two EEG channels to accurately differentiate between patients with insomnia or controls with no sleep complaints. We investigated two different approaches to achieve this. The first approach used EEG data from the whole sleep recording irrespective of the sleep stage (stage-independent classification), while the second used only EEG data from insomnia-impacted specific sleep stages (stage-dependent classification). We trained and tested our system using both healthy and disordered sleep collected from 41 controls and 42 primary insomnia patients. When compared with manual assessments, a NREM+REM based classifier had an overall discrimination accuracy of 92% and 86% between two groups using both two and one EEG channels respectively. These results demonstrate that deep learning can be used to assist in the diagnosis of sleep disorders such as insomnia.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28092583,pubmed,2017,5a5ffac2-567f-47b9-96b6-9b4996cf4263,1
multimodal neural network for rapid serial visual presentation brain computer interface,/pubmed/28066220,"Manor R, Mishali L, Geva AB.",Front Comput Neurosci. 2016 Dec 20;10:130. doi: 10.3389/fncom.2016.00130. eCollection 2016.,Front Comput Neurosci.  2016,PubMed,citation,PMID:28066220 | PMCID:PMC5168930,pubmed,28066220,create date:2017/01/10 | first author:Manor R,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Brain computer interfaces allow users to preform various tasks using only the electrical activity of the brain. BCI applications often present the user a set of stimuli and record the corresponding electrical response. The BCI algorithm will then have to decode the acquired brain response and perform the desired task. In rapid serial visual presentation (RSVP) tasks, the subject is presented with a continuous stream of images containing rare target images among standard images, while the algorithm has to detect brain activity associated with target images. In this work, we suggest a multimodal neural network for RSVP tasks. The network operates on the brain response and on the initiating stimulus simultaneously, providing more information for the BCI application. We present two variants of the multimodal network, a supervised model, for the case when the targets are known in advanced, and a semi-supervised model for when the targets are unknown. We test the neural networks with a RSVP experiment on satellite imagery carried out with two subjects. The multimodal networks achieve a significant performance improvement in classification metrics. We visualize what the networks has learned and discuss the advantages of using neural network models for BCI applications.</abstracttext></p></div></div>",,BCI; EEG; RSVP; computer vision; deep learning; neural network; single trial,https://www.ncbi.nlm.nih.gov//pubmed/28066220,pubmed,2016,3b2a07ad-9fc5-485b-a8ba-c428ab216886,1
the extraction of motion-onset vep bci features based on deep learning and compressed sensing,/pubmed/27845150,"Ma T, Li H, Yang H, Lv X, Li P, Liu T, Yao D, Xu P.",J Neurosci Methods. 2017 Jan 1;275:80-92. doi: 10.1016/j.jneumeth.2016.11.002. Epub 2016 Nov 11.,J Neurosci Methods.  2017,PubMed,citation,PMID:27845150,pubmed,27845150,create date:2016/11/16 | first author:Ma T,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>Motion-onset visual evoked potentials (mVEP) can provide a softer stimulus with reduced fatigue, and it has potential applications for brain computer interface(BCI)systems. However, the mVEP waveform is seriously masked in the strong background EEG activities, and an effective approach is needed to extract the corresponding mVEP features to perform task recognition for BCI control.</abstracttext></p><h4>NEW METHOD: </h4><p><abstracttext label='NEW METHOD' nlmcategory='UNASSIGNED'>In the current study, we combine deep learning with compressed sensing to mine discriminative mVEP information to improve the mVEP BCI performance.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The deep learning and compressed sensing approach can generate the multi-modality features which can effectively improve the BCI performance with approximately 3.5% accuracy incensement over all 11 subjects and is more effective for those subjects with relatively poor performance when using the conventional features.</abstracttext></p><h4>COMPARISON WITH EXISTING METHODS: </h4><p><abstracttext label='COMPARISON WITH EXISTING METHODS' nlmcategory='UNASSIGNED'>Compared with the conventional amplitude-based mVEP feature extraction approach, the deep learning and compressed sensing approach has a higher classification accuracy and is more effective for subjects with relatively poor performance.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>According to the results, the deep learning and compressed sensing approach is more effective for extracting the mVEP feature to construct the corresponding BCI system, and the proposed feature extraction framework is easy to extend to other types of BCIs, such as motor imagery (MI), steady-state visual evoked potential (SSVEP)and P300.</abstracttext></p><p class='copyright'>Copyright Â© 2016 Elsevier B.V. All rights reserved.</p></div></div>",dyao@uestc.edu.cn,Brain computer interface; Compressed sensing; Deep learning; Motion-onset VEP; Multi-modality feature,https://www.ncbi.nlm.nih.gov//pubmed/27845150,pubmed,2017,8602d924-f9fd-4057-b61b-67a3e7ca61da,1
interpretable deep neural networks for single-trial eeg classification,/pubmed/27746229,"Sturm I, Lapuschkin S, Samek W, Müller KR.",J Neurosci Methods. 2016 Dec 1;274:141-145. doi: 10.1016/j.jneumeth.2016.10.008. Epub 2016 Oct 13.,J Neurosci Methods.  2016,PubMed,citation,PMID:27746229,pubmed,27746229,create date:2016/11/05 | first author:Sturm I,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>BACKGROUND: </h4><p><abstracttext label='BACKGROUND' nlmcategory='BACKGROUND'>In cognitive neuroscience the potential of deep neural networks (DNNs) for solving complex classification tasks is yet to be fully exploited. The most limiting factor is that DNNs as notorious 'black boxes' do not provide insight into neurophysiological phenomena underlying a decision. Layer-wise relevance propagation (LRP) has been introduced as a novel method to explain individual network decisions.</abstracttext></p><h4>NEW METHOD: </h4><p><abstracttext label='NEW METHOD' nlmcategory='UNASSIGNED'>We propose the application of DNNs with LRP for the first time for EEG data analysis. Through LRP the single-trial DNN decisions are transformed into heatmaps indicating each data point's relevance for the outcome of the decision.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>DNN achieves classification accuracies comparable to those of CSP-LDA. In subjects with low performance subject-to-subject transfer of trained DNNs can improve the results. The single-trial LRP heatmaps reveal neurophysiologically plausible patterns, resembling CSP-derived scalp maps. Critically, while CSP patterns represent class-wise aggregated information, LRP heatmaps pinpoint neural patterns to single time points in single trials.</abstracttext></p><h4>COMPARISON WITH EXISTING METHOD(S): </h4><p><abstracttext label='COMPARISON WITH EXISTING METHOD(S)' nlmcategory='UNASSIGNED'>We compare the classification performance of DNNs to that of linear CSP-LDA on two data sets related to motor-imaginary BCI.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>We have demonstrated that DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of high-resolution assessment of neural activity can be reached. LRP is a potential remedy for the lack of interpretability of DNNs that has limited their utility in neuroscientific applications. The extreme specificity of the LRP-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes.</abstracttext></p><p class='copyright'>Copyright © 2016 Elsevier B.V. All rights reserved.</p></div></div>",irene.sturm@tu-berlin.de,Brain–computer interfacing; Interpretability; Neural networks,https://www.ncbi.nlm.nih.gov//pubmed/27746229,pubmed,2016,59d63b98-9611-4c03-a168-dcf8d40289da,1
a deep learning scheme for motor imagery classification based on restricted boltzmann machines,/pubmed/27542114,"Lu N, Li T, Ren X, Miao H.",IEEE Trans Neural Syst Rehabil Eng. 2017 Jun;25(6):566-576. doi: 10.1109/TNSRE.2016.2601240. Epub 2016 Aug 17.,IEEE Trans Neural Syst Rehabil Eng.  2017,PubMed,citation,PMID:27542114,pubmed,27542114,create date:2016/08/20 | first author:Lu N,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Motor imagery classification is an important topic in brain-computer interface (BCI) research that enables the recognition of a subject's intension to, e.g., implement prosthesis control. The brain dynamics of motor imagery are usually measured by electroencephalography (EEG) as nonstationary time series of low signal-to-noise ratio. Although a variety of methods have been previously developed to learn EEG signal features, the deep learning idea has rarely been explored to generate new representation of EEG features and achieve further performance improvement for motor imagery classification. In this study, a novel deep learning scheme based on restricted Boltzmann machine (RBM) is proposed. Specifically, frequency domain representations of EEG signals obtained via fast Fourier transform (FFT) and wavelet package decomposition (WPD) are obtained to train three RBMs. These RBMs are then stacked up with an extra output layer to form a four-layer neural network, which is named the frequential deep belief network (FDBN). The output layer employs the softmax regression to accomplish the classification task. Also, the conjugate gradient method and backpropagation are used to fine tune the FDBN. Extensive and systematic experiments have been performed on public benchmark datasets, and the results show that the performance improvement of FDBN over other selected state-of-the-art methods is statistically significant. Also, several findings that may be of significant interest to the BCI community are presented in this article.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27542114,pubmed,2017,5489a542-6d94-4231-9c2d-1c3df41a0ee0,1
deep learning representation from electroencephalography of early-stage creutzfeldt-jakob disease and features for differentiation from rapidly progressive dementia,/pubmed/27440465,"Morabito FC, Campolo M, Mammone N, Versaci M, Franceschetti S, Tagliavini F, Sofia V, Fatuzzo D, Gambardella A, Labate A, Mumoli L, Tripodi GG, Gasparini S, Cianci V, Sueri C, Ferlazzo E, Aguglia U.",Int J Neural Syst. 2017 Mar;27(2):1650039. doi: 10.1142/S0129065716500398. Epub 2016 May 3.,Int J Neural Syst.  2017,PubMed,citation,PMID:27440465,pubmed,27440465,create date:2016/07/22 | first author:Morabito FC,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>A novel technique of quantitative EEG for differentiating patients with early-stage Creutzfeldt-Jakob disease (CJD) from other forms of rapidly progressive dementia (RPD) is proposed. The discrimination is based on the extraction of suitable features from the time-frequency representation of the EEG signals through continuous wavelet transform (CWT). An average measure of complexity of the EEG signal obtained by permutation entropy (PE) is also included. The dimensionality of the feature space is reduced through a multilayer processing system based on the recently emerged deep learning (DL) concept. The DL processor includes a stacked auto-encoder, trained by unsupervised learning techniques, and a classifier whose parameters are determined in a supervised way by associating the known category labels to the reduced vector of high-level features generated by the previous processing blocks. The supervised learning step is carried out by using either support vector machines (SVM) or multilayer neural networks (MLP-NN). A subset of EEG from patients suffering from Alzheimer's Disease (AD) and healthy controls (HC) is considered for differentiating CJD patients. When fine-tuning the parameters of the global processing system by a supervised learning procedure, the proposed system is able to achieve an average accuracy of 89%, an average sensitivity of 92%, and an average specificity of 89% in differentiating CJD from RPD. Similar results are obtained for CJD versus AD and CJD versus HC.</abstracttext></p></div></div>",,Alzheimer’s disease; CJD; EEG; SVM; classification; continuous wavelet transform; deep learning; dementia; subacute encephalopathies,https://www.ncbi.nlm.nih.gov//pubmed/27440465,pubmed,2017,b606b2a2-ddba-4149-9c00-4157e0eb6efb,1
investigating deep learning for fnirs based bci,/pubmed/26736884,"Hennrich J, Herff C, Heger D, Schultz T.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:2844-7. doi: 10.1109/EMBC.2015.7318984.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736884,pubmed,26736884,create date:2016/01/07 | first author:Hennrich J,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736884,pubmed,2015,6e61b9f6-2b51-4e5f-99e3-ac81f3b0f290,1
sleep spindle detection using deep learning: a validation study based on crowdsourcing,/pubmed/26736880,"Dakun Tan, Rui Zhao, Jinbo Sun, Wei Qin.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:2828-31. doi: 10.1109/EMBC.2015.7318980.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736880,pubmed,26736880,create date:2016/01/07 | first author:Dakun Tan,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Sleep spindles are significant transient oscillations observed on the electroencephalogram (EEG) in stage 2 of non-rapid eye movement sleep. Deep belief network (DBN) gaining great successes in images and speech is still a novel method to develop sleep spindle detection system. In this paper, crowdsourcing replacing gold standard was applied to generate three different labeled samples and constructed three classes of datasets with a combination of these samples. An F1-score measure was estimated to compare the performance of DBN to other three classifiers on classifying these samples, with the DBN obtaining an result of 92.78%. Then a comparison of two feature extraction methods based on power spectrum density was made on same dataset using DBN. In addition, the DBN trained in dataset was applied to detect sleep spindle from raw EEG recordings and performed a comparable capacity to expert group consensus. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736880,pubmed,2015,0553910f-de67-494a-85b7-a7044328547e,1
automatic sleep stage scoring using time-frequency analysis and stacked sparse autoencoders,/pubmed/26464268,"Tsinalis O, Matthews PM, Guo Y.",Ann Biomed Eng. 2016 May;44(5):1587-97. doi: 10.1007/s10439-015-1444-y. Epub 2015 Oct 13.,Ann Biomed Eng.  2016,PubMed,citation,PMID:26464268 | PMCID:PMC4837220,pubmed,26464268,create date:2015/10/16 | first author:Tsinalis O,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>We developed a machine learning methodology for automatic sleep stage scoring. Our time-frequency analysis-based feature extraction is fine-tuned to capture sleep stage-specific signal features as described in the American Academy of Sleep Medicine manual that the human experts follow. We used ensemble learning with an ensemble of stacked sparse autoencoders for classifying the sleep stages. We used class-balanced random sampling across sleep stages for each model in the ensemble to avoid skewed performance in favor of the most represented sleep stages, and addressed the problem of misclassification errors due to class imbalance while significantly improving worst-stage classification. We used an openly available dataset from 20 healthy young adults for evaluation. We used a single channel of EEG from this dataset, which makes our method a suitable candidate for longitudinal monitoring using wearable EEG in real-world settings. Our method has both high overall accuracy (78%, range 75-80%), and high mean [Formula: see text]-score (84%, range 82-86%) and mean accuracy across individual sleep stages (86%, range 84-88%) over all subjects. The performance of our method appears to be uncorrelated with the sleep efficiency and percentage of transitional epochs in each recording. </abstracttext></p></div></div>",y.guo@imperial.ac.uk,Deep learning; EEG; Electroencephalography; Ensemble learning,https://www.ncbi.nlm.nih.gov//pubmed/26464268,pubmed,2016,a4a4211d-50d8-43e0-b68a-49581888a3eb,1
feature extraction with stacked autoencoders for epileptic seizure detection,/pubmed/25570914,"Supratak A, Ling Li, Yike Guo.",Conf Proc IEEE Eng Med Biol Soc. 2014;2014:4184-7. doi: 10.1109/EMBC.2014.6944546.,Conf Proc IEEE Eng Med Biol Soc.  2014,PubMed,citation,PMID:25570914,pubmed,25570914,create date:2015/01/09 | first author:Supratak A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Scalp electroencephalogram (EEG), a recording of the brain's electrical activity, has been used to diagnose and detect epileptic seizures for a long time. However, most researchers have implemented seizure detectors by manually hand-engineering features from observed EEG data, and used them in seizure detection, which might not scale well to new patterns of seizures. In this paper, we investigate the possibility of utilising unsupervised feature learning, the recent development of deep learning, to automatically learn features from raw, unlabelled EEG data that are representative enough to be used in seizure detection. We develop patient-specific seizure detectors by using stacked autoencoders and logistic classifiers. A two-step training consisting of the greedy layer-wise and the global fine-tuning was used to train our detectors. The evaluation was performed by using labelled dataset from the CHB-MIT database, and the results showed that all of the test seizures were detected with a mean latency of 3.36 seconds, and a low false detection rate. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25570914,pubmed,2014,b908ecb0-cddd-4b20-b1a1-9a9db1c25f09,1
joint optimization of algorithmic suites for eeg analysis,/pubmed/25570621,"Santana E, Brockmeier AJ, Principe JC.",Conf Proc IEEE Eng Med Biol Soc. 2014;2014:2997-3000. doi: 10.1109/EMBC.2014.6944253.,Conf Proc IEEE Eng Med Biol Soc.  2014,PubMed,citation,PMID:25570621,pubmed,25570621,create date:2015/01/09 | first author:Santana E,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Electroencephalogram (EEG) data analysis algorithms consist of multiple processing steps each with a number of free parameters. A joint optimization methodology can be used as a wrapper to fine-tune these parameters for the patient or application. This approach is inspired by deep learning neural network models, but differs because the processing layers for EEG are heterogeneous with different approaches used for processing space and time. Nonetheless, we treat the processing stages as a neural network and apply backpropagation to jointly optimize the parameters. This approach outperforms previous results on the BCI Competition II - dataset IV; additionally, it outperforms the common spatial patterns (CSP) algorithm on the BCI Competition III dataset IV. In addition, the optimized parameters in the architecture are still interpretable. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25570621,pubmed,2014,cb5bd8d5-69b0-4e0a-9c06-8784b8e1b961,1
eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation,/pubmed/25258728,"Jirayucharoensak S, Pan-Ngum S, Israsena P.",ScientificWorldJournal. 2014;2014:627892. doi: 10.1155/2014/627892. Epub 2014 Sep 1.,ScientificWorldJournal.  2014,PubMed,citation,PMID:25258728 | PMCID:PMC4165739,pubmed,25258728,create date:2014/09/27 | first author:Jirayucharoensak S,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Automatic emotion recognition is one of the most challenging tasks. To detect emotion from nonstationary EEG signals, a sophisticated learning algorithm that can represent high-level abstraction is required. This study proposes the utilization of a deep learning network (DLN) to discover unknown feature correlation between input signals that is crucial for the learning task. The DLN is implemented with a stacked autoencoder (SAE) using hierarchical feature learning approach. Input features of the network are power spectral densities of 32-channel EEG signals from 32 subjects. To alleviate overfitting problem, principal component analysis (PCA) is applied to extract the most important components of initial input features. Furthermore, covariate shift adaptation of the principal components is implemented to minimize the nonstationary effect of EEG signals. Experimental results show that the DLN is capable of classifying three different levels of valence and arousal with accuracy of 49.52% and 46.03%, respectively. Principal component based covariate shift adaptation enhances the respective classification accuracy by 5.55% and 6.53%. Moreover, DLN provides better performance compared to SVM and naive Bayes classifiers. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/25258728,pubmed,2014,4a818743-0e43-445c-8970-3fcfcc65cb9a,1
"accuracy of deep learning, a machine-learning technology, using ultra-wide-field fundus ophthalmoscopy for detecting rhegmatogenous retinal detachment",/pubmed/28842613,"Ohsugi H, Tabuchi H, Enno H, Ishitobi N.",Sci Rep. 2017 Aug 25;7(1):9425. doi: 10.1038/s41598-017-09891-x.,Sci Rep.  2017,PubMed,citation,PMID:28842613 | PMCID:PMC5573327,pubmed,28842613,create date:2017/08/27 | first author:Ohsugi H,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Rhegmatogenous retinal detachment (RRD) is a serious condition that can lead to blindness; however, it is highly treatable with timely and appropriate treatment. Thus, early diagnosis and treatment of RRD is crucial. In this study, we applied deep learning, a machine-learning technology, to detect RRD using ultra-wide-field fundus images and investigated its performance. In total, 411 images (329 for training and 82 for grading) from 407 RRD patients and 420 images (336 for training and 84 for grading) from 238 non-RRD patients were used in this study. The deep learning model demonstrated a high sensitivity of 97.6% [95% confidence interval (CI), 94.2-100%] and a high specificity of 96.5% (95% CI, 90.2-100%), and the area under the curve was 0.988 (95% CI, 0.981-0.995). This model can improve medical care in remote areas where eye clinics are not available by using ultra-wide-field fundus ophthalmoscopy for the accurate diagnosis of RRD. Early diagnosis of RRD can prevent blindness.</abstracttext></p></div></div>",h.ohsugi@tsukazaki-eye.net,,https://www.ncbi.nlm.nih.gov//pubmed/28842613,pubmed,2017,1143f453-5e16-4ed0-96ea-aad150e1a308,1
automatic recognition of severity level for diagnosis of diabetic retinopathy using deep visual features,/pubmed/28353133,"Abbas Q, Fondon I, Sarmiento A, Jiménez S, Alemany P.",Med Biol Eng Comput. 2017 Mar 28. doi: 10.1007/s11517-017-1638-6. [Epub ahead of print],Med Biol Eng Comput.  2017,PubMed,citation,PMID:28353133,pubmed,28353133,create date:2017/03/30 | first author:Abbas Q,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Diabetic retinopathy (DR) is leading cause of blindness among diabetic patients. Recognition of severity level is required by ophthalmologists to early detect and diagnose the DR. However, it is a challenging task for both medical experts and computer-aided diagnosis systems due to requiring extensive domain expert knowledge. In this article, a novel automatic recognition system for the five severity level of diabetic retinopathy (SLDR) is developed without performing any pre- and post-processing steps on retinal fundus images through learning of deep visual features (DVFs). These DVF features are extracted from each image by using color dense in scale-invariant and gradient location-orientation histogram techniques. To learn these DVF features, a semi-supervised multilayer deep-learning algorithm is utilized along with a new compressed layer and fine-tuning steps. This SLDR system was evaluated and compared with state-of-the-art techniques using the measures of sensitivity (SE), specificity (SP) and area under the receiving operating curves (AUC). On 750 fundus images (150 per category), the SE of 92.18%, SP of 94.50% and AUC of 0.924 values were obtained on average. These results demonstrate that the SLDR system is appropriate for early detection of DR and provide an effective treatment for prediction type of diabetes.</abstracttext></p></div></div>",qaisarabbasphd@gmail.com,Color dense SIFT features; Computer-aided diagnosis; Deep learning; Deep visual feature; Diabetic retinopathy; Gradient location-orientation histogram; Retinal fundus images,https://www.ncbi.nlm.nih.gov//pubmed/28353133,pubmed,2017,57dd2580-23b2-446c-861c-4744d8f577e9,1
deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images,/pubmed/26736930,"Maji D, Santara A, Ghosh S, Sheet D, Mitra P.",Conf Proc IEEE Eng Med Biol Soc. 2015 Aug;2015:3029-32. doi: 10.1109/EMBC.2015.7319030.,Conf Proc IEEE Eng Med Biol Soc.  2015,PubMed,citation,PMID:26736930,pubmed,26736930,create date:2016/01/07 | first author:Maji D,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195. </abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/26736930,pubmed,2015,0c089f23-ccba-4466-95ba-a351a95afa3f,1
protein function prediction using deep restricted boltzmann machines,/pubmed/28744460,"Zou X, Wang G, Yu G.",Biomed Res Int. 2017;2017:1729301. doi: 10.1155/2017/1729301. Epub 2017 Jun 28.,Biomed Res Int.  2017,PubMed,citation,PMID:28744460 | PMCID:PMC5506480,pubmed,28744460,create date:2017/07/27 | first author:Zou X,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Accurately annotating biological functions of proteins is one of the key tasks in the postgenome era. Many machine learning based methods have been applied to predict functional annotations of proteins, but this task is rarely solved by deep learning techniques. Deep learning techniques recently have been successfully applied to a wide range of problems, such as video, images, and nature language processing. Inspired by these successful applications, we investigate deep restricted Boltzmann machines (DRBM), a representative deep learning technique, to predict the missing functional annotations of partially annotated proteins. Experimental results on <i>Homo sapiens</i>, <i>Saccharomyces cerevisiae</i>, <i>Mus musculus,</i> and <i>Drosophila</i> show that DRBM achieves better performance than other related methods across different evaluation metrics, and it also runs faster than these comparing methods.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28744460,pubmed,2017,82a2ed26-7976-4c84-88e6-8d1dcc14e914,1
deep monocular 3d reconstruction for assisted navigation in bronchoscopy,/pubmed/28508345,"Visentini-Scarzanella M, Sugiura T, Kaneko T, Koto S.",Int J Comput Assist Radiol Surg. 2017 Jul;12(7):1089-1099. doi: 10.1007/s11548-017-1609-2. Epub 2017 May 15.,Int J Comput Assist Radiol Surg.  2017,PubMed,citation,PMID:28508345,pubmed,28508345,create date:2017/05/17 | first author:Visentini-Scarzanella M,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>In bronchoschopy, computer vision systems for navigation assistance are an attractive low-cost solution to guide the endoscopist to target peripheral lesions for biopsy and histological analysis. We propose a decoupled deep learning architecture that projects input frames onto the domain of CT renderings, thus allowing offline training from patient-specific CT data.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>A fully convolutional network architecture is implemented on GPU and tested on a phantom dataset involving 32 video sequences and [Formula: see text]60k frames with aligned ground truth and renderings, which is made available as the first public dataset for bronchoscopy navigation.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>An average estimated depth accuracy of 1.5 mm was obtained, outperforming conventional direct depth estimation from input frames by 60%, and with a computational time of [Formula: see text]30 ms on modern GPUs. Qualitatively, the estimated depth and renderings closely resemble the ground truth.</abstracttext></p><h4>CONCLUSIONS: </h4><p><abstracttext label='CONCLUSIONS' nlmcategory='CONCLUSIONS'>The proposed method shows a novel architecture to perform real-time monocular depth estimation without losing patient specificity in bronchoscopy. Future work will include integration within SLAM systems and collection of in vivo datasets.</abstracttext></p></div></div>",marco.visentiniscarzanella@gmail.com,3D reconstruction; Assisted navigation; Bronchoscopy; Deep learning,https://www.ncbi.nlm.nih.gov//pubmed/28508345,pubmed,2017,afb0c5a5-f6f1-4206-ad1b-230c3440fdea,1
deep pain: exploiting long short-term memory networks for facial expression classification,/pubmed/28207407,"Rodriguez P, Cucurull G, Gonalez J, Gonfaus JM, Nasrollahi K, Moeslund TB, Roca FX.",IEEE Trans Cybern. 2017 Feb 9. doi: 10.1109/TCYB.2017.2662199. [Epub ahead of print],IEEE Trans Cybern.  2017,PubMed,citation,PMID:28207407,pubmed,28207407,create date:2017/02/17 | first author:Rodriguez P,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively, there is the need for automatic systems to measure it. In this paper, contrary to current state-of-the-art techniques in pain assessment, which are based on facial features only, we suggest that the performance can be enhanced by feeding the raw frames to deep learning models, outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline, our approach first uses convolutional neural networks (CNNs) to learn facial features from VGG_Faces, which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result, we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition, to evaluate the generalization properties of our proposed methodology on facial motion recognition, we also report competitive results in the Cohn Kanade+ facial expression database.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/28207407,pubmed,2017,da67ed5e-86e8-44f1-a2d6-3bad4c93817d,1
deep learning for polyp recognition in wireless capsule endoscopy images,/pubmed/28160514,"Yuan Y, Meng MQ.",Med Phys. 2017 Apr;44(4):1379-1389. doi: 10.1002/mp.12147.,Med Phys.  2017,PubMed,citation,PMID:28160514,pubmed,28160514,create date:2017/02/06 | first author:Yuan Y,"<div class='abstr'><h3>Abstract</h3><div class=''><h4>PURPOSE: </h4><p><abstracttext label='PURPOSE' nlmcategory='OBJECTIVE'>Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.</abstracttext></p><h4>METHODS: </h4><p><abstracttext label='METHODS' nlmcategory='METHODS'>We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.</abstracttext></p><h4>RESULTS: </h4><p><abstracttext label='RESULTS' nlmcategory='RESULTS'>The average overall recognition accuracy (ORA) of our method for WCE images is 98.00%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00%, 99.50%, 99.00%, and 95.50%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.</abstracttext></p><h4>CONCLUSION: </h4><p><abstracttext label='CONCLUSION' nlmcategory='CONCLUSIONS'>The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work.</abstracttext></p><p class='copyright'>© 2017 American Association of Physicists in Medicine.</p></div></div>",,image manifold information; polyp recognition; stacked sparse autoencoder with image manifold (SSAEIM); wireless capsule endoscopy images,https://www.ncbi.nlm.nih.gov//pubmed/28160514,pubmed,2017,c2d3dbac-9f73-4cc3-b47d-35c24f7c377f,1
robust individual-cell/object tracking via pcanet deep network in biomedicine and computer vision,/pubmed/27689090,"Zhong B, Pan S, Wang C, Wang T, Du J, Chen D, Cao L.",Biomed Res Int. 2016;2016:8182416. Epub 2016 Aug 25.,Biomed Res Int.  2016,PubMed,citation,PMID:27689090 | PMCID:PMC5015430,pubmed,27689090,create date:2016/10/01 | first author:Zhong B,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Tracking individual-cell/object over time is important in understanding drug treatment effects on cancer cells and video surveillance. A fundamental problem of individual-cell/object tracking is to simultaneously address the cell/object appearance variations caused by intrinsic and extrinsic factors. In this paper, inspired by the architecture of deep learning, we propose a robust feature learning method for constructing discriminative appearance models without large-scale pretraining. Specifically, in the initial frames, an unsupervised method is firstly used to learn the abstract feature of a target by exploiting both classic principal component analysis (PCA) algorithms with recent deep learning representation architectures. We use learned PCA eigenvectors as filters and develop a novel algorithm to represent a target by composing of a PCA-based filter bank layer, a nonlinear layer, and a patch-based pooling layer, respectively. Then, based on the feature representation, a neural network with one hidden layer is trained in a supervised mode to construct a discriminative appearance model. Finally, to alleviate the tracker drifting problem, a sample update scheme is carefully designed to keep track of the most representative and diverse samples during tracking. We test the proposed tracking method on two standard individual cell/object tracking benchmarks to show our tracker's state-of-the-art performance.</abstracttext></p></div></div>",,,https://www.ncbi.nlm.nih.gov//pubmed/27689090,pubmed,2016,d5f2559e-3963-4e45-bd0a-4008e22bfb9c,1
drugan: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico,/pubmed/28703000,"Kadurin A, Nikolenko S, Khrabrov K, Aliper A, Zhavoronkov A.",Mol Pharm. 2017 Sep 5;14(9):3098-3104. doi: 10.1021/acs.molpharmaceut.7b00346. Epub 2017 Aug 4.,Mol Pharm.  2017,PubMed,citation,PMID:28703000,pubmed,28703000,create date:2017/07/14 | first author:Kadurin A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Deep generative adversarial networks (GANs) are the emerging technology in drug discovery and biomarker development. In our recent work, we demonstrated a proof-of-concept of implementing deep generative adversarial autoencoder (AAE) to identify new molecular fingerprints with predefined anticancer properties. Another popular generative model is the variational autoencoder (VAE), which is based on deep neural architectures. In this work, we developed an advanced AAE model for molecular feature extraction problems, and demonstrated its advantages compared to VAE in terms of (a) adjustability in generating molecular fingerprints; (b) capacity of processing very large molecular data sets; and (c) efficiency in unsupervised pretraining for regression model. Our results suggest that the proposed AAE model significantly enhances the capacity and efficiency of development of the new molecules with specific anticancer properties using the deep generative models.</abstracttext></p></div></div>",,adversarial autoencoder; deep learning; drug discovery; generative adversarial network; variational autoencoder,https://www.ncbi.nlm.nih.gov//pubmed/28703000,pubmed,2017,9b12ccd9-e136-4af6-8fa3-6294b9d85451,1
the cornucopia of meaningful leads: applying deep adversarial autoencoders for new molecule development in oncology,/pubmed/28029644,"Kadurin A, Aliper A, Kazennov A, Mamoshina P, Vanhaelen Q, Khrabrov K, Zhavoronkov A.",Oncotarget. 2017 Feb 14;8(7):10883-10890. doi: 10.18632/oncotarget.14073.,Oncotarget.  2017,PubMed,citation,PMID:28029644 | PMCID:PMC5355231,pubmed,28029644,create date:2016/12/29 | first author:Kadurin A,"<div class='abstr'><h3>Abstract</h3><div class=''><p><abstracttext>Recent advances in deep learning and specifically in generative adversarial networks have demonstrated surprising results in generating new images and videos upon request even using natural language as input. In this paper we present the first application of generative adversarial autoencoders (AAE) for generating novel molecular fingerprints with a defined set of parameters. We developed a 7-layer AAE architecture with the latent middle layer serving as a discriminator. As an input and output the AAE uses a vector of binary fingerprints and concentration of the molecule. In the latent layer we also introduced a neuron responsible for growth inhibition percentage, which when negative indicates the reduction in the number of tumor cells after the treatment. To train the AAE we used the NCI-60 cell line assay data for 6252 compounds profiled on MCF-7 cell line. The output of the AAE was used to screen 72 million compounds in PubChem and select candidate molecules with potential anti-cancer properties. This approach is a proof of concept of an artificially-intelligent drug discovery engine, where AAEs are used to generate new molecular fingerprints with the desired molecular properties.</abstracttext></p></div></div>",,adversarial autoencoder; artificial intelligence; deep learning; drug discovery; generative adversarian networks,https://www.ncbi.nlm.nih.gov//pubmed/28029644,pubmed,2017,1e115bfa-3c22-4da7-8789-be77325ee065,1
