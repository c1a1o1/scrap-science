"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3Ddeep+learning+microscopy",2017/09/18 11:18:09
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction","R. Li; T. Zeng; H. Peng; S. Ji","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1533","1541","Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.","0278-0062;02780062","","10.1109/TMI.2017.2679713","10.13039/100000001 - National Science Foundation; 10.13039/100007588 - Washington State University; 10.13039/100009980 - Old Dominion University; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874113","BigNeuron;Deep learning;image denoising;image segmentation;neuron reconstruction","Convolution;Image reconstruction;Image segmentation;Microscopy;Morphology;Neurons;Three-dimensional displays","biomedical optical imaging;brain;image denoising;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optical microscopy","3-D convolutional neural networks;3-D microscopy images;3-D neuron reconstruction;3-D neuron structure;CNN architecture;brain anatomy;brain wiring;deep learning segmentation;digital reconstruction;digital tracing;discontinued segments;image segmentation methods;neurite patterns;neuronal microscopy images;neuronal voxels;noise removal;optical microscopy images;organisms;preprocessing step;reconstruction algorithms;reversing engineering;tracing performance;volumetric images;voxel-wise segmentation maps","","","","","","","20170308","July 2017","","IEEE","IEEE Journals & Magazines"
"Hybrid deep learning for Reflectance Confocal Microscopy skin images","P. Kaur; K. J. Dana; G. O. Cula; M. C. Mack","Department of Electrical and Computer Engineering, Rutgers University, NJ, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1466","1471","Reflectance Confocal Microscopy (RCM) is used for evaluation of human skin disorders and the effects of skin treatments by imaging the skin layers at different depths. Traditionally, clinical experts manually categorize the images captured into different skin layers. This time-consuming labeling task impedes the convenient analysis of skin image datasets. In recent automated image recognition tasks, deep learning with convolutional neural nets (CNN) has achieved remarkable results. However in many clinical settings, training data is often limited and insufficient for CNN training. For recognition of RCM skin images, we demonstrate that a CNN trained on a moderate size dataset leads to low accuracy. We introduce a hybrid deep learning approach which uses traditional texton-based feature vectors as input to train a deep neural network. This hybrid method uses fixed filters in the input layer instead of tuned filters, yet superior performance is achieved. Our dataset consists of 1500 images from 15 RCM stacks belonging to six different categories of skin layers. We show that our hybrid deep learning approach performs with a test accuracy of 82% compared with 51% for CNN. We also compare the results with additional proposed methods for RCM image recognition and show improved accuracy.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899844","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899844","","Epidermis;Histograms;Image recognition;Libraries;Machine learning;Neural networks","convolution;data analysis;filtering theory;image recognition;learning (artificial intelligence);medical image processing;microscopy;vectors;visual databases","CNN training;RCM;automated image recognition tasks;clinical experts;convolutional neural nets;fixed filters;human skin disorders;hybrid deep learning approach;reflectance confocal microscopy skin images;skin image datasets;skin layers;traditional texton-based feature vectors","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep learning for automatic cell detection in wide-field microscopy zebrafish images","B. Dong; L. Shao; M. Da Costa; O. Bandmann; A. F. Frangi","Centre of Computational Imaging & Simulation Technologies in Biomedicine (CISTIB)","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","772","776","The zebrafish has become a popular experimental model organism for biomedical research. In this paper, a unique framework is proposed for automatically detecting Tyrosine Hydroxylase-containing (TH-labeled) cells in larval zebrafish brain z-stack images recorded through the wide-field microscope. In this framework, a supervised max-pooling Convolutional Neural Network (CNN) is trained to detect cell pixels in regions that are preselected by a Support Vector Machine (SVM) classifier. The results show that the proposed deep-learned method outperforms hand-crafted techniques and demonstrate its potential for automatic cell detection in wide-field microscopy z-stack zebrafish images.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163986","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163986","","Computer architecture;Histograms;Microprocessors;Microscopy;Neurons;Three-dimensional displays;Training","biomedical optical imaging;brain;cellular biophysics;convolution;enzymes;feature extraction;image classification;learning (artificial intelligence);medical image processing;molecular biophysics;neural nets;neurophysiology;optical microscopy;support vector machines","SVM classifier;automatic TH-labeled cell detection;automatic tyrosine hydroxylase-containing cell detection;biomedical research;cell pixel detection;convolutional neural network;deep learning;experimental model organism;hand-crafted technique;larval zebrafish brain z-stack image recording;region preselection;supervised max-pooling CNN training;support vector machine;wide-field microscopy","","3","","26","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Human induced pluripotent stem cell region recognition in microscopy images using Convolutional Neural Networks","Y. H. Chang; K. Abe; H. Yokota; K. Sudo; Y. Nakamura; C. Y. Lin; M. D. Tsai","Department of Information and Computer Engineering, Chung-Yuan Christian University, Chung-Li, 32023, Taiwan","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","4058","4061","We present a deep learning architecture Convolutional Neural Networks (CNNs) for automatic classification and recognition of reprogramming and reprogrammed human Induced Pluripotent Stem (iPS) cell regions in microscopy images. The differentiated cells that possibly undergo reprogramming to iPS cells can be detected by this method for screening reagents or culture conditions in iPS induction. The learning results demonstrate that our CNNs can achieve the Top-1 and Top-2 error rates of 9.2% and 0.84%, respectively, to produce probability maps for the automatic analysis. The implementation results show that this automatic method can successfully detect and localize the human iPS cell formation, thereby yield a potential tool for helping iPS cell culture.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8037747","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037747","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images","Y. T. Su; Y. Lu; M. Chen; A. A. Liu","School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China.","IEEE Access","","2017","PP","99","1","1","We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a Convolutional Neural Network (CNN) and a Long Short Term Memory network (LSTM) to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data we validate the proposed method on include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 dataset, and the F score of 96.5% on the C3H10 dataset. The results on both datasets outperform traditional graph model based approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of CNN-LSTM network.","","","10.1109/ACCESS.2017.2745544","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019789","biomedical imaging;computer vision;machine learning;mitosis detection;stem cell","Computer architecture;Feature extraction;Hidden Markov models;Machine learning;Microscopy;Spatiotemporal phenomena;Visualization","","","","","","","","","20170829","","","IEEE","IEEE Early Access Articles"
"An efficient method for neuronal tracking in electron microscopy images","L. Yin; C. Xiao; Q. Xie; X. Chen; L. Shen; H. Han","Computational Mathematics, Hubei University, Wuhan, 430062, China","2017 IEEE International Conference on Mechatronics and Automation (ICMA)","20170824","2017","","","1865","1870","With the introduction of deep learning, a wave of artificial intelligence research has been set off again. Scientists focus on brain-inspired intelligence, namely, try to get inspiration from the brain nervous system and cognitive behavior mechanism, to develop intelligent computing models as well as algorithms with stronger information representation, processing and learning ability. So, the study of neurons and the connections between neurons of brain are needed. One major obstacle of reconstruction lies in segmenting and tracking neuronal processes. Electron microscopy is producing neurons images rapidly. In response, we propose an efficient method for neuronal tracking in electron microscopy images to help scientists reconstruct complex neurons. First, we track neurons by kernelized correlation filter to get candidate neuron; then we calculate overlap area and distance of the contours between two consecutive images to get final neuron. We evaluate the performance of our method on a public electron microscopy dataset. The method is superior in accuracy and efficiency.","","CD:978-1-5090-6757-2; Electronic:978-1-5090-6759-6; POD:978-1-5090-6760-2; Paper:978-1-5090-6758-9","10.1109/ICMA.2017.8016102","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016102","3D reconstruction;correlation filters;electron microscopy;neuron;tracking","Correlation;Electron microscopy;Image segmentation;Neurons;Target tracking","","","","","","","","","","6-9 Aug. 2017","","IEEE","IEEE Conference Publications"
"MIMO-Net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images","S. E. A. Raza; L. Cheung; D. Epstein; S. Pelengaris; M. Khan; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","337","340","We propose a novel multiple-input multiple-output convolution neural network (MIMO-Net) for cell segmentation in fluorescence microscopy images. The proposed network trains the network parameters using multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The MIMO-Net allows us to deal with variable intensity cell boundaries and highly variable cell size in the mouse pancreatic tissue by adding extra convolutional layers which bypass the max-pooling operation. The results show that our method outperforms state-of-the-art deep learning based approaches for segmentation.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950532","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950532","Cell Segmentation;Deep Learning;Fluorescence Microscopy","Biomembranes;Computer architecture;Convolution;Image segmentation;Machine learning;Microprocessors;Microscopy","biomedical optical imaging;fluorescence;image resolution;image segmentation;medical image processing;neural nets;optical microscopy","cell segmentation;deep learning based approaches;fluorescence microscopy images;mouse pancreatic tissue;multipleinput multipleoutput convolution neural network;multiresolution deconvolution filters;variable intensity cell boundaries","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep convolutional neural networks for detecting secondary structures in protein density maps from cryo-electron microscopy","R. Li; D. Si; T. Zeng; S. Ji; J. He","Department of Computer Science, Old Dominion University, Norfolk, Virginia 23529, United States of America","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","41","46","The detection of secondary structure of proteins using three dimensional (3D) cryo-electron microscopy (cryo-EM) images is still a challenging task when the spatial resolution of cryo-EM images is at medium level (5-10Å). Prior researches focused on the usage of local features that may not capture the global information of image objects. In this study, we propose to use deep learning methods to extract high representative global features and then automatically detect secondary structures of proteins. In particular, we build a convolutional neural network (CNN) classifier that predicts the probability of label for every individual voxel in 3D cryo-EM image with respect to the secondary structure elements of proteins such as α-helix, β-sheet and background. To effectively incorporate the 3D spatial information in protein structures, we propose to perform 3D convolutions in the convolutional layers of CNNs. We show that the proposed CNN classifier can outperform existing SVM method on identifying the secondary structure elements of proteins from 3D cryo-EM medium resolution images.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822490","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822490","","Computational modeling;Convolution;Deconvolution;Image resolution;Protein engineering;Proteins;Three-dimensional displays","biology computing;electron microscopy;feature extraction;image resolution;molecular biophysics;molecular configurations;neural nets;probability;proteins;support vector machines","α-helix;β-sheet;3D convolutions;3D cryo-EM image;3D spatial information;CNN classifier;SVM;convolutional layers;convolutional neural network classifier;deep convolutional neural networks;deep learning methods;high-representative global feature extraction;image objects;local features;probability;protein density maps;secondary structure detection;spatial resolution;three-dimensional cryo-electron microscopy images","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"[Title page i]","","","2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20161219","2016","","","i","i","The following topics are dealt with: computer vision; vehicle technology; biometrics; egocentric vision; DeepVision; deep learning; biomedical image registration; large scale 3D data; satellite visual analysis; street imagery; face analysis workshop; embedded vision; computational cameras; differential geometry; machine learning; tracking performance evaluation; microscopy image analysis; moving cameras; video surveillance; body-borne cameras; context-based affect recognition; affective face in-the-wild and automatic traffic surveillance.","","Electronic:978-1-5090-1437-8; POD:978-1-5090-1438-5","10.1109/CVPRW.2016.1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789491","","","cameras;computer vision;differential geometry;face recognition;geophysical image processing;image registration;learning (artificial intelligence);medical image processing;object tracking;traffic engineering computing;vehicles;video surveillance","DeepVision;affective face in-the-wild;automatic traffic surveillance;biomedical image registration;biometrics;body-borne cameras;computational cameras;computer vision;context-based affect recognition;deep learning;differential geometry;egocentric vision;embedded vision;face analysis workshop;large scale 3D data;machine learning;microscopy image analysis;moving cameras;satellite visual analysis;street imagery;tracking performance evaluation;vehicle technology;video surveillance","","","","","","","","June 26 2016-July 1 2016","","IEEE","IEEE Conference Publications"
"[Copyright notice]","","","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1","1","The following topics are dealt with: brain CAD; cardiac imaging; fMRI analysis; image reconstruction; interventional imaging; microscopy imaging and reconstruction; musculo-skeletal imaging; retinal imaging; segmentation methods for microscopy images; modeling and simulation; brain segmentation; motion tracking; neuron image analysis; optical imaging; segmentation and quantification of biological images; tissue quantification; ultrasound; visualization; fast MR acquisition and reconstruction; structural brain connectivity; CT reconstruction; image analysis of neurons; perspectives on deep learning for biomedical and biological imaging and image analysis; shape analysis; imaging cellular processes; breast imaging; EEG; foetal imaging; histological image analysis; imaging genetics; and machine learning.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493196","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493196","","","biomedical MRI;biomedical optical imaging;biomedical ultrasonics;brain;cardiology;cellular biophysics;computerised tomography;eye;genetics;image reconstruction;image segmentation;learning (artificial intelligence);mammography;medical image processing;neurophysiology;optical microscopy","CT reconstruction;EEG;biological images;brain CAD;brain segmentation;breast imaging;cardiac imaging;cellular processes;deep learning;fMRI analysis;fast MR acquisition;fetal imaging;histology image analysis;image reconstruction;imaging genetics;interventional imaging;machine learning;microscopy imaging;microscopy reconstruction;motion tracking;musculo-skeletal imaging;neuron image analysis;optical imaging;retinal imaging;segmentation methods;shape analysis;structural brain connectivity;tissue quantification;ultrasound;visualization","","","","","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks","J. T. Kwak; S. M. Hewitt","Department of Computer Science and Engineering, Sejong University, Seoul, Korea 05006.","IEEE Access","","2017","PP","99","1","1","In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei were utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training dataset including 73 benign and 89 cancer samples and validated on the testing dataset comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison to the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value<5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.","","","10.1109/ACCESS.2017.2747838","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023758","artificial neural network;cancer detection;computer-aided diagnosis;microscopy;pattern recognition","Biological tissues;Kernel;Machine learning;Neurons;Pathology;Prostate cancer","","","","","","","","","20170831","","","IEEE","IEEE Early Access Articles"
"Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning","M. I. Razzak; S. Naz","","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20170824","2017","","","801","807","Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.","","Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3","10.1109/CVPRW.2017.111","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014845","Blood Sample Analysis;ELM;KWFLICM;RBC;cell morphology;image analysis","Blood;Diseases;Feature extraction;Image color analysis;Image segmentation;Microscopy;Shape","","","","","","","","","","21-26 July 2017","","IEEE","IEEE Conference Publications"
"Crowdsourcing for Chromosome Segmentation and Deep Classification","M. Sharma; O. Saha; A. Sriraman; R. Hebbalaguppe; L. Vig; S. Karande","","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20170824","2017","","","786","793","Metaphase chromosome analysis is one of the primary techniques utilized in cytogenetics. Observations of chromosomal segments or translocations during metaphase can indicate structural changes in the cell genome, and is often used for diagnostic purposes. Karyotyping of the chromosomes micro-photographed under metaphase is done by characterizing the individual chromosomes in cell spread images. Currently, considerable effort and time is spent to manually segment out chromosomes from cell images, and classifying the segmented chromosomes into one of the 24 types, or for diseased cells to one of the known translocated types. Segmenting out the chromosomes in such images can be especially laborious and is often done manually, if there are overlapping chromosomes in the image which are not easily separable by image processing techniques. Many techniques have been proposed to automate the segmentation and classification of chromosomes from spread images with reasonable accuracy, but given the criticality of the domain, a human in the loop is often still required. In this paper, we present a method to segment out and classify chromosomes for healthy patients using a combination of crowdsourcing, preprocessing and deep learning, wherein the non-expert crowd from CrowdFlower is utilized to segment out the chromosomes from the cell image, which are then straightened and fed into a (hierarchical) deep neural network for classification. Experiments are performed on 400 real healthy patient images obtained from a hospital. Results are encouraging and promise to significantly reduce the cognitive burden of segmenting and karyotyping chromosomes.","","Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3","10.1109/CVPRW.2017.109","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014843","","Biological cells;Conferences;Crowdsourcing;Image segmentation;Machine learning;Microscopy;Pipelines","","","","","","","","","","21-26 July 2017","","IEEE","IEEE Conference Publications"
"FORMS-Locks: A Dataset for the Evaluation of Similarity Measures for Forensic Toolmark Images","M. Keglevic; R. Sablatnig","","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20170824","2017","","","1890","1897","We present a toolmark dataset created using lock cylinders seized during criminal investigations of break-ins. A total number of 197 cylinders from 48 linked criminal cases were photographed under a comparison microscope used by forensic experts for toolmark comparisons. In order to allow an assessment of the influence of different lighting conditions, all images were captured using a ring light with 11 different lighting settings. Further, matching image regions in the toolmark images were manually annotated. In addition to the annotated toolmark images and the annotation tool, extracted toolmark patches are provided for training and testing to allow a quantitative comparison of the performance of different similarity measures. Finally, results from an evaluation using a publicly available state-of-the-art image descriptor based on deep learning are presented to provide a baseline for future publications.","","Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3","10.1109/CVPRW.2017.236","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014970","","Cloning;Forensics;Image edge detection;Lighting;Manuals;Microscopy;Tools","","","","","","","","","","21-26 July 2017","","IEEE","IEEE Conference Publications"
"Semantic segmentation of microscopic images of H&E stained prostatic tissue using CNN","J. Isaksson; I. Arvidsson; K. Åaström; A. Heyden","Lund University, Centre for Mathematical Sciences, Lund, Sweden","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","1252","1256","There is a need for an automatic Gleason scoring system that can be used for prostate cancer diagnosis. Today the diagnoses are determined by pathologists manually, which is both a complex and a time-consuming task. To reduce the pathologists' workload, but also to reduce variations between different pathologists, an automatic classification system would be of great use. Some previous works have aimed for this, but still more work needs to be done. It is probable that such a tool would benefit from having access to individually segmented, pathologically relevant objects from the images. Therefore, we have developed an algorithm for semantic segmentation of the microscopic images of H&E stained prostate tissue into Background, Stroma, Epithelial Cytoplasm and Nuclei. This algorithm is based on deep learning, or more specifically a convolutional neural network. The network design is inspired by architectures that previously have been proved successful in different applications. It consists of a contracting and an expanding part, which are symmetrical. We have reached an accuracy of 80 %, as measured by the mean of the intersection over union, for segmentation into four classes. Previous works have only investigated nuclei segmentation, and our network performed similar but for the more challenging task of four class segmentation.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7965996","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965996","","Cancer;Gold;Image segmentation;Microscopy;Neural networks;Semantics;Standards","biological tissues;cancer;image classification;image segmentation;medical image processing;neural nets;patient diagnosis","CNN;H&E stained prostatic tissue;automatic Gleason scoring system;automatic classification system;epithelial cytoplasm;microscopic image semantic segmentation;prostate cancer diagnosis","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"Automating Papanicolaou Test Using Deep Convolutional Activation Feature","J. Hyeon; H. J. Choi; K. N. Lee; B. D. Lee","Sch. of Comput., KAIST, Daejeon, South Korea","2017 18th IEEE International Conference on Mobile Data Management (MDM)","20170703","2017","","","382","385","Cervical cancer is the women's fourth most common cancer worldwide, with 266,000 deaths in a year. Cervical cancer can be diagnosed by the Papanicolaou test. In this test, a cytopathologist observes a microscopic image of the cervix cells and decides whether the patient is abnormal or not. According to research, the accuracy of the cervical cytology is reported as 89.7%. Because it is associated with the patient's life, it is important to improve the accuracy of this test. Many systems have been proposed to help judge experts to improve the accuracy of tests in the medical field, but development has been limited to areas where there are cleanly quantified test data. In this paper, we design and train a model to automatically classify the normal/abnormal state of cervical cells from microscopic images by using a convolutional neural network and several machine learning classifiers. As a result, the support vector machine achieves the highest performance with 78% F1 score.","","Electronic:978-1-5386-3932-0; POD:978-1-5386-3933-7","10.1109/MDM.2017.66","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962484","Cervical Cancer;Cervical Cancer Screening Test;Convolutional Neural Network;Deep Convolutional Activation Feature;Papanicolaou Test","Biological neural networks;Cervical cancer;Feature extraction;Microscopy;Support vector machines","biomedical optical imaging;cancer;cellular biophysics;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;microscopes;support vector machines","F1 score;Papanicolaou test automation;automatic abnormal state classification;automatic normal state classification;cervical cancer;cervical cytology;cervix cells;convolutional neural network;deep convolutional activation feature;machine learning classifiers;medical field;microscopic image;support vector machine","","","","","","","","May 29 2017-June 1 2017","","IEEE","IEEE Conference Publications"
"Mitosis Detection in Phase Contrast Microscopy Image Sequences of Stem Cell Populations: A Critical Review","A. A. Liu; Y. Lu; M. Chen; Y. Su","Electronics Information Engineering, Tianjin University, Tianjin, Tianjin China 300072 (e-mail: anan0422@gmail.com)","IEEE Transactions on Big Data","","2017","PP","99","1","1","Detecting mitosis from cell population is a fundamental problem in many biological researches and biomedical applications. In modern researches, advanced imaging technologies have been applied to generate large amount of microscope images of cells. However, detecting all mitotic cells from these images with human eye is tedious and time-consuming. In recent years, several approaches have been proposed to help humans finish this job automatically with high efficiency and accuracy. In this review paper, we first described some commonly used datasets for mitosis detection, and then discussed different kinds of methods for mitosis detection, like tracking based methods, tracking free methods, hybrid methods, and the most recently proposed works based on deep learning architecture. We compared these methods on same datasets, and found that deep learning based approaches have achieved a great improvement in performance. At last, we discussed the future possible approaches on mitosis detection, to combine the success of previous works and the advantage of big data in modern researches. Considering expertise is highly required in biomedical area, we will further discuss the possibility to learn information from biomedical big data with less expert annotation.","","","10.1109/TBDATA.2017.2721438","China Scholarship Council; Elite Scholar Program of Tianjin University; National Natural Science Foundation of China; Tianjin Research Program of Application Foundation and Advanced Technology; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962189","big data;biomedical image;computer vision;microscopy image;mitosis detection;stem cell","Big Data;Computer architecture;Image sequences;Microscopy;Sociology;Statistics;Stem cells","","","","","","","","","20170629","","","IEEE","IEEE Early Access Articles"
"Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1405","1417","In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitativ- results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.","0278-0062;02780062","","10.1109/TMI.2017.2677479","10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Experienced Researchers; 10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Postdoctoral Researchers; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869416","Microscopy;deep learning;high-order loss functions;structured output learning;weakly-supervised training","Biomedical imaging;Cancer;Computational modeling;Manuals;Medical treatment;Training;Tumors","biomedical optical imaging;cancer;fluorescence;image classification;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;tumours","HE images;IF images;acutely hypoxic regions;automatic quantification;baseline method;cancer patients;chronically hypoxic regions;clinical annotations;estimated annotations;fixed grid;fully-convolutional neural network;hematoxylin and eosin stained images;high resolution immunofluorescence images;high-order loss function;histological specimen;hypoxia-modified therapies;input images;latent structured output learning formulation;machine learning-based methodologies;majority classifier;manual annotations;multimodal microscopy images;naive method;normoxic regions;pixel-wise labeling;standard cancer treatments;standardized measurement;tumor hypoxia;tumour hypoxia;weakly-supervised learning methods;weakly-supervised structured output classification problem","","","","","","","20170302","July 2017","","IEEE","IEEE Journals & Magazines"
"A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology","N. Kumar; R. Verma; S. Sharma; S. Bhargava; A. Vahadane; A. Sethi","IIT Guwahati, Guwahati, India","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1550","1560","Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.","0278-0062;02780062","","10.1109/TMI.2017.2677499","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382","Annotation;boundaries;dataset;deep learning;nuclear segmentation;nuclei","Diseases;Image color analysis;Image segmentation;Machine learning;Measurement;Pathology;Training","biological organs;biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;object recognition;optical microscopy","H&E-stained images;Otsu thresholding;chromatin-sparse;computational pathology;conventional image processing techniques;crowded nuclei;deep learning;digital microscopic tissue images;disease states;generalized nuclear segmentation;hematoxylin and eosin-stained tissue images;high-quality feature extraction;image classification problems;machine learning algorithms;machine learning-based segmentation;nuclear appearances;nuclear boundaries;nuclear morphometrics;object recognition;object-level errors;organs;overlapping nuclei;pixel-level errors;right out-of-the-box;segmentation technique;watershed segmentation","","","","","","","20170306","July 2017","","IEEE","IEEE Journals & Magazines"
"Malaria Parasite Detection From Peripheral Blood Smear Images Using Deep Belief Networks","D. Bibin; M. S. Nair; P. Punitha","Department of Research and Development Centre, Bharathiar University, Coimbatore, India","IEEE Access","20170619","2017","5","","9099","9108","In this paper, we propose a novel method to identify the presence of malaria parasites in human peripheral blood smear images using a deep belief network (DBN). This paper introduces a trained model based on a DBN to classify 4100 peripheral blood smear images into the parasite or non-parasite class. The proposed DBN is pre-trained by stacking restricted Boltzmann machines using the contrastive divergence method for pre-training. To train the DBN, we extract features from the images and initialize the visible variables of the DBN. A concatenated feature of color and texture is used as a feature vector in this paper. Finally, the DBN is discriminatively fine-tuned using a backpropagation algorithm that computes the probability of class labels. The optimum size of the DBN architecture used in this paper is 484-600-600-600-600-2, in which the visible layer has 484 nodes and the output layer has two nodes with four hidden layers containing 600 hidden nodes in every layer. The proposed method has performed significantly better than the other state-of-the-art methods with an F-score of 89.66%, a sensitivity of 97.60%, and specificity of 95.92%. This paper is the first application of a DBN for malaria parasite detection in human peripheral blood smear images.","2169-3536;21693536","","10.1109/ACCESS.2017.2705642","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931565","Deep learning;contrastive divergence;deep belief network;discriminative training;malaria parasite detection;restricted Boltzmann machine","Blood;Computer architecture;Diseases;Feature extraction;Image color analysis;Microscopy;Training","Boltzmann machines;backpropagation;belief networks;biology computing;blood;feature extraction;image classification;learning (artificial intelligence)","DBN;backpropagation algorithm;contrastive divergence method;deep belief networks;feature extraction;feature vector;human peripheral blood smear images;malaria parasite detection;restricted Boltzmann machines","","","","","","","20170518","2017","","IEEE","IEEE Journals & Magazines"
"Deep residual Hough voting for mitotic cell detection in histopathology images","T. Wollmann; K. Rohr","University of Heidelberg, BIOQUANT, IPMB, and DKFZ Heidelberg, Dept. Bioinformatics and Functional Genomics, Biomedical Computer Vision Group, Im Neuenheimer Feld 267, 69120, Germany","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","341","344","Cell detection in microscopy images is a common and challenging task. We propose a new approach for mitotic cell detection in histopathology images, which is based on a Deep Residual Network architecture combined with Hough voting. We propose a voting layer for neural networks and introduce a novel loss function. Our approach is learned from scratch using cell centroids and the original images. We benchmarked our approach on the challenging AMIDA13 dataset containing histology images of invasive breast carcinoma. It turned out that our approach achieved competitive results.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950533","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950533","Deep Learning;Dilated Convolution;Hough transform;Microscopy;Residual Network","Biomedical imaging;Computer architecture;Convolution;Microprocessors;Neural networks;Training;Transforms","Hough transforms;biological organs;cellular biophysics;learning (artificial intelligence);medical image processing;neural net architecture","AMIDA13 dataset;cell centroids;deep residual Hough voting;deep residual network architecture;histopathology images;invasive breast carcinoma;microscopy images;mitotic cell detection","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep learning for magnification independent breast cancer histopathology image classification","N. Bayramoglu; J. Kannala; J. Heikkilä","Center for Machine Vision and Signal Analysis, University of Oulu, Finland","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2440","2445","Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7900002","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900002","","Breast cancer;Databases;Microscopy;Pathology;Training;Training data","cancer;computer vision;image classification;learning (artificial intelligence);medical image processing;neural nets","BreaKHis dataset;breast tissues;computer vision;convolutional neural networks;deep learning;diagnostic pathology workflow;digital imaging techniques;image classification;machine learning;magnification independent breast cancer histopathology image classification;microscopic analysis;multitask CNN;single task CNN","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Learned vs. engineered features for fine-grained classification of aquatic macroinvertebrates","E. Riabchenko; K. Meissner; I. Ahmad; A. Iosifidis; V. Tirronen; M. Gabbouj; S. Kiranyaz","Department of Signal Processing, Tampere University of Technology, Finland","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2276","2281","Aquatic macroinvertebrate biomonitoring is an efficient way of assessment of slow and subtle anthropogenic changes and their effect on water quality. It is imperative to have reliable identification and counts of the various taxa occurring in samples as these form the basis for the quality indices used to infer the ecological status of the aquatic ecosystem. In this paper, we try to close the gap between human taxa identification accuracy (typically 90-95% on 30-40 classes of macroinvertebrates) and results of automatic fine-grained classification by introducing a novel technique based on Convolutional Neural Networks (CNN). CNN learns optimal features for macroinvertebrate classification and achieves near human accuracy when tested on 29 macroinvertebrate classes. Moreover, we perform comparative evaluation of the learned features against the hand-crafted features, which have been commonly used in classical approaches, and confirm superiority of the learned deep features over the engineered ones.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899975","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899975","","Databases;Ecosystems;Feature extraction;Machine vision;Microscopy;Water resources","aquaculture;biology computing;feedforward neural nets;learning (artificial intelligence);pattern classification","CNN;anthropogenic changes;aquatic ecosystem;aquatic macroinvertebrate biomonitoring;automatic fine-grained classification;convolutional neural networks;ecological status;engineered features;hand-crafted features;human taxa identification accuracy;learned deep features;macroinvertebrate classification;quality indices","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks","L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Medical Imaging","20170331","2017","36","4","994","1004","Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.","0278-0062;02780062","","10.1109/TMI.2016.2642839","Research Grants Council of the Hong Kong Special Administrative Region; 10.13039/501100003453 - Guangdong Natural Science Foundation; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699","Automated melanoma recognition;fully convolutional neural networks;residual learning;skin lesion analysis;very deep convolutional neural networks","Biomedical imaging;Feature extraction;Image segmentation;Lesions;Malignant tumors;Skin;Training data","biomedical optical imaging;cancer;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;skin","automated melanoma recognition;classification network;deep convolutional neural networks;deep residual networks;dermoscopy images;fully convolutional residual network;low-level hand-crafted features;medical image analysis;skin lesion segmentation;training data","","","","","","","20161221","April 2017","","IEEE","IEEE Journals & Magazines"
"Materials discovery: Understanding polycrystals from large-scale electron patterns","R. Liu; A. Agrawal; W. k. Liao; A. Choudhary; M. De Graef","Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2261","2269","This paper explores the idea of modeling a large image data collection of polycrystal electron patterns, in order to detect insights in understanding materials discovery. There is an emerging interest in applying big data processing, management and modeling methods to scientific images, which often come in a form and with patterns only interpretable to domain experts. While large-scale machine learning approaches have demonstrated certain superiority in analyzing, summarizing, and providing an understandable route to data types like natural images, speeches and texts, scientific images is still a relatively unexplored area. Deep convolutional neural networks, despite their recent triumph in natural image understanding, are still rarely seen adapted to experimental microscopic images, especially in a large scale. To the best of our knowledge, we present the first deep learning solution towards a scientific image indexing problem using a collection of over 300K microscopic images. The result obtained is 54% better than a dictionary lookup method which is state-of-the-art in the materials science society.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840857","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840857","Deep learning;EBSD;convolutional neural networks;electronic images;materials design;materials discovery","Biological neural networks;Computational modeling;Electron microscopy;Indexing;Machine learning","Big Data;convolution;crystals;image processing;learning (artificial intelligence);materials science computing;neural nets","big data processing;deep convolutional neural networks;experimental microscopic images;image data collection;large-scale machine learning;large-scale polycrystal electron patterns;materials discovery;materials science society;scientific image indexing problem;scientific images","","1","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation","A. Fakhry; T. Zeng; S. Ji","Department of Computer Science, Old Dominion University, Norfolk, VA, USA","IEEE Transactions on Medical Imaging","20170201","2017","36","2","447","456","Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.","0278-0062;02780062","","10.1109/TMI.2016.2613019","10.13039/100000153 - National Science Foundation, Old Dominion University, and Washington State University; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638","Brain circuit reconstruction;deconvolutional networks;deep learning;electron microscopy;image segmentation;residual learning","Convolution;Deconvolution;Feature extraction;Image reconstruction;Image segmentation;Predictive models;Three-dimensional displays","brain;deconvolution;electron microscopy;image segmentation;medical image processing;neural nets;neurophysiology","3D neurite segmentation;EM image segmentation;anatomical connection reconstruction;brain electron microscopy;circuit mapping;contextual information;dense output prediction;electron microscopy images;full-resolution features;full-resolution predictions;hand-crafted features;human-level performance;information pathways;neurite segmentation challenge dataset;neurons;residual deconvolutional networks","","","","","","","20160923","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"CNN-based image analysis for malaria diagnosis","Z. Liang; A. Powell; I. Ersoy; M. Poostchi; K. Silamut; K. Palaniappan; P. Guo; M. A. Hossain; A. Sameer; R. J. Maude; J. X. Huang; S. Jaeger; G. Thoma","School of Information Technology, York University, Toronto, ON, M3J1P3, Canada","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","493","496","Malaria is a major global health threat. The standard way of diagnosing malaria is by visually examining blood smears for parasite-infected red blood cells under the microscope by qualified technicians. This method is inefficient and the diagnosis depends on the experience and the knowledge of the person doing the examination. Automatic image recognition technologies based on machine learning have been applied to malaria blood smears for diagnosis before. However, the practical performance has not been sufficient so far. This study proposes a new and robust machine learning model based on a convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. In a ten-fold cross-validation based on 27,578 single cell images, the average accuracy of our new 16-layer CNN model is 97.37%. A transfer learning model only achieves 91.99% on the same images. The CNN model shows superiority over the transfer learning model in all performance indicators such as sensitivity (96.99% vs 89.00%), specificity (97.75% vs 94.98%), precision (97.73% vs 95.12%), F1 score (97.36% vs 90.24%), and Matthews correlation coefficient (94.75% vs 85.25%).","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822567","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822567","computer-aided diagnosis;convolutional neural network;deep learning;machine learning;malaria","Blood;Data models;Diseases;Machine learning;Mathematical model;Microscopy;Training","blood;cellular biophysics;diseases;image recognition;learning (artificial intelligence);medical image processing;neural nets","CNN-based image analysis;Matthews correlation coefficient;automatic image recognition technology;convolutional neural network;machine learning model;malaria diagnosis;parasite-infected red blood cell;single cell classification;single cell images","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Cell proposal network for microscopy image analysis","S. U. Akram; J. Kannala; L. Eklund; J. Heikkilä","Center for Machine Vision Research, University of Oulu, Finland","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","3199","3203","Robust cell detection plays a key role in the development of reliable methods for automated analysis of microscopy images. It is a challenging problem due to low contrast, variable fluorescence, weak boundaries, conjoined and overlapping cells, causing most cell detection methods to fail in difficult situations. One approach for overcoming these challenges is to use cell proposals, which enable the use of more advanced features from ambiguous regions and/or information from adjacent frames to make better decisions. However, most current methods rely on simple proposal generation and scoring methods, which limits the performance they can reach. In this paper, we propose a convolutional neural network based method which generates cell proposals to facilitate cell detection, segmentation and tracking. We compare our method against commonly used proposal generation and scoring methods and show that our method generates significantly better proposals, and achieves higher final recall and average precision.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532950","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532950","cell detection;cell proposals;cell tracking;deep learning;fully convolutional network","Feature extraction;Image analysis;Image segmentation;Microscopy;Proposals;Shape;Training","image segmentation;medical image processing;neural nets","cell proposal network;cell segmentation;cell tracking;convolutional neural network based method;microscopy image analysis;robust cell detection","","1","","16","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Membrane segmentation via active learning with deep networks","U. Gaur; M. Kourakis; E. Newman-Smith; W. Smith; B. S. Manjunath","Department of Computer Science, University of California Santa Barbara","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","1943","1947","Segmentation is a key component of several bio-medical image processing systems. Recently, segmentation methods based on supervised learning such as deep convolutional networks have enjoyed immense success for natural image datasets and biological datasets alike. These methods require large volumes of data to avoid overfitting which limits their applicability. In this work, we present a transfer learning mechanism based on active learning which allows us to utilize pre-trained deep networks for segmenting new domains with limited labelled data. We introduce a novel optimization criterion to allow feedback on the most uncertain, yet abundant image patterns thus provisioning for an expert in the loop albeit with minimum amount of guidance. Our experiments demonstrate the effectiveness of the proposed method in improving segmentation performance with very limited labelled data.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532697","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532697","Active Learning;Deep Networks;Image Segmentation;Transfer Learning","Computer architecture;Image segmentation;Microprocessors;Microscopy;Optimization;Training;Uncertainty","convolution;image segmentation;learning (artificial intelligence);optimisation","active learning;deep convolutional networks;deep networks;membrane segmentation;optimization criterion;supervised learning;transfer learning mechanism","","","","17","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Multi-loss convolutional networks for gland analysis in microscopy","A. BenTaieb; J. Kawahara; G. Hamarneh","Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","642","645","Manual tissue diagnosis is the most prevalent approach to cancer diagnosis. However, it mainly relies on a subjective visual quantification of specific morphometric features, which often leads to a relatively limited reproducibility among experts. In most computational techniques proposed to automate the diagnostic procedure, accurate segmentation is paramount as a precursor to the extraction of relevant morphometric features. Since the ultimate goal of segmentation is generally classification, yet a given class imparts an expected tissue appearance beneficial to segmentation, we pose the problem of automatic tissue analysis as the joint task of segmentation and classification. We propose a novel multi-objective learning method that optimizes a single unified deep fully convolutional neural network with two distinct loss functions. We illustrate our reasoning on the task of colon adenocarcinomas diagnosis and show how glands' classification can facilitate their segmentation by adding class-specific spatial priors. The final classification also benefits from this joint learning framework yielding an improvement of 6% over classification-only models.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493349","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493349","Classification;Deep Learning;Histopathology;Segmentation","Cancer;Colon;Feature extraction;Glands;Image segmentation;Training;Tumors","cancer;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation","automatic tissue analysis;cancer diagnosis;colon adenocarcinoma diagnosis;gland classification;image segmentation;microscopy;morphometric feature extraction;multiloss convolutional networks;multiobjective learning method;optimization","","","","6","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Structure-based assessment of cancerous mitochondria using deep networks","M. Mishra; S. Schmitt; L. Wang; M. K. Strasser; C. Marr; N. Navab; H. Zischka; T. Peng","Computer Aided Medical Procedures (CAMP), Technische Universitaet Muenchen, Germany","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","545","548","Mitochondrial functions are essential for cell survival. Pathologic situations, e.g. cancer, can impair mitochondrial function which is frequently reflected by an altered morphology. So far, feature description of mitochondrial structure in cancer remains largely qualitative. In this study, we propose a learning-based approach to quantitatively assess the structure of mitochondria isolated from liver tumor cell lines using convolutional neural network (CNN). Besides achieving a high classification accuracy on isolated mitochondria from healthy tissue and different tumor cell lines which the CNN model was trained on, CNN is also able to classify unseen tumor cell lines, which suggests its superior capability to capture the intrinsic structural transition from healthy to tumor mitochondria.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493327","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493327","Mitochondria;convolutional neural network;deep learning;electron microscopy","Indexes;Liver;Standards;Support vector machines;Training;Tumors","cancer;cellular biophysics;learning (artificial intelligence);liver;medical image processing;microorganisms;neurophysiology;tumours","CNN model;altered morphology;cancer;cancerous mitochondria;cell survival;convolutional neural network;deep networks;healthy tumor mitochondria;high classification accuracy;learning-based approach;liver tumor cell lines;mitochondria isolated structure;mitochondrial functions;mitochondrial structure;pathologic situations;structural transition;structure-based assessment;tumor cell lines","","","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia","2015 IEEE International Conference on Computer Vision (ICCV)","20160218","2015","","","648","656","We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.","","Electronic:978-1-4673-8391-2; POD:978-1-4673-8392-9","10.1109/ICCV.2015.81","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410438","","Cancer;Computer vision;Manuals;Microscopy;Support vector machines;Training;Tumors","cancer;graph theory;medical image processing;neural nets;support vector machines;tumours","DCNN model;FLSSVM;cancer treatment;deep convolutional neural network;flexible graph;flexible latent structure support vector machine;high-order loss function;latent graph;malignant tumour;microcirculatory supply units;microscopy imaging;weakly-supervised structured output learning","","","","37","","","","7-13 Dec. 2015","","IEEE","IEEE Conference Publications"
"Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy","B. Jiang; X. Wang; J. Luo; X. Zhang; Y. Xiong; H. Pang","Guangzhou Inst. of Biomed. & Health, Guangzhou, China","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","20160215","2015","","","122","126","The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.","","Electronic:978-1-4673-7723-2; POD:978-1-4673-7724-9","10.1109/IMCCC.2015.33","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405812","bright-field microscopy;convolutional neural networks;deep learning;machine learning;non-invasive;non-labelled;trans-differentiated neural progenitor cells","Biological neural networks;Electronic mail;Feature extraction;Image recognition;Machine learning;Microscopy;Morphology","cellular biophysics;image classification;medical image processing;neural nets;optical microscopy","NPC classification;automatic recognition;bright-field microscopic imaging analysis;convolutional neural networks;image pre-processing;nonNPC classification;trans-differentiated neural progenitor cells","","","","17","","","","18-20 Sept. 2015","","IEEE","IEEE Conference Publications"
"Maximum Margin Learning of t-SPNs for Cell Classification With Filtered Input","H. Kang; C. D. Yoo; Y. Na","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea","IEEE Journal of Selected Topics in Signal Processing","20160121","2016","10","1","130","139","An algorithm based on a deep probabilistic architecture referred to as tree-structured sum-product network (t-SPN) is considered for cells classification. The t-SPN is a rooted acyclic graph constructed as a tree of several sum-product networks where each network is constructed over a subset of most confusing class features. The constructed t-SPN architecture is learned by maximizing the margin which is defined to be the difference in the conditional probability between the true and the most competitive false labels. To enhance generalization, l<sub>2</sub>-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate compared to other state-of-the-art algorithms that include convolutional neural network (CNN) based algorithms. Ideal high-pass filter was more effective on the HEp-2 dataset which is based on immunofluorescence staining while the LOG was more effective on Feulgen dataset which is based on Feulgen staining.","1932-4553;19324553","","10.1109/JSTSP.2015.2502542","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332738","confusing classes;maximum margin;sub-SPNs;t-SPNs","Computer architecture;Input variables;Microprocessors;Microscopy;Signal processing;Special issues and sections","biomedical optical imaging;cellular biophysics;fluorescence;high-pass filters;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","Feulgen dataset;Feulgen staining;HEp-2 dataset;Laplacian-of-Gaussian filtering;cell classification;cell feature;convolutional neural network;deep probabilistic architecture;filtered input;generic high-pass filter;immunofluorescence staining;l2-regularization;learning process;maximum margin criterion;maximum margin learning;rooted acyclic graph;t-SPN architecture;tree-structured sum-product network","","","","18","","","20151120","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Predictive data analytics and machine learning enabling metrology and process control for advanced node IC fabrication","N. Rana; Y. Zhang; D. Wall; B. Dirahoui","IBM Semiconductor Research & Development Center, Hopewell Junction, NY-12533, USA","2015 26th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)","20150727","2015","","","313","319","Processor technology is going through multiple changes in terms of patterning techniques (multipatterning, EUV and DSA), device architectures (FinFET, nanowire, graphene) and patterning scale (few nanometers). These changes require tighter controls on processes and measurements to achieve the required device performance, and challenge the metrology and process control in terms of capability and quality. Predictive metrology and analytics offer Multivariate data with non-linear trends and complex correlations generally cannot be described well by mathematical models but can be relatively easily learned by computing machines and used to predict or extrapolate. In this paper we present the application of machine learning and analytics to accurately predict the electrical performance of deep trenches and metal lines. Machine learning models can be used in process control where, for example, the electrical test results are predicted early in the processing flow invoking appropriate actionable decisions. It is demonstrated that metal line resistance can be modeled directly by the raw reflectance spectra obtained using scatterometry tool. This obviates the need to make complex geometrical models to measure the CDs and then establishing the correlation of CDs to resistance. It is shown that dimensional parameters such as height and CD can be derived from the predicted electrical measurements. Such information can be used in feedforward or feedback flow to optimize, control or monitor processes in fab. Results show improved correlation of neural network model predicted deep trench capacitance to the measured capacitance compared to the capacitance predicted by multivariate linear regression model that is currently in use. This paper presents the concept of predictive metrology with the use of machine learning and predictive analytics for CD and electrical test predictions. Predictive metrology can be used in conjunction with hybrid metrology to enable APC and novel metrolog- pathways in gap areas in the advanced semiconductor research, development and manufacturing.","1078-8743;10788743","Electronic:978-1-4799-9930-9; POD:978-1-4799-9931-6","10.1109/ASMC.2015.7164502","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164502","Critical Dimension Atomic Force Microscopy (CD-AFM);Critical Dimension Scanning Electron Microscopy (CD-SEM);Deep Trench Capacitance and Metal Line Resistance;Electrical CD (ECD);Hybrid Metrology (HM);Machine Learning (ML);Model Based Infrared Reflectometry (MBIR);Neural Network (NN);Optical Critical Dimension Metrology (OCD);PM: Predictive Metrology (PM);Partial Least Square regression (PLS);Principal Components Analysis (PCA)","Capacitance;Data models;Metrology;Neural networks;Predictive models;Resistance;Semiconductor device modeling","learning (artificial intelligence);process control;regression analysis","DSA;EUV;FinFET;advanced node IC fabrication;deep trench capacitance;graphene;machine learning;multipatterning;multivariate linear regression model;nanowire;neural network model;patterning scale;predictive data analytics;process control;scatterometry tool","","3","","7","","","","3-6 May 2015","","IEEE","IEEE Conference Publications"
"Forest Species Recognition Using Deep Convolutional Neural Networks","L. G. Hafemann; L. S. Oliveira; P. Cavalin","Dept. of Inf., Fed. Univ. of Parana, Curitiba, Brazil","2014 22nd International Conference on Pattern Recognition","20141206","2014","","","1103","1107","Forest species recognition has been traditionally addressed as a texture classification problem, and explored using standard texture methods such as Local Binary Patterns (LBP), Local Phase Quantization (LPQ) and Gabor Filters. Deep learning techniques have been a recent focus of research for classification problems, with state-of-the art results for object recognition and other tasks, but are not yet widely used for texture problems. This paper investigates the usage of deep learning techniques, in particular Convolutional Neural Networks (CNN), for texture classification in two forest species datasets - one with macroscopic images and another with microscopic images. Given the higher resolution images of these problems, we present a method that is able to cope with the high-resolution texture images so as to achieve high accuracy and avoid the burden of training and defining an architecture with a large number of free parameters. On the first dataset, the proposed CNN-based method achieves 95.77% of accuracy, compared to state-of-the-art of 97.77%. On the dataset of microscopic images, it achieves 97.32%, beating the best published result of 93.2%.","1051-4651;10514651","Electronic:978-1-4799-5209-0; POD:978-1-4799-5210-6","10.1109/ICPR.2014.199","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976909","","Accuracy;Feature extraction;Image recognition;Image resolution;Microscopy;Support vector machines;Training","forestry;image classification;image resolution;image texture;learning (artificial intelligence);neural nets;object recognition","CNN-based method;Gabor filters;LBP;LPQ;deep convolutional neural networks;forest species recognition;high resolution images;local binary patterns;local phase quantization;macroscopic images;microscopic images;object recognition;texture classification problem","","3","","19","","","","24-28 Aug. 2014","","IEEE","IEEE Conference Publications"
"Modeling two-photon calcium fluorescence of episodic V1 recordings using multifrequency analysis","H. W. Zheng; W. Q. Malik; C. A. Runyan; M. Sur; E. N. Brown","Yale University, New Haven, CT 06520","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20111201","2011","","","3016","3019","The use of two-photon microscopy allows for imaging of deep neural tissue in vivo. This paper examines frequency-based analysis to two-photon calcium fluorescence images with the goal of deriving smooth tuning curves. We present a multifrequency analysis approach for improved extraction of calcium responses in episodic stimulation experiments, that is, when the stimulus is applied for a number of frames, then turned off for the next few frames, and so on. Episodic orientation stimulus was applied while recording from the primary visual cortex of an anesthetized mouse. The multifrequency model demonstrated improved tuning curve descriptions of the neurons. It also offers perspective regarding the characteristics of calcium fluorescence imaging of the brain.","1094-687X;1094687X","Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8","10.1109/IEMBS.2011.6090826","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090826","Two-photon microscopy;calcium imaging;multifrequency analysis;tuning curve;visual cortex","Brain modeling;Calcium;Fluorescence;Microscopy;Neurons;Tuning","biological tissues;biomedical optical imaging;brain;calcium;fluorescence;medical image processing;neurophysiology;optical microscopy;physiological models;two-photon spectra","anesthetized mouse;brain;deep neural tissue imaging;episodic V1 recordings;episodic orientation stimulus;frequency-based analysis;multifrequency analysis;neurons;primary visual cortex;smooth tuning curves;two-photon calcium fluorescence images;two-photon microscopy","Animals;Calcium;Fluorescence;Models, Theoretical;Photons","0","","9","","","","Aug. 30 2011-Sept. 3 2011","","IEEE","IEEE Conference Publications"
"A decision support system to detect morphologic changes of chromatin arrangement in normal-appearing cells","R. Sacile; E. Montaldo; C. Ruggiero; H. E. Nieburgs; G. Nicolo","Dept. of Commun., Univ. of Genoa, Italy","IEEE Transactions on NanoBioscience","20030709","2003","2","2","118","123","Several studies have described malignancy-associated changes (MACs) of chromatin arrangement in the nuclei of apparently normal cells adjacent to and distant from an invasive cancer area. MAC assessment is a hard task, since it requires a deep knowledge of morphologic features of chromatin arrangement. The aim of this work is to verify the reproducibility of the subjective evaluation of the expert on the basis of a decision support system (DSS) that automatically and objectively reproduces MAC diagnosis. A set of 61 patients with suspected clinical diagnosis for lung cancer has been taken into account. The scientist who first described MAC defined each patient as MAC positive or negative on the basis of the MAC diagnosis performed on all cells of the related cytologic sample. A DSS based on an artificial neural network has been set up to learn the relation between 14 morphometric and texture parameters, computed on each nucleus by image processing techniques, with the MAC diagnosis of the expert on each cell. The results show that an objective automatic assessment on MAC by the DSS can effectively support the MAC diagnosis. The method adopted in this approach may be also appropriate for other problems, where an automatic classification of visually inspected patterns of biological micro- and submicrostructure is needed.","1536-1241;15361241","","10.1109/TNB.2003.813939","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1209640","","Artificial neural networks;Cancer;Clinical diagnosis;Computer networks;Decision support systems;Lungs;Malignant tumors;Microscopy;Reproducibility of results;Shape","biomedical optical imaging;cancer;cellular biophysics;decision support systems;image classification;image texture;lung;neural nets;optical microscopy","automatic classification;biological microstructure;biological submicrostructure;cell nucleus;chromatin arrangement detection;cytologic sample;lung cancer;malignancy-associated changes;morphologic changes detection;morphometric parameters;subjective evaluation reproducibility;suspected clinical diagnosis;texture parameters;visually inspected patterns","Algorithms;Chromatin;Decision Support Techniques;Expert Systems;Humans;Image Interpretation, Computer-Assisted;Lung Neoplasms;Neural Networks (Computer);Reproducibility of Results;Sensitivity and Specificity","2","","39","","","","June 2003","","IEEE","IEEE Journals & Magazines"
