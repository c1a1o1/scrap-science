"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&queryText%3Ddeep+learning+pathology",2017/09/18 11:17:07
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Voice Pathology Detection Using Deep Learning: a Preliminary Study","P. Harar; J. B. Alonso-Hernandezy; J. Mekyska; Z. Galaz; R. Burget; Z. Smekal","Department of Telecommunications Brno University of Technology, Technicka 10, 61600 Brno, Czech Republic","2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI)","20170724","2017","","","1","4","This paper describes a preliminary investigation of Voice Pathology Detection using Deep Neural Networks (DNN). We used voice recordings of sustained vowel /a/ produced at normal pitch from German corpus Saarbruecken Voice Database (SVD). This corpus contains voice recordings and electroglottograph signals of more than 2 000 speakers. The idea behind this experiment is the use of convolutional layers in combination with recurrent Long-Short-Term-Memory (LSTM) layers on raw audio signal. Each recording was split into 64 ms Hamming windowed segments with 30 ms overlap. Our trained model achieved 71.36% accuracy with 65.04% sensitivity and 77.67% specificity on 206 validation files and 68.08% accuracy with 66.75% sensitivity and 77.89% specificity on 874 testing files. This is a promising result in favor of this approach because it is comparable to similar previously published experiment that used different methodology. Further investigation is needed to achieve the state-of-the-art results.","","Electronic:978-1-5386-0850-0; POD:978-1-5386-0851-7","10.1109/IWOBI.2017.7985525","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985525","","Convolution;Databases;Feature extraction;Pathology;Support vector machines;Testing;Training","audio signal processing;bioelectric phenomena;electric impedance measurement;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;speech;speech processing","German corpus Saarbruecken voice database;Hamming windowed segments;audio signal;convolutional layers;deep learning;deep neural networks;electroglottograph signals;long-short-term-memory layers;sustained vowel;voice pathology detection;voice recordings","","","","","","","","10-12 July 2017","","IEEE","IEEE Conference Publications"
"Deep Learning for Automated Extraction of Primary Sites from Cancer Pathology Reports","J. Qiu; H. J. Yoon; P. A. Fearn; G. D. Tourassi","University of Tennessee, Knoxville, TN, 37996, and Health Data Sciences Institute, Oak Ridge National Laboratory, Oak Ridge, TN 37831 (e-mail: jqiu1@utk.edu)","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study we investigated deep learning and a convolutional neural network (CNN), for extracting ICDO- 3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro and macro-F score increases of up to 0.132 and 0.226 respectively when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.","2168-2194;21682194","","10.1109/JBHI.2017.2700722","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918552","Deep learning;convolutional neural network;information extraction;natural language processing;pathology reports;primary cancer site","","","","","","","","","","20170503","","","IEEE","IEEE Early Access Articles"
"Chest pathology detection using deep learning with non-medical training","Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan","The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","294","297","In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163871","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871","CNN;Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks","Biomedical imaging;Diagnostic radiography;Feature extraction;Machine learning;Pathology;Visualization;X-rays","convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets","AUC;CNN algorithm;CNN deep architecture classification;CNN learning;GIST feature;ImageNet;area under curve;chest X-ray image dataset;chest pathology detection;chest radiograph;convolutional neural network;deep learning;domain specific representation;general medical image recognition task;high level image representation learning;large scale nonmedical image database;mid level image representation learning;nonmedical learning;nonmedical training;pathology identification;pathology type","","10","","15","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Deep learning based Nucleus Classification in pancreas histological images","Y. H. Chang; G. Thibault; O. Madin; V. Azimi; C. Meyers; B. Johnson; J. Link; A. Margolin; J. W. Gray","Oregon Health and Science University (OHSU), Portland, United States of America","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","672","675","Tumor specimens contain a variety of healthy cells as well as cancerous cells, and this heterogeneity underlies resistance to various cancer therapies. But this problem has not been thoroughly investigated until recently. Meanwhile, technological breakthroughs in imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples, and modern machine learning approaches including deep learning have been shown to produce encouraging results by finding hidden structures and make accurate predictions. In this paper, we propose a Deep learning based Nucleus Classification (DeepNC) approach using paired histopathology and immunofluorescence images (for label), and demonstrate its classification prediction power. This method can solve current issue on discrepancy between genomic- or transcriptomic-based and pathology-based tumor purity estimates by improving histological evaluation. We also explain challenges in training a deep learning model for huge dataset.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8036914","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036914","Deep Learning;Histopathology;Immunofluorescence;Segmentation","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks","J. T. Kwak; S. M. Hewitt","Department of Computer Science and Engineering, Sejong University, Seoul, Korea 05006.","IEEE Access","","2017","PP","99","1","1","In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei were utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training dataset including 73 benign and 89 cancer samples and validated on the testing dataset comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison to the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value<5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.","","","10.1109/ACCESS.2017.2747838","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023758","artificial neural network;cancer detection;computer-aided diagnosis;microscopy;pattern recognition","Biological tissues;Kernel;Machine learning;Neurons;Pathology;Prostate cancer","","","","","","","","","20170831","","","IEEE","IEEE Early Access Articles"
"A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology","N. Kumar; R. Verma; S. Sharma; S. Bhargava; A. Vahadane; A. Sethi","IIT Guwahati, Guwahati, India","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1550","1560","Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.","0278-0062;02780062","","10.1109/TMI.2017.2677499","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382","Annotation;boundaries;dataset;deep learning;nuclear segmentation;nuclei","Diseases;Image color analysis;Image segmentation;Machine learning;Measurement;Pathology;Training","biological organs;biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;object recognition;optical microscopy","H&E-stained images;Otsu thresholding;chromatin-sparse;computational pathology;conventional image processing techniques;crowded nuclei;deep learning;digital microscopic tissue images;disease states;generalized nuclear segmentation;hematoxylin and eosin-stained tissue images;high-quality feature extraction;image classification problems;machine learning algorithms;machine learning-based segmentation;nuclear appearances;nuclear boundaries;nuclear morphometrics;object recognition;object-level errors;organs;overlapping nuclei;pixel-level errors;right out-of-the-box;segmentation technique;watershed segmentation","","","","","","","20170306","July 2017","","IEEE","IEEE Journals & Magazines"
"Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images","T. H. Song; V. Sanchez; H. EIDaly; N. M. Rajpoot","Department of Computer Science, University of Warwick, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1040","1043","Automated cell detection is a critical step for a number of computer-assisted pathology related image analysis algorithm. However, automated cell detection is complicated due to the variable cytomorphological and histological factors associated with each cell. In order to efficiently resolve the challenge of automated cell detection, deep learning strategies are widely applied and have recently been shown to be successful in histopathological images. In this paper, we concentrate on bone marrow trephine biopsy images and propose a hybrid deep autoencoder (HDA) network with Curvature Gaussian model for efficient and precise bone marrow hematopoietic stem cell detection via related high-level feature correspondence. The accuracy of our proposed method is up to 94%, outperforming other supervised and unsupervised detection approaches.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950694","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950694","Autoencoder;Bone marrow;Deep learning;Nuclei Detection","Biological system modeling;Biopsy;Bones;Decoding;Feature extraction;Shape;Training","Gaussian processes;bone;cellular biophysics;learning (artificial intelligence);medical image processing","automated cell detection;bone marrow hematopoietic stem cell detection;bone marrow trephine biopsy images;curvature Gaussian model;deep learning;hybrid deep autoencoder","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","76","84","Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","2168-2194;21682194","","10.1109/JBHI.2016.2636929","Bern University Hospital; 10.13039/501100001711 - Swiss National Science Foundation (SNSF); ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792","Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning","Biomedical imaging;Computed tomography;Databases;Knowledge engineering;Lungs;Machine learning;Training","biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets","CT images;computed tomography;computer-aided diagnosis;convolutional neural networks;fused knowledge compression;interstitial lung disease diagnosis;lung pattern analysis;lung tissue data;medical image analysis;multisource transfer learning;texture classification;texture databases","","","","","","","20161207","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Evaluation of feature descriptors for cancerous tissue recognition","P. Stanitsas; A. Cherian; Xinyan Li; A. Truskinovsky; V. Morellas; N. Papanikolopoulos","Department of Computer Science and Engineering, University of Minnesota, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1490","1495","Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899848","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848","","Cancer;Covariance matrices;Feature extraction;Geometry;Histograms;Image color analysis;Symmetric matrices","cancer;computer vision;feature extraction;image representation;image texture;medical image processing;neural nets;object recognition","CAD;CKD;RCD;breast carcinomas;cancerous tissue recognition;computer vision;computer-aided diagnosis;convolutional neural networks;covariance based descriptors;covariance kernel descriptor;feature descriptors;feature representation;image descriptors;myometrial leiomyosarcomas;pathologists;prostate carcinomas;region covariance descriptors;texture recognition","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images","Y. Huang; H. ZHENG; C. LIU; X. Ding; G. Rohde","Electrical Engineering Department and Biomedical Engineering Department, University of Virginia, VA, U.S.A.","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.","2168-2194;21682194","","10.1109/JBHI.2017.2691738","CCF-TENCENT; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893702","convolutional neural networks;domain adaptation;epitheliumstroma classification;histopathological image analysis;transfer learning","Adaptation models;Feature extraction;Image analysis;Kernel;Machine learning;Neural networks;Training","","","","","","","","","20170406","","","IEEE","IEEE Early Access Articles"
"Advanced deep learning for blood vessel segmentation in retinal fundus images","Lua Ngo; Jae-Ho Han","Dept. Brain and Cognitive Engineering, Korea University, Seoul, South Korea","2017 5th International Winter Conference on Brain-Computer Interface (BCI)","20170220","2017","","","91","92","Rising of deep learning methodologies draws huge attention to their application in image processing and classification. Catching up the trends, this study briefly presents state-of-the-art of deep learning applications in medical imaging interfered with achievements of blood vessel segmentation methods in neurosensory retinal fundus images. Successful segmentation based on deep learning offers advantage in diagnosing ophthalmological disease or pathology.","","Electronic:978-1-5090-5096-3; POD:978-1-5090-5097-0","10.1109/IWW-BCI.2017.7858169","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858169","Biomedical optical imaging;blood vessels;fundus images;image segmentation;medical image processing","","blood vessels;image segmentation;learning (artificial intelligence);medical image processing","advanced deep learning methodologies;blood vessel segmentation methods;image classification;image processing;ophthalmological disease;pathology;retinal fundus images","","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Customizing CNNs for blood vessel segmentation from fundus images","S. K. Vengalil; N. Sinha; S. S. S. Kruthiventi; R. V. Babu","International Institute of Information Technology, Bangalore, India","2016 International Conference on Signal Processing and Communications (SPCOM)","20161117","2016","","","1","4","For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 × 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.","","Electronic:978-1-5090-1746-1; POD:978-1-5090-1747-8","10.1109/SPCOM.2016.7746702","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746702","","Biomedical imaging;Blood vessels;Image resolution;Image segmentation;Pathology;Testing;Training","blood vessels;convolution;diseases;image colour analysis;image resolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets","CNN;DRIVE database;HRF databases;ROC curve;automatic screening;blood vessel network;blood vessel segmentation;convolutional neural network;deep learning framework;eye diseases;fundal images;fundus images;gray scale image;image resolution;semantic image segmentation","","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Automatic Lumbar Vertebrae Detection Based on Feature Fusion Deep Learning for Partial Occluded C-arm X-ray Images","Y. Li; W. Liang; Y. Zhang; H. An; J. Tan","Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","647","650","Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590785","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590785","","Computational modeling;Feature extraction;Kernel;Machine learning;Pathology;Surgery;X-ray imaging","computational complexity;computerised tomography;image fusion;image segmentation;image texture;learning (artificial intelligence);medical image processing;neural nets;prosthetics;surgery","CNN;DRR;FFDL;Gabor kernel;IG-MISS;ROI;abnormal pathological condition;automatic lumbar vertebrae detection;automatic segmentation;computational complexity;convolutional neural network model;feature fusion deep learning model;image-guided minimally invasive spine surgery;imaging angle;lumbar vertebrae X-ray images;lumbar vertebrae contour;lumbar vertebrae texture;partial occluded C-arm X-ray images;sobel kernel;surgical implants;training data","","1","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images","K. Sirinukunwattana; S. E. A. Raza; Y. W. Tsang; D. R. J. Snead; I. A. Cree; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, UK","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1196","1206","Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.","0278-0062;02780062","","10.1109/TMI.2016.2525803","10.13039/100008982 - Qatar National Research Fund; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399414","Convolutional neural network;deep learning;histology image analysis;nucleus detection","Cancer;Computer architecture;Feature extraction;Machine learning;Microprocessors;Shape;Tumors","biological organs;biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;probability;tumours","NEP;SC-CNN;cancerous tissue;cell nuclei classification;cell nuclei detection;cellular heterogeneity;colorectal adenocarcinoma images;dataset;eosin stain;high-probability values;highest average F1 score;histopathology images;joint detection;locality sensitive deep learning;neighboring ensemble predictor;quantitative analysis;routine colon cancer histology images;spatially constrained convolutional neural network;standard hematoxylin;tissue constituents;whole-slide images","","16","","38","","","20160204","May 2016","","IEEE","IEEE Journals & Magazines"
"Retinal vessel landmark detection using deep learning and hessian matrix","T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang","Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China","2015 8th International Congress on Image and Signal Processing (CISP)","20160218","2015","","","387","392","The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.","","Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2","10.1109/CISP.2015.7407910","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910","Deep learning;Hessian response;Image registration;Landmark detection;The probability map;The retinal image;Vessel Segmentation","Feature extraction;Image color analysis;Image registration;Image segmentation;Machine learning;Neural networks;Retina","Hessian matrices;eye;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery","Hessian matrix;bifurcation points;crossover points;curvature extreme points;deep learning;feature detection;image noise;multimodel retinal image;multiscale Hessian response;ophthalmological surgery;pathology;retinal image probability map;retinal image registration;retinal vessel landmark detection;vessel segmentation","","","","9","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"An Automatic Learning-Based Framework for Robust Nucleus Segmentation","F. Xing; Y. Xie; L. Yang","Department of Electrical and Computer Engineering, University of Florida, Gainesville","IEEE Transactions on Medical Imaging","20160202","2016","35","2","550","566","Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.","0278-0062;02780062","","10.1109/TMI.2015.2481436","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274740","Deep convolutional neural network;nucleus segmentation;sparse representation","Breast cancer;Computational modeling;Image color analysis;Image segmentation;Robustness;Shape;Tumors","cancer;diagnostic radiography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;probability;tumours","automated nucleus segmentation;automatic learning-based framework;automatic morphological feature computation;brain tumor;breast cancer;computer-aided image analysis;deep CNN model;deep convolutional neural network model;diseases;early detection;feature learning characteristic;high level shape prior modeling;histopathology imaging;histopathology specimens;iterative region merging approach;large-scale pathology image datasets;local repulsive deformable model;pancreatic neuroendocrine tumor;probability map;quantitative analysis;robust nucleus segmentation;robust selection-based sparse shape model;staining histopathology images;tissue","","7","","83","","","20150923","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images","J. Xu; L. Xiang; Q. Liu; H. Gilmore; J. Wu; J. Tang; A. Madabhushi","Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Medical Imaging","20160104","2016","35","1","119","130","Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.","0278-0062;02780062","","10.1109/TMI.2015.2458702","; 10.13039/100000062 - National Institute of Diabetes and Digestive and Kidney Diseases; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353","Automated nuclei detection;breast cancer histopathology;deep learning;digital pathology;feature representation learning;stacked sparse autoencoder","Breast cancer;Decoding;Feature extraction;Image color analysis;Pathology;Training","biological tissues;cancer;image classification;image coding;image representation;image resolution;learning (artificial intelligence);medical image processing","Nottingham histologic score system;automated grading;automated nuclear detection;average area under Precision-Recall curve;breast cancer tissue specimens;computer assisted pathology related image analysis algorithms;deep learning strategy;high resolution digitized pathology images;high-level features;high-resolution breast cancer histopathological images;image classifier;image patch representation;nuclei detection;pixel intensity;sliding window operation;stacked sparse autoencoder","","18","","43","","","20150720","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"A comparative study for chest radiograph image retrieval using binary texture and deep learning classification","Y. Anavi; I. Kogan; E. Gelbart; O. Geva; H. Greenspan","Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","2940","2943","In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the “descriptor”-based approach); the second approach (“classification”-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319008","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319008","","Biomedical imaging;Feature extraction;Heart;Machine learning;Measurement;Pathology;Support vector machines","diagnostic radiography;image classification;image retrieval;image texture;learning (artificial intelligence);medical image processing;probability;support vector machines","CNN;SVM classifier;X-ray image retrieval;binary features;binary texture;chest pathology retrieval;chest radiograph image retrieval;classification-based approach;decision values;deep learning classification;deep learning features;descriptor-based approach;image descriptors;pair-wise classification;probability descriptor;query image;texture features","","3","","12","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"General-pupose technology for a general-purpose nervous system","G. E. Loeb; J. Wills","Alfred Mann Institute for Biomedical Engineering, University of Southern California, Los Angeles, 90089-1112, USA","2008 IEEE International Symposium on Circuits and Systems","20080613","2008","","","340","343","The nervous system is a one-trick pony, using general- purpose neurons with the same basic signal transduction, transmission and integration mechanisms to handle essentially all information processing needs in the body: sensation and perception, posture and movement, autonomic and visceral function, memory and learning. Over the past fifty years, scientists and engineers have developed many different interfaces between neurons and electronic instrumentation in order to study how individual subsystems work and to fix some of them when they malfunction (e.g. pacemakers, cochlear implants, deep brain stimulators, etc.). While the various interfaces and their applications may look different, they are all based on strikingly similar, fundamental principles of biophysics, electrochemistry and information theory, and enabled by similar microfabrication and microelectronic technologies. Neural control is gradually converging on principles of design and best practices that can and should give rise to engineering standards and interchangeable components for recurring functions such as bioelectric recording and stimulation, transmission of power and data, and physical packaging and user interfaces. As such general tools become available, the clinical applications will be limited only by our understanding of the underlying pathologies, which are often best studied by those same tools. This virtuous circle consists of accessible technology enabling basic science enabling clinical applications generating business success motivating yet more technology.","0271-4302;02714302","CD-ROM:978-1-4244-1684-4; POD:978-1-4244-1683-7","10.1109/ISCAS.2008.4541424","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4541424","","Biophysics;Cochlear implants;Information processing;Information theory;Instruments;Microelectronics;Nervous system;Neurons;Pacemakers;Signal processing","neuromuscular stimulation","general-purpose nervous system;neural control","","1","","10","","","","18-21 May 2008","","IEEE","IEEE Conference Publications"
"Mixed Neural Network Approach for Temporal Sleep Stage Classification","H. Dong; A. Supratak; W. Pan; C. Wu; P. M. Matthews; Y. Guo","Department of Computing, Imperial College London, London, SW7 2AZ, UK.","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2017","PP","99","1","1","This paper proposes a practical approach to addressing limitations posed by using of single-channel electroencephalography (EEG) for sleep stage classification. EEG-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages at lower cost in the home. However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which are both uncomfortable and difficult to position. Previous studies of sleep stage scoring that used only frontal electrodes with a hierarchical decision tree motivated this paper, in which we have taken advantage of rectifier neural network for detecting hierarchical features and long short-term memory (LSTM) network for sequential data learning to optimize classification performance with single-channel recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electrooculogram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm than is available from existing approaches with vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical.","1534-4320;15344320","","10.1109/TNSRE.2017.2733220","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995122","Deep learning;EEG signal;Long short-term memory;Sleep stage classification;electroencephalography","Electrodes;Electroencephalography;Electrooculography;Feature extraction;Sleep;Standards","","","","","","","","","20170728","","","IEEE","IEEE Early Access Articles"
"Nuclei segmentation in histopathology images using deep neural networks","P. Naylor; M. Laé; F. Reyal; T. Walter","MINES ParisTech, PSL Research University, CBIO - Centre de Bioinformatique, 77300 Fontainebleau, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","933","936","Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950669","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950669","Breast Cancer;Cellular Phenotyping;Convolutional Neural Networks;Deep Learning;Digital Pathology;Histopathology;Nuclei Segmentation","Cancer;Computer architecture;Image segmentation;Machine learning;Microprocessors;Neural networks;Semantics","biomedical optical imaging;cancer;cellular biophysics;image segmentation;medical image processing;neural nets;probability;tumours","automatic segmentation;cancer diagnosis;cancer prognosis;cell nuclei;cellular phenotypes;complex image data;deep neural networks;digital pathology;fully automated workflow;histopathology image data;image data set;manually annotated images;nuclei segmentation;pathologists;posterior probability maps;scientific community;split jointly segmented nuclei;stained tumor sections;tissular phenotypes","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images","B. Ehteshami Bejnordi; J. Lin; B. Glass; M. Mullooly; G. L. Gierach; M. E. Sherman; N. Karssemeijer; J. van der Laak; A. H. Beck","Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, Netherlands","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","929","932","Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950668","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950668","Breast Cancer;Convolutional Neural Networks;Digital pathology;Tumor Associated Stroma","Breast cancer;Feature extraction;Training;Tumors","biomedical optical imaging;cancer;cellular biophysics;learning (artificial intelligence);mammography;medical image processing;patient diagnosis;tumours","breast cancer diagnosis;breast carcinomas diagnosis;breast tissue biopsies;convolutional neural networks;deep learning-based assessment;epithelial cells;epithelial regions;epithelial tissue architecture;hematoxyli-and-eosin stained breast specimens;histopathology images;tumor-associated stroma","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Wide residual networks for mitosis detection","E. Zerhouni; D. Lányi; M. Viana; M. Gabrani","IBM Research Zurich, S&#x00E4;umerstrasse 4, 8803 R&#x00FC;schlikon, Switzerland","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","924","928","One of the most important prognostic markers to assess proliferation activity of breast tumors is estimating the number of mitotic figures in H&E stained tissue. We propose the use of a recently published convolutional neural network architecture, Wide Residual Networks, for mitosis detection in breast histology images. The model is trained to classify each pixel of on an image using as context a patch centered on the pixel. We apply post-processing on the network output in order to filter out noise and select true mitosis. Finally, we combine the output of several networks using majority vote. Our approach ranked 2nd in the MICCAI TUPAC 2016 competition for mitosis detection, outperforming most other contestants by a significant margin.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950667","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950667","Mitotic activity;convolutional network;deep learning;tumor proliferation;wide-residual network","Image processing;Machine learning;Neural networks;Pathology;Shape;Testing;Training","cellular biophysics;diagnostic radiography;image classification;image filtering;mammography;medical image processing;neural nets;tumours","breast histology images;breast tumors;convolutional neural network architecture;hematoxylin-and-eosin-stained-tissue;image classification;image filtering;mitosis detection;mitotic figures;proliferation activity;wide residual networks","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Disease grading of heterogeneous tissue using convolutional autoencoder","E. Zerhouni; B. Prisacari; Q. Zhong; P. Wild; M. Gabrani","IBM Research-Z&#x00FC;rich, Saeumerstrasse 4, 8803 Rueschlikon, Switzerland","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","596","599","One of the main challenges of histological image analysis is the high dimensionality of the images. This can be addressed via summarizing techniques or feature engineering. However, such approaches can limit the performance of subsequent machine learning models, particularly when dealing with highly heterogeneous tissue samples. One possible alternative is to employ unsupervised learning to determine the most relevant features automatically. In this paper, we propose a method of generating representative image signatures that are robust to tissue heterogeneity. At the core of our approach lies a novel deep-learning based mechanism to simultaneously produce representative image features as well as perform dictionary learning to further reduce dimensionality. By integrating this mechanism in a broader framework for disease grading, we show significant improvement in terms of grading accuracy compared to alternative local feature extraction methods.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950591","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950591","Convolutional Autoencoder;Dimensionality Reduction;Image Description;Tissue Heterogeneity","Dictionaries;Diseases;Feature extraction;Image color analysis;Image reconstruction;Morphology;Training","biological tissues;convolutional codes;diseases;feature extraction;image coding;learning (artificial intelligence);medical image processing","convolutional autoencoder;deep-learning;dictionary learning;disease grading;feature engineering;feature extraction;heterogeneous tissue;histological image analysis;image features;machine learning;tissue heterogeneity;unsupervised learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"The importance of stain normalization in colorectal tissue classification with convolutional networks","F. Ciompi; O. Geessink; B. E. Bejnordi; G. S. de Souza; A. Baidoshvili; G. Litjens; B. van Ginneken; I. Nagtegaal; J. van der Laak","Dept. of Pathology, Radboud University Medical Center, Nijmegen, Netherlands","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","160","163","The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950492","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950492","Colorectal Cancer;Deep learning;Digital pathology","Algorithm design and analysis;Biomarkers;Blood;Cancer;Image color analysis;Training;Tumors","biological tissues;cancer;image classification;medical image processing;neural nets","ConvNets;colorectal tissue classification;convolutional networks;hematoxylin-eosin stained histopathology images;imaging biomarkers;stain normalization","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"On the impact of non-modal phonation on phonological features","M. Cernak; E. Nöth; F. Rudzicz; H. Christensen; J. R. Orozco-Arroyave; R. Arora; T. Bocklet; H. Chinaei; J. Hannink; P. S. Nidadavolu; J. C. Vásquez; M. Yancheva; A. Vann; N. Vogler","Idiap Research Institute, Switzerland","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","5090","5094","Different modes of vibration of the vocal folds contribute significantly to the voice quality. The neutral mode phonation, often used in a modal voice, is one against which the other modes can be contrastively described, also called non-modal phonations. This paper investigates the impact of non-modal phonation on phonological posteriors, the probabilities of phonological features inferred from the speech signal using a deep learning approach. Five different non-modal phonations are considered: falsetto, creaky, harshness, tense and breathiness. The impact of such non-modal phonation on phonological features, the Sound Patterns of English (SPE), is investigated in both speech analysis and synthesis tasks. We found that breathy and tense phonation impact the SPE features less, creaky phonation impacts the features moderately, and harsh and falsetto phonation impact the phonological features the most. We also report invariant and the most different SPE features impacted by non-modal phonation.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7953126","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953126","Phonological features;non-modal phonation;phonological vocoding","Cepstral analysis;Databases;Hidden Markov models;Pathology;Speech;Training;Vibrations","feature extraction;learning (artificial intelligence);speech processing;speech synthesis","SPE features;breathy phonation;creaky phonation;deep learning approach;falsetto phonation;harsh phonation;modal voice;neutral mode phonation;non-modal phonation;phonological features;sound patterns of English;speech analysis;speech signal;speech synthesis;tense phonation;vibration modes;voice quality","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Closed-Loop Modulation of the Pathological Disorders of the Basal Ganglia Network","C. Liu; J. Wang; H. Li; M. Lu; B. Deng; H. Yu; X. Wei; C. Fietkiewicz; K. A. Loparo","School of Electrical Engineering and Automation, Tianjin University, Tianjin, China","IEEE Transactions on Neural Networks and Learning Systems","20170520","2017","28","2","371","382","A generalized predictive closed-loop control strategy to improve the basal ganglia activity patterns in Parkinson's disease (PD) is explored in this paper. Based on system identification, an input-output model is established to reveal the relationship between external stimulation and neuronal responses. The model contributes to the implementation of the generalized predictive control (GPC) algorithm that generates the optimal stimulation waveform to modulate the activities of neuronal nuclei. By analyzing the roles of two critical control parameters within the GPC law, optimal closed-loop control that has the capability of restoring the normal relay reliability of the thalamus with the least stimulation energy expenditure can be achieved. In comparison with open-loop deep brain stimulation and traditional static control schemes, the generalized predictive closed-loop control strategy can optimize the stimulation waveform without requiring any particular knowledge of the physiological properties of the system. This type of closed-loop control strategy generates an adaptive stimulation waveform with low energy expenditure with the potential to improve the treatments for PD.","2162-237X;2162237X","","10.1109/TNNLS.2015.2508599","10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004543 - China Scholarship Council¿s Study Abroad Project; 10.13039/501100006606 - Natural Science Foundation of Tianjin, China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381666","Basal ganglia (BG) network;Parkinsonian state;energy expenditure;generalized predictive closed-loop control;system identification","Adaptation models;Basal ganglia;Computational modeling;Data models;Modulation;Neurons;Pathology","brain;closed loop systems;diseases;identification;medical disorders;neurophysiology;open loop systems;optimal control;patient treatment;predictive control","GPC law;PD treatment;Parkinson's disease;basal ganglia activity patterns;basal ganglia network;closed-loop modulation;external stimulation;generalized predictive closed-loop control;input-output model;least stimulation energy expenditure;neuronal nuclei;neuronal responses;normal relay reliability;open-loop deep brain stimulation;optimal closed-loop control;optimal stimulation waveform;pathological disorders;physiological properties;static control;system identification;thalamus","","1","","","","","20160113","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"DeepPap: Deep Convolutional Networks for Cervical Cell Classification","L. Zhang; L. Lu; I. Nogues; R. Summers; S. Liu; J. Yao","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA.(email:ling.zhang3@nih.gov)","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into ”abnormal” and ”normal” categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells – without prior segmentation – based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.","2168-2194;21682194","","10.1109/JBHI.2017.2705583","10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065","Cell classification;Cervical cytology;Deep learning;Neural networks;Pap smear","Feature extraction;Image segmentation;Imaging;Informatics;Neural networks;Testing;Training","","","","","","","","","20170519","","","IEEE","IEEE Early Access Articles"
"Deep learning for magnification independent breast cancer histopathology image classification","N. Bayramoglu; J. Kannala; J. Heikkilä","Center for Machine Vision and Signal Analysis, University of Oulu, Finland","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2440","2445","Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7900002","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900002","","Breast cancer;Databases;Microscopy;Pathology;Training;Training data","cancer;computer vision;image classification;learning (artificial intelligence);medical image processing;neural nets","BreaKHis dataset;breast tissues;computer vision;convolutional neural networks;deep learning;diagnostic pathology workflow;digital imaging techniques;image classification;machine learning;magnification independent breast cancer histopathology image classification;microscopic analysis;multitask CNN;single task CNN","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Evaluations of deep convolutional neural networks for automatic identification of malaria infected cells","Y. Dong; Z. Jiang; H. Shen; W. David Pan; L. A. Williams; V. V. B. Reddy; W. H. Benjamin; A. W. Bryan","Dept. of Electrical and Computer Engineering, University of Alabama in Huntsville, Huntsville, AL 35899, USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","101","104","This paper studied automatic identification of malaria infected cells using deep learning methods. We used whole slide images of thin blood stains to compile an dataset of malaria-infected red blood cells and non-infected cells, as labeled by a group of four pathologists. We evaluated three types of well-known convolutional neural networks, including the LeNet, AlexNet and GoogLeNet. Simulation results showed that all these deep convolution neural networks achieved classification accuracies of over 95%, higher than the accuracy of about 92% attainable by using the support vector machine method. Moreover, the deep learning methods have the advantage of being able to automatically learn the features from the input data, thereby requiring minimal inputs from human experts for automated malaria diagnosis.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897215","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897215","","Diseases;Feature extraction;Machine learning;Neural networks;Support vector machines;Testing;Training","biology computing;cellular biophysics;diseases;neural nets;support vector machines","AlexNet;GoogLeNet;cell automatic identification;deep convolutional neural networks;deep learning;malaria infected cells;support vector machine;thin blood stains;whole slide images","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Modeling and Analysis of Beta Oscillations in the Basal Ganglia","C. Liu; J. Wang; H. Li; C. Fietkiewicz; K. A. Loparo","School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072 China, the Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, Ohio, 44106 USA, and also with the Department of Physics, Hong Kong Baptist University, Hong Kong.","IEEE Transactions on Neural Networks and Learning Systems","","2017","PP","99","1","12","Enhanced beta (12-30 Hz) oscillatory activity in the basal ganglia (BG) is a prominent feature of the Parkinsonian state in animal models and in patients with Parkinson's disease. Increased beta oscillations are associated with severe dopaminergic striatal depletion. However, the mechanisms underlying these pathological beta oscillations remain elusive. Inspired by the experimental observation that only subsets of neurons within each nucleus in the BG exhibit oscillatory activities, a computational model of the BG-thalamus neuronal network is proposed, which is characterized by subdivided nuclei within the BG. Using different currents externally applied to the neurons within a given nucleus, neurons behave according to one of the two subgroups, named “-N” and “-P,” where “-N” and “-P” denote the normal and the Parkinsonian states, respectively. The ratio of “-P” to “-N” neurons indicates the degree of the Parkinsonian state. Simulation results show that if “-P” neurons have a high degree of connectivity in the subthalamic nucleus (STN), they will have a significant downstream effect on the generation of beta oscillations in the globus pallidus. Interestingly, however, the generation of beta oscillations in the STN is independent of the selection of the “-P” neurons in the external segment of the globus pallidus (GPe), despite the reciprocal structure between STN and GPe. This computational model may pave the way to revealing the mechanism of such pathological behaviors in a realistic way that can replicate experimental observations. The simulation results suggest that the STN is more suitable than GPe as a deep brain stimulation target.","2162-237X;2162237X","","10.1109/TNNLS.2017.2688426","Hong Kong Scholars Programs; National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898504","Basal ganglia (BG);Izhikevich model;beta oscillations;parkinsonian state;relay reliability.","Brain models;Computational modeling;Mathematical model;Neurons;Oscillators;Pathology","","","","","","","","","20170412","","","IEEE","IEEE Early Access Articles"
"Deep convolutional neural network for survival analysis with pathological images","X. Zhu; J. Yao; J. Huang","Department of Computer Science and Engineering, The University of Texas at Arlington, USA","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","544","547","Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822579","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822579","Deep learning;Lung cancer;Pathological images;Survival analysis","Analytical models;Data models;Feature extraction;Hazards;Lungs;Pathology;Predictive models","cancer;feature extraction;lung;medical image processing;neural nets","DeepConvSurv;NLST lung cancer data;cox proportional hazard model;deep convolutional neural network;hand-crafted feature;medical imaging technology;national lung screening trial;pathological image;survival analysis;survival prediction;unstructured medical image","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling","A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers","Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Image Processing","20161124","2017","26","1","386","399","Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.","1057-7149;10577149","","10.1109/TIP.2016.2624198","10.13039/100000098 - Intramural Research Program of the National Institutes of Health Clinical Center; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966","Abdominal computed tomography (CT);cascaded random forest;deep convolutional neural networks;dense image patch labeling;pancreas segmentation","Computed tomography;Image segmentation;Labeling;Liver;Pancreas;Shape","biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability","CT slice image decomposition;abdominal computed tomography scans;bottom-up approach;cascaded superpixels;computer-aided diagnosis;deep convolutional neural network classification;deep image patch labeling;dense patch labeling;disjoint boundary-preserving superpixels;image histogram;image location;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach;pancreas class probability maps;pancreas segmentation;pathology detection;probability features;quantitative imaging analysis;random forest classification;superpixel classification;surgical assistance;texture features","","","","","","","20161101","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Segmenting Retinal Blood Vessels With Deep Neural Networks","P. Liskowski; K. Krawiec","Institute of Computing Science, Poznan University of Technology, Poland","IEEE Transactions on Medical Imaging","20161103","2016","35","11","2369","2380","The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.","0278-0062;02780062","","10.1109/TMI.2016.2546227","10.13039/501100005632 - Narodowe Centrum Bada? i Rozwoju; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440871","Classification;deep learning;feature learning;fundus;neural networks;retina;retinopathy;structured prediction;vessel segmentation","Biomedical imaging;Blood vessels;Convolution;Databases;Image segmentation;Neural networks;Pathology","blood vessels;eye;image classification;image segmentation;medical image processing;neural nets;sensitivity analysis;unsupervised learning","CHASE databases;DRIVE databases;ROC curve;STARE databases;central vessel reflex;deep neural networks;diagnostic factor;fundus imaging;gamma corrections;geometric transformations;global contrast normalization;hemorrhages;human eye;image classification;microaneurysms;nontrivial task;ophthalmology;retinal blood vessel segmentation;structured prediction;supervised segmentation;vascular network;zero-phase whitening","","7","","","","","20160324","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography","A. Lahiri; A. G. Roy; D. Sheet; P. K. Biswas","Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology Kharagpur, India","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","1340","1343","Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590955","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590955","","Biomedical imaging;Feature extraction;Image segmentation;Kernel;Retinal vessels;Standards;Training","biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;feature extraction;image classification;image coding;image denoising;image segmentation;medical image processing;unsupervised learning","2-level fusion;DRIVE dataset;Kappa agreement coefficient;SoftMax classifier;architectural revision;autoencoders fosters diversity;automated segmentation;bootstrap samples;cardiovascular diseases;classification task;computed aided diagnosis;deep neural ensemble;diabetic retinopathy;first level training;hypertensive disorders;label-free angiography;label-free fundus images;learning dictionary;maximum average accuracy;medical image analysis;network architectures;noisy background;ophthalmic pathologies;physical appearance;retinal blood vessels;retinal vessel segmentation;sparsely trained denoised stacked autoencoder;standard deviation;unsupervised hierarchical feature learning;visual kernels","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation","A. Mansoor; J. J. Cerrolaza; R. Idrees; E. Biggs; M. A. Alsharid; R. A. Avery; M. G. Linguraru","Children's National Health System, Washington","IEEE Transactions on Medical Imaging","20160729","2016","35","8","1856","1865","Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.","0278-0062;02780062","","10.1109/TMI.2016.2535222","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420737","Anterior visual pathway;MRI;intensity normalization;partitioned statistical model;shape model;sparse learning","Biomedical optical imaging;Machine learning;Magnetic resonance imaging;Optical imaging;Pathology;Robustness;Shape","biomedical MRI;curvelet transforms;eye;image segmentation;image sequences;medical image processing;paediatrics;statistical analysis","Dice similarity coefficient;MRI sequences;anatomic structures;anterior visual pathway segmentation;conditional space deep learning;cranial nerve systems;deep learning feature representation;deep learning guided partitioned shape model;joint partitioned statistical shape model;low-grade gliomas;optic nerve;pathologic AVP segmentation;pediatric subjects;robust statistical model;shape localization method;sparse appearance-based learning approach;volumetric multiscale curvelet transform-based intensity normalization method","","3","","","","","20160226","Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Automated mitosis detection with deep regression networks","H. Chen; X. Wang; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1204","1207","Mitosis counting is one of the strongest prognostic markers for invasive breast cancer diagnosis. Clinical visual examination on histology slides by pathologists is tedious, error-prone and time-consuming. Furthermore, with the advent of whole slide imaging for high-throughput digitization, a large quantity of histology images need to be analyzed. Therefore, automated mitosis detection methods are highly demanded in clinical practice. In this paper, we proposed a deep regression network (DRN) to meet these challenges. It consisted of a downsampling path for extracting the high level information and an upsampling path for outputting the score map with original input size, thus it can be trained in an end-to-end way. In addition, we transferred knowledge learned from cross domains to mitigate the issue of insufficient medical training data. Experimental results on the benchmark dataset 2012ICPR Mitosis Detection Challenge demonstrated the efficacy of our approach, which achieved comparable or better performance than the state-of-the-art methods.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493482","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493482","Mitosis detection;convolutional neural network;deep learning;regression","Benchmark testing;Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Pathology;Training","biological tissues;cancer;learning (artificial intelligence);medical image processing;regression analysis","automated mitosis detection;benchmark dataset 2012 ICPR Mitosis Detection Challenge;clinical visual examination;deep regression network;deep regression networks;downsampling path;high level information;high-throughput digitization;histology imaging;histology slides;invasive breast cancer diagnosis;medical training data;prognostic markers;upsampling path;whole slide imaging","","","","20","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Handcrafted features with convolutional neural networks for detection of tumor cells in histology images","M. N. Kashif; S. E. A. Raza; K. Sirinukunwattana; M. Arif; N. Rajpoot","Department of Electrical Engineering, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1029","1032","Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493441","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493441","Convolutional Neural Network;Digital Pathology;Scattering Transform;Tumor Nuclei Detection","Feature extraction;Image color analysis;Neural networks;Scattering;Standards;Transforms;Tumors","cellular biophysics;feature extraction;medical image processing;neural nets;tumours","cancer histology images;handcrafted features;nonlinear invariant texture features;scattering transform;spatially constrained convolutional neural networks;tumor cell detection","","","","13","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Retinal vessel segmentation via deep learning network and fully-connected conditional random fields","H. Fu; Y. Xu; D. W. K. Wong; J. Liu","Ocular Imaging Department, Institute for Infocomm Research, Agency for Science, Technology and Research (A&#8727;STAR), Singapore","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","698","701","Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493362","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493362","Conditional Random Fields;Convolutional Neural Networks;Vessel segmentation","Computer architecture;Image segmentation;Machine learning;Neural networks;Pathology;Retinal vessels","blood vessels;eye;image segmentation;medical image processing;neural nets","DRIVE dataset;STARE dataset;binary vessel segmentation;boundary detection;convolutional neural networks;deep learning network;fully-connected conditional random fields;fundus image;pathological region;retinal vessel segmentation;vessel probability map","","3","","18","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","20160503","2016","35","5","1170","1181","Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","0278-0062;02780062","","10.1109/TMI.2015.2482920","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156","Computer aided diagnosis;artificial neural networks;computed tomography;deep learning;machine learning;medical diagnostic imaging;multi-layer neural network;object detection","Colonic polyps;Computed tomography;Feature extraction;Lymph nodes;Three-dimensional displays;Training","computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","classification probability;colonic polyp detection;computed tomography;computer-aided detection;deep convolutional neural network classifier training;false positives;lymph node detection;medical imaging;random rotations;random translations;random view aggregation;scale transformations;sclerotic metastasis detection;two-tiered coarse-to-fine cascade framework","","11","","60","","","20150928","May 2016","","IEEE","IEEE Journals & Magazines"
"Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation","Y. Xu; Z. Jia; Y. Ai; F. Zhang; M. Lai; E. I. C. Chang","Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beihang University, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","947","951","We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178109","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109","classification;deep convolutional activation features;deep learning;feature learning;segmentation","Biomedical imaging;Feature extraction;Image segmentation;Support vector machines;Training;Training data;Tumors","brain;feature extraction;image classification;image segmentation;medical image processing;tumours","CNN activations;ImageNet;MICCAI 2014 Brain Tumor Digital Pathology Challenge;brain tumor histopathology;deep convolutional activation features;features extraction;image classification;image dimensions;image segmentation","","3","","23","","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Deep learning of feature representation with multiple instance learning for medical image analysis","Y. Xu; T. Mo; Q. Feng; P. Zhong; M. Lai; E. I. C. Chang","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20140714","2014","","","1626","1630","This paper studies the effectiveness of accomplishing high-level tasks with a minimum of manual annotation and good feature representations for medical images. In medical image analysis, objects like cells are characterized by significant clinical features. Previously developed features like SIFT and HARR are unable to comprehensively represent such objects. Therefore, feature representation is especially important. In this paper, we study automatic extraction of feature representation through deep learning (DNN). Furthermore, detailed annotation of objects is often an ambiguous and challenging task. We use multiple instance learning (MIL) framework in classification training with deep learning features. Several interesting conclusions can be drawn from our work: (1) automatic feature learning outperforms manual feature; (2) the unsupervised approach can achieve performance that's close to fully supervised approach (93.56%) vs. (94.52%); and (3) the MIL performance of coarse label (96.30%) outweighs the supervised performance of fine label (95.40%) in supervised deep learning features.","1520-6149;15206149","Electronic:978-1-4799-2893-4; POD:978-1-4799-2894-1; USB:978-1-4799-2892-7","10.1109/ICASSP.2014.6853873","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853873","deep learning;feature learning;multiple instance learning;supervised;un-supervised","Biomedical imaging;Cancer;Feature extraction;Manuals;Supervised learning;Training;Vectors","feature extraction;image representation;learning (artificial intelligence);medical image processing","DNN;HARR features;MIL framework;SIFT features;automatic feature representation extraction;classification training;clinical features;feature representation;manual annotation;medical image analysis;multiple instance learning;supervised deep learning features;unsupervised approach","","14","","26","","","","4-9 May 2014","","IEEE","IEEE Conference Publications"
"System for processing and simulation of brain signals","G. J. E. Duque; P. A. Munera; C. D. Trujillo; H. D. A. Urrego; V. A. M. Hernandez","Grupo de Investigaci&#243;n en Bioelectr&#243;nica e Ingenier&#237;a Cl&#237;nica (GIBIC), Programa de Bioingenier&#237;a - Universidad de Antioquia (UdeA), Medell&#237;n, Colombia","2009 IEEE Latin-American Conference on Communications","20091030","2009","","","1","6","Processing and simulation of brain signals from stochastic models have allowed going deep into the study of brain function during sleeping and pathological situations, it had also facilitated the quantification and assessment of the effect of different drugs. In order to make well know with this type of signals is necessary to have highly specialized and expensive equipments, it makes difficult the training and learning processes of medical helper and biomedical engineers. Virtual laboratories have been used successfully in learning processes in a biomedical engineering field because of their educational benefits and their low economic cost. This paper presents a virtual laboratory developed in Matlabreg, which facilitates the understanding and usefulness of the techniques to processing signals from brain origin. To implement the application, EEG signals have been modelled and real intracerebral and cortical signals recorded during surgical procedures also have been used. Besides the application allows to assess characteristics of brain signals, it allows also to evaluate the usefulness of processing techniques used in clinical practice.","2330-989X;2330989X","POD:978-1-4244-4387-1","10.1109/LATINCOM.2009.5304853","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304853","","Biomedical engineering;Brain modeling;Computer languages;Costs;Drugs;Laboratories;Mathematical model;Pathology;Signal processing;Stochastic processes","biomedical education;brain;computer aided instruction;digital simulation;drugs;mathematics computing;medical signal processing;stochastic processes;virtual reality","Matlab;biomedical engineering field;brain signal processing;digital simulation;drug effect assessment;educational benefit;learning process;medical helper;pathological situation;stochastic model;virtual laboratory","","3","","14","","","","10-11 Sept. 2009","","IEEE","IEEE Conference Publications"
