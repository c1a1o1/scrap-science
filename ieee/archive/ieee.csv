,Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
0,Multi-Scale Rotation-Invariant Convolutional Neural Networks for Lung Texture Classification,Q. Wang; Y. Zheng; g. yang; W. Jin; X. Chen; y. yin,"School of Computer Science and Technology, Shandong University, Jinan 250101, China.(email:shdyn2000@mail.sdu.edu.cn)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"We propose a new Multi-scale Rotation-invariant Convolutional Neural Network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography (HRCT). MRCNN employs Gabor-local binary pattern (Gabor-LBP) which introduces a good property in image analysis - invariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public Interstitial Lung Disease (ILD) database show a superior performance of the proposed method to state-of-the-art.",2168-2194;21682194,,10.1109/JBHI.2017.2685586,NSFC Joint Fund with Guangdong under Key Project; National Natural Science Foundation of China under Grant; the Fostering Project of Dominant Discipline and Talent Team of Shandong Province Higher Education Institutions; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883849,Gabor filter;Interstitial Lung Disease (ILD) classification;convolutional neural network (CNN);local binary pattern (LBP);lung classification,Biomedical imaging;Computed tomography;Feature extraction;Informatics;Lungs;Neural networks;Support vector machines,,,,,,,,,20170321.0,,,IEEE,IEEE Early Access Articles
1,Adrenal lesions detection on low-contrast CT images using fully convolutional networks with multi-scale integration,L. Bi; J. Kim; T. Su; M. Fulham; D. Feng; G. Ning,"School of Information Technologies, University of Sydney, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,895,898,"Adrenal lesions include a wide variety of benign and malignant neoplasms of the adrenal gland, and are seen in up to 5% of computed tomography (CT) examinations of the abdomen. Better identification of these lesions is important for effective management and patient prognosis. Detection on low-contrast CT images, however, even for experienced physicians can be difficult and error-prone, because the lesions are often problematic to be separated from the normal surrounding structures. Existing lesion detection techniques have problems in identifying and differentiating low-contrast tumors, which is related to their use of low-level features rather than high level of semantics. Hence we propose an automated approach using fully convolutional networks (FCNs) and multi-scale integration to detect adrenal lesion on low-contrast CT scans. The architecture of FCNs includes deep, coarse, semantic information and shallow, fine, appearance information in a hierarchical manner and it enables the encoding of image-wide location and semantics, which are desirable characteristics for adrenal lesion detection. We also propose a multi-scale integration with a superpixel based random walk (MI-SRW) approach to refine the lesion boundaries on different scales. The MI-SRW technique enables us to constrain the spatial and appearance consistency and then use complementary information provided on different scales to detect adrenal lesions of various sizes and characteristics. We used 38 adrenal lesions detected on low-contrast CT and compared our approach to existing `state-of-the-art' methods and found that our approach had superior detection performance.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950660,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950660,Adrenal lesions;Detection;Fully Convolutional Networks (FCN),Biomedical imaging;Computed tomography;Feature extraction;Lesions;Semantics;Support vector machines,biological organs;cancer;computerised tomography;feature extraction;image coding;medical image processing;tumours,MI-SRW approach;adrenal gland;adrenal lesion;adrenal lesion detection;adrenal lesions detection;appearance consistency;benign neoplasms;complementary information;computed tomography examinations;deep coarse semantic information;fully convolutional networks;hierarchical manner;image-wide location encoding;lesion boundaries;lesion detection techniques;low-contrast CT image detection;low-contrast CT images;low-contrast CT scans;low-contrast tumors;low-level features;malignant neoplasms;multiscale integration;normal surrounding structures;patient prognosis;semantics encoding;shallow fine appearance information;spatial consistency;superpixel based random walk,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
2,Anatomy-specific classification of medical images using deep convolutional nets,H. R. Roth; C. T. Lee; H. C. Shin; A. Seff; L. Kim; J. Yao; L. Lu; R. M. Summers,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,101,104,"Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. “Deep learning” methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163826,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163826,Computed tomography (CT);Convolutional Networks;Deep Learning;Image Classification,Computed tomography;Convolution;Lungs;Medical diagnostic imaging;Neural networks;Training,PACS;biological organs;computerised tomography;image classification;medical image processing,ConvNets;anatomy variability;anatomy-specific classification;anatomy-specific classification error;area-under-the-curve;automated classification;axial 2D key-images;body part-specific anatomical classification;computed tomography;computer-aided diagnosis systems;convolutional networks;data augmentation;data augmentation approach;deep convolutional nets;deep learning methods;hospital PACS archive;human anatomy;image classification;medical images;organ-specific anatomical classification;spatial complexity,,5.0,,16.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
3,Deep Convolutional Neural Network for Inverse Problems in Imaging,K. H. Jin; M. T. McCann; E. Froustey; M. Unser,"Biomedical Imaging Group, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Lausanne, Switzerland",IEEE Transactions on Image Processing,20170711.0,2017,26,9.0,4509,4522,"In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.",1057-7149;10577149,,10.1109/TIP.2017.2713099,10.13039/100000070 - National Institute of Biomedical Imaging and Bioengineering; 10.13039/100010661 - European Union¿¿¿s Horizon 2020 Framework Programme for Research and Innovation (call 2015); 10.13039/501100000781 - European Research Council (H2020-ERC Project GlobalBioIm); 10.13039/501100006391 - Center for Biomedical Imaging of the Geneva-Lausanne Universities and EPFL; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949028,Image restoration;biomedical imaging;biomedical signal processing;computed tomography;image reconstruction;magnetic resonance imaging;reconstruction algorithms;tomography,Computed tomography;Convolution;Image reconstruction;Inverse problems;Iterative methods;Neural networks,computerised tomography;feedforward neural nets;image resolution;iterative methods;learning (artificial intelligence);medical image processing,CNN;GPU;adjoint operators;deep convolutional neural network;direct inversion;forward model;forward operators;hyperparameter selection;ill-posed inverse problems;image structure;multiresolution decomposition;normal-convolutional inverse problems;parallel beam X-ray computed tomography;regularized iterative algorithms;residual learning;synthetic phantoms;total variation-regularized iterative reconstruction,,,,,,,20170615.0,Sept. 2017,,IEEE,IEEE Journals & Magazines
4,Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?,N. Tajbakhsh; J. Y. Shin; S. R. Gurudu; R. T. Hurst; C. B. Kendall; M. B. Gotway; J. Liang,"Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1299,1312,"Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.",0278-0062;02780062,,10.1109/TMI.2016.2535302,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426826,Carotid intima-media thickness;computer-aided detection;convolutional neural networks;deep learning;fine-tuning;medical image analysis;polyp detection;pulmonary embolism detection;video quality assessment,Biomedical imaging;Computed tomography;Feature extraction;Image analysis;Image segmentation;Training;Tuning,biomedical optical imaging;endoscopes;image classification;image segmentation;medical image processing;neural nets,cardiology;classification;deep convolutional neural network;distinct medical imaging applications;gastroenterology;imaging modalities;labeled training data;layer-wise fine-tuning scheme;medical image analysis;radiology;segmentation,,34.0,,76.0,,,20160307.0,May 2016,,IEEE,IEEE Journals & Magazines
5,Size and Texture-Based Classification of Lung Tumors with 3D CNNs,Z. Luo; M. A. Brubaker; M. Brudno,,2017 IEEE Winter Conference on Applications of Computer Vision (WACV),20170515.0,2017,,,806,814,"In this paper, we explore the use of current deep learning methods in the field of computer-aided diagnosis (CAD). Specifically we propose the use of 3D convolutional neural nets (CNN) in classifying lung nodules based off of their appearance in CT scans. We explore the choices of network architectures, learning parameters and problem formulations. Comparing these results to other methods we show that the proposed method has close to perfect performance on the publicly available LIDC dataset, achieving an AUC of 0:9685 and a false positive rate of 0:46% with a true positive rate of 90% where the ground truth is the expert opinion of a radiologist.",,Electronic:978-1-5090-4822-9; POD:978-1-5090-4823-6,10.1109/WACV.2017.95,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926678,,Biological neural networks;Cancer;Computed tomography;Lungs;Three-dimensional displays;Tumors,,,,,,,,,,24-31 March 2017,,IEEE,IEEE Conference Publications
6,Lung nodule detection in CT images using deep convolutional neural networks,R. Golan; C. Jacob; J. Denzinger,"Dept. of Computer Science, University of Calgary, Alberta, Canada T2N 1N4",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,243,250,"Early detection of lung nodules in thoracic Computed Tomography (CT) scans is of great importance for the successful diagnosis and treatment of lung cancer. Due to improvements in screening technologies, and an increased demand for their use, radiologists are required to analyze an ever increasing amount of image data, which can affect the quality of their diagnoses. Computer-Aided Detection (CADe) systems are designed to assist radiologists in this endeavor. Here, we present a CADe system for the detection of lung nodules in thoracic CT images. Our system is based on (1) the publicly available Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) database, which contains 1018 thoracic CT scans with nodules of different shape and size, and (2) a deep Convolutional Neural Network (CNN), which is trained, using the back-propagation algorithm, to extract valuable volumetric features from the input data and detect lung nodules in sub-volumes of CT images. Considering only those test nodules that have been annotated by four radiologists, our CADe system achieves a sensitivity (true positive rate) of 78.9% with 20 false positives (FPs) per scan, or a sensitivity of 71.2% with 10 FPs per scan. This is achieved without using any segmentation or additional FP reduction procedures, both of which are commonly used in other CADe systems. Furthermore, our CADe system is validated on a larger number of lung nodules compared to other studies, which increases the variation in their appearance, and therefore, makes their detection by a CADe system more challenging.",,,10.1109/IJCNN.2016.7727205,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727205,,Biological neural networks;Biomedical imaging;Cancer;Computed tomography;Image databases;Lungs,backpropagation;cancer;computerised tomography;feature extraction;medical image processing;neural nets;object detection;patient diagnosis;patient treatment,CADe systems;CNN;IDRI database;LIDC database;backpropagation algorithm;computer-aided detection system;deep convolutional neural network;deep convolutional neural networks;image data;image database resource initiative database;lung cancer diagnosis;lung cancer treatment;lung image database consortium database;lung nodule detection;screening technology;thoracic CT images;thoracic computed tomography scans;valuable volumetric feature extraction,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
7,Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN),H. Chen; Y. Zhang; M. K. Kalra; F. Lin; Y. Chen; P. Liao; J. Zhou; G. Wang,"College of Computer Science, Sichuan University, Chengdu 610065, China.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.",0278-0062;02780062,,10.1109/TMI.2017.2715284,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947200,Low-dose CT;auto-encoder;convolutional;deconvolutional;deep learning;residual neural network,Computed tomography;Convolution;Decoding;Feature extraction;Image reconstruction;X-ray imaging,,,,,,,,,20170613.0,,,IEEE,IEEE Early Access Articles
8,Automatic segmentation of the left ventricle in cardiac CT angiography using convolutional neural networks,M. Zreik; T. Leiner; B. D. de Vos; R. W. van Hamersvelt; M. A. Viergever; I. Išgum,"Image Sciences Institute, University Medical Center Utrecht, The Netherlands",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,40,43,"Accurate delineation of the left ventricle (LV) is an important step in evaluation of cardiac function. In this paper, we present an automatic method for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation is performed in two stages. First, a bounding box around the LV is detected using a combination of three convolutional neural networks (CNNs). Subsequently, to obtain the segmentation of the LV, voxel classification is performed within the defined bounding box using a CNN. The study included CCTA scans of sixty patients, fifty scans were used to train the CNNs for the LV localization, five scans were used to train LV segmentation and the remaining five scans were used for testing the method. Automatic segmentation resulted in the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1 mm. The results demonstrate that automatic segmentation of the LV in CCTA scans using voxel classification with convolutional neural networks is feasible.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493206,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493206,Cardiac CT Angiography;Classification;Convolutional Neural Network;Deep learning;Left ventricle segmentation,Biomedical imaging;Computed tomography;Heart;Image segmentation;Manuals;Neural networks;Observers,,,,,,16.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
9,Classification of radiology reports using neural attention models,B. Shin; F. H. Chokshi; T. Lee; J. D. Choi,"Mathematics and Computer Science, Emory University, Atlanta, GA 30322",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,4363,4370,"The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure “black-box” models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7966408,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966408,,Computational modeling;Convolution;Machine learning;Neural networks;Radiology;Sentiment analysis;Support vector machines,computerised tomography;convolution;data analysis;electronic health records;neural nets;pattern classification;radiology,CNN;EHR;attention analysis;convolutional neural networks;double-annotated dataset;electronic health record;neural attention mechanism;neural attention models;radiology head computed tomography reports classification,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
10,TumorNet: Lung nodule characterization using multi-view Convolutional Neural Network with Gaussian Process,S. Hussein; R. Gillies; K. Cao; Q. Song; U. Bagci,"Center for Research in Computer Vision (CRCV) at University of Central Florida, Orlando, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1007,1010,"Characterization of lung nodules as benign or malignant is one of the most important tasks in lung cancer diagnosis, staging and treatment planning. While the variation in the appearance of the nodules remains large, there is a need for a fast and robust computer aided system. In this work, we propose an end-to-end trainable multi-view deep Convolutional Neural Network (CNN) for nodule characterization. First, we use median intensity projection to obtain a 2D patch corresponding to each dimension. The three images are then concatenated to form a tensor, where the images serve as different channels of the input image. In order to increase the number of training samples, we perform data augmentation by scaling, rotating and adding noise to the input image. The trained network is used to extract features from the input image followed by a Gaussian Process (GP) regression to obtain the malignancy score. We also empirically establish the significance of different high level nodule attributes such as calcification, sphericity and others for malignancy determination. These attributes are found to be complementary to the deep multi-view CNN features and a significant improvement over other methods is obtained.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950686,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950686,Computer-aided diagnosis;computed tomography;deep learning;lung cancer;pulmonary nodule,Cancer;Computed tomography;Feature extraction;Lungs;Neural networks;Training,Gaussian processes;computerised tomography;feature extraction;neural nets;tumours,Gaussian process;TumorNet;calcification;computed tomography;feature extraction;lung nodule characterization;malignancy determination;multiview convolutional neural network;sphericity,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
11,Ischemic stroke identification based on EEG and EOG using ID convolutional neural network and batch normalization,E. P. Giri; M. I. Fanany; A. M. Arymurthy; S. K. Wijaya,"Computer Sciences Department, Faculty of Mathematics and Natural Sciences, Bogor Agricultural University, Bogor 16680, West Java, Indonesia",2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS),20170309.0,2016,,,484,491,"In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of ID Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72). The feature used in our study were only 24 handcrafted feature with simple feature extraction process.",,Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7,10.1109/ICACSIS.2016.7872780,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872780,EEG;ID CNN;deep learning;ischemic;stroke,Brain modeling;Computed tomography;Convolution;Electroencephalography;Electrooculography;Feature extraction;Hospitals,blood;convolution;electro-oculography;electroencephalography;feature extraction;medical signal processing;neural nets;patient diagnosis,1D convolutional neural network;1DCNN;CBF;CT-Scan;EEG;EOG;batch normalization;cerebral blood flow;electroencephalography;electrooculography;feature extraction;ischemic stroke identification;stroke diagnosis,,,,,,,,15-16 Oct. 2016,,IEEE,IEEE Conference Publications
12,Lung nodule detection in CT using 3D convolutional neural networks,X. Huang; J. Shan; V. Vaidya,"GE Global Research, Niskayuna, NY, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,379,383,"We propose a new computer-aided detection system that uses 3D convolutional neural networks (CNN) for detecting lung nodules in low dose computed tomography. The system leverages both a priori knowledge about lung nodules and confounding anatomical structures and data-driven machine-learned features and classifier. Specifically, we generate nodule candidates using a local geometric-model-based filter and further reduce the structure variability by estimating the local orientation. The nodule candidates in the form of 3D cubes are fed into a deep 3D convolutional neural network that is trained to differentiate nodule and non-nodule inputs. We use data augmentation techniques to generate a large number of training examples and apply regularization to avoid overfitting. On a set of 99 CT scans, the proposed system achieved state-of-the-art performance and significantly outperformed a similar hybrid system that uses conventional shallow learning. The experimental results showed benefits of using a priori models to reduce the problem space for data-driven machine learning of complex deep neural networks. The results also showed the advantages of 3D CNN over 2D CNN in volumetric medical image analysis.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950542,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950542,3D convolutional neural networks;CT;Lung nodule;computer-aided detection;deep learning,Computed tomography;Lungs;Neural networks;Solid modeling;Three-dimensional displays;Training;Two dimensional displays,computerised tomography;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets,3D CNN;CT scans;complex deep neural networks;computer-aided detection;conventional shallow learning;data augmentation;data-driven machine-learned classifier;data-driven machine-learned features;deep 3D convolutional neural networks;local geometric-model-based filter;low dose computed tomography;lung nodule detection;structure variability;volumetric medical image analysis,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
13,Combining deep neural network and traditional image features to improve survival prediction accuracy for lung cancer patients from diagnostic CT,R. Paul; S. H. Hawkins; L. O. Hall; D. B. Goldgof; R. J. Gillies,"Department of Computer Science and Engineering, University of South Florida, Tampa, USA","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",20170209.0,2016,,,2570,2575,"Lung cancer is caused by abnormal and uncontrolled growth of cells in the lungs and the mortality rate of lung cancer is the highest among all types of cancer. It can be identified and treated with the help of computed tomography (CT) images. For an automated classifier, identifying good features from an image is a key concern. Deep feature extraction using pre-trained convolutional neural networks has been successful for some image domains recently. In our study, we apply a pre-trained convolutional neural network (CNN) to extract deep features from lung cancer CT images and then train classifiers to predict short and long term survivors. The best accuracy of 77.5% was with a cropping approach using a decision tree classifier in a leave one out cross validation with ten features chosen using symmetric uncertainty feature ranking. We mixed extracted deep neural network features along with quantitative (traditional image) features and obtained the best accuracy of 82.5% with a nearest neighbor classifier in a leave one out cross validation using the symmetric uncertainty feature ranking algorithm.",,Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2,10.1109/SMC.2016.7844626,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844626,CNN;Radiomics;computed tomography;deep features,Cancer;Classification algorithms;Computed tomography;Feature extraction;Lungs;Neural networks;Tumors,cancer;computerised tomography;convolution;decision trees;image classification;learning (artificial intelligence);lung;patient diagnosis,automated classifier;classifier training;computed tomography images;decision tree classifier;deep feature extraction;deep neural network;diagnostic CT;lung cancer patients;nearest neighbor classifier;pretrained convolutional neural networks;quantitative image features;survival prediction accuracy;symmetric uncertainty feature ranking,,,,,,,,9-12 Oct. 2016,,IEEE,IEEE Conference Publications
14,Application of big data analytics for automated estimation of CT image quality,M. D. Naeemi; J. Ren; N. Hollcroft; A. M. Alessio; S. Roychowdhury,"Department of Electrical Engineering, University of Washington, Bothell WA",2016 IEEE International Conference on Big Data (Big Data),20170206.0,2016,,,3422,3431,"With the increasing applications of Big Data analytics in medical image processing systems, there has been a growing need for quantitative medical image quality assessment techniques. Specifically for computed tomography (CT) images, quantitative image assessment can allow for benchmarking image processing methods and optimization of image acquisition parameters. In this work, large volumes of CT images from phantoms and patients are analyzed using 3 data models that vary in their implementation time complexities. The goal here is to identify the optimal method that scales across data set variabilities for predictive modeling of CT image quality (CTIQ). The first two models rely on spatial segmentation of regions-of-interest (ROIs) and estimate CTIQs in terms of segmented pixel variabilities. The third, convolutional neural network (CNN) model relies on error back-propagation from the training set of images to learn the regions indicative of CTIQ. We observe that for 70/30 data split, the average multi-class classification accuracies for CTIQ prediction using the 3 data models range from 73.6-100% and 50-100% for the phantom and patient CT images, respectively. Using variance of pixels within the segmented ROIs as a CTIQ classification parameter, the spatial segmentation data models are found to be more generalizable that the CNN model. However, the CNN model is found to be more suitable for CT image texture classification in the absence of structural variabilities. Our analysis demonstrates that spatial ROI segmentation data models are consistent CTIQ estimators while the CNN models are consistent identifiers of structural similarities for CT image data sets.",,Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4,10.1109/BigData.2016.7841003,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841003,CT image;Convolutional neural network;Image variability;Region of interest,Big data;Computed tomography;Data models;Image quality;Image segmentation;Lungs;Measurement,Big Data;backpropagation;computerised tomography;convolution;data analysis;image segmentation;medical image processing;neural nets,Big Data analytics;CNN model;CT image quality;CTIQ;ROI;automated estimation;computed tomography images;convolutional neural network;data models;data set variabilities;error back-propagation;image acquisition parameters;medical image processing systems;medical image quality assessment techniques;predictive modeling;quantitative image assessment;regions-of-interest;segmented pixel variabilities;spatial segmentation;time complexities,,,,,,,,5-8 Dec. 2016,,IEEE,IEEE Conference Publications
15,Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection,Q. Dou; H. Chen; L. Yu; J. Qin; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Transactions on Biomedical Engineering,20170615.0,2017,64,7.0,1558,1567,"Objective: False positive reduction is one of the most crucial components in an automated pulmonary nodule detection system, which plays an important role in lung cancer diagnosis and early treatment. The objective of this paper is to effectively address the challenges in this task and therefore to accurately discriminate the true nodules from a large number of candidates. Methods: We propose a novel method employing three-dimensional (3-D) convolutional neural networks (CNNs) for false positive reduction in automated pulmonary nodule detection from volumetric computed tomography (CT) scans. Compared with its 2-D counterparts, the 3-D CNNs can encode richer spatial information and extract more representative features via their hierarchical architecture trained with 3-D samples. More importantly, we further propose a simple yet effective strategy to encode multilevel contextual information to meet the challenges coming with the large variations and hard mimics of pulmonary nodules. Results: The proposed framework has been extensively validated in the LUNA16 challenge held in conjunction with ISBI 2016, where we achieved the highest competition performance metric (CPM) score in the false positive reduction track. Conclusion: Experimental results demonstrated the importance and effectiveness of integrating multilevel contextual information into 3-D CNN framework for automated pulmonary nodule detection in volumetric CT data. Significance: While our method is tailored for pulmonary nodule detection, the proposed framework is general and can be easily extended to many other 3-D object detection tasks from volumetric medical images, where the targeting objects have large variations and are accompanied by a number of hard mimics.",0018-9294;00189294,,10.1109/TBME.2016.2613502,Shenzhen-Hong Kong Innovation Circle; The Hong Kong Special Administrative Region; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576695,3-D convolutional neural networks;Computer-aided diagnosis;deep learning;false positive reduction;pulmonary nodule detection,Cancer;Computed tomography;Feature extraction;Kernel;Lungs;Three-dimensional displays;Two dimensional displays,cancer;computerised tomography;feature extraction;lung;medical image processing;neural nets,3D convolutional neural network;3D object detection tasks;CPM score;ISBI 2016;LUNA16 challenge;automated pulmonary nodule detection system;competition performance metric score;contextual 3D CNN;false positive reduction track;feature extraction;lung cancer diagnosis;multilevel contextual information;volumetric CT scans;volumetric computed tomography;volumetric medical images,,,,,,,20160926.0,July 2017,,IEEE,IEEE Journals & Magazines
16,Low-dose CT denoising with convolutional neural network,H. Chen; Y. Zhang; W. Zhang; P. Liao; K. Li; J. Zhou; G. Wang,"College of Computer Science, Sichuan University, Chengdu 610065, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,143,146,"To reduce the potential radiation risk, low-dose CT has attracted much attention. However, simply lowering the radiation dose will lead to significant deterioration of the image quality. In this paper, we propose a noise reduction method for low-dose CT via deep neural network without accessing original projection data. A deep convolutional neural network is trained to transform low-dose CT images towards normal-dose CT images, patch by patch. Visual and quantitative evaluation demonstrates a competing performance of the proposed method.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950488,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950488,Low-dose CT;convolutional neural network;deep learning;noise reduction,Computed tomography;Dictionaries;Filtering;Image reconstruction;Neural networks;Noise reduction;Training,computerised tomography;image denoising;medical image processing;neural nets,deep convolutional neural network;image quality;low-dose CT denoising;low-dose CT images;noise reduction method;normal-dose CT images;original projection data;quantitative evaluation;radiation dose;visual evaluation,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
17,ConvNet-Based Localization of Anatomical Structures in 3-D Medical Images,B. D. de Vos; J. M. Wolterink; P. A. de Jong; T. Leiner; M. A. Viergever; I. Išgum,"Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands",IEEE Transactions on Medical Imaging,20170630.0,2017,36,7.0,1470,1481,"Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.",0278-0062;02780062,,10.1109/TMI.2017.2673121,10.13039/100007065 - NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research; 10.13039/501100003958 - Netherlands Organization for Scientific Research Foundation for Technology Sciences Project 12726; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862905,CT;Localization;convolutional neural networks;deep learning;detection,Abdomen;Anatomical structure;Computed tomography;Heart;Three-dimensional displays;Two dimensional displays,angiocardiography;computerised tomography;feature extraction;liver;medical image processing;neural nets;stereo image processing,3D medical images;ConvNet-based localization;abdomen CT scans;anatomical structure;aortic arch;ascending aorta;cardiac CT angiography;chest CT;convolutional neural network;descending aorta;heart;left cardiac ventricle;liver,,,,,,,20170223.0,July 2017,,IEEE,IEEE Journals & Magazines
18,"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning",H. C. Shin; H. R. Roth; M. Gao; L. Lu; Z. Xu; I. Nogues; J. Yao; D. Mollura; R. M. Summers,Imaging Biomarkers and Computer-Aided Diagnosis Laboratory,IEEE Transactions on Medical Imaging,20160503.0,2016,35,5.0,1285,1298,"Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.",0278-0062;02780062,,10.1109/TMI.2016.2528162,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404017,Biomedical imaging;computer aided diagnosis;image analysis;machine learning;neural networks,Biomedical imaging;Computational modeling;Computed tomography;Diseases;Lungs;Lymph nodes;Solid modeling,computerised tomography;diseases;image classification;image representation;learning (artificial intelligence);lung;medical image processing;neurophysiology;reviews,CNN architectures;CNN model analysis;axial CT slices;computer-aided detection;computer-aided detection problems;dataset characteristics;deep convolutional neural networks;fine-tuning CNN models;five-fold cross-validation classification;high performance CAD systems;highly representative hierarchical image features;image recognition;interstitial lung disease classification;learning data-driven;mediastinal LN detection;medical image classification;medical image tasks;medical imaging domain;natural image dataset;off-the-shelf pretrained CNN features;pretrained imagenet;spatial image context;state-of-the-art performance;supervised fine-tuning;thoraco-abdominal lymph node detection;transfer learning;unsupervised CNN pretraining,,35.0,,73.0,,,20160211.0,May 2016,,IEEE,IEEE Journals & Magazines
19,Generative Adversarial Networks for Noise Reduction in Low-Dose CT,J. M. Wolterink; T. Leiner; M. A. Viergever; I. Isgum,,IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"Noise is inherent to low-dose CT acquisition. We propose to train a convolutional neural network (CNN) jointly with an adversarial CNN to estimate routine-dose CT images from low-dose CT images and hence reduce noise. A generator CNN was trained to transform low-dose CT images into routine-dose CT images using voxel-wise loss minimization. An adversarial discriminator CNN was simultaneously trained to distinguish the output of the generator from routinedose CT images. The performance of this discriminator was used as an adversarial loss for the generator. Experiments were performed using CT images of an anthropomorphic phantom containing calcium inserts, as well as patient non-contrast-enhanced cardiac CT images. The phantom and patients were scanned at 20% and 100% routine clinical dose. Three training strategies were compared: the first used only voxel-wise loss, the second combined voxel-wise loss and adversarial loss, and the third used only adversarial loss. The results showed that training with only voxel-wise loss resulted in the highest peak signal-to-noise ratio with respect to reference routine-dose images. However, the CNNs trained with adversarial loss captured image statistics of routine-dose images better. Noise reduction improved quantification of low-density calcified inserts in phantom CT images and allowed coronary calcium scoring in low-dose patient CT images with high noise levels. Testing took less than 10 seconds per CT volume. CNN-based low-dose CT noise reduction in the image domain is feasible. Training with an adversarial network improves the CNN’s ability to generate images with an appearance similar to that of reference routine-dose CT images.",0278-0062;02780062,,10.1109/TMI.2017.2708987,Netherlands Organization for Health Research and Development ZonMw; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934380,"Low-dose cardiac CT;coronary calcium scoring;deep learning,;generative adversarial networks;noise reduction",Calcium;Computed tomography;Convolution;Generators;Noise reduction;Training;Transforms,,,,,,,,,20170526.0,,,IEEE,IEEE Early Access Articles
20,A deep learning based approach to classification of CT brain images,X. W. Gao; R. Hui,"Department of Computer Science, Middlesex University, London NW4 4BT, UK",2016 SAI Computing Conference (SAI),20160901.0,2016,,,28,31,"This study explores the applicability of the state of the art of deep learning convolutional neural network (CNN) to the classification of CT brain images, aiming at bring images into clinical applications. Towards this end, three categories are clustered, which contains subjects' data with either Alzheimer's disease (AD) or lesion (e.g. tumour) or normal ageing. Specifically, due to the characteristics of CT brain images with larger thickness along depth (z) direction (~5mm), both 2D and 3D CNN are employed in this research. The fusion is therefore conducted based on both 2D CT images along axial direction and 3D segmented blocks with the accuracy rates are 88.8%, 76.7% and 95% for classes of AD, lesion and normal respectively, leading to an average of 86.8%.",,Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2,10.1109/SAI.2016.7555958,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555958,3D CNN;CT brain images;Classification;convolutional neural network,Alzheimer's disease;Brain;Computed tomography;Kernel;Lesions;Three-dimensional displays;Two dimensional displays,computerised tomography;diseases;image classification;learning (artificial intelligence);medical image processing;neural nets,2D CNN;2D CT images;3D CNN;3D segmented blocks;AD;Alzheimer's disease;CT brain image classification;axial direction;clinical applications;deep learning based approach;deep learning convolutional neural network;lesion;normal ageing,,,,,,,,13-15 July 2016,,IEEE,IEEE Conference Publications
21,Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network,M. Anthimopoulos; S. Christodoulidis; L. Ebner; A. Christe; S. Mougiakakou,"ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1207,1216,"Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.",0278-0062;02780062,,10.1109/TMI.2016.2535865,Bern University hospital Inselspital; Swiss National Science Foundation SNSF; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422082,Convolutional neural networks;interstitial lung diseases;texture classification,Computed tomography;Convolution;Design automation;Diseases;Feature extraction;Lungs;Neural networks,biological tissues;computerised tomography;convolution;diseases;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets,CT volume scans;ILD pattern classification;automated tissue characterization;computer aided diagnosis system;computer vision problems;consolidation;deep convolutional neural network;deep learning techniques;feature maps;ground glass opacity;honeycombing;interstitial lung diseases;lung pattern classification;medical image analysis;micronodules;reticulation,,17.0,,42.0,,,20160229.0,May 2016,,IEEE,IEEE Journals & Magazines
22,Segmentation of Pulmonary CT Image by Using Convolutional Neural Network Based on Membership Function,J. Xu; H. Liu,"Sch. of Comput. Sci. & Technol., Shandong Univ. of Finance & Econ., Jinan, China",2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC),20170810.0,2017,1,,198,203,"The accurate segmentation of pulmonary CT images is of great significance to clinical computer-aided diagnosis and treatment. In order to avoid the explicit extraction of image features and improve the efficiency of image segmentation and reduce the influence of human factors on the segmentation results, this paper proposes a method of segmenting pulmonary CT image based on membership function convolution neural network (MFCNN). First, the method uses pulmonary CT image filtered by the Gaussian as the input data of the convolution neural network. Then, that uses the improved convolution neural network to achieve the initial segmentation of the image. Finally, the final segmentation result is obtained by setting the threshold based on this paper method. After experimental comparison, this paper demonstrates the feasibility and effectiveness of convolution neural network in the segmentation of pulmonary CT images.",,Electronic:978-1-5386-3221-5; POD:978-1-5386-3222-2; Paper:978-1-5386-3220-8,10.1109/CSE-EUC.2017.42,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005794,convolutional neural network;image segmentation;membership function;pulmonary CT images,Computed tomography;Convolution;Feature extraction;Image segmentation;Medical diagnostic imaging;Neural networks,Gaussian processes;computerised tomography;feature extraction;feedforward neural nets;image filtering;image segmentation;medical image processing,Gaussian process;clinical computer-aided diagnosis;clinical computer-aided treatment;image feature extraction;input data;membership function convolution neural network;pulmonary CT image filtering;pulmonary CT image segmentation,,,,,,,,21-24 July 2017,,IEEE,IEEE Conference Publications
23,Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis,S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou,"ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,76,84,"Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.",2168-2194;21682194,,10.1109/JBHI.2016.2636929,Bern University Hospital; 10.13039/501100001711 - Swiss National Science Foundation (SNSF); ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792,Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning,Biomedical imaging;Computed tomography;Databases;Knowledge engineering;Lungs;Machine learning;Training,biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets,CT images;computed tomography;computer-aided diagnosis;convolutional neural networks;fused knowledge compression;interstitial lung disease diagnosis;lung pattern analysis;lung tissue data;medical image analysis;multisource transfer learning;texture classification;texture databases,,,,,,,20161207.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
24,Deep-learning strategy for pulmonary artery-vein classification of non-contrast CT images,P. Nardelli; D. Jimenez-Carretero; D. Bermejo-Peláez; M. J. Ledesma-Carbayo; F. N. Rahaghi; R. S. J. Estépar,"Applied Chest Imaging Laboratory, Brigham and Women's Hospital, Boston, MA, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,384,387,"Artery-vein classification on pulmonary computed tomography (CT) images is becoming of high interest in the scientific community due to the prevalence of pulmonary vascular disease that affects arteries and veins through different mechanisms. In this work, we present a novel approach to automatically segment and classify vessels from chest CT images. We use a scale-space particle segmentation to isolate vessels, and combine a convolutional neural network (CNN) to graph-cut (GC) to classify the single particles. Information about proximity of arteries to airways is learned by the network by means of a bronchus enhanced image. The methodology is evaluated on the superior and inferior lobes of the right lung of twenty clinical cases. Comparison with manual classification and a Random Forests (RF) classifier is performed. The algorithm achieves an overall accuracy of 87% when compared to manual reference, which is higher than the 73% accuracy achieved by RF.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950543,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950543,Artery-vein segmentation;Frangi filter;convolutional neural networks;lung,Arteries;Computed tomography;Image segmentation;Lungs;Manuals;Radio frequency;Veins,blood vessels;computerised tomography;image classification;image segmentation;learning (artificial intelligence);neural nets,computed tomography;convolutional neural network;deep-learning strategy;noncontrast CT images;pulmonary artery-vein classification;random forests classifier;scale-space particle segmentation,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
25,Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines,G. van Tulder; M. de Bruijne,"Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1262,1272,"The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.",0278-0062;02780062,,10.1109/TMI.2016.2526687,10.13039/501100003246 - Nederlandse Organisatie voor Wetenschappelijk Onderzoek; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401039,Deep learning;X-ray imaging and computed tomography;lung;machine learning;neural network;pattern recognition and classification;representation learning;restricted Boltzmann machine;segmentation,Computed tomography;Feature extraction;Learning systems;Lungs;Neural networks;Standards;Training data,Boltzmann machines;biological tissues;channel bank filters;computerised tomography;feature extraction;image classification;image filtering;image texture;lung;medical image processing;pneumodynamics,airway detection;convolutional restricted Boltzmann machines;discriminative representation learning;feature description;generative learning objective;generative representation learning;lung CT analysis;lung texture classification;lung tissue classification accuracy;standard predefined filter banks;tissue classification system;training data;unlabeled data representations,,5.0,,47.0,,,20160208.0,May 2016,,IEEE,IEEE Journals & Magazines
26,Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans,B. van Ginneken; A. A. A. Setio; C. Jacobs; F. Ciompi,"Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, The Netherlands",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,286,289,"Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163869,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163869,Nodule detection;computed tomography;convolutional neural networks,Biomedical imaging;Cancer;Computed tomography;Design automation;Feature extraction;Lesions;Lungs,computerised tomography;feature extraction;image classification;medical image processing;neural nets;support vector machines,2D sagittal;CNN features;LIDC data set;OverFeat features;axial patches;computed tomography scans;coronal patches;feature extraction;image classification;linear support vector machines;object detection;off-the-shelf convolutional neural network features;pulmonary nodule detection;thoracic radiologists;volumetric medical data,,12.0,,11.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
27,Atherosclerotic vascular calcification detection and segmentation on low dose computed tomography scans using convolutional neural networks,K. Chellamuthu; J. Liu; J. Yao; M. Bagheri; L. Lu; V. Sandfort; R. M. Summers,"Imaging Biomarkers and Computer-aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Building 10 Room 1C224 MSC 1182, Bethesda, MD 20892-1182, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,388,391,"We propose an automated platform for extra-coronary calcification detection on low dose CT scans. We utilize faster regional convolutional neural networks (R-CNN) to directly detect calcifications at the lesion-level without performing vessel extraction. To segment detected calcifications at the voxel-level, we employ holistically nested edge detection (HED). CT scans of 112 vasculitis patients and 3219 images with labeled calcifications were used to develop and evaluate our method. By employing a two-class faster R-CNN, the average precision (AP) increased from 49.2% to 84.4% for calcification detection. In addition, sensitivity of 85.0% at 1 false positive per image was observed. The Dice Similarity Coefficient (DSC) for calcification segmentation using HED (0.83±0.08) was significantly better (p≪0.01) than the traditional threshold-based method (0.59±0.26).",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950544,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950544,CNNs;Calcification;HED;plaque;region proposal,Atherosclerosis;Computed tomography;Computer vision;Image edge detection;Image segmentation;Proposals,blood vessels;computerised tomography;diseases;feature extraction;image segmentation;medical image processing;neural nets,Dice similarity coefficient;atherosclerotic vascular calcification detection;average precision;convolutional neural networks;extracoronary calcification detection;holistically nested edge detection;low dose CT scans;low dose computed tomography scans;segmentation method;threshold-based method;vasculitis patients;vessel extraction;voxel-level,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
28,Convolutional neural networks for lung cancer screening in computed tomography (CT) scans,P. Rao; N. A. Pereira; R. Srinivasan,"Department of Electronics and Communication Engineering, M. S. Ramaiah Institute of Technology, Bengaluru 560054, India",2016 2nd International Conference on Contemporary Computing and Informatics (IC3I),20170504.0,2016,,,489,493,"Diagnosis and cure of cancer has been one of the biggest challenges faced by mankind in the last few decades. Early detection of cancer would facilitate in saving millions of lives across the globe every year. This paper presents an approach which uses a Convolutional Neural Network (CNNs) to classify tumours seen in lung cancer screening computed tomography scans as malignant or benign. CNNs have special properties such as spatial invariance, and allow for multiple feature extraction. When such layers are cascaded, leading to Deep CNNs, it has been shown widely that the accuracy of prediction increases dramatically. In this work, we have designed a CNN suitable for the analysis of CT scans with tumours, using domain knowledge from both medicine and neural networks. The results show that the accuracy of classification for our network performs better than both the traidtional neural networks, and also existing CNNs built for image classification purposes.",,Electronic:978-1-5090-5256-1; POD:978-1-5090-5257-8; USB:978-1-5090-5255-4,10.1109/IC3I.2016.7918014,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918014,,Cancer;Computed tomography;Computer architecture;Convolution;Feature extraction;Neural networks;Tumors,cancer;computerised tomography;feature extraction;feedforward neural nets;image classification;lung;medical image processing,CT scan analysis;cancer detection;cancer diagnosis;computed tomography scans;convolutional neural networks;deep CNN;feature extraction;image classification purposes;lung cancer screening;tumour classification,,,,,,,,14-17 Dec. 2016,,IEEE,IEEE Conference Publications
29,Deep Features Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network,M. Chen; X. Shi; Y. Zhang; D. Wu; M. Guizani,"School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, HuBei China (e-mail: minchen@ieee.org)",IEEE Transactions on Big Data,,2017,PP,99.0,1,1,"At present, computed tomography (CT) are widely used to assist diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) is an extremely important research field in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it evaluates that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during of artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.",,,10.1109/TBDATA.2017.2717439,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954012,Convolutional autoencoder neural network;Feature learning;Hand-craft feature;Lung nodule;Unsupervised learning,Biomedical imaging;Computed tomography;Convolutional codes;Feature extraction;Image analysis;Lungs;Training,,,,,,,,,20170620.0,,,IEEE,IEEE Early Access Articles
30,Colitis detection on computed tomography using regional convolutional neural networks,J. Liu; D. Wang; Z. Wei; L. Lu; L. Kim; E. Turkbey; R. M. Summers,"Imaging Biomarkers and Computer-aided Diagnosis Laboratory, Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Building 10 Room 1C224 MSC 1182, Bethesda, MD 20892-1182",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,863,866,"Colitis is inflammation of the colon that is frequently associated with infection and immune compromise. The wall of a colon afflicted with colitis is much thicker than normal. Colitis can be debilitating or life threatening, and early detection is essential to initiate proper treatment. In this work, we apply high-capacity convolutional neural net-works (CNNs) to bottom-up region proposals to detect potential colitis on CT scans. Our method first generates around 3000 category-independent region proposals for each slice of the input CT scan using selective search. Then, a fixed-length feature vector is extracted from each region proposal using a CNN. Finally, each region proposal is classified and assigned a confidence score with a linear SVM. We applied the detection method to 448 images from 56 CT scans of patients with colitis for evaluation. The detection system achieved 85% sensitivity at 1 false positive per image.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493402,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493402,CNNs;Colitis;Region proposal;SVM,Biological neural networks;Colon;Computed tomography;Feature extraction;Image segmentation;Proposals;Support vector machines,computerised tomography;diseases;feature extraction;image classification;medical image processing;support vector machines,CT scans;bottom-up region;category-independent region;classification;colitis detection;colon inflammation;computed tomography;detection method;early detection;fixed-length feature vector;high-capacity convolutional neural networks;immune compromise;infection;life threatening;linear SVM;potential colitis;regional convolutional neural networks,,,,20.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
31,Detecting Anatomical Landmarks From Limited Medical Imaging Data Using Two-Stage Task-Oriented Deep Neural Networks,J. Zhang; M. Liu; D. Shen,"Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA",IEEE Transactions on Image Processing,20170718.0,2017,26,10.0,4753,4764,"One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.",1057-7149;10577149,,10.1109/TIP.2017.2721106,10.13039/100000002 - NIH; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961205,Anatomical landmark detection;deep convolutional neural networks;limited medical imaging data;real-time;task-oriented,Biological neural networks;Biomedical imaging;Machine learning;Testing;Three-dimensional displays;Training;Training data,biomedical MRI;computerised tomography;feedforward neural nets;learning (artificial intelligence);medical image processing;object detection;regression analysis,3D T1-weighted magnetic resonance images;3D computed tomography images;CNN based regression model;anatomical landmark coordinate prediction;anatomical landmark detection;brain landmarks;deep convolutional neural networks;fully convolutional network;local image patches;medical imaging data;network learning;prostate landmarks;training data;two-stage task-oriented deep learning method;two-stage task-oriented deep neural networks,,,,,,,20170628.0,Oct. 2017,,IEEE,IEEE Journals & Magazines
32,A CNN Regression Approach for Real-Time 2D/3D Registration,S. Miao; Z. J. Wang; R. Liao,"Department of Electrical and Computer Engineering, University of British Columbia, Vancouver",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1352,1363,"In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.",0278-0062;02780062,,10.1109/TMI.2016.2521800,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393571,2-D/3-D registration;convolutional neural network;deep learning;image guided intervention,Attenuation;Biomedical imaging;Computed tomography;Feature extraction;Real-time systems;X-ray imaging,diagnostic radiography;feature extraction;image reconstruction;image registration;iterative methods;medical image processing;neural nets;optimisation;regression analysis,3D pose-indexed features;CNN regression approach;CNN regressors;X-ray images;automatic feature extraction step;complex regression task;convolutional neural network regression approach;digitally reconstructed radiograph;formation parameters;intensity-based 2D-3D registration technology;intensity-based methods;iterative optimization;memory footprint;multiple simpler subtasks;optimization-based methods;real-time 2D-3D registration;scalar-valued metric function;transformation parameters,,4.0,,31.0,,,20160126.0,May 2016,,IEEE,IEEE Journals & Magazines
33,Convolutional neural networks for predicting molecular profiles of non-small cell lung cancer,D. Yu; M. Zhou; F. Yang; D. Dong; O. Gevaert; Z. Liu; J. Shi; J. Tian,"The Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,569,572,"Quantitative imaging biomarkers identification has become a powerful tool for predictive diagnosis given increasingly available clinical imaging data. In parallel, molecular profiles have been well documented in non-small cell lung cancers (NSCLCs). However, there has been limited studies on leveraging the two major sources for improving lung cancer computer-aided diagnosis. In this paper, we investigate the problem of predicting molecular profiles with CT imaging arrays in NSCLC. In particular, we formulate a discriminative convolutional neural network to learn deep features for predicting epidermal growth factor receptor (EGFR) mutation states that are associated with cancer cell growth. We evaluated our approach on two independent datasets including a discovery set with 595 patients (Datset1) and a validation set with 89 patients (Dataset2). Extensive experimental results demonstrated that the learned CNN-based features are effective in predicting EGFR mutation states (AUC=0.828, ACC=76.16%) on Dataset1, and it further demonstrated generalized predictive performance (AUC=0.668, ACC=67.55%) on Dataset2.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950585,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950585,Computed tomography;Computed-aided diagnosis;Convolutional neural networks;Non-Small Cell Lung Carcinoma,Cancer;Computed tomography;Convolution;Feature extraction;Lungs;Neural networks,cancer;cellular biophysics;computerised tomography;feature extraction;lung;medical image processing;molecular biophysics;neural nets;proteins,CT imaging arrays;EGFR mutation state prediction;biomarker identification;cancer cell growth;clinical imaging data;discriminative convolutional neural network;epidermal growth factor receptor;learned CNN-based feature;lung cancer computer-aided diagnosis;molecular profile;nonsmall cell lung cancer;quantitative imaging,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
34,Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks,A. A. A. Setio; F. Ciompi; G. Litjens; P. Gerke; C. Jacobs; S. J. van Riel; M. M. W. Wille; M. Naqibullah; C. I. Sánchez; B. van Ginneken,"Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1160,1169,"We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.",0278-0062;02780062,,10.1109/TMI.2016.2536809,The Netherlands Organization for Scientific Research; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422783,Computed tomography;computer-aided detection;convolutional networks;deep learning;lung cancer;pulmonary nodule,Cancer;Computed tomography;Design automation;Feature extraction;Lesions;Lungs;Solids,cancer;computerised tomography;feature extraction;image classification;image fusion;medical image processing;tumours,2D ConvNets;2D patches;ANODE09 challenge;CAD system;CT images;computer-aided detection system;data augmentation;dedicated fusion method;differently oriented planes;discriminative features;false positive reduction;final classification;multiple streams;multiview ConvNets;multiview convolutional networks;nodule candidates;publicly available LIDC-IDRI dataset;pulmonary nodule detection;training data,,10.0,1.0,47.0,,,20160301.0,May 2016,,IEEE,IEEE Journals & Magazines
35,A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling,A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers,"Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA",IEEE Transactions on Image Processing,20161124.0,2017,26,1.0,386,399,"Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.",1057-7149;10577149,,10.1109/TIP.2016.2624198,10.13039/100000098 - Intramural Research Program of the National Institutes of Health Clinical Center; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966,Abdominal computed tomography (CT);cascaded random forest;deep convolutional neural networks;dense image patch labeling;pancreas segmentation,Computed tomography;Image segmentation;Labeling;Liver;Pancreas;Shape,biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability,CT slice image decomposition;abdominal computed tomography scans;bottom-up approach;cascaded superpixels;computer-aided diagnosis;deep convolutional neural network classification;deep image patch labeling;dense patch labeling;disjoint boundary-preserving superpixels;image histogram;image location;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach;pancreas class probability maps;pancreas segmentation;pathology detection;probability features;quantitative imaging analysis;random forest classification;superpixel classification;surgical assistance;texture features,,,,,,,20161101.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
36,Self supervised deep representation learning for fine-grained body part recognition,P. Zhang; F. Wang; Y. Zheng,"Medical Imaging Technologies, Siemens Medical Solutions USA Inc., Princeton, NJ 08540, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,578,582,"Difficulty on collecting annotated medical images leads to lack of enough supervision and makes discrimination tasks challenging. However, raw data, e.g., spatial context information from 3D CT images, even without annotation, may contain rich useful information. In this paper, we exploit spatial context information as a source of supervision to solve discrimination tasks for fine-grained body part recognition with conventional 3D CT and MR volumes. The proposed pipeline consists of two steps: 1) pre-train a convolutional network for an auxiliary task of 2D slices ordering in a self-supervised manner; 2) transfer and fine-tune the pre-trained network for fine-grained body part recognition. Without any use of human annotation in the first stage, the pre-trained network can still outperform CNN trained from scratch on CT as well as M-R data. Moreover, by comparing with pre-trained CNN from ImageNet, we discover that the distance between source and target tasks plays a crucial role in transfer learning. Our experiments demonstrate that our approach can achieve high accuracy with a slice location estimation error of only a few slices on CT and MR data. To the best of our knowledge, our work is the first attempt studying the problem of robust body part recognition at a continuous level.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950587,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950587,Body Part Recognition;Self Supervised Learning;Slice Ordering,Biomedical imaging;Computed tomography;Context;Image recognition;Three-dimensional displays;Training;Two dimensional displays,biomedical MRI;computerised tomography;image recognition;learning (artificial intelligence);medical image processing,3D CT;MR volumes;fine-grained body part recognition;self supervised deep representation learning;spatial context information;transfer learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
37,Classification of thyroid nodules in ultrasound images using deep model based transfer learning and hybrid features,T. Liu; S. Xie; J. Yu; L. Niu; W. Sun,"Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619.0,2017,,,919,923,"Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful semantic features to the classification. Firstly, a CNN model trained with a massive natural dataset is transferred to the ultrasound image domain, to generate semantic deep features and handle the small sample problem. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Local Binary Patterns (LBP) together, to form a hybrid feature space. Finally, a positive-sample-first majority voting and a feature-selected based strategy are employed for the hybrid classification. Experimental results on 1037 images show that the accuracy of our proposed method is 0.931, which outperformed other relative methods by over 10%.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7952290,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952290,classification;deep learning;feature fusion;transfer learning;ultrasound image,Biomedical imaging;Cancer;Feature extraction;Indexes;Machine learning;Semantics;Ultrasonic imaging,biomedical ultrasonics;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;ultrasonic imaging,CNN model trained;HOG;LBP;benign nodules;convolution neural networks;deep model based transfer learning;feature extraction method;feature-selected based strategy;histogram of oriented gradient;hybrid feature space;local binary patterns;malignant nodules;positive-sample-first majority voting;semantic deep features;thyroid nodule classification;thyroid nodules;ultrasonography;ultrasound images,,,,,,,,5-9 March 2017,,IEEE,IEEE Conference Publications
38,Automatic 3D ultrasound segmentation of the first trimester placenta using deep learning,P. Looney; G. N. Stevenson; K. H. Nicolaides; W. Plasencia; M. Molloholli; S. Natsis; S. L. Collins,"Nuffield Department of Obstetrics and Gynaecology, University of Oxford, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,279,282,"Placental volume measured with 3D ultrasound in the first trimester has been shown to be correlated to adverse pregnancy outcomes. This could potentially be used as a screening test to predict the “at risk” pregnancy. However, manual segmentation whilst previously shown to be accurate and repeatable is very time consuming and semi-automated methods still require operator input. To generate a screening tool, fully automated placental segmentation is required. In this work, a deep convolutional neural network (cNN), DeepMedic, was trained using the output of the semi-automated Random Walker method as ground truth. 300 3D ultrasound scans of first trimester placentas were used to train, validate and test the cNN. Compared against the semi-automated segmentation, resultant median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Dice Similarity Coefficient was 0.73 (0.66, 0.76). The median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Hausdorff distance was 27 mm (18 mm, 36 mm). We present the first attempt at using a deep cNN for segmentation of 3D ultrasound of the placenta. This work shows that feasible results compared to ground truth were obtained that could form the basis of a fully automatic segmentation method.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950519,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950519,3D;automatic segmentation;deep learning;neural network;placenta;random walker;ultrasound,Biological neural networks;Image segmentation;Magnetic resonance imaging;Pregnancy;Three-dimensional displays;Ultrasonic imaging,biomedical ultrasonics;image segmentation;learning (artificial intelligence);medical image processing;neural nets;obstetrics,DeepMedic;automatic 3D ultrasound segmentation;deep convolutional neural network;deep learning;first trimester placenta,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
39,Hybrid approach for automatic segmentation of fetal abdomen from ultrasound images using deep learning,H. Ravishankar; S. M. Prabhu; V. Vaidya; N. Singhal,"GE Global Research, Bangalore, India",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,779,782,"In this paper, we propose a hybrid approach combining traditional texture analysis methods with deep learning for the automatic detection and measurement of abdominal contour from 2-D fetal ultrasound images. Following a learning-based procedure for region of interest (ROI) localization to segment the abdominal boundary, we show that convolutional neural networks (CNNs) outperform other state-of-the-art texture features and conventional classifiers, in addressing the binary classification problem of distinguishing between abdomen versus non-abdomen regions. However, we obtain significantly better segmentation results in identifying the best ROI containing fetal abdomen, when the predictions from CNN are combined with those from gradient boosting machine (GBM) using histogram of oriented gradient (HOG) features. We trained our method on a set of 70 images and tested them on another distinct set of 70 images. We obtained a mean DICE similarity coefficient of 0.90, which shows excellent overlap with the ground truth. We report that the mean computed gestational age difference between our segmentation results and the ground truth, is within two weeks for 90% (and within one week for 70%) of the testing cases.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493382,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493382,,Abdomen;Feature extraction;Image segmentation;Machine learning;Training;Ultrasonic imaging;Ultrasonic variables measurement,,,,,,14.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
40,Automatic fetal body and amniotic fluid segmentation from fetal ultrasound images by encoder-decoder network with inner layers,Y. Li; R. Xu; J. Ohya; H. Iwata,"Faculty of Science and Engineering, Waseda University, Tokyo, Japan",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,1485,1488,"This paper explores the effectiveness of applying a deep learning based method to segment the amniotic fluid and fetal tissues in fetal ultrasound (US) images. The deeply learned model firstly encodes the input image into down scaled feature maps by convolution and pooling structures, then up-scale the feature maps to confidence maps by corresponded un-pooling and convolution layers. Additional convolution layers with 1×1 sized kernels are adopted to enhance the feature representations, which could be used to further improve the discriminative learning of our model. We effectively update the weights of the network by fine-tuning on part of the layers from a pre-trained model. By conducting experiments using clinical data, the feasibility of our proposed approach is compared and discussed. The result proves that this work achieves satisfied results for segmentation of specific anatomical structures from US images.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037116,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037116,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
41,Automated embolic signal detection using Deep Convolutional Neural Network,P. Sombune; P. Phienphanich; S. Phuechpanpaisal; S. Muengtaweepongsa; A. Ruamthanthong; C. Tantibundhit,"Faculty of Engineering, Thammasat University, Thailand",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,3365,3368,"This work investigated the potential of Deep Neural Network in detection of cerebral embolic signal (ES) from transcranial Doppler ultrasound (TCD). The resulting system is aimed to couple with TCD devices in diagnosing a risk of stroke in real-time with high accuracy. The Adaptive Gain Control (AGC) approach developed in our previous study is employed to capture suspected ESs in real-time. By using spectrograms of the same TCD signal dataset as that of our previous work as inputs and the same experimental setup, Deep Convolutional Neural Network (CNN), which can learn features while training, was investigated for its ability to bypass the traditional handcrafted feature extraction and selection process. Extracted feature vectors from the suspected ESs are later determined whether they are of an ES, artifact (AF) or normal (NR) interval. The effectiveness of the developed system was evaluated over 19 subjects going under procedures generating emboli. The CNN-based system could achieve in average of 83.0% sensitivity, 80.1% specificity, and 81.4% accuracy, with considerably much less time consumption in development. The certainly growing set of training samples and computational resources will contribute to high performance. Besides having potential use in various clinical ES monitoring settings, continuation of this promising study will benefit developments of wearable applications by leveraging learnable features to serve demographic differentials.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037577,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037577,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
42,Automated Breast Ultrasound Lesions Detection using Convolutional Neural Networks,M. H. Yap; G. Pons; J. Martí; S. Ganau; M. Sentís; R. Zwiggelaar; A. K. Davison; R. Martí,"School of Computing, Mathematics, and Digital Technology, Manchester Metropolitan University, Manchester, Lancashire United Kingdom of Great Britain and Northern Ireland M1 5GD (e-mail: M.Yap@mmu.ac.uk)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"Breast lesion detection using ultrasound imaging is considered an important step of Computer-Aided Diagnosis systems. Over the past decade, researchers have demonstrated the possibilities to automate the initial lesion detection. However, the lack of a common dataset impedes research when comparing the performance of such algorithms. This paper proposes the use of deep learning approaches for breast ultrasound lesion detection and investigates three different methods: a Patch-based LeNet, a U-Net, and a transfer learning approach with a pretrained FCN-AlexNet. Their performance is compared against four state-of-the-art lesion detection algorithms (i.e. Radial Gradient Index, Multifractal Filtering, Rule-based Region Ranking and Deformable Part Models). In addition, this paper compares and contrasts two conventional ultrasound image datasets acquired from two different ultrasound systems. Dataset A comprises 306 (60 malignant and 246 benign) images and Dataset B comprises 163 (53 malignant and 110 benign) images. To overcome the lack of public datasets in this domain, Dataset B will be made available for research purposes. The results demonstrate an overall improvement by the deep learning approaches when assessed on both datasets in terms of True Positive Fraction, False Positives per image, and F-measure.",2168-2194;21682194,,10.1109/JBHI.2017.2731873,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003418,Lesion detection;breast cancer;convolutional neural networks;transfer learning;ultrasound imaging,Breast cancer;Filtering;Fractals;Imaging;Lesions;Ultrasonic imaging,,,,,,,,,20170807.0,,,IEEE,IEEE Early Access Articles
43,Deep Learning on Sparse Manifolds for Faster Object Segmentation,J. C. Nascimento; G. Carneiro,"Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Lisboa, Portugal",IEEE Transactions on Image Processing,20170804.0,2017,26,10.0,4978,4990,"We propose a new combination of deep belief networks and sparse manifold learning strategies for the 2D segmentation of non-rigid visual objects. With this novel combination, we aim to reduce the training and inference complexities while maintaining the accuracy of machine learning-based non-rigid segmentation methodologies. Typical non-rigid object segmentation methodologies divide the problem into a <italic>rigid detection</italic> followed by a <italic>non-rigid segmentation</italic>, where the low dimensionality of the rigid detection allows for a robust training (i.e., a training that does not require a vast amount of annotated images to estimate robust appearance and shape models) and a fast search process during inference. Therefore, it is desirable that the dimensionality of this rigid transformation space is as small as possible in order to enhance the advantages brought by the aforementioned division of the problem. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection space. Furthermore, we propose the use of deep belief networks to allow for a training process that can produce robust appearance models without the need of large annotated training sets. We test our approach in the segmentation of the left ventricle of the heart from ultrasound images and lips from frontal face images. Our experiments show that the use of sparse manifolds and deep belief networks for the rigid detection stage leads to segmentation results that are as accurate as the current state of the art, but with lower search complexity and training processes that require a small amount of annotated training data.",1057-7149;10577149,,10.1109/TIP.2017.2725582,FCT; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973159,Deep belief netwolks;defonnable objects;non-rigid segmentation;sparse manifold,Image segmentation;Manifolds;Robustness;Search problems;Shape;Training;Visualization,,,,,,,,,20170711.0,Oct. 2017,,IEEE,IEEE Journals & Magazines
44,Automated assessment of endometrium from transvaginal ultrasound using Deep Learned Snake,N. Singhal; S. Mukherjee; C. Perrey,"GE Global Research, Bangalore, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,283,286,Endometrium assessment via thickness measurement is commonly performed in routine gynecological ultrasound examination for assessing the reproductive health of patients undergoing fertility related treatments and endometrium cancer screening in women with post-menopausal bleeding. This paper introduces a fully automated technique for endometrium thickness measurement from three-dimensional transvaginal ultrasound (TVUS) images. The algorithm combines the robustness of deep neural networks with the more interpretable level set method for segmentation. We propose a hybrid variational curve propagation model which embeds a deep-learned endometrium probability map in the segmentation energy functional. This solution provides approximately 30% performance improvement over a contemporary supervised learning method on a database of 59 TVUS images and the thickness measurement is found to be within ±2mm of the manual measurement in 87% of the cases.,,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950520,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950520,Endometrium;deep learning;level set;segmentation;ultrasound;uterus,Image segmentation;Level set;Shape;Thickness measurement;Three-dimensional displays;Training;Ultrasonic imaging,biomedical measurement;biomedical ultrasonics;cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines;thickness measurement,3D TVUS images;3D transvaginal ultrasound images;automated endometrium assessment;deep learned snake;deep neural networks;deep-learned endometrium probability map;endometrium cancer screening;endometrium thickness measurement;gynecological ultrasound examination;image segmentation;post-menopausal bleeding;supervised learning method;variational curve propagation model,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
45,Liver Fibrosis Classification Based on Transfer Learning and FCNet for Ultrasound Images,D. Meng; L. Zhang; G. Cao; W. Cao; G. Zhang; B. Hu,"MOE Research Center for Software/Hardware Co-Design Engineering, East China Normal University, Shanghai, China",IEEE Access,20170520.0,2017,5,,5804,5810,"Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.",2169-3536;21693536,,10.1109/ACCESS.2017.2689058,NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Informatization; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890483,Deep neural networks;fully connected layers;liver fibrosis;transfer learning,Feature extraction;Heating systems;Liver;Medical services;Neural networks;Training;Ultrasonic imaging,,,,,,,,,20170330.0,2017,,IEEE,IEEE Journals & Magazines
46,A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound,K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel,"Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,48,55,"Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.",2168-2194;21682194,,10.1109/JBHI.2016.2631401,European Regions Development; FIS; Marie-Curie Actions Program of the European Union; 10.13039/100000002 - NIH; 10.13039/100007065 - NVIDIA; 10.13039/501100000783 - REA; 10.13039/501100003741 - ICREA; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798,Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound,Atherosclerosis;Feature extraction;Imaging;Lipidomics;Machine learning;Neural networks;Ultrasonic imaging,biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets,calcified tissue;cardiovascular events;carotid plaque composition;carotid ultrasound;cerebrovascular events;convolutional neural network;deep learning framework;fibrous cap;fibrous tissue;image noise;imaging features;lipid core;plaque constituents;thresholding values,,,,,,,20161122.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
47,Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks,H. Chen; D. Ni; J. Qin; S. Li; X. Yang; T. Wang; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Journal of Biomedical and Health Informatics,20170520.0,2015,19,5.0,1627,1636,"Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.",2168-2194;21682194,,10.1109/JBHI.2015.2425041,Hong Kong Innovation and Technology Fund; Research Grants Council of Hong Kong; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943,Convolutional neural network (CNN);Ultrasound;convolutional neural network;deep learning;domain transfer;knowledge transfer;standard plane;ultrasound (US),Biomedical imaging;Dictionaries;Feature extraction;Informatics;Standards;Training;Videos,biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics,FASP localization;US videos;automatic standard plane localization;classification performance;domain transferred deep convolutional neural network;fetal abdominal standard plane;fetal ultrasound;high-level features;learning-based approach;low-level features;medical image computing problems;natural images;overfitting problem;task-specific CNN;transfer learning strategy;ultrasound videos,"0;Abdomen;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;ROC Curve;Ultrasonography, Prenatal",32.0,,37.0,,,20150421.0,Sept. 2015,,IEEE,IEEE Journals & Magazines
48,A Deep Convolutional Neural Network Based Framework for Automatic Fetal Facial Standard Plane Recognition,Z. Yu; E. L. Tan; D. Ni; J. Qin; S. Chen; S. Li; B. Lei; T. Wang,,IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"Ultrasound imaging has become a prevalent examination method in prenatal diagnosis. Accurate acquisition of fetal facial standard plane (FFSP) is the most important precondition for subsequent diagnosis and measurement. In the past few years, considerable effort has been devoted to FFSP recognition using various hand-crafted features, but the recognition performance is still unsatisfactory due to the high intra-class variation of FFSPs and the high degree of visual similarity between FFSPs and other non-FFSPs. To improve the recognition performance, we propose a method to automatically recognize FFSP via a deep convolutional neural network (DCNN) architecture. The proposed DCNN consists of 16 convolutional layers with small 3×3 size kernels and three fully connected layers. A global average pooling (GAP) is adopted in the last pooling layer to significantly reduce network parameters, which alleviates the overfitting problems and improves the performance under limited training data. Both the transfer learning strategy and a data augmentation technique tailored for FFSP are implemented to further boost the recognition performance. Extensive experiments demonstrate the advantage of our proposed method over traditional approaches and the effectiveness of DCNN to recognize FFSP for clinical diagnosis.",2168-2194;21682194,,10.1109/JBHI.2017.2705031,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930382,Deep convolutional neural network;Standard plane recognition;Transfer learning;Ultrasound image,Biomedical imaging;Biomedical measurement;Feature extraction;Image recognition;Neural networks;Standards;Ultrasonic imaging,,,,,,,,,20170517.0,,,IEEE,IEEE Early Access Articles
49,Deep learning of submerged body images from 2D sonar sensor based on convolutional neural network,S. Lee,"Division of Mechanical and Automotive Engineering, Kongju University, Cheonan, 31080, Korea",2017 IEEE Underwater Technology (UT),20170403.0,2017,,,1,3,"Given the harsh working conditions such as high-speed flow rate, turbid watch, and steep terrain, it is a very challenging task to find submerged bodies in disaster site occurred at sea or river or for the military purpose. Therefore, if it is possible to utilize the unmanned robot, such as the USV(Unmanned Surface Vehicle) and UUV (Unmanned Underwater Vehicle) for the navigational operation of these special purpose, it has a great effect. Underwater ultrasound image information is pretty difficult to make the geometric modeling of submerged body due to heavy noise on its characteristics. This study presents the robust method of submerged body recognition based on the CNN(Convolutional Neural Network), which is one of the deep learning approach.",,Electronic:978-1-5090-5266-0; POD:978-1-5090-5267-7,10.1109/UT.2017.7890309,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890309,Convolutional Neural Network;Submerged Body Recognition;Underwater Sonar Image;Unmanned Surface Vehicle;Unmanned Underwater Vehicle,Kernel;Machine learning;Neural networks;Robot sensing systems;Sea surface;Sonar,image recognition;image sensors;learning (artificial intelligence);neural nets;sonar imaging,2D sonar sensor;CNN;USV;UUV;convolutional neural network;deep learning;disaster;geometric modeling;high-speed flow rate;military purpose;navigational operation;steep terrain;submerged body image recognition;turbid watch;underwater ultrasound image information;unmanned robot;unmanned surface vehicle;unmanned underwater vehicle,,,,,,,,21-24 Feb. 2017,,IEEE,IEEE Conference Publications
50,Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images,Y. Zhang; M. T. C. Ying; L. Yang; A. T. Ahuja; D. Z. Chen,"Department of Computer Science and Engineering, University of Notre Dame, IN 46556, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,443,448,"Ultrasound as a well-established imaging modality is widely used in imaging lymph nodes for clinical diagnosis and disease analysis. Quantitative analysis of lymph node features, morphology, and relations can provide valuable information for diagnosis and immune system studies. For such analysis, it is necessary to first accurately segment the lymph node areas in ultrasound images. In this paper, we develop a new deep learning method, called Coarse-to-Fine Stacked Fully Convolutional Nets (CFS-FCN), for automatically segmenting lymph nodes in ultrasound images. Our method consists of multiple stages of FCN modules. We train the CFS-FCN model to learn the segmentation knowledge from a coarse-to-fine, simple-to-complex manner. A data set of 80 ultrasound images containing both normal and diseased lymph nodes is used in our experiments, which show that our method considerably outperforms the state-of-the-art deep learning methods for lymph node segmentation.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822557,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822557,,Biological system modeling;Biomedical imaging;Image segmentation;Lymph nodes;Machine learning;Training;Ultrasonic imaging,biomedical ultrasonics;diseases;image segmentation;learning (artificial intelligence);medical image processing,CFS-FCN;FCN module;clinical diagnosis;coarse-to-fine stacked fully convolutional net;deep learning method;disease analysis;lymph node imaging;lymph node segmentation;ultrasound images;ultrasound imaging modality,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
51,Fetal facial standard plane recognition via very deep convolutional networks,Z. Yu; D. Ni; S. Chen; S. Li; T. Wang; B. Lei,"School of Biomedical Engineering, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,627,630,"The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3×3) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590780,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590780,,Computer architecture;Convolution;Image recognition;Imaging;Standards;Training;Ultrasonic imaging,biomedical ultrasonics;face recognition;learning (artificial intelligence);medical image processing;neural nets;obstetrics,DCNN;FFSP dataset;US examination;automatic FFSP recognition method;convolution filters;deep learning approach;fetal facial standard plane recognition;hand-crafted visual features;ultrasound images;very deep convolutional networks,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
52,Multi-atlas segmentation using manifold learning with deep belief networks,J. C. Nascimento; G. Carneiro,"Instituto de Sistemas e Rob&#243;tica, Instituto Superior T&#233;cnico, 1049-001 Lisboa, Portugal",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,867,871,"This paper proposes a novel combination of manifold learning with deep belief networks for the detection and segmentation of left ventricle (LV) in 2D — ultrasound (US) images. The main goal is to reduce both training and inference complexities while maintaining the segmentation accuracy of machine learning based methods for non-rigid segmentation methodologies. The manifold learning approach used can be viewed as an atlas-based segmentation. It partitions the data into several patches. Each patch proposes a segmentation of the LV that somehow must be fused. This is accomplished by a deep belief network (DBN) multi-classifier that assigns a weight for each patch LV segmentation. The approach is thus threefold: (i) it does not rely on a single segmentation, (ii) it provides a great reduction in the rigid detection phase that is performed at lower dimensional space comparing with the initial contour space, and (iii) DBN's allows for a training process that can produce robust appearance models without the need of large annotated training sets.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493403,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493403,,Complexity theory;Context;Image segmentation;Manifolds;Principal component analysis;Training;Visualization,,,,,,17.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
53,Describing ultrasound video content using deep convolutional neural networks,Y. Gao; M. A. Maraci; J. A. Noble,"Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, UK",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,787,790,"We address the task of object recognition in obstetric ultrasound videos using deep Convolutional Neural Networks (CNNs). A transfer learning based design is presented to study the transferability of features learnt from natural images to ultrasound image object recognition which on the surface is a very different problem. Our results demonstrate that CNNs initialised with large-scale pre-trained networks outperform those directly learnt from small-scale ultrasound data (91.5% versus 87.9%), in terms of object identification.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493384,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493384,Classification;Convolutional neural networks;Obstetric ultrasound;Transfer learning,Abdomen;Data visualization;Feature extraction;Heart;Standards;Training;Ultrasonic imaging,biomedical ultrasonics;image recognition;medical image processing;neural nets;object recognition;obstetrics,deep convolutional neural networks;object identification;obstetric ultrasound videos;small-scale ultrasound data;transfer learning-based design;ultrasound image object recognition;ultrasound video content,,1.0,,11.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
54,Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing,F. C. Ghesu; E. Krubasik; B. Georgescu; V. Singh; Y. Zheng; J. Hornegger; D. Comaniciu,"Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1217,1228,"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% o- er the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.",0278-0062;02780062,,10.1109/TMI.2016.2538802,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426845,Deep learning;image parsing;marginal space learning;sparse representations;three-dimensional (3D) object detection and segmentation,Context;Feature extraction;Image segmentation;Machine learning;Robustness;Shape;Three-dimensional displays,biomedical ultrasonics;feature extraction;image classification;image sampling;image segmentation;learning (artificial intelligence);medical image processing;pattern clustering;probability;ultrasonic imaging,3D context;DL-based active shape model;anatomical object detection;anatomical pose estimation;annotated image databases;aortic valve;automated feature design;boundary delineation;clinical workflow;clustered high-probability regions;computational efficiency;deep learning network architectures;deep learning systems;diagnosis;extensive dataset;full 3D data detection;full 3D data segmentation;hierarchical marginal spaces;learning classifiers;machine learning methods;marginal space deep learning;nonrigid object boundary;object localization;object parametrization;parametrized representations;patient stratification;representative image features;restricted affine transformation;run-time performance;scanning high-dimensional parametric spaces;scanning hypotheses;segmentation support;sparse adaptive data sampling patterns;therapy planning;two-step learning problem;ultrasound;volumetric medical image data parsing,,9.0,,46.0,,,20160307.0,May 2016,,IEEE,IEEE Journals & Magazines
55,A novel method with a deep network and directional edges for automatic detection of a fetal head,S. Nie; J. Yu; P. Chen; J. Zhang; Y. Wang,"Department of Electronic Engineering, Fudan University, Shanghai, China",2015 23rd European Signal Processing Conference (EUSIPCO),20151228.0,2015,,,654,658,"In this paper, we propose a novel method for the automatic detection of fetal head in 2D ultrasound images. Fetal head detection has been a challenging task, as the ultrasound images usually have poor quality, the structures contained in the images are complex, and the gray scale distribution is highly variable. Our approach is based on a deep belief network and a modified circle detection method. The whole process can be divided into two steps: first, a deep learning architecture is applied to search the whole image and determine the result patch that contains the entire fetal head; second, a modified circle detection method is used along with Hough transform to detect the position and size of the fetal head. In order to validate our method, experiments are performed on both synthetic data and clinic ultrasound data. A good performance of the proposed method is shown in the paper.",,Electronic:978-0-9928-6263-3; POD:978-1-4799-8851-8,10.1109/EUSIPCO.2015.7362464,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362464,Fetal head;circle detection;deep learning,Europe;Head;Image edge detection;Magnetic heads;Signal processing;Training;Ultrasonic imaging,Hough transforms;belief networks;medical signal detection;ultrasonic imaging,2D ultrasound images;Hough transform;automatic detection;clinic ultrasound data;deep belief network;deep learning architecture;deep network;directional edges;fetal head detection;gray scale distribution;modified circle detection;synthetic data,,1.0,,19.0,,,,Aug. 31 2015-Sept. 4 2015,,IEEE,IEEE Conference Publications
56,Mapping between ultrasound and vowel speech using DNN framework,X. Zheng; J. Wei; W. Lu; Q. Fang; J. Dang,"School of Computer Science and Technology, Tianjin University, China",The 9th International Symposium on Chinese Spoken Language Processing,20141027.0,2014,,,372,376,"Building up the mapping between articulatory movements and corresponding speech could great facility the speech training and speech aid for voiceless patients. In this paper, we propose a deep learning framework for building up a mapping between articulatory information and corresponding speech, which were recorded by ultrasound system. The dataset includes six Chinese vowels. We use Bimodal Deep Autoencoder algorithm based on RBM to learn the relationship between speech and articulation, the weights matrix of representation of them. Speech and ultrasound images have been reconstructed using the extracted features. The reconstruction error of articulation by our method is less than that of PCA based approach. The reconstructed speech is similar to the original one. We propose a mapping from ultrasound tongue image to acoustic signal with a revised Denoising Autoencoder, the results show that it is a promising approach. In contrast, another experiment is conducted to synthesize the ultrasound tongue image from the speech, but the result should be improved.",,Electronic:978-1-4799-4219-0; POD:978-1-4799-4218-3; USB:978-1-4799-4220-6,10.1109/ISCSLP.2014.6936700,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936700,DBN;Denoising Autoencoder;articulatory-acoustic mapping,Acoustics;Feature extraction;Image reconstruction;Speech;Synchronization;Tongue;Ultrasonic imaging,acoustic signal processing;feature extraction;handicapped aids;image reconstruction;natural language processing;neural nets;speech processing;ultrasonic imaging,Chinese vowels;DNN framework;RBM;acoustic signal;articulatory movements;bimodal deep autoencoder algorithm;deep neural network framework;denoising autoencoder;feature extraction;speech aid;speech reconstruction;speech training;ultrasound image reconstruction;ultrasound system;ultrasound tongue image;voiceless patients;vowel speech,,1.0,,9.0,,,,12-14 Sept. 2014,,IEEE,IEEE Conference Publications
57,Small Sample Deep Learning for Newborn Gestational Age Estimation,M. T. Torres; M. F. Valstar; C. Henry; C. Ward; D. Sharkey,"Sch. of Comput. Sci., Univ. of Nottingham, Nottingham, UK",2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),20170629.0,2017,,,79,86,"A baby's gestational age determines whether or not they are preterm, which helps clinicians decide on suitable post-natal treatment. The most accurate dating methods use Ultrasound Scan (USS) machines, but these machines are expensive, require trained personnel and cannot always be deployed to remote areas. In the absence of USS, the Ballard Score can be used, which is a manual postnatal dating method. However, this method is highly subjective and results can vary widely depending on the experience of the rater. In this paper, we present an automatic system for postnatal gestational age estimation aimed to be deployed on mobile phones, using small sets of images of a newborn's face, foot and ear. We present a novel two-stage approach that makes the most out of Convolutional Neural Networks trained on small sets of images to predict broad classes of gestational age, and then fuse the outputs of these discrete classes with a baby's weight to make fine-grained predictions of gestational age. On a purpose=collected dataset of 88 babies, experiments show that our approach attains an expected error of 6 days and is three times more accurate than the manual postnatal method (Ballard). Making use of images improves predictions by 30% compared to using weight only. This indicates that even with a very small set of data, our method is a viable candidate for postnatal gestational age estimation in areas were USS is not available.",,Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7,10.1109/FG.2017.19,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961726,,Biomedical imaging;Ear;Estimation;Face;Image segmentation;Machine learning;Pediatrics,biomedical ultrasonics;convolution;image classification;image segmentation;learning (artificial intelligence);medical image processing;mobile computing;paediatrics,Ballard score;USS machines;convolutional neural network training;deep learning;image segmentation;mobile phones;newborn gestational age estimation;post-natal treatment;postnatal dating method;postnatal gestational age estimation;ultrasound scan machines,,,,,,,,May 30 2017-June 3 2017,,IEEE,IEEE Conference Publications
58,Automated characterization of the fetal heart in ultrasound images using fully convolutional neural networks,V. Sundaresan; C. P. Bridge; C. Ioannou; J. A. Noble,"Institute of Biomedical Engineering, University of Oxford, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,671,674,Automatic analysis of fetal echocardiography screening images could aid in the identification of congenital heart diseases. The first step towards automatic fetal echocardiography analysis is locating the fetal heart in an image and identifying the viewing (imaging) plane. This is highly challenging since the fetal heart is small with relatively indistinct anatomical structural appearance. This is further compounded by the presence of artefacts in ultrasound images. Herein we provide a state-of-art solution for detecting the fetal heart and classifying each individual frame as belonging to one of the standard viewing planes using fully convolutional neural networks (FCNs). Our FCN model achieves a classification error rate of 23.48% on real-world clinical ultrasound data. We also present comparative performance for analysis of different FCN architectures.,,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950609,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950609,Fetal ultrasound images;deep learning;fetal echocardiography;fully convolutional neural networks,Echocardiography;Fetal heart;Neural networks;Standards;Training;Ultrasonic imaging,echocardiography;image classification;medical image processing;neural nets,FCN model;classification error rate;congenital heart diseases;fetal echocardiography screening images;fetal heart;fully convolutional neural networks;real-world clinical ultrasound data;ultrasound images,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
59,Feature selection and thyroid nodule classification using transfer learning,T. Liu; S. Xie; Y. Zhang; J. Yu; L. Niu; W. Sun,"Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1096,1099,"Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful and specific features to the classification. A CNN model trained with ImageNet data is transferred to the ultrasound image domain, to generate semantic deep features under small sample condition. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Scale Invariant Feature Transform (SIFT) together to form a hybrid feature space. Furthermore, to make the general deep features more pertinent to our problem, a feature subset selection process is employed for the hybrid nodule classification, followed by a detailed discussion on the influence of feature number and feature composition method. Experimental results on 1037 images show that the accuracy of our proposed method is 0.929, which outperforms other relative methods by over 10%.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950707,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950707,feature subset selection;thyroid nodules classification;transfer learning;ultrasound image,Biomedical imaging;Cancer;Convolution;Feature extraction;Indexes;Training;Ultrasonic imaging,biomedical ultrasonics;feature extraction;feature selection;image classification;learning (artificial intelligence);medical image processing;neural nets;programming language semantics,ImageNet data;benign nodules;convolution neural networks;diagnosis accuracy;feature extraction method;feature selection;histogram-of-oriented-gradient;malignant nodules;scale-invariant-feature-transform;semantic deep features;thyroid nodule classification;transfer learning;ultrasonography;ultrasound images,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
60,Classification of breast lesions using cross-modal deep learning,O. Hadad; R. Bakalo; R. Ben-Ari; S. Hashoul; G. Amit,"IBM Research, Haifa, Israel",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,109,112,"Automatic detection and classification of lesions in medical images is a desirable goal, with numerous clinical applications. In breast imaging, multiple modalities such as X-ray, ultrasound and MRI are often used in the diagnostic workflow. Training robust classifiers for each modality is challenging due to the typically small size of the available datasets. We propose to use cross-modal transfer learning to improve the robustness of the classifiers. We demonstrate the potential of this approach on a problem of identifying masses in breast MRI images, using a network that was trained on mammography images. Comparison between cross-modal and cross-domain transfer learning showed that the former improved the classification performance, with overall accuracy of 0.93 versus 0.90, while the accuracy of de-novo training was 0.94. Using transfer learning within the medical imaging domain may help to produce standard pre-trained shared models, which can be utilized to solve a variety of specific clinical problems.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950480,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950480,breast imaging;computer-aided diagnosis;deep learning;multimodal analysis;transfer learning,Biomedical imaging;Breast;Data models;Lesions;Magnetic resonance imaging;Training,biomedical MRI;image classification;learning (artificial intelligence);mammography;medical image processing,breast MRI images;breast imaging;breast lesion classification;cross-domain transfer learning;cross-modal deep learning;mammography images,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
61,Handcrafted features vs ConvNets in 2D echocardiographic images,C. Raynaud; H. Langet; M. S. Amzulescu; E. Saloux; H. Bertrand; P. Allain; P. Piro,"Philips Research Medisys, Paris, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1116,1119,"In this paper, we address the problem of automated pose classification and segmentation of the left ventricle (LV) in 2D echocardiographic images. For this purpose, we compare two complementary approaches. The first one is based on engineering ad-hoc features according to the traditional machine learning paradigm. Namely, we extract phase features to build an unsupervised LV pose estimator, as well as a global image descriptor for view type classification. We also apply the Supervised Descent Method (SDM) to iteratively refine the LV contour. The second approach follows the deep learning framework, where a Convolutional Network (ConvNet) learns the visual features automatically. Our experiments on a large database of apical sequences show that the two approaches yield comparable results on view classification, but SDM outperforms ConvNet on LV segmentation at a significantly lower training computational cost.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950712,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950712,2D ultrasound;ConvNets;Supervised Descent Method;left ventricle segmentation;phase features,Databases;Image segmentation;Pose estimation;Robustness;Shape;Training;Two dimensional displays,echocardiography;feature extraction;image classification;image segmentation;image sequences;iterative methods;learning (artificial intelligence);medical image processing,2D echocardiographic images;ConvNet;LV contour;LV segmentation;SDM;ad-hoc features;apical sequences;automated pose classification;convolutional network;deep learning framework;handcrafted features;image classification;iterative method;left ventricle segmentation;machine learning paradigm;phase feature extraction;supervised descent method;unsupervised LV pose estimator;visual features,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
62,Detection of lumen and media-adventitia borders in IVUS images using sparse auto-encoder neural network,S. Su; Z. Gao; H. Zhang; Q. Lin; W. K. Hau; S. Li,"College of Sciences, Zhejiang University of Technology, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1120,1124,"This paper describes an artificial neural network (ANN) method that employs a feature-learning algorithm to detect the lumen and MA borders in intravascular ultrasound (IVUS) images. Three types of imaging features including spatial, neighboring, and gradient features were used as the input features to the neural network, and then the different vascular layers were distinguished using two sparse autoencoders and one softmax classifier. To smooth the lumen and MA borders detected by the ANN method, we used the active contour model. The performance of our approach was compared with the manual drawing method and another existing method on 538 IVUS images from six subjects. Results showed that our approach had a high correlation (r = 0.9284 ~ 0.9875 for all measurements) and good agreement (bias = 0.0148 ~ 0.4209 mm) with the manual drawing method, and small detection error (lumen border: 0.0928±0.0935 mm, MA border: 0.1056±0.1088 mm). The average time to process each image was 14±4.6 seconds. The obtained results indicate that our proposed approach can be used to efficiently and accurately detect the lumen and MA borders in IVUS images.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950713,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950713,Deep neural network;Image segmentation;Intravascular image;Sparse autoencoder,Active contours;Artificial neural networks;Biomedical imaging;Feature extraction;Manuals;Training,biomedical ultrasonics;blood vessels;feature extraction;image classification;medical image processing;neural nets,ANN method;IVUS image;artificial neural network;feature-learning algorithm;imaging feature;intravascular ultrasound;lumen detection;media-adventitia border detection;softmax classifier;sparse autoencoder neural network;vascular layer,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
63,Feature extraction using multimodal convolutional neural networks for visual speech recognition,E. Tatulli; T. Hueber,"CNRS / Univ. Grenoble-Alpes / GIPSA-lab, France","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619.0,2017,,,2971,2975,"This article addresses the problem of continuous speech recognition from visual information only, without exploiting any audio signal. Our approach combines a video camera and an ultrasound imaging system for monitoring simultaneously the speaker's lips and the movement of the tongue. We investigate the use of convolutional neural networks (CNN) to extract visual features directly from the raw ultrasound and video images. We propose different architectures among which a multimodal CNN processing jointly the two visual modalities. Combined with an HMM-GMM decoder, the CNN-based approach outperforms our previous baseline based on Principal Component Analysis. Importantly, the recognition accuracy is only 4% lower than the one obtained when decoding the audio signal, which makes it a good candidate for a practical visual speech recognition system.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7952701,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952701,Convolutional Neural Networks;Deep Learning;Visual Speech Recognition,Decoding;Feature extraction;Hidden Markov models;Speech;Speech recognition;Ultrasonic imaging;Visualization,Gaussian processes;convolution;feature extraction;hidden Markov models;mixture models;neural nets;principal component analysis;speech recognition;ultrasonic imaging;video cameras;video signal processing,Gaussian mixture model;HMM-GMM decoder;continuous speech recognition;feature extraction;hidden Markov model;multimodal CNN processing;multimodal convolutional neural networks;principal component analysis;speaker lips monitoring;tongue movement;ultrasound images;ultrasound imaging system;video camera;video images;visual modalities;visual speech recognition,,,,,,,,5-9 March 2017,,IEEE,IEEE Conference Publications
64,Coronary luminal and wall mask prediction using convolutional neural network,Y. Hong; Y. M. Hong; Y. Jang; S. Kim; B. Jeon; S. Jung; S. Ha; D. Han; H. Shim; H. J. Chang,"Brain Korea 21 PLUS Project for Medical Science, Yonsei University, South Korea",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1049,1052,"A significant amount of research has been done on the segmentation of coronary arteries. However, the resulting automated boundary delineation is still not suitable for clinical utilization. The convolutional neural network was driving advances in the medical image processing. We propose the brief convolutional network (BCN) that automatically produces the labeled mask with the luminal and wall boundaries of the coronary artery. We utilized 50 patients of CCTA - intravascular ultrasound matched image data sets. Training and testing were performed on 40 and 10 patient data sets, respectively. The prediction of luminal and wall mask was performed using stacked BCN on the each image view: axial, coronal, and sagittal of straightened curved planar reformation. We defined the vector that includes probability from BCN result on each image view and proposed amplified probability. We used an Adaptive Boost regressor with an extremely randomized tree regressor to determine the label for unknown probability vector.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950696,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950696,Classification;Convolutional neural network;Coronary artery;Deep learning;Plaque quantification,Arteries;Computer architecture;Feature extraction;Image segmentation;Neural networks;Training;Ultrasonic imaging,biomedical ultrasonics;blood vessels;cardiology;image segmentation;learning (artificial intelligence);medical image processing;neural nets;probability;regression analysis,CCTA-intravascular ultrasound image;adaptive boost regressor;brief convolutional network;convolutional neural network;coronary artery segmentation;coronary luminal;coronary wall mask prediction;extremely randomized tree regressor;image view;medical image processing,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
65,Ultrasound Standard Plane Detection Using a Composite Neural Network Framework,H. Chen; L. Wu; Q. Dou; J. Qin; S. Li; J. Z. Cheng; D. Ni; P. A. Heng,"Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong",IEEE Transactions on Cybernetics,20170520.0,2017,47,6.0,1576,1586,"Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.",2168-2267;21682267,,10.1109/TCYB.2017.2685080,"National Basic Research Program of China, 973 Program; National Natural Science Foundation of China; Research Grants Council of Hong Kong Special Administrative Region; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445,Convolutional neural network (CNN);deep learning;knowledge transfer;recurrent neural network (RNN);standard plane;ultrasound (US),Biomedical imaging;Feature extraction;Fetus;Machine learning;Standards;Training data;Videos,,,,,,,,,20170330.0,June 2017,,IEEE,IEEE Journals & Magazines
66,Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks,Q. Dou; H. Chen; L. Yu; L. Zhao; J. Qin; D. Wang; V. C. Mok; L. Shi; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1182,1195,"Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.",0278-0062;02780062,,10.1109/TMI.2016.2528129,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403984,3D convolutional neural networks;biomarker detection;cerebral microbleeds;deep learning;susceptibility-weighted imaging,Biomarkers;Feature extraction;Kernel;MIMICs;Medical diagnostic imaging;Three-dimensional displays,biomedical MRI;blood;blood vessels;brain;cognition;diseases;feature extraction;haemodynamics;medical image processing;neurophysiology;probability,3D FCN strategy;3D convolutional neural networks;3D fully convolutional network strategy;CMB detection;MR volume extraction;MRI;automatic cerebral microbleed detection;blood vessels;cerebrovascular diseases;cognitive dysfunctions;current clinical routine;diagnostic biomarkers;haemorrhages;low-level hand-crafted descriptors;magnetic resonance images;massive redundant computations;probabilities;radiologists;representative high-level features;spatial contextual information;traditional sliding window strategy;well-trained 3D CNN discrimination,,15.0,,52.0,,,20160211.0,May 2016,,IEEE,IEEE Journals & Magazines
67,Retinal vessel landmark detection using deep learning and hessian matrix,T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang,"Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China",2015 8th International Congress on Image and Signal Processing (CISP),20160218.0,2015,,,387,392,"The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.",,Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2,10.1109/CISP.2015.7407910,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910,Deep learning;Hessian response;Image registration;Landmark detection;The probability map;The retinal image;Vessel Segmentation,Feature extraction;Image color analysis;Image registration;Image segmentation;Machine learning;Neural networks;Retina,Hessian matrices;eye;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery,Hessian matrix;bifurcation points;crossover points;curvature extreme points;deep learning;feature detection;image noise;multimodel retinal image;multiscale Hessian response;ophthalmological surgery;pathology;retinal image probability map;retinal image registration;retinal vessel landmark detection;vessel segmentation,,,,9.0,,,,14-16 Oct. 2015,,IEEE,IEEE Conference Publications
68,A supervised method using convolutional neural networks for retinal vessel delineation,Q. Li; L. Xie; Q. Zhang; S. Qi; P. Liang; H. Zhang; T. Wang,"National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060",2015 8th International Congress on Image and Signal Processing (CISP),20160218.0,2015,,,418,422,"Retinal vessel delineation is a hot research topic owing to its importance in a lot of clinic application. Several methods have been proposed in the past decades. Here we will present a new supervised method for retinal vessel segmentation. The method is designed to explore the complex relationship between retinal images and their corresponding vessel label maps. Specifically, in order to build a model describing the direct transformation from retinal image to vessel map, we introduce a deep convolutional neural network (abbreviation as CNN), which has strong enough induction ability. For the purpose of constructing the whole vessel probability map, we also design a synthesis method. Our method shows better performance on DRIVE dataset than state-of-the-art of reported approaches in the light of sensitivity (abbreviation as Se), specificity (abbreviation as Sp) and accuracy (abbreviation as Acc). Our proposed method has great potential to be applied in existing computer-assisted diagnostic system of ophthalmologic diseases. Meanwhile, the method may offer a novel, general computing framework for segmentation in other fields.",,Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2,10.1109/CISP.2015.7407916,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407916,CNN;deep learning;retinal image;vessel delineation,Feature extraction;Image segmentation;Measurement;Neural networks;Retinal vessels;Training,blood vessels;eye;feedforward neural nets;image segmentation;medical image processing;patient diagnosis;probability,DRIVE dataset;clinic application;computer-assisted diagnostic system;convolutional neural networks;deep convolutional neural network;general computing framework;ophthalmologic diseases;retinal images;retinal vessel delineation;retinal vessel segmentation;supervised method;synthesis method;vessel label maps;vessel probability map,,,,9.0,,,,14-16 Oct. 2015,,IEEE,IEEE Conference Publications
69,A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images,Q. Li; B. Feng; L. Xie; P. Liang; H. Zhang; T. Wang,"Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China",IEEE Transactions on Medical Imaging,20151229.0,2016,35,1.0,109,118,"This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.",0278-0062;02780062,,10.1109/TMI.2015.2457891,Project of the National Science Foundation of China; Shenzhen Science Plan of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161344,Cross-modality learning;deep learning;retinal image;vessel segmentation,Accuracy;Deformable models;Feature extraction;Image segmentation;Neural networks;Retina;Training,eye;image segmentation;medical image processing;neural nets,center pixel;computing framework;cross-modality learning approach;cross-training evaluation;image diagnosis;image segmentation;induction ability;neural network;ophthalmologic diseases;retinal images;vessel map;vessel segmentation,,14.0,,35.0,,,20150717.0,Jan. 2016,,IEEE,IEEE Journals & Magazines
70,Accurate Segmentation of Cervical Cytoplasm and Nuclei Based on Multiscale Convolutional Network and Graph Partitioning,Y. Song; L. Zhang; S. Chen; D. Ni; B. Lei; T. Wang,Shenzhen University,IEEE Transactions on Biomedical Engineering,20150916.0,2015,62,10.0,2421,2433,"In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.",0018-9294;00189294,,10.1109/TBME.2015.2430895,48th Scientific Research Foundation for the Returned Overseas Chinese Scholars; Shenzhen Key Basic Research; Shenzhen-Hong Kong Innovation Circle Funding; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004608 - National Natural Science Foundation of Guangdong Province; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103332,Cervical segmentation;coarse to fine;graph partitioning;graph-partitioning;multi-scale convolutional network;multiscale convolutional network (MSCN);touching-cell splitting,Computer architecture;Feature extraction;Image color analysis;Image edge detection;Image segmentation;Microprocessors;Shape,biomedical optical imaging;cancer;cellular biophysics;computational complexity;feature extraction;image segmentation;image texture;learning (artificial intelligence);medical image processing,MSCN;automated graph partitioning method;cervical cytoplasm segmentation;cervical nucleus cell segmentation;coarse segmentation;coarse-to-fine nucleus segmentation framework;computational complexity;contextual information;deep learning;multiscale convolutional network;pretrained feature;raw pixels;scale invariant feature extraction;shape information;superpixel;target objects;texture information,,12.0,,51.0,,,20150507.0,Oct. 2015,,IEEE,IEEE Journals & Magazines
71,A deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei,Y. Song; L. Zhang; S. Chen; D. Ni; B. Li; Y. Zhou; B. Lei; T. Wang,"Department of Biomedical Engineering, School of Medicine, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106.0,2014,,,2903,2906,"In this paper, a superpixel and convolution neural network (CNN) based segmentation method is proposed for cervical cancer cell segmentation. Since the background and cytoplasm contrast is not relatively obvious, cytoplasm segmentation is first performed. Deep learning based on CNN is explored for region of interest detection. A coarse-to-fine nucleus segmentation for cervical cancer cell segmentation and further refinement is also developed. Experimental results show that an accuracy of 94.50% is achieved for nucleus region detection and a precision of 0.9143±0.0202 and a recall of 0.8726±0.0008 are achieved for nucleus cell segmentation. Furthermore, our comparative analysis also shows that the proposed method outperforms the related methods.",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6944230,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944230,,Accuracy;Cervical cancer;Image color analysis;Image segmentation;Neural networks;Training,biological organs;cancer;cellular biophysics;image segmentation;medical image processing;neural nets,CNN based segmentation method;cervical cancer cell segmentation;cervical cytoplasm;coarse-to-fine nucleus segmentation;convolution neural network;cytoplasm segmentation;deep learning based framework;nucleus cell segmentation;nucleus region detection,,1.0,,9.0,,,,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
72,Non-rigid Segmentation Using Sparse Low Dimensional Manifolds and Deep Belief Networks,J. C. Nascimento; G. Carneiro,"Inst. de Sist. e Robot., Inst. Super. Tecnico, Lisbon, Portugal",2014 IEEE Conference on Computer Vision and Pattern Recognition,20140925.0,2014,,,288,295,"In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the search procedure is onducted directly on a sparse low-dimensional manifold, guided by the classification results computed from a deep belief network. Our main contribution is the fact that we do not rely on the typical sub-division of segmentation tasks into rigid detection and non-rigid delineation. Instead, the non-rigid segmentation is performed directly, where points in the sparse low-dimensional can be mapped to an explicit contour representation in image space. Our proposal shows significantly smaller search and training complexities given that the dimensionality of the manifold is much smaller than the dimensionality of the search spaces for rigid detection and non-rigid delineation aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database. Our experiments show that the use of sparse low dimensional manifolds reduces the search and training complexities of current segmentation approaches without a significant impact on the segmentation accuracy shown by state-of-the-art approaches.",1063-6919;10636919,Electronic:978-1-4799-5118-5; POD:978-1-4799-5119-2,10.1109/CVPR.2014.44,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909438,Deep Belief Nets;Non-rigid segmentation;Sparse manifolds,Complexity theory;Image segmentation;Manifolds;Search problems;Shape;Training;Visualization,belief networks;biomedical ultrasonics;cardiology;image classification;image representation;image segmentation;learning (artificial intelligence);medical image processing;visual databases,contour representation;deep belief networks;extended Cohn-Kanade database;image classification;left ventricle endocardial segmentation;lip segmentation;nonrigid visual object segmentation;sparse low dimensional manifolds;ultrasound images,,3.0,,24.0,,,,23-28 June 2014,,IEEE,IEEE Conference Publications
73,Image quality classification for DR screening using deep learning,F. Yu; J. Sun; A. Li; J. Cheng; C. Wan; J. Liu,"Nanjing University of Aeronautics and Astronautics, China",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,664,667,"The quality of input images significantly affects the outcome of automated diabetic retinopathy (DR) screening systems. Unlike the previous methods that only consider simple low-level features such as hand-crafted geometric and structural features, in this paper we propose a novel method for retinal image quality classification (IQC) that performs computational algorithms imitating the working of the human visual system. The proposed algorithm combines unsupervised features from saliency map and supervised features coming from convolutional neural networks (CNN), which are fed to an SVM to automatically detect high quality vs poor quality retinal fundus images. We demonstrate the superior performance of our proposed algorithm on a large retinal fundus image dataset and the method could achieve higher accuracy than other methods. Although retinal images are used in this study, the methodology is applicable to the image quality assessment and enhancement of other types of medical images.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036912,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036912,convolutional neural networks;image quality classification;saliency map,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
74,Analysis of Disfluencies for automatic detection of Mild Cognitive Impartment: a deep learning approach,K. Lopez-de-Ipina; U. Martinez-de-Lizarduy; P. M. Calvo; B. Beitia; J. Garcia-Melero; M. Ecay-Torres; A. Estanga; M. Faundez-Zanuy,"Faculty of Engineering 20018, Donostia-San Sebastian Spain Universidad del Pais Vasco/Euskal Herriko Unibertsitatea (UPV/EHU) {karmele.ipina, unai.martinezdelizarduy, pilarmaria.calvo, mariablanca.beitia",2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI),20170724.0,2017,,,1,4,"The so-called Mild Cognitive Impairment (MCI) or cognitive loss appears in a previous stage before Alzheimer's Disease (AD), but it does not seem sufficiently severe to interfere in independent abilities of daily life, so it usually does not receive an appropriate diagnosis. Its detection is a challenging issue to be addressed by medical specialists. This work presents a novel proposal based on automatic analysis of speech and disfluencies aimed at supporting MCI diagnosis. The approach includes deep learning by means of Convolutional Neural Networks (CNN) and non-linear multifeature modelling. Moreover, to select the most relevant features non-parametric Mann-Whitney U-testt and Support Vector Machine Attribute (SVM) evaluation are used.",,Electronic:978-1-5386-0850-0; POD:978-1-5386-0851-7,10.1109/IWOBI.2017.7985526,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985526,Automatic speechanalysis;Convolutional Neural Networks;Deep Learning;Disfluencies;Mild Cognitive Impairment;Nonlinearfeatures,Dementia;Frequency modulation;Machine learning;Neurons;Speech;Support vector machines,cognition;diseases;learning (artificial intelligence);medical signal detection;neural nets;speech;speech processing;support vector machines,Alzheimer disease;MCI diagnosis;automatic mild cognitive impartment detection;convolutional neural networks;deep learning approach;disfluencies analysis;nonlinear multifeature modelling;nonparametric Mann-Whitney U-test;speech analysis;support vector machine attribute evaluation,,,,,,,,10-12 July 2017,,IEEE,IEEE Conference Publications
75,Cell classification using convolutional neural networks in medical hyperspectral imagery,Xiang Li; W. Li; Xiaodong Xu; Wei Hu,"College of Information Science & Technology, Beijing University of Chemical Technology, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)",20170720.0,2017,,,501,504,"Hyperspectral imaging is a rising imaging modality in the field of medical applications, and the combination of both spectral and spatial information provides wealth information for cell classification. In this paper, deep convolutional neural network (CNN) is employed to achieve blood cell discrimination in medical hyperspectral images (MHSI). As a deep learning architecture, CNNs are expected to get more discriminative and semantic features, which effect classification accuracy to a certain extent. Experimental results based on two real medical hyperspectral image data sets demonstrate that cell classification using CNNs is effective. In addition, compared to traditional support vector machine (SVM), the proposed method, which jointly exploits spatial and spectral features, can achieve better classification performance, showcasing the CNN-based methods' tremendous potential for accurate medical hyperspectral data classification.",,DVD:978-1-5090-6236-2; Electronic:978-1-5090-6238-6; POD:978-1-5090-6239-3,10.1109/ICIVC.2017.7984606,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984606,blood cell classification;convolutional neural network;deep learning;medical hyperspectral imagery,Blood;Computer architecture;Hyperspectral imaging;Medical diagnostic imaging;Microprocessors;Support vector machines,blood;cellular biophysics;feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);medical image processing;neural nets,CNN;blood cell discrimination;cell classification;deep convolutional neural network;deep learning architecture;discriminative features;hyperspectral imaging;medical applications;medical hyperspectral data classification;medical hyperspectral imagery;semantic features;spatial features;spatial information;spectral features;spectral information,,,,,,,,2-4 June 2017,,IEEE,IEEE Conference Publications
76,A Unified Deep Learning Model for Protein Structure Prediction,L. Bai; L. Yang,"Sch. of Comput., Electron. & Inf. Guangxi Univ., Nanning, China",2017 3rd IEEE International Conference on Cybernetics (CYBCONF),20170720.0,2017,,,1,6,"Predicting protein tertiary structure from its primary amino acid sequence is one of the most challenging problems in bioinformatics, which makes an important impact in the field of medical science. The mainly difficult is how to learn the most useful and suitable protein features to improve the prediction. In this paper, we propose a novel unified deep learning model for improving protein tertiary structure prediction. The core contribution of this work is the group of deep convolution neural networks (deep CNNs) that can directly learn the high-level relational features from a pair of the query and target protein sequences. The deep CNN can learn high-level relational features from the pairwise protein sequences in a hierarchy by progressively integrating convergent protein property representations from lower levels. Multiple deep CNNs are designed to achieve robust protein structure similarities from different aspects. These relational features are fully connected to the top two Restricted Boltzmann Machines (RBMS) layer to further extract global relational features, which significantly improve the protein structure prediction. Experiments conducted on a well-known benchmark, SCOPe dataset, show that our model significantly outperforms the state-of-the-art methods in various statistical measurements. The results also demonstrate that our deep learning model can improve the learning of protein properties.",,Electronic:978-1-5386-2201-8; POD:978-1-5386-2202-5,10.1109/CYBConf.2017.7985752,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985752,,Convolution;Feature extraction;Hidden Markov models;Machine learning;Predictive models;Protein sequence,Boltzmann machines;bioinformatics;feedforward neural nets;learning (artificial intelligence);proteins;statistical analysis,RBMS layer;SCOPe dataset benchmark;bioinformatics;convergent protein property representations;deep CNN;deep-convolution neural networks;global relational feature extraction;high-level relational feature learning;pairwise protein sequences;primary amino acid sequence;protein feature learning;protein tertiary structure prediction;query protein sequences;restricted Boltzmann machine layer;robust protein structure similarities;statistical measurements;target protein sequences;unified deep-learning model,,,,,,,,21-23 June 2017,,IEEE,IEEE Conference Publications
77,An Automatic Detection System of Lung Nodule Based on Multi-Group Patch-Based Deep Learning Network,H. Jiang; H. Ma; W. Qian; M. Gao; Y. Li,shenyang China (e-mail: hongyang1020@126.com),IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"High-efficiency lung nodule detection dramatically contributes to the risk assessment of lung cancer. It is a significant and challenging task to quickly locate the exact positions of lung nodules. Extensive work has been done by researchers around this domain for approximately two decades. However, previous computer aided detection (CADe) schemes are mostly intricate and time-consuming since they may require more image processing modules, such as the computed tomography (CT) image transformation, the lung nodule segmentation and the feature extraction, to construct a whole CADe system. It is difficult for those schemes to process and analyze enormous data when the medical images continue to increase. Besides, some state of the art deep learning schemes may be strict in the standard of database. This study proposes an effective lung nodule detection scheme based on multi-group patches cut out from the lung images, which are enhanced by the Frangi filter. Through combining two groups of images, a four-channel convolution neural networks (CNN) model is designed to learn the knowledge of radiologists for detecting nodules of four levels. This CADe scheme can acquire the sensitivity of 80.06% with 4.7 false positives per scan and the sensitivity of 94% with 15.1 false positives per scan. The results demonstrate that the multi-group patch-based learning system is efficient to improve the performance of lung nodule detection and greatly reduce the false positives under a huge amount of image data.",2168-2194;21682194,,10.1109/JBHI.2017.2725903,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981333,Frangi filter;computed tomography (CT) images;computer aided detection (CADe);deep learning network;lung nodule detection,Biomedical imaging;Cancer;Computed tomography;Databases;Feature extraction;Image segmentation;Lungs,,,,,,,,,20170714.0,,,IEEE,IEEE Early Access Articles
78,Domain specific convolutional neural nets for detection of architectural distortion in mammograms,R. Ben-Ari; A. Akselrod-Ballin; L. Karlinsky; S. Hashoul,"IBM Research - Haifa, Israel",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,552,556,"Detection of Architectural distortion (AD) is important for ruling out possible pre-malignant lesions in breast, but due to its subtlety, it is often missed on the screening mammograms. In this work we suggest a novel AD detection method based on region proposal convolution neural nets (R-CNN). When the data is scarce, as typically the case in medical domain, R-CNN yields poor results. In this study, we suggest a new R-CNN method addressing this shortcoming by using a pretrained network on a candidate region guided by clinical observations. We test our method on the publicly available DDSM data set, with comparison to the latest faster R-CNN and previous works. Our detection accuracy allows binary image classification (normal vs. containing AD) with over 80% sensitivity and specificity, and yields 0.46 false-positives per image at 83% true-positive rate, for localization accuracy. These measures significantly improve the best results in the literature.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950581,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950581,Architectural Distortion;Breast Mammography;Computer-Aided Diagnosis;Convolution Neural Net;Deep Learning;Region Proposal,Breast;Convolution;Mammography;Neural networks;Proposals;Sensitivity;Training,cancer;image classification;mammography;medical image processing;neural nets,AD detection method;R-CNN method;architectural distortion detection;image classification;mammograms;region proposal convolution neural nets,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
79,HEp-2 cell classification based on a Deep Autoencoding-Classification convolutional neural network,J. Liu; B. Xu; L. Shen; J. Garibaldi; G. Qiu,"The Universiy of Nottingham Ningbo China, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1019,1023,"In this paper, we present a novel deep learning model termed Deep Autoencoding-Classification Network (DACN) for HEp-2 cell classification. The DACN consists of an autoencoder and a normal classification convolutional neural network (CNN), while the two architectures shares the same encoding pipeline. The DACN model is jointly optimized for the classification error and the image reconstruction error based on a multi-task learning procedure. We evaluate the proposed model using the publicly available ICPR2012 benchmark dataset. We show that this architecture is particularly effective when the training dataset is small which is often the case in medical imaging applications. We present experimental results to show that the proposed approach outperforms all known state of the art HEp-2 cell classification methods.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950689,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950689,HEp2 cells;autoencoder;classification;convolutional neural networks;indirect immunofluorescence,Benchmark testing;Computer architecture;Image reconstruction;Machine learning;Microprocessors;Solid modeling;Training,cellular biophysics;image classification;image coding;image reconstruction;learning (artificial intelligence);medical image processing;neural nets,HEp-2 cell classification methods;deep autoencoding-classification convolutional neural network;image reconstruction error;medical imaging applications;multitask learning procedure,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
80,A convolutional neural network approach for abnormality detection in Wireless Capsule Endoscopy,A. K. Sekuboyina; S. T. Devarakonda; C. S. Seelamantula,"Klinikum rechts der Isar der Technische Universit&#x00E4;t M&#x00FC;nchen, 81675, Germany",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1057,1060,"In wireless capsule endoscopy (WCE), a swallowable miniature optical endoscope is used to transmit color images of the gastrointestinal tract. However, the number of images transmitted is large, taking a significant amount of the medical expert's time to review the scan. In this paper, we propose a technique to automate the abnormality detection in WCE images. We split the image into several patches and extract features pertaining to each block using a convolutional neural network (CNN) to increase their generality while overcoming the drawbacks of manually crafted features. We intend to exploit the importance of color information for the task. Experiments are performed to determine the optimal color space components for feature extraction and classifier design. We obtained an area under receiver-operating-characteristic (ROC) curve of approximately 0.8 on a dataset containing multiple abnormalities.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950698,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950698,Classification;Convolutional neural networks;Gastrointestinal tract;Wireless capsule endoscopy,Endoscopes;Feature extraction;Hemorrhaging;Image color analysis;Lesions;Neural networks;Training,biomedical optical imaging;endoscopes;feature extraction;medical image processing;neural nets;sensitivity analysis,abnormality detection;convolutional neural network;feature extraction;receiver-operating-characteristic curve;wireless capsule endoscopy,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
81,FUIQA: Fetal Ultrasound Image Quality Assessment With Deep Convolutional Networks,L. Wu; J. Z. Cheng; S. Li; B. Lei; T. Wang; D. Ni,"National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China",IEEE Transactions on Cybernetics,20170520.0,2017,47,5.0,1336,1349,"The quality of ultrasound (US) images for the obstetric examination is crucial for accurate biometric measurement. However, manual quality control is a labor intensive process and often impractical in a clinical setting. To improve the efficiency of examination and alleviate the measurement error caused by improper US scanning operation and slice selection, a computerized fetal US image quality assessment (FUIQA) scheme is proposed to assist the implementation of US image quality control in the clinical obstetric examination. The proposed FUIQA is realized with two deep convolutional neural network models, which are denoted as L-CNN and C-CNN, respectively. The L-CNN aims to find the region of interest (ROI) of the fetal abdominal region in the US image. Based on the ROI found by the L-CNN, the C-CNN evaluates the image quality by assessing the goodness of depiction for the key structures of stomach bubble and umbilical vein. To further boost the performance of the L-CNN, we augment the input sources of the neural network with the local phase features along with the original US data. It will be shown that the heterogeneous input sources will help to improve the performance of the L-CNN. The performance of the proposed FUIQA is compared with the subjective image quality evaluation results from three medical doctors. With comprehensive experiments, it will be illustrated that the computerized assessment with our FUIQA scheme can be comparable to the subjective ratings from medical doctors.",2168-2267;21682267,,10.1109/TCYB.2017.2671898,National Key Research and Development Program of China; Natural Science Foundation of SZU; Open Fund Project of Fujian Provincial Key Laboratory of Information Processing and Intelligent Control (Minjiang University); Shenzhen Basic Research Project; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875138,Deep convolutional neural network (DCNN);fetal ultrasound (US);local phase;quality control,Biomedical imaging;Biomedical measurement;Image quality;Quality assessment;Quality control;Standards;Ultrasonic imaging,biomedical ultrasonics;feedforward neural nets;medical image processing;obstetrics;ultrasonic imaging,C-CNN;FUIQA;L-CNN;ROI;clinical obstetric examination;deep convolutional neural network;fetal US image quality assessment;fetal ultrasound image quality assessment;region of interest,,,,,,,20170309.0,May 2017,,IEEE,IEEE Journals & Magazines
82,An Ensemble of Fine-Tuned Convolutional Neural Networks for Medical Image Classification,A. Kumar; J. Kim; D. Lyndon; M. Fulham; D. Feng,"Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,31,40,"The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than - ethods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.",2168-2194;21682194,,10.1109/JBHI.2016.2635663,10.13039/100000163 - ARC; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769199,Convolutional neural network (CNN);deep learning;ensembles;fine-tuning;image classification,Biomedical imaging;Computer architecture;Feature extraction;Informatics;Neural networks;Training;Training data,feature extraction;image classification;image representation;medical image processing;neural nets;probability,ImageCLEF 2016 medical image public dataset;feature extraction;fine-tuned convolutional neural networks;image features;medical image classification;natural image dataset;posterior probability;semantic image representation,,,,,,,20161205.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
83,Unsupervised Joint Mining of Deep Features and Image Labels for Large-Scale Radiology Image Categorization and Scene Recognition,X. Wang; L. Lu; H. C. Shin; L. Kim; M. Bagheri; I. Nogues; J. Yao; R. M. Summers,,2017 IEEE Winter Conference on Applications of Computer Vision (WACV),20170515.0,2017,,,998,1007,"The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of ""Google Search"" and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized ""convergence"" of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75:3%, compared to the state-of-the-art supervised classification accuracy of 81:0% (when both are based on the VGG-VD model).",,Electronic:978-1-5090-4822-9; POD:978-1-5090-4823-6,10.1109/WACV.2017.116,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926699,,Biomedical imaging;Computational modeling;Feature extraction;Image recognition;Image representation;Optimization;Radiology,,,,,,,,,,24-31 March 2017,,IEEE,IEEE Conference Publications
84,L-CNN: Exploiting labeling latency in a CNN learning framework,M. J. Afridi; A. Ross; E. M. Shapiro,"Department of Computer Science and Engineering, Michigan State University, United States of America",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,2156,2161,"A supervised learning system requires labeled data during the training phase. Obtaining labels can be an expensive process, especially in medical imaging applications where a qualified expert may be needed to carefully analyze images and annotate them. This constrains the amount of labeled data available. This study explores the possibility of incorporating labeling behavior (viz., labeling latency) in a supervised convolutional neural network (CNN) framework in order to improve its performance in the presence of limited labeled data. The problem of “spot” detection in MRI scans is considered in this work. In this two-class problem, (a) labeling behavior is available only during the training phase unlike traditional features that are available both during training and testing; and (b) the labeling behavior is associated with only one class (the positive samples) unlike other side information that is available for all classes. To address these issues, a new CNN architecture referred to as L-CNN is designed. The proposed method utilizes the labeling behavior of the expert to cluster the labeled data into multiple categories; a source CNN is then trained to distinguish between these categories. Next, a transfer learning paradigm is used where a target CNN is initialized using this source CNN and its weights updated with the limited labeled data that is available. Experimental results on an existing MRI database show that the proposed L-CNN performs better than a conventional CNN and, further, significantly outperforms the previous state-of-the-art, thereby establishing a new baseline for “spot” detection in MRI.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899955,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899955,,Biomedical imaging;Computer architecture;Labeling;Magnetic resonance imaging;Microprocessors;Testing;Training,biomedical MRI;learning (artificial intelligence);medical image processing;neural nets,L-CNN;MRI scans;labeling behavior;supervised convolutional neural network learning framework;supervised learning system;transfer learning paradigm;two-class problem,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
85,An efficient radiographic Image Retrieval system using Convolutional Neural Network,M. Chowdhury; S. R. Bulò; R. Moreno; M. K. Kundu; Ö. Smedby,"KTH, School of Technology and Health, H&#x00E4;lsov&#x00E4;gen 11c, SE-141 57 Huddinge, Sweden",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,3134,3139,"Content-Based Medical Image Retrieval (CBMIR) is an important research field in the context of medical data management. In this paper we propose a novel CBMIR system for the automatic retrieval of radiographic images. Our approach employs a Convolutional Neural Network (CNN) to obtain high-level image representations that enable a coarse retrieval of images that are in correspondence to a query image. The retrieved set of images is refined via a non-parametric estimation of putative classes for the query image, which are used to filter out potential outliers in favour of more relevant images belonging to those classes. The refined set of images is finally re-ranked using Edge Histogram Descriptor, i.e. a low-level edge-based image descriptor that allows to capture finer similarities between the retrieved set of images and the query image. To improve the computational efficiency of the system, we employ dimensionality reduction via Principal Component Analysis (PCA). Experiments were carried out to evaluate the effectiveness of the proposed system on medical data from the “Image Retrieval in Medical Applications” (IRMA) benchmark database. The obtained results show the effectiveness of the proposed CBMIR system in the field of medical image retrieval.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7900116,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900116,,Histograms;Image edge detection;Image retrieval;Medical diagnostic imaging;Principal component analysis;Radiography,diagnostic radiography;image retrieval;medical image processing;neural nets;principal component analysis;query processing;set theory,CBMIR system;CNN;IRMA;PCA;content-based medical image retrieval;convolutional neural network;edge histogram descriptor;efficient radiographic image retrieval system;high-level image representations;image retrieval-in-medical applications;medical data management;nonparametric estimation;principal component analysis;query image,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
86,Cardiac left ventricle segmentation using convolutional neural network regression,L. K. Tan; Y. M. Liew; E. Lim; R. A. McLaughlin,"Faculty of Medicine, University of Malaya, Kuala Lumpur, Malaysia",2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES),20170206.0,2016,,,490,493,"Cardiac MRI is important for the diagnosis and assessment of various cardiovascular diseases. Automated segmentation of the left ventricular (LV) endocardium at end-diastole (ED) and end-systole (ES) enables automated quantification of various clinical parameters including ejection fraction. Neural networks have been used for general image segmentation, usually via per-pixel categorization e.g. “foreground” and “background”. In this paper we propose that the generally circular LV endocardium can be parameterized and the endocardial contour determined via neural network regression. We designed two convolutional neural networks (CNN), one for localization of the LV, and the other for determining the endocardial radius. We trained the networks against 100 datasets from the Medical Image Computing and Computer Assisted Intervention (MICCAI) 2011 challenge, and tested the networks against 45 datasets from the MICCAI 2009 challenge. The networks achieved 0.88 average Dice metric, 2.30 mm average perpendicular distance, and 97.9% good contours, the latter being the highest published result to date. These results demonstrate that CNN regression is a viable and highly promising method for automated LV endocardial segmentation at ED and ES phases, and is capable of generalizing learning between highly distinct training and testing data sets.",,Electronic:978-1-4673-7791-1; POD:978-1-4673-7792-8,10.1109/IECBES.2016.7843499,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843499,cardiac MRI;left ventricle;neural network regression;segmentation,Blood;Image segmentation;Magnetic resonance imaging;Measurement;Neural networks;Standards;Training,biomedical MRI;cardiology;diseases;image segmentation;medical image processing;neural nets;regression analysis,Medical Image Computing and Computer Assisted Intervention;automated segmentation;cardiac MRI;cardiac left ventricle segmentation;cardiovascular disease assessment;cardiovascular disease diagnosis;circular left ventricular endocardium;convolutional neural network regression;ejection fraction;endocardial contour;general image segmentation,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
87,Dependency-based convolutional neural network for drug-drug interaction extraction,S. Liu; Kai Chen; Q. Chen; B. Tang,"Intelligent Computing Research Center, Harbin Institute of Technology Shenzhen Graduate School, 518055, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,1074,1080,"Drug-drug interactions (DDIs) are crucial for healthcare. Besides DDIs reported in medical knowledge bases such as DrugBank, a large number of latest DDI findings are also reported in unstructured biomedical literature. Extracting DDIs from unstructured biomedical literature is a worthy addition to the existing knowledge bases. Currently, convolutional neural network (CNN) is a state-of-the-art method for DDI extraction. One limitation of CNN is that it neglects long distance dependencies between words in candidate DDI instances, which may be helpful for DDI extraction. In order to incorporate the long distance dependencies between words in candidate DDI instances, in this work, we propose a dependency-based convolutional neural network (DCNN) for DDI extraction. Experiments conducted on the DDIExtraction 2013 corpus show that DCNN using a public state-of-the-art dependency parser achieves an F-score of 70.19%, outperforming CNN by 0.44%. By analyzing errors of DCNN, we find that errors from dependency parsers are propagated into DCNN and affect the performance of DCNN. To reduce error propagation, we design a simple rule to combine CNN with DCNN, that is, using DCNN to extract DDIs in short sentences and CNN to extract DDIs in long distances as most dependency parsers work well for short sentences but bad for long sentences. Finally, our system that combines CNN and DCNN achieves an F-score of 70.81%, outperforming CNN by 1.06% and DNN by 0.62% on the DDIExtraction 2013 corpus.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822671,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822671,Biomedical literature;Dependency-based convolutional neural network;Drug-drug interaction,Drugs;Neural networks;TV,drugs;feature extraction;health care;medical computing;neural nets,DDIExtraction 2013 corpus;DrugBank;F-score;dependency-based convolutional neural network;drug-drug interaction extraction;healthcare;medical knowledge bases;public state-of-the-art dependency parser;unstructured biomedical literature,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
88,Deep learning-based pipeline to recognize Alzheimer's disease using fMRI data,S. Sarraf; G. Tofighi,"Department of Electrical and Computer Engineering, McMaster University Hamilton, ON, L8S 4L8, Canada, Rotman Research Institue at Baycrest, University of Toronto",2016 Future Technologies Conference (FTC),20170119.0,2016,,,816,820,"Over the past decade, machine learning techniques and in particular predictive modeling and pattern recognition in biomedical sciences, from drug delivery systems to medical imaging, have become one of the most important methods of assisting researchers in gaining a deeper understanding of issues in their entirety and solving complex medical problems. Deep learning is a powerful machine learning algorithm in classification that extracts low-to high-level features. In this paper, we employ a convolutional neural network to distinguish an Alzheimers brain from a normal, healthy brain. The importance of classifying this type of medical data lies in its potential to develop a predictive model or system in order to recognize the symptoms of Alzheimers disease when compared with normal subjects and to estimate the stages of the disease. Classification of clinical data for medical conditions such as Alzheimers disease has always been challenging, and the most problematic aspect has always been selecting the strongest discriminative features. Using the Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimers subjects from normal controls, where the accuracy of testing data reached 96.85%. This experiment suggests that the shift and scale invariant features extracted by CNN followed by deep learning classification represents the most powerful method of distinguishing clinical data from healthy data in fMRI. This approach also allows for expansion of the methodology to predict more complicated systems.",,Electronic:978-1-5090-4171-8; POD:978-1-5090-4172-5,10.1109/FTC.2016.7821697,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821697,Alzheimer's Disease;Deep learning;fMRI,Alzheimer's disease;Biological neural networks;Biomedical imaging;Feature extraction;Machine learning;Neurons,biomedical MRI;convolution;diseases;feature extraction;learning (artificial intelligence);medical computing;neural nets;pattern classification,Alzheimer disease;Alzheimers brain;CNN;LeNet-5 architecture;biomedical sciences;clinical data classification;convolutional neural network;deep learning-based pipeline;drug delivery systems;functional MRI data classification;machine learning;medical imaging;pattern recognition;predictive modeling;scale invariant features extraction,,,,,,,,6-7 Dec. 2016,,IEEE,IEEE Conference Publications
89,A deep tongue image features analysis model for medical application,Dan Meng; Guitao Cao; Y. Duan; Minghua Zhu; Liping Tu; Jiatuo Xu; D. Xu,"School of Computer Scinence and Software Engineering, East China Normal University, Shanghai, China 200062",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,1918,1922,"With the improvement of people's living standards, there is no doubt that people are paying more and more attention to their health. However, shortage of medical resources is a critical global problem. As a result, an intelligent prognostics system has a great potential to play important roles in computer aided diagnosis. Numerous papers reported that tongue features have been closely related to a human's state. Among them, the majority of the existing tongue image analyses and classification methods are based on the low-level features, which may not provide a holistic view of the tongue. Inspired by a deep convolutional neural network (CNN), we propose a deep tongue image feature analysis system to extract unbiased features and reduce human labor for tongue diagnosis. With the unbalanced sample distribution, it is hard to form a balanced classification model based on feature representations obtained by existing low-level and high-level methods. Our proposed deep tongue image feature analysis model learns high-level features and provide more classification information during training time, which may result in higher accuracy when predicting testing samples. We tested the proposed system on a set of 267 gastritis patients, and a control group of 48 healthy volunteers (labeled according to Western medical practices). Test results show that the proposed deep tongue image feature analysis model can classify a given tongue image into healthy and diseased state with an average accuracy of 91.49%, which demonstrates the relationship between human body's state and its deep tongue image features.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822815,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822815,Tongue image;deep model;feature analysis;weighted SVM,Analytical models;Biomedical imaging;Computational modeling;Radio frequency;Sensitivity;Shape;Support vector machines,convolution;feature extraction;image classification;medical image processing;neural nets,computer aided diagnosis;deep convolutional neural network;deep tongue image classification methods;deep tongue image feature analysis model;intelligent prognostic system;medical application,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
90,EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos,A. P. Twinanda; S. Shehata; D. Mutter; J. Marescaux; M. de Mathelin; N. Padoy,"ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France",IEEE Transactions on Medical Imaging,20161230.0,2017,36,1.0,86,97,"Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, surgical phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the used visual features are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.",0278-0062;02780062,,10.1109/TMI.2016.2593957,French state funds managed by the ANR within the Investissements d¿Avenir program; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519080,Laparoscopic videos;cholecystectomy;convolutional neural network;phase recognition;tool presence detection,Computer architecture;Feature extraction;Image recognition;Laparoscopes;Surgery;Videos;Visualization,biomedical optical imaging;endoscopes;feature extraction;medical image processing;neural nets;optimisation;surgery,CNN architecture;EndoNet;automatic indexing;cataract surgeries;cholecystectomy videos;convolutional neural network;deep architecture;laparoscopic surgeries;laparoscopic videos;manual annotation process;medical applications;neurological surgeries;optimization;phase recognition task;real-time operating room scheduling;recognition tasks;surgical phase recognition;surgical video databases;surgical workflow recognition;tool presence detection tasks;tool usage signals;visual features,,1.0,,,,,20160722.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
91,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,F. Milletari; N. Navab; S. A. Ahmadi,,2016 Fourth International Conference on 3D Vision (3DV),20161219.0,2016,,,565,571,"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",,Electronic:978-1-5090-5407-7; POD:978-1-5090-5408-4,10.1109/3DV.2016.79,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785132,Deep learning;convolutional neural networks;machine learning;prostate;segmentation,Biomedical imaging;Feature extraction;Image segmentation;Magnetic resonance imaging;Neural networks;Three-dimensional displays;Two dimensional displays,biomedical MRI;image segmentation;medical image processing;neural nets,3D image segmentation;CNN;Dice coefficient;MRI volumes;V-Net;background voxel;clinical practice;computer vision;foreground voxel;fully convolutional neural networks;histogram matching;magnetic resonance imaging;medical image analysis;random nonlinear transformations;volumetric medical image segmentation,,1.0,,,,,,25-28 Oct. 2016,,IEEE,IEEE Conference Publications
92,Lesion border detection using deep learning,P. Sabouri; H. GholamHosseini,"Department of Electrical and Electronics Engineering, School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Private bag 92006, Auckland 1142, New Zealand",2016 IEEE Congress on Evolutionary Computation (CEC),20161121.0,2016,,,1416,1421,"Computer aided diagnosis of medical images can result in (better) detection in addition to early diagnosis of many symptoms to assist health physicians and therefore reducing the mortality rate. Realization of an efficient mobile device for automatic diagnosis of melanoma would greatly enhance the applicability of medical image classification scheme and make it useful in clinical contexts. In this paper, a deep learning method using convolutional neural networks (CNN) is proposed for border detection of skin lesions based on clinical images. Prepossessing of clinical and dermoscopy images has been common and necessary in the lesion segmentation realm; however, the result of the study shows that CNN can be used with relatively much less prepossessing algorithm compared with previous methods.",,Electronic:978-1-5090-0623-6; POD:978-1-5090-0624-3; USB:978-1-5090-0622-9,10.1109/CEC.2016.7743955,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743955,Deep learning;border detection;convolutional neural networks;lesion segmentation;melanoma detection,Feature extraction;Hair;Image color analysis;Image segmentation;Lesions;Malignant tumors;Skin,cancer;image classification;image segmentation;learning (artificial intelligence);medical image processing;mobile computing;neural nets;skin,CNN;automatic melanoma diagnosis;clinical image prepossessing;computer aided diagnosis;convolutional neural networks;deep learning;dermoscopy image prepossessing;lesion segmentation realm;medical image classification;mobile device;skin lesion border detection,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
93,A multiclass classification method based on deep learning for named entity recognition in electronic medical records,X. Dong; L. Qian; Y. Guan; L. Huang; Q. Yu; J. Yang,"Center of Excellence in Research and Education for Big Military Data Intelligence (CREDIT), Department of Electrical and Computer Engineering, Prairie View A&M University Houston, USA",2016 New York Scientific Data Summit (NYSDS),20161121.0,2016,,,1,10,"Research of named entity recognition (NER) on electrical medical records (EMRs) focuses on verifying whether methods to NER in traditional texts are effective for that in EMRs, and there is no model proposed for enhancing performance of NER via deep learning from the perspective of multiclass classification. In this paper, we annotate a real EMR corpus to accomplish the model training and evaluation. And, then, we present a Convolutional Neural Network (CNN) based multiclass classification method for mining named entities from EMRs. The method consists of two phases. In the phase 1, EMRs are pre-processed for representing samples with word embedding. In the phase 2, the method is built by segmenting training data into many subsets and training a CNN binary classification model on each of subset. Experimental results showed the effectiveness of our method.",,Electronic:978-1-4673-9051-4; POD:978-1-4673-9052-1,10.1109/NYSDS.2016.7747810,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747810,convolutional neural network;electrical medical records;machine learning;named entity recognition;natural language processing,Diseases;Feature extraction;Medical diagnostic imaging;Support vector machines;Unified modeling language,data mining;electronic health records;learning (artificial intelligence);neural nets;pattern classification,CNN based multiclass classification method;CNN binary classification model;EMR corpus;NER;convolutional neural network based multiclass classification method;deep learning;electronic medical records;named entity mining;named entity recognition;training data segmentation,,,,,,,,14-17 Aug. 2016,,IEEE,IEEE Conference Publications
94,Weakly-supervised Convolutional learning for detection of inflammatory gastrointestinal lesions,S. V. Georgakopoulos; D. K. Iakovidis; M. Vasilakakis; V. P. Plagianakos; A. Koulaouzidis,"Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece",2016 IEEE International Conference on Imaging Systems and Techniques (IST),20161110.0,2016,,,510,514,"Graphic image annotations provide the necessary ground truth information for supervised machine learning in image-based computer-aided medical diagnosis. Performing such annotations is usually a time-consuming and cost-inefficient process requiring knowledge from domain experts. To cope with this problem we propose a novel weakly-supervised learning method based on a Convolutional Neural Network (CNN) architecture. The advantage of the proposed method over conventional supervised approaches is that only image-level semantic annotations are used in the training process, instead of pixel-level graphic annotations. This can drastically reduce the required annotation effort. Its advantage over the few state-of-the-art weakly-supervised CNN architectures is its simplicity. The performance of the proposed method is evaluated in the context of computer-aided detection of inflammatory gastrointestinal lesions in wireless capsule endoscopy videos. This is a broad category of lesions, for which early detection and treatment can be of vital importance. The results show that the proposed weakly-supervised learning method can be more effective than the conventional supervised learning, with an accuracy of 90%.",,Electronic:978-1-5090-1817-8; POD:978-1-5090-1818-5,10.1109/IST.2016.7738279,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738279,convolutional neural networks;inflammatory lesions;lesion detection;medical image analysis;weakly supervised learning,Biomedical imaging;Computer architecture;Graphics;Lesions;Support vector machines;Training;Videos,endoscopes;learning (artificial intelligence);medical image processing;neural nets;object detection;patient diagnosis,convolutional neural network;graphic image annotations;image-based computer-aided medical diagnosis;image-level semantic annotations;inflammatory gastrointestinal lesion detection;supervised machine learning;training process;weakly-supervised CNN architectures;weakly-supervised convolutional learning;wireless capsule endoscopy videos,,,,,,,,4-6 Oct. 2016,,IEEE,IEEE Conference Publications
95,Generating binary tags for fast medical image retrieval based on convolutional nets and Radon Transform,X. Liu; H. R. Tizhoosh; J. Kofman,"Department of Systems Design Engineering, University of Waterloo, ON, Canada N2L 3G1",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,2872,2878,"Content-based image retrieval (CBIR) in large medical image archives is a challenging and necessary task. Generally, different feature extraction methods are used to assign expressive and invariant features to each image such that the search for similar images comes down to feature classification and/or matching. The present work introduces a new image retrieval method for medical applications that employs a convolutional neural network (CNN) with recently introduced Radon barcodes. We combine neural codes for global classification with Radon barcodes for the final retrieval. We also examine image search based on regions of interest (ROI) matching after image retrieval. The IRMA dataset with more than 14,000 x-rays images is used to evaluate the performance of our method. Experimental results show that our approach is superior to many published works.",,,10.1109/IJCNN.2016.7727562,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727562,,Biomedical imaging;Convolutional codes;Feature extraction;Image retrieval;Neural networks;Radon;Training,Radon transforms;bar codes;content-based retrieval;image classification;image matching;image retrieval;medical image processing;neural nets,CBIR;CNN;IRMA dataset;ROI matching;Radon barcodes;Radon transform;X-ray images;binary tag generation;content-based image retrieval;convolutional neural network;fast medical image retrieval;feature classification;feature extraction;feature matching;image search;neural codes;regions of interest,,1.0,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
96,Brain MRI segmentation with patch-based CNN approach,Z. Cui; J. Yang; Y. Qiao,"Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China",2016 35th Chinese Control Conference (CCC),20160829.0,2016,,,7026,7031,"Brain Magnetic Resonance Image (MRI) plays a non-substitutive role in clinical diagnosis. The symptom of many diseases corresponds to the structural variants of brain. Automatic structure segmentation in brain MRI is of great importance in modern medical research. Some methods were developed for automatic segmenting of brain MRI but failed to achieve desired accuracy. In this paper, we proposed a new patch-based approach for automatic segmentation of brain MRI using convolutional neural network (CNN). Each brain MRI acquired from a small portion of public dataset is firstly divided into patches. All of these patches are then used for training CNN, which is used for automatic segmentation of brain MRI. Experimental results showed that our approach achieved better segmentation accuracy compared with other deep learning methods.",,Electronic:978-9-8815-6391-0; POD:978-1-5090-0910-7,10.1109/ChiCC.2016.7554465,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7554465,Brain MRI Segmentation;CNN;Deep Learning;Patch-based,Biology;Biomedical imaging;Computer architecture;Image segmentation;Magnetic resonance imaging;Visualization,biomedical MRI;brain;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets,CNN training;automatic brain MRI segmentation;automatic structure segmentation;brain magnetic resonance image;clinical diagnosis;convolutional neural network;disease symptoms;patch-based CNN approach;public dataset;segmentation accuracy;structural variants,,,,,,,,27-29 July 2016,,IEEE,IEEE Conference Publications
97,On the generality of neural image features,R. Venkatesan; V. Gatupalli; B. Li,"School of Computing Informatics and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,41,45,"Often the filters learned by Convolutional Neural Networks (CNNs) from different image datasets appear similar. This similarity of filters is often exploited for the purposes of transfer learning. This is also being used as an initialization technique for different tasks in the same dataset or for the same task in similar datasets. Off-the-shelf CNN features have capitalized on this idea to promote their networks as best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks. While the filters learned by these CNNs are related to the atomic structures of the images from which they are learnt, all datasets learn similar looking low-level filters. With the understanding that a dataset that contains many such atomic structures learn general filters and are therefore useful to initialize other networks with, we propose a way to analyse and quantify generality. We applied this metric on several popular character recognition, natural image and a medical image dataset, and arrive at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset themselves are more general than others.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532315,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532315,,Atomic layer deposition;Colonoscopy;Computer vision;Detectors;Feature extraction;Image edge detection;Training,feature extraction;image processing;learning (artificial intelligence);neural nets,CNN;convolutional neural networks;image datasets;initialization technique;low-level filters;medical image dataset;neural image features;transfer learning,,,,16.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
98,Colonic Polyp Classification with Convolutional Neural Networks,E. Ribeiro; A. Uhl; M. Häfner,"Dept. of Comput. Sci., Univ. of Salzburg, Salzburg, Austria",2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS),20160818.0,2016,,,253,258,"Texture patch classification is an important task in many different computer-aided medical systems. Convolutional Neural Networks (CNN's) have become state-of-the-art for many computer vision tasks in recent years. In this paper, we propose the use of CNN's for the automated classification of colonic mucosa for colon polyp staging in the context of colon cancer screening. This deep learning approach has the property of extracting features and classifying images in the same architecture by exploiting directly the input image pixels being successful in handling distortions such as different light conditions, presence of partial occlusions, etc. For this type of deep learning approach it is common to require that the database contains large amounts of data, which is quite rare in the medical field. The method proposed allows the use of small patches (subimages) to increase the size of the database as well to classify different regions in the same image. We show experimentally that this model is more efficient than some of the commonly used features for colonic polyp classification.",,Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8,10.1109/CBMS.2016.39,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545996,Colonic Polyp Classification;Convolutional Neural Networks;Deep Learning,Biological neural networks;Cancer;Colonic polyps;Endoscopes;Feature extraction;Training,biological organs;cancer;computer vision;feature extraction;image classification;image texture;learning (artificial intelligence);medical image processing;neural nets,CNN;automated colonic mucosa classification;colon cancer screening;colon polyp staging;colonic polyp classification;computer vision;computer-aided medical system;convolutional neural networks;deep learning;distortion handling;feature extraction;image classification;image pixels;partial occlusion;texture patch classification,,1.0,,,,,,20-24 June 2016,,IEEE,IEEE Conference Publications
99,Deep vessel tracking: A generalized probabilistic approach via deep learning,A. Wu; Z. Xu; M. Gao; M. Buty; D. J. Mollura,"Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1363,1367,"Analysis of vascular geometry is important in many medical imaging applications, such as retinal, pulmonary, and cardiac investigations. In order to make reliable judgments for clinical usage, accurate and robust segmentation methods are needed. Due to the high complexity of biological vasculature trees, manual identification is often too time-consuming and tedious to be used in practice. To design an automated and computerized method, a major challenge is that the appearance of vasculatures in medical images has great variance across modalities and subjects. Therefore, most existing approaches are specially designed for a particular task, lacking the flexibility to be adapted to other circumstances. In this paper, we present a generic approach for vascular structure identification from medical images, which can be used for multiple purposes robustly. The proposed method uses the state-of-the-art deep convolutional neural network (CNN) to learn the appearance features of the target. A Principal Component Analysis (PCA)-based nearest neighbor search is then utilized to estimate the local structure distribution, which is further incorporated within the generalized probabilistic tracking framework to extract the entire connected tree. Qualitative and quantitative results over retinal fundus data demonstrate that the proposed framework achieves comparable accuracy as compared with state-of-the-art methods, while efficiently producing more information regarding the candidate tree structure.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493520,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493520,Deep Learning;Generalized Probabilistic Tracking;Nearest Neighbor Search;Principal Component Analysis;Vascular Structure,Biomedical imaging;Dictionaries;Image segmentation;Machine learning;Probabilistic logic;Robustness,,,,1.0,,10.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
100,A hybrid learning approach for semantic labeling of cardiac CT slices and recognition of body position,M. Moradi; Y. Gur; H. Wang; P. Prasanna; T. Syeda-Mahmood,"IBM Research - Almaden Research Center, San Jose, CA",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1418,1421,"We work towards efficient methods of categorizing visual content in medical images as a precursor step to segmentation and anatomy recognition. In this paper, we address the problem of automatic detection of level/position for a given cardiac CT slice. Specifically, we divide the body area depicted in chest CT into nine semantic categories each representing an area most relevant to the study of a disease and/or key anatomic cardiovascular feature. Using a set of handcrafted image features together with features derived form a deep convolutional neural network (CNN), we build a classification scheme to map a given CT slice to the relevant level. Each feature group is used to train a separate support vector machine classifier. The resulting labels are then combined in a linear model, also learned from training data. We report margin zero and margin one accuracy of 91.7% and 98.8% and show that this hybrid approach is a very effective methodology for assigning a given CT image to a relatively narrow anatomic window.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493533,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493533,cardiac CT;image category classification;slice level recognition,Biomedical imaging;Computed tomography;Convolution;Feature extraction;Image recognition;Semantics;Support vector machines,cardiovascular system;computerised tomography;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines,anatomic cardiovascular feature;anatomy recognition;automatic detection;body area;body position recognition;cardiac CT slices;chest CT;classification scheme;deep convolutional neural network;disease;handcrafted image features;hybrid learning approach;medical images;narrow anatomic window;segmentation;semantic categories;semantic labeling;separate support vector machine classifier;training data;visual content,,1.0,,13.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
101,Discriminative feature extraction from X-ray images using deep convolutional neural networks,M. Srinivas; D. Roy; C. K. Mohan,"VIsual learninG and InteLligence (VIGIL) group, Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519.0,2016,,,917,921,"Feature extraction is one of the most important phases of medical image classification which requires extensive domain knowledge. Convolutional Neural Networks (CNN) have been successfully used for feature extraction in images from different domains involving a lot of classes. In this paper, CNNs are exploited to extract a hierarchical and discriminative representation of X-ray images. This representation is then used for classification of the X-ray images as various parts of the body. Visualization of the feature maps in the hidden layers show that features learnt by the CNN resemble the essential features which help discern the discrimination among different body parts. A comparison on the standard IRMA X-ray image dataset demonstrates that the CNNs easily outperform classifiers with hand-engineered features.",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7471809,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471809,Convolutional Neural Networks (CNN);Feature Extraction;X-ray image,Biomedical imaging;Convolution;Feature extraction;Support vector machines;Training;Visualization;X-ray imaging,X-ray imaging;feature extraction;image classification;image representation;medical image processing;neural nets,CNN;X-ray image classification;deep convolutional neural networks;discriminative feature extraction;discriminative representation;feature map visualization;hierarchical representation;medical image classification;standard IRMA X-ray image dataset,,,,25.0,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
102,Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images,M. J. J. P. van Grinsven; B. van Ginneken; C. B. Hoyng; T. Theelen; C. I. Sánchez,"Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1273,1284,"Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.",0278-0062;02780062,,10.1109/TMI.2016.2526689,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401052,Convolutional neural network;deep learning;hemorrhage;selective sampling,Biomedical imaging;Databases;Hemorrhaging;Image analysis;Image color analysis;Observers;Training,biomedical optical imaging;blood;computer vision;image classification;image colour analysis;image sampling;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis,CNN learning process;CNN training iteration;color fundus images;computer vision applications;deep learning network architectures;dynamically selecting misclassified negative samples;fast convolutional neural network training;hemorrhage detection;independent test set;medical image analysis tasks;receiver operating characteristics curve;selective data sampling;selective sampling method,,7.0,,48.0,,,20160208.0,May 2016,,IEEE,IEEE Journals & Magazines
103,Multi-Instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition,Z. Yan; Y. Zhan; Z. Peng; S. Liao; Y. Shinagawa; S. Zhang; D. N. Metaxas; X. S. Zhou,"Department of Computer Science, Rutgers University, Piscataway, NJ, USA",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1332,1343,"In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. “Bodypart identity” of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.",0278-0062;02780062,,10.1109/TMI.2016.2524985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398101,CNN;discriminative local information discovery;multi-instance;multi-stage,Algorithm design and analysis;DICOM;Image analysis;Image recognition;Machine learning;Three-dimensional displays,cardiology;computerised tomography;face recognition;image classification;learning (artificial intelligence);medical image processing,aorta arch slice;body-part recognition;cardiac slice;convolutional neural network;discriminative information;discriminative local anatomies;discriminative local appearances;global image context;human faces;human identity information;image classification problem;image recognition problems;image-level classifier;large scale CT dataset;local image information;local image patches;mediastinum region;multiinstance deep learning;multiinstance learning fashion;multistage deep learning framework;prelearned CNN;pretrain stage;synthetic dataset;transversal slice,,10.0,,51.0,,,20160203.0,May 2016,,IEEE,IEEE Journals & Magazines
104,Convolutional Neural Networks for Branch Retinal Vein Occlusion recognition?,R. Zhao; Z. Chen; Z. Chi,"Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Kowloon, Hong Kong",2015 IEEE International Conference on Information and Automation,20151001.0,2015,,,1633,1636,"Branch Retinal Vein Occlusion (BRVO) is one of the most common retinal diseases that could impair people's vision seriously if it is not timely diagnosed and treated. It would save a lot of time and money for both medical institutions and patients if BRVO could be well recognized automatically. In this paper, we propose to exploit Convolutional Neural Networks (CNN) for BRVO recognition. We propose patch-based method and image-based voting method to implement the recognition. As it could learn abstract and useful features, CNN can achieve a high recognition accuracy. The accuracy of CNN is over 97%. Experimental results demonstrate the efficiency of our proposed CNN based methods for BRVO recognition.",,Electronic:978-1-4673-9104-7; POD:978-1-4673-9105-4; USB:978-1-4673-9103-0,10.1109/ICInfA.2015.7279547,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279547,Branch Retinal Vein Occlusion;Convolutional Neural Networks;Feature Extraction,Accuracy;Feature extraction;Image recognition;Neural networks;Retina;Training;Veins,diseases;medical image processing;neural nets;retinal recognition,BRVO recognition;CNN;branch retinal vein occlusion recognition;convolutional neural networks;image-based voting method;patch-based method;retinal diseases,,,,11.0,,,,8-10 Aug. 2015,,IEEE,IEEE Conference Publications
105,Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation,Y. Xu; Z. Jia; Y. Ai; F. Zhang; M. Lai; E. I. C. Chang,"Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beihang University, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20150806.0,2015,,,947,951,"We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.",1520-6149;15206149,Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1,10.1109/ICASSP.2015.7178109,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109,classification;deep convolutional activation features;deep learning;feature learning;segmentation,Biomedical imaging;Feature extraction;Image segmentation;Support vector machines;Training;Training data;Tumors,brain;feature extraction;image classification;image segmentation;medical image processing;tumours,CNN activations;ImageNet;MICCAI 2014 Brain Tumor Digital Pathology Challenge;brain tumor histopathology;deep convolutional activation features;features extraction;image classification;image dimensions;image segmentation,,3.0,,23.0,,,,19-24 April 2015,,IEEE,IEEE Conference Publications
106,Chest pathology detection using deep learning with non-medical training,Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan,"The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,294,297,"In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871,CNN;Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks,Biomedical imaging;Diagnostic radiography;Feature extraction;Machine learning;Pathology;Visualization;X-rays,convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets,AUC;CNN algorithm;CNN deep architecture classification;CNN learning;GIST feature;ImageNet;area under curve;chest X-ray image dataset;chest pathology detection;chest radiograph;convolutional neural network;deep learning;domain specific representation;general medical image recognition task;high level image representation learning;large scale nonmedical image database;mid level image representation learning;nonmedical learning;nonmedical training;pathology identification;pathology type,,10.0,,15.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
107,Automated anatomical landmark detection ondistal femur surface using convolutional neural network,D. Yang; S. Zhang; Z. Yan; C. Tan; K. Li; D. Metaxas,"CBIM, Rutgers University, Piscataway, NJ, US",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,17,21,"Accurate localization of the anatomical landmarks on distal femur bone in the 3D medical images is very important for knee surgery planning and biomechanics analysis. However, the landmark identification process is often conducted manually or by using the inserted auxiliaries, which is time-consuming and lacks of accuracy. In this paper, an automatic localization method is proposed to determine positions of initial geometric landmarks on femur surface in the 3D MR images. Based on the results from the convolutional neural network (CNN) classifiers and shape statistics, we use the narrow-band graph cut optimization to achieve the 3D segmentation of femur surface. Finally, the anatomical landmarks are located on the femur according to the geometric cues of surface mesh. Experiments demonstrate that the proposed method is effective, efficient, and reliable to segment femur and locate the anatomical landmarks.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163806,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163806,Deep learning;anatomical landmark detection;convolutional neural network;graph cut;mesh curvature,Biomedical imaging;Bones;Image segmentation;Neural networks;Shape;Three-dimensional displays;Training,biomechanics;biomedical MRI;bone;graph theory;image classification;medical image processing;neural nets;optimisation;surgery,3D MR images;3D medical images;3D segmentation;CNN classifiers;automated anatomical landmark detection;automatic localization method;biomechanics analysis;convolutional neural network;distal femur bone;distal femur surface;knee surgery planning;landmark identification process;magnetic resonance images;narrow-band graph cut optimization;shape statistics;surface mesh,,0.0,,20.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
108,Medical image classification with convolutional neural network,Q. Li; W. Cai; X. Wang; Y. Zhou; D. D. Feng; M. Chen,"Biomedical and Multimedia Information Technology (BMIT) Research Group, School of IT, University of Sydney, Australia",2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),20150323.0,2014,,,844,848,"Image patch classification is an important task in many different medical imaging applications. In this work, we have designed a customized Convolutional Neural Networks (CNN) with shallow convolution layer to classify lung image patches with interstitial lung disease (ILD). While many feature descriptors have been proposed over the past years, they can be quite complicated and domain-specific. Our customized CNN framework can, on the other hand, automatically and efficiently learn the intrinsic image features from lung image patches that are most suitable for the classification purpose. The same architecture can be generalized to perform other medical image or texture classification tasks.",,Electronic:978-1-4799-5199-4; POD:978-1-4799-5200-7; USB:978-1-4799-5198-7,10.1109/ICARCV.2014.7064414,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064414,,Biological neural networks;Biomedical imaging;Feature extraction;Kernel;Lungs;Neurons;Training,diseases;feature extraction;image classification;lung;medical image processing;neural nets,CNN;ILD;convolutional neural network;feature descriptors;interstitial lung disease;intrinsic image features;lung image patches classification;medical image classification;medical imaging applications;shallow convolution layer;texture classification,,11.0,,26.0,,,,10-12 Dec. 2014,,IEEE,IEEE Conference Publications
109,Cellular neural networks assisted automatic detection of elements in microscopic medical images. A preliminary study,C. Botoca,"Communications Department, Electronics and Telecommunications Faculty, University Politehnica Timisoara Timisoara, Romania",2014 11th International Symposium on Electronics and Telecommunications (ISETC),20150115.0,2014,,,1,4,"This paper presents a new algorithm for object recognition in medical microscopic images, assisted by a cellular neural network (CNN). The CNN flowchart and its component parts are described based on successions of interconnections templates. The experiments results are shown and they appear to be promising. Our results sustain the usability of CNN as a real time processing tool for assisting the medical act.",,CD-ROM:978-1-4799-7265-4; Electronic:978-1-4799-7267-8; POD:978-1-4799-7268-5,10.1109/ISETC.2014.7010801,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010801,cellular neural networks;cloning template;medical images processing,Cellular neural networks;Computers;Crystals;Medical diagnostic imaging;Microscopy;Noise reduction,biomedical optical imaging;cellular neural nets;flowcharting;medical image processing;object detection;optical microscopy;real-time systems,CNN component part;CNN flowchart;cellular neural network assisted automatic detection;interconnection template succession;microscopic medical image element detection;object recognition algorithm;real time processing tool,,0.0,,12.0,,,,14-15 Nov. 2014,,IEEE,IEEE Conference Publications
110,cellular neural network based medical image segmentation using artificial bee colony algorithm,M. Duraisamy; F. M. M. Jane,"Department of Computer Applications, Dr NGP Institute of Technology, Coimbatore - 641 048",2014 International Conference on Green Computing Communication and Electrical Engineering (ICGCCEE),20141016.0,2014,,,1,6,"Magnetic Resonance Imaging (MRI) has become an efficient instrument for clinical diagnoses and research in recent years. It has become a very useful medical modality for the detection of various diseases through segmentation methods. In this paper, we have presented an effective CNN based segmentation method with lung and brain MRI images. This approach hits the target with the aid of the following major steps, which includes, 1) Preprocessing of the brain and lung images, 2) Segmentation using cellular neural network. Initially, the MRI image is pre-processed to make it fit for segmentation. Here, in the pre-processing step, image de-noising is done using the linear smoothing filters, such as Gaussian Filter. Then, the pre-processed image is segmented according to our proposed technique, CNN-based image segmentation. Finally, the different MRI images (brain and lung) are given to the proposed approach to evaluate the performance of the proposed approach in segmentation process. The Comparative analysis is carried out Fuzzy C-means (FCM) and K-means classification. From the comparative analysis, the accuracy of proposed segmentation approach produces better results (83.7% for lung and 93% for brain images) than that of existing Fuzzy C-means (FCM) and K-means classification.",,Electronic:978-1-4799-4982-3; POD:978-1-4799-4981-6,10.1109/ICGCCEE.2014.6922413,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6922413,Brain and Lung MRI image;Cellular neural network;Gaussian filter;employed bee;onlooker bee;scout bee;template design,Algorithm design and analysis;Biomedical imaging;Cellular neural networks;Image segmentation;Lungs;Magnetic resonance imaging,ant colony optimisation;biomedical MRI;brain;cellular neural nets;diseases;fuzzy set theory;image classification;image denoising;image segmentation;lung;medical image processing;smoothing methods,CNN based segmentation method;FCM classification;Gaussian filter;K-means classification;artificial bee colony algorithm;brain MRI images;brain image preprocessing;cellular neural network based medical image segmentation;clinical diagnoses;diseases;fuzzy c-means classification;image denoising;linear smoothing filters;lung MRI images;lung image preprocessing;magnetic resonance imaging;medical modality,,1.0,,37.0,,,,6-8 March 2014,,IEEE,IEEE Conference Publications
111,Intervertebral disc detection in X-ray images using faster R-CNN,R. Sa; W. Owens; R. Wiegand; M. Studin; D. Capoferri; K. Barooha; A. Greaux; R. Rattray; A. Hutton; J. Cintineo; V. Chaudhary,"State University of New York (SUNY) at Buffalo, United States of America",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,564,567,"Automatic identification of specific osseous landmarks on the spinal radiograph can be used to automate calculations for correcting ligament instability and injury, which affect 75% of patients injured in motor vehicle accidents. In this work, we propose to use deep learning based object detection method as the first step towards identifying landmark points in lateral lumbar X-ray images. The significant breakthrough of deep learning technology has made it a prevailing choice for perception based applications, however, the lack of large annotated training dataset has brought challenges to utilizing the technology in medical image processing field. In this work, we propose to fine tune a deep network, Faster-RCNN, a state-of-the-art deep detection network in natural image domain, using small annotated clinical datasets. In the experiment we show that, by using only 81 lateral lumbar X-Ray training images, one can achieve much better performance compared to traditional sliding window detection method on hand crafted features. Furthermore, we fine-tuned the network using 974 training images and tested on 108 images, which achieved average precision of 0.905 with average computation time of 3 second per image, which greatly outperformed traditional methods in terms of accuracy and efficiency.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036887,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036887,X-Ray;deep learning;detection;intervertebral disc,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
112,Surgical-tools detection based on Convolutional Neural Network in laparoscopic robot-assisted surgery,B. Choi; K. Jo; S. Choi; J. Choi,"Biomedical Engineering Research Center, Asan Institute for Life Sciences, Asan Medical Center, Seoul, South Korea",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,1756,1759,"Laparoscopic surgery, a type of minimally invasive surgery, is used in a variety of clinical surgeries because it has a faster recovery rate and causes less pain. However, in general, the robotic system used in laparoscopic surgery can cause damage to the surgical instruments, organs, or tissues during surgery due to a narrow field of view and operating space, and insufficient tactile feedback. This study proposes real-time models for the detection of surgical instruments during laparoscopic surgery by using a CNN(Convolutional Neural Network). A dataset included information of the 7 surgical tools is used for learning CNN. To track surgical instruments in real time, unified architecture of YOLO apply to the models. So as to evaluate performance of the suggested models, degree of recall and precision is calculated and compared. Finally, we achieve 72.26% mean average precision over our dataset.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037183,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037183,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
113,Cerebral Micro-Bleed Detection Based on the Convolution Neural Network With Rank Based Average Pooling,S. Wang; Y. Jiang; X. Hou; H. Cheng; S. Du,"School of Electronic Engineering, Nanjing University, Nanjing, China",IEEE Access,20170906.0,2017,5,,16576,16583,"Cerebral micro-bleed (CMB) is small perivascular hemosiderin deposits from leakage through cerebral small vessels. They can result from cerebra-vascular disease, dementia, or simply from normal aging. It can be visualized via the susceptibility weighted imaging (SWI). Based on the SWI, we propose to use different structures of the CNN with rank-based average pooling to detect the CMB, and compare this method used in this paper to the current state-of-the-art methods. We can find that the CNN with five layers obtains the best performance, with a sensitivity of 96.94%, a specificity of 97.18%, and an accuracy of 97.18%.",,,10.1109/ACCESS.2017.2736558,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013653,Convolutional neural network;cerebral micro-bleed;network structure;rank based average pooling,Convolution;Diseases;Electronic mail;Magnetic resonance imaging;Manuals;Visualization,,,,,,,,,20170821.0,2017,,IEEE,IEEE Journals & Magazines
114,Nursing-care text classification using word vector representation and convolutional neural networks,M. Nii; Y. Tsuchida; Y. Kato; A. Uchinuno; R. Sakashita,"Graduate School of Engineering, University of Hyogo, Himeji, Hyogo, Japan",2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS),20170831.0,2017,,,1,5,"In this paper, we propose a convolutional neural network (CNN) based classification method for nursing-care classification. CNNs have obtained strong performance in computer vision speech recognition areas. Recently, CNNs have been also applied sentence classification. We have studied nursing-care text classification [6]-[18]. In our former works, we proposed several types of feature definitions and examined some classification models. In this paper, each text is represented as a concatenated word vector. Then, every text is classified using CNN-based classification methods. We examined some classification models at the classification layer in CNNs. From our experimental results, the proposed CNN-based method obtained better performance than our former works.",,Electronic:978-1-5090-4917-2; POD:978-1-5090-4918-9,10.1109/IFSA-SCIS.2017.8023240,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023240,Nursing-care text classification;convolutional neural networks;word vector representation,Computational modeling;Feature extraction;Medical services;Predictive models;Tools;Training,,,,,,,,,,27-30 June 2017,,IEEE,IEEE Conference Publications
115,Deep Learning for Categorization of Lung Cancer CT Images,A. M. Rossetto; W. Zhou,"Dept. of Comput. Sci., Univ. of Massachusetts Lowell, Lowell, MA, USA","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20170817.0,2017,,,272,273,"Lung cancer is a serious health problem. In the United States alone, approximately 225,000 people each year are diagnosed with lung cancer. Early detection is a crucial part of giving patients the best chance of recovery. Deep learning gives us an opportunity to increase the accuracy of the automated initial diagnosis. Here we present an ensemble of Convolution Neural Networks(CNN) using multiple preprocessing methods to increase the accuracy of the automated labeling of the scans. We have done this by implementing ensembles of CNNs along with a voting system to get the consensus of the two networks. The initial results of our best method show both a consistently high accuracy (97.5%) and a low percentage of false positives (<;10%).",,Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5,10.1109/CHASE.2017.98,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010653,,Cancer;Convolution;Lungs;Machine learning;Pipelines;Testing;Training,computerised tomography;learning (artificial intelligence);medical image processing;neural nets,CNN;automated initial diagnosis;automated labeling;categorization;convolution neural networks;deep learning;false positives;health problem;lung cancer CT images;multiple preprocessing methods,,,,,,,,17-19 July 2017,,IEEE,IEEE Conference Publications
116,Learning to Read Chest X-Ray Images from 16000+ Examples Using CNN,Y. Dong; Y. Pan; J. Zhang; W. Xu,"Inst. for Interdiscipl. Inf. Sci., Tsinghua Univ., Beijing, China","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20170817.0,2017,,,51,57,"Chest radiography (chest X-ray) is a low-cost yet effective and widely used medical imaging procedures. The lacking of qualified radiologist seriously limits the applicability of the technique. We explore the possibility of designing a computer-aided diagnosis for chest X-rays using deep convolutional neural networks. Using a real-world dataset of 16,000 chest X-rays with natural language diagnosis reports, we can train a multi-class classification model from images and preform accurate diagnosis, without any prior domain knowledge.",,Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5,10.1109/CHASE.2017.59,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010614,,Biomedical imaging;Convolution;Diseases;Feature extraction;Lungs;Neural networks;X-ray imaging,computerised tomography;diagnostic radiography;image classification;medical image processing;neural nets,CNN;chest X-ray image;chest X-rays;chest radiography;computer-aided diagnosis;deep convolutional neural networks;medical imaging procedures;multiclass image classification;natural language diagnosis reports,,,,,,,,17-19 July 2017,,IEEE,IEEE Conference Publications
117,Deep learning for tumour classification in homogeneous breast tissue in medical microwave imaging,B. Gerazov; R. C. Conceicao,"Faculty of Electrical Engineering and Information Technologies, Ss Cyril and Methodius University in Skopje, Skopje, Macedonia",IEEE EUROCON 2017 -17th International Conference on Smart Technologies,20170817.0,2017,,,564,569,"Deep learning has become the state-of-the-art in the area of biomedical imaging, leading to a large boost in performance that approaches human levels. Medical microwave imaging is an emerging technology that has great potential especially in the area of breast cancer diagnosis. Moreover, the obtained backscatter signals have also been shown to be a good basis for differentiating malignant and benign tumour type. We further analyse these results by applying deep learning methods to a dataset of Finite Difference Time Domain (FDTD) numerical simulations of tumour models embedded in homogeneous breast adipose tissue. Specifically we use Deep and Convolutional Neural Networks and obtain an accuracy of 93.44% which outperforms conventional machine learning previously used on the analysed dataset.",,Electronic:978-1-5090-3843-5; POD:978-1-5090-3844-2; USB:978-1-5090-3842-8,10.1109/EUROCON.2017.8011175,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011175,CNN;DNN;breast tumour classification;deep learning;feature embedding;medical microwave imaging,Biomedical imaging;Finite difference methods;Machine learning;Microwave theory and techniques;Numerical models;Time-domain analysis;Tumors,cancer;convolution;finite difference time-domain analysis;image classification;learning (artificial intelligence);medical image processing;microwave imaging;neural nets;tumours,FDTD numerical simulations;backscatter signals;breast cancer diagnosis;convolutional neural networks;deep learning;deep neural networks;finite difference time domain;homogeneous breast tissue;medical microwave imaging;tumour classification,,,,,,,,6-8 July 2017,,IEEE,IEEE Conference Publications
118,A medical image fusion method based on convolutional neural networks,Y. Liu; X. Chen; J. Cheng; H. Peng,"Department of Biomedical Engineering, Hefei University of Technology, Hefei 230009, China",2017 20th International Conference on Information Fusion (Fusion),20170814.0,2017,,,1,7,"Medical image fusion technique plays an an increasingly critical role in many clinical applications by deriving the complementary information from medical images with different modalities. In this paper, a medical image fusion method based on convolutional neural networks (CNNs) is proposed. In our method, a siamese convolutional network is adopted to generate a weight map which integrates the pixel activity information from two source images. The fusion process is conducted in a multi-scale manner via image pyramids to be more consistent with human visual perception. In addition, a local similarity based strategy is applied to adaptively adjust the fusion mode for the decomposed coefficients. Experimental results demonstrate that the proposed method can achieve promising results in terms of both visual quality and objective assessment.",,Electronic:978-0-9964-5270-0; POD:978-1-5090-4582-2,10.23919/ICIF.2017.8009769,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009769,,Image fusion;Laplace equations;Medical diagnostic imaging;Training;Transforms,image fusion;medical image processing;neural nets;visual perception,CNN;clinical applications;convolutional neural networks;human visual perception;image pyramids;medical image fusion;pixel activity information;siamese convolutional network;visual quality,,,,,,,,10-13 July 2017,,IEEE,IEEE Conference Publications
119,EEG-based biometric identification with deep learning,Z. Mao; W. X. Yao; Y. Huang,"Department of Electrical and Computer Engineering, University of Texas at San Antonio, USA",2017 8th International IEEE/EMBS Conference on Neural Engineering (NER),20170814.0,2017,,,609,612,"Despite the recent increasing interest in biometric identification using electroencephalogram (EEG) signals, the state of the art still lacks a simple and robust model that is useful in real applications. This work proposes a new approach based on convolutional neural network CNN. The proposed CNN works directly on raw EEG data, thus alleviating the need for engineering features. We investigate the performance of the CNN on datasets of 100 subjects collected from one driving fatigue experiment. Our results show that the CNN model is fast highly efficient in training (<;0.5h on >100K training epochs) and highly robust, achieving 97% accuracy in identifying ~14K testing epochs from 100 subjects with non-time-locked natural driving fatigue data and much higher than from randomly sampled epochs (90%). Overall, this work demonstrates the potential of deep learning solutions for real-life EEG-based biometric identification.",,Electronic:978-1-5090-4603-4; POD:978-1-5090-4604-1,10.1109/NER.2017.8008425,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008425,biometric identification;brain-computer interface;convolutional neural networks;deconvolutional networks;deep learning,Brain modeling;Convolution;Electroencephalography;Fatigue;Feature extraction;Machine learning;Training,electroencephalography;feature extraction;medical signal processing;neural nets;neurophysiology;signal sampling,CNN;EEG-based biometric identification;convolutional neural network;datasets;deep learning solutions;driving fatigue experiment;electroencephalogram signals;engineering features;non-time-locked natural driving fatigue data;randomly sampled epochs;raw EEG data,,,,,,,,25-28 May 2017,,IEEE,IEEE Conference Publications
120,Super-resolution of Magnetic Resonance Images using deep Convolutional Neural Networks,K. Srinivasan; A. Ankur; A. Sharma,"Department of Computer Science & Information Engineering, National Ilan University, China",2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),20170727.0,2017,,,41,42,This research focuses on developing a Super-resolution magnetic resonance (MR) Image restoration method using Convolutional Neural Networks (CNN). The main aim is to train an end to end mapping that takes low-resolution image as input and returns a high-resolution output. Low overhead and a state of the art reconstruction makes the model perform efficiently.,,Electronic:978-1-5090-4017-9; POD:978-1-5090-4018-6,10.1109/ICCE-China.2017.7990985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990985,,Conferences;Image reconstruction;Image resolution;Image restoration;Interpolation;Magnetic resonance;Neural networks,biomedical MRI;feedforward neural nets;image resolution;image restoration;medical image processing,CNN;MR;deep convolutional neural networks;end-to-end mapping;high-resolution output;image restoration method;low-resolution image;magnetic resonance images;super-resolution,,,,,,,,12-14 June 2017,,IEEE,IEEE Conference Publications
121,Automatic 1D convolutional neural network-based detection of artifacts in MEG acquired without electrooculography or electrocardiography,P. Garg; E. Davenport; G. Murugesan; B. Wagner; C. Whitlow; J. Maldjian; A. Montillo,"UT Southwestern Medical Center, Dallas, Texas, USA",2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI),20170720.0,2017,,,1,4,"Magnetoencephalography (MEG) is a functional neuroimaging tool that records the magnetic fields induced by electrical neuronal activity; however, signal from non-neuronal sources can corrupt the data. Eye-Blinks (EB) and Cardiac Activity (CA) are two of the most common types of non-neuronal artifacts. They can be measured by affixing eye proximal electrodes, as in electrooculography (EOG) and chest electrodes, as in electrocardiography (EKG), however this complicates imaging setup, decreases patient comfort, and often induces further artifacts from facial twitching and postural muscle movement. We propose an EOG- and EKG-free approach to identify eye-blink, cardiac, or neuronal signals for automated artifact suppression. Our contributions are two-fold. First, we combine a data driven, multivariate decomposition approach based on Independent Component Analysis (ICA) and a highly accurate classifier constructed as a deep 1-D Convolutional Neural Network. Second, we visualize the features learned to reveal what features the model uses and to bolster user confidence in our model's training and potential for generalization. We train and test three variants of our method on resting state MEG data from 49 subjects. Our cardiac model achieves a 96% sensitivity and 99% specificity on the set-aside test-set. Our eye-blink model achieves a sensitivity of 85% and specificity of 97%. This work facilitates automated MEG processing for both, clinical and research use, and can obviate the need for EOG or EKG electrodes.",,Electronic:978-1-5386-3159-1; POD:978-1-5386-3160-7,10.1109/PRNI.2017.7981506,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981506,CNN;EKG;EOG;MEG;artifact;deep learning,Brain modeling;Convolution;Electrocardiography;Electrodes;Electrooculography;Neuroscience;Sensitivity,cardiology;eye;gait analysis;independent component analysis;magnetoencephalography;medical signal detection;medical signal processing;neural nets,ICA;MEG acquisition;MEG processing;affixing eye proximal electrodes;automatic 1D convolutional neural network-based detection;cardiac activity;cardiac model;chest electrodes;electrical neuronal activity;eye-blink model;eye-blinks activity;facial twitching;feature learning;functional neuroimaging tool;independent component analysis;magnetic fields;magnetoencephalography;multivariate decomposition approach;nonneuronal artifacts;nonneuronal sources;postural muscle movement,,,,,,,,21-23 June 2017,,IEEE,IEEE Conference Publications
122,Tongue shape classification integrating image preprocessing and Convolution Neural Network,C. M. Huo; H. Zheng; H. Y. Su; Z. L. Sun; Y. J. Cai; Y. F. Xu,"Key Lab of Intelligent Information Technology, Beijing Institute of Technology, Beijing, China",2017 2nd Asia-Pacific Conference on Intelligent Robot Systems (ACIRS),20170720.0,2017,,,42,46,"Tongue diagnosis is one of the most important parts in “inspection diagnosis” of Traditional Chinese Medicine (TCM). Observing tongue shape can help to understand the changes in human body and thereby to estimate the illness. This paper presents a method of recognizing tongue shapes based on Convolution Neural Network. The proposed method enhances the features of tongue images with preprocessing to ensure the data suitable for tongue shape binary classification. In view of the special texture and outline of tongue, the whole tongue images of dot-sting tongue and fissured tongue is transformed by Gabor filter, and the tooth-marked are processed by boundary detection approach. CNN is adopted because it has achieved remarkable results in computer vision and pattern recognition, and the model training through neural network coincides with the Chinese medicine dialectics through experience. Based on commonly used Alex-net, network is optimized with batch normalization to improve efficiency. The experimental results indicate that the preprocessing methods increase the accuracy and decreases the time of training process of tongue shape classification, which proves that the method is effective for the recognition of different tongue shapes.",,DVD:978-1-5090-6791-6; Electronic:978-1-5090-6793-0; POD:978-1-5090-6794-7; Paper:978-1-5090-6792-3,10.1109/ACIRS.2017.7986062,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986062,Gabor filtering;convolution neural network;deep learning;tongue shape classification,Biological neural networks;Convolution;Feature extraction;Gabor filters;Shape;Tongue;Training,Gabor filters;computer vision;image classification;image texture;medical computing;neural nets;patient diagnosis;shape recognition,Alex-net;Chinese medicine dialectics;Gabor filter;TCM;computer vision;convolution neural network;dot-sting tongue;image preprocessing;image texture;inspection diagnosis;pattern recognition;tongue diagnosis;tongue images;tongue shape binary classification;traditional Chinese medicine,,,,,,,,16-18 June 2017,,IEEE,IEEE Conference Publications
123,Segmentation of Fetal Left Ventricle in Echocardiographic Sequences Based on Dynamic Convolutional Neural Networks,L. Yu; Y. Guo; Y. Wang; J. Yu; P. Chen,Department of Electronic EngineeringFudan University,IEEE Transactions on Biomedical Engineering,20170714.0,2017,64,8.0,1886,1895,"Segmentation of fetal left ventricle (LV) in echocardiographic sequences is important for further quantitative analysis of fetal cardiac function. However, image gross inhomogeneities and fetal random movements make the segmentation a challenging problem. In this paper, a dynamic convolutional neural networks (CNN) based on multiscale information and fine-tuning is proposed for fetal LV segmentation. The CNN is pretrained by amount of labeled training data. In the segmentation, the first frame of each echocardiographic sequence is delineated manually. The dynamic CNN is fine-tuned by deep tuning with the first frame and shallow tuning with the rest of frames, respectively, to adapt to the individual fetus. Additionally, to separate the connection region between LV and left atrium (LA), a matching approach, which consists of block matching and line matching, is used for mitral valve (MV) base points tracking. Advantages of our proposed method are compared with an active contour model (ACM), a dynamical appearance model (DAM), and a fixed multiscale CNN method. Experimental results in 51 echocardiographic sequences show that the segmentation results agree well with the ground truth, especially in the cases with leakage, blurry boundaries, and subject-to-subject variations. The CNN architecture can be simple, and the dynamic fine-tuning is efficient.",0018-9294;00189294,,10.1109/TBME.2016.2628401,Clinical Technology Innovation Project of Hospital Development Center of Shanghai ShenKang; National Basic Research Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744576,Dynamic convolutional neural networks (CNN);echocardiographic sequences;fine-tuning;mitral valve (MV) base points,Biomedical imaging;Convolution;Image segmentation;Kernel;Training data;Tuning;Valves,blood vessels;echocardiography;image matching;image segmentation;image sequences;medical image processing;neural nets;obstetrics,ACM;CNN architecture;DAM;active contour model;block matching;blurry boundaries;connection region;deep tuning;dynamic CNN;dynamic convolutional neural networks;dynamic fine-tuning;dynamical appearance model;echocardiographic sequence;fetal LV segmentation;fetal cardiac function;fetal random movements;fixed multiscale CNN method;ground truth;image gross inhomogeneities;individual fetus;labeled training data;left atrium;line matching;matching approach;mitral valve base points;multiscale information;shallow tuning;subject-to-subject variations,,,,,,,20161115.0,Aug. 2017,,IEEE,IEEE Journals & Magazines
124,Computer aided diagnosis in digital pathology application: Review and perspective approach in lung cancer classification,A. K. AlZubaidi; F. B. Sideseq; A. Faeq; M. Basil,"Biomedical Engineering Department, AlMustaqbal University College, Babil, Iraq",2017 Annual Conference on New Trends in Information & Communications Technology Applications (NTICT),20170713.0,2017,,,219,224,"This electronic document is a “live” template and already defines the components of your paper [title, text, heads, etc.] in its style sheet This paper provide a broad review for most important algorithms used in the CAD application for lung tissue diagnostics and highlighted the performance of each distinctive algorithm. Moreover, ROC characteristics have been made for each selected algorithms (support vector machine (SVM), Fuzzy C-mean (FCM), Conventional Neural network (CNN) and CAD-FCM). The features for each algorithm discussed and related performance in clinical aided diagnosis (CAD) discussed and explained. Moreover, comparison of different research groups has been made to spotlight each criterion for different algorithms and approach used in CAD platforms in lung cancer. Finally, limitation and constrains for these algorithms has been discussed in order to optimize performance for each of these algorithms.",,Electronic:978-1-5386-2962-8; POD:978-1-5386-2963-5,10.1109/NTICT.2017.7976109,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976109,Digital pathology;artificial intelligence (AI);classification algorithms;computer aided diagnosis;histopathology;machine learning (ML);tumor staging and grading,Cancer;Classification algorithms;Feature extraction;Imaging;Lungs;Pathology;Support vector machines,CAD;biological tissues;cancer;fuzzy set theory;image classification;lung;medical image processing;neural nets;optimisation;support vector machines,CAD application;CAD-FCM;CNN;FCM algorithms;ROC characteristics;SVM;clinical aided diagnosis;computer aided diagnosis;conventional neural network;digital pathology application;electronic document;fuzzy C-mean algorithms;lung cancer classification;lung tissue diagnostics;performance optimization;support vector machine,,,,,,,,7-9 March 2017,,IEEE,IEEE Conference Publications
125,Semantic segmentation of microscopic images of H&E stained prostatic tissue using CNN,J. Isaksson; I. Arvidsson; K. Åaström; A. Heyden,"Lund University, Centre for Mathematical Sciences, Lund, Sweden",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,1252,1256,"There is a need for an automatic Gleason scoring system that can be used for prostate cancer diagnosis. Today the diagnoses are determined by pathologists manually, which is both a complex and a time-consuming task. To reduce the pathologists' workload, but also to reduce variations between different pathologists, an automatic classification system would be of great use. Some previous works have aimed for this, but still more work needs to be done. It is probable that such a tool would benefit from having access to individually segmented, pathologically relevant objects from the images. Therefore, we have developed an algorithm for semantic segmentation of the microscopic images of H&E stained prostate tissue into Background, Stroma, Epithelial Cytoplasm and Nuclei. This algorithm is based on deep learning, or more specifically a convolutional neural network. The network design is inspired by architectures that previously have been proved successful in different applications. It consists of a contracting and an expanding part, which are symmetrical. We have reached an accuracy of 80 %, as measured by the mean of the intersection over union, for segmentation into four classes. Previous works have only investigated nuclei segmentation, and our network performed similar but for the more challenging task of four class segmentation.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7965996,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965996,,Cancer;Gold;Image segmentation;Microscopy;Neural networks;Semantics;Standards,biological tissues;cancer;image classification;image segmentation;medical image processing;neural nets;patient diagnosis,CNN;H&E stained prostatic tissue;automatic Gleason scoring system;automatic classification system;epithelial cytoplasm;microscopic image semantic segmentation;prostate cancer diagnosis,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
126,Deeply-supervised CNN for prostate segmentation,Q. Zhu; B. Du; B. Turkbey; P. L. Choyke; P. Yan,"School of Computer, Wuhan University, WuHan, China, 430079",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,178,184,"Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided intervention. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7965852,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965852,,Convolution;Feature extraction;Image segmentation;Machine learning;Medical diagnostic imaging;Training,biomedical MRI;feature extraction;feedforward neural nets;image segmentation;medical image processing,MR images;convolutional information;deeply supervised convolutional neural network;deeply supervised layers;deeply-supervised CNN;feature extraction;image guided intervention;magnetic resonance images;prostate region detection;prostate segmentation,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
127,Similarities and differences between stimulus tuning in the inferotemporal visual cortex and convolutional networks,B. P. Tripp,"Department of Systems Design Engineering & Centre for Theoretical Neuroscience, Waterloo, Ontario, Canada",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,3551,3560,"Deep convolutional neural networks (CNNs) trained for object classification have a number of striking similarities with the primate ventral visual stream. In particular, activity in early, intermediate, and late layers is closely related to activity in V1, V4, and the inferotemporal cortex (IT). This study further compares activity in late layers of object-classification CNNs to activity patterns reported in the IT electrophysiology literature. There are a number of close similarities, including the distributions of population response sparseness across stimuli, and the distribution of size tuning bandwidth. Statisics of scale invariance, responses to clutter and occlusion, and orientation tuning are less similar. Statistics of object selectivity are quite different. These results agree with recent studies that highlight strong parallels between object-categorization CNNs and the ventral stream, and also highlight differences that could perhaps be reduced in future CNNs.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7966303,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966303,,Bandwidth;Correlation;Neurons;Sociology;Tuning;Visualization,bioelectric phenomena;image classification;learning (artificial intelligence);medical image processing;object detection;scaling phenomena;statistics,CNN training;IT electrophysiology literature;activity patterns;clutter responses;deep convolutional neural networks;inferotemporal visual cortex;object classification;object selectivity statistics;object-categorization CNNs;object-classification CNNs;occlusion responses;orientation tuning;primate ventral visual stream;scale invariance statistics;size tuning bandwidth;stimulus tuning;ventral stream,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
128,Multi-Task Convolutional Neural Network for Patient Detection and Skin Segmentation in Continuous Non-Contact Vital Sign Monitoring,S. Chaichulee; M. Villarroel; J. Jorge; C. Arteta; G. Green; K. McCormick; A. Zisserman; L. Tarassenko,"Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK",2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),20170629.0,2017,,,266,272,"Patient detection and skin segmentation are important steps in non-contact vital sign monitoring as skin regions contain pulsatile information required for the estimation of vital signs such as heart rate, respiratory rate and peripheral oxygen saturation (SpO2). Previous methods based on face detection or colour-based image segmentation are less reliable in a hospital setting. In this paper, we develop a multi-task convolutional neural network (CNN) for detecting the presence of a patient and segmenting the patient's skin regions. The multi-task model has a shared core network with two branches: a segmentation branch which was implemented using a fully convolutional network, and a classification branch which was implemented using global average pooling. The whole network was trained using images from a clinical study conducted in the neonatal intensive care unit (NICU) of the John Radcliffe hospital, Oxford, UK. Our model can produce accurate results and is robust to changes in different skin tones, pose variations, lighting variations, and routine interaction of clinical staff.",,Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7,10.1109/FG.2017.41,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961751,,Cameras;Hospitals;Image color analysis;Image segmentation;Monitoring;Pediatrics;Skin,face recognition;hospitals;image classification;image colour analysis;image segmentation;medical image processing;neural nets;patient monitoring;skin,CNN;John Radcliffe hospital;NICU;Oxford;UK;colour-based image segmentation;continuous noncontact vital sign monitoring;face detection;multitask convolutional neural network;neonatal intensive care unit;patient detection;patient skin region segmentation;pulsatile information,,,,,,,,May 30 2017-June 3 2017,,IEEE,IEEE Conference Publications
129,A deep learning based approach for classification of CerbB2 tumor cells in breast cancer,G. A. Tataroğlu; A. Genç; K. A. Kabakçı; A. Çapar; B. U. Töreyin; H. K. Ekenel; İ. Türkmen; A. Çakır,"Bili&#x015F;im Enstit&#x00FC;s&#x00FC;, &#x0130;stanbul Teknik &#x00DC;niversitesi",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"This study proposes a unique approach to classify CerbB2 tumor cell scores in breast cancer based on deep learning models. Another contribution of the study is the creation of a dataset from original breast cancer tissues. On the purpose of training, validating and testing with deep learning models cell fragments were generated from sample tissue images. CerbB2 tumor scores were generated for the cell fragments were classified with high performance by the aid of convolutional neural networks (CNN).",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960587,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960587,CerbB2 marker;Convolutional Neural Networks (CNN);classification;score;tumor,Breast cancer;Fasteners;Fish;Machine learning;Neural networks;Proteins;Tumors,cancer;learning (artificial intelligence);medical image processing;neural nets;tumours,CNN;CerbB2 tumor cells;breast cancer;breast cancer tissues;cell fragments;convolutional neural networks;deep learning;deep learning models;sample tissue images,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
130,Segmentation of precursor lesions in cervical cancer using convolutional neural networks,A. Albayrak; A. Ünlü; N. Çalık; G. Bilgin; İ. Türkmen; A. Çakır; A. Çapar; B. U. Töreyin; L. D. Ata,"Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Y&#x0131;ld&#x0131;z Teknik &#x00DC;niversitesi",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"Cervical carcinoma is one of the frequently seen cancers in the world and in our country, develops from precursor lesions. These precursor lesions are analyzed by pathologists so that the diagnosis of the disease can be made. In this study, a system that performs automatic detection of pre-cancerous lesions was performed using the convolutional neural networks (CNNs). In the training phase, lesion recognition performance of the proposed system has reached 92%. Thereafter, whole image was segmented by using 60 × 60 pixel tiles during the training phase. After all, the precursor lesions were segmented with 81.71% Dice coefficient.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960459,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960459,Cervical cancer;convolutional neural networks;histopathological images;precursor lesions;segmentation,Biomedical imaging;Cancer;Dogs;Image segmentation;Lesions;Neural networks;Training,cancer;feedforward neural nets;gynaecology;image recognition;image segmentation;medical image processing,CNN;Dice coefficient;cervical cancer;cervical carcinoma;convolutional neural networks;disease diagnosis;lesion recognition performance;precancerous lesions;precursor lesions segmentation,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
131,Cerebral vessel classification with convolutional neural networks,Y. H. Şahin; G. Ünal,"Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, &#x0130;stanbul Teknik &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"Analysing brain magnetic resonance angiography (MRA) images is important for detecting arteriovenous malformations and aneurysms. To detect these diseases, extracting the vessel structure in the image can be seen as a first step. In this paper, it was aimed to classify the cubic image parts obtained from brain MRA images according to whether they belong to vein structure or not. For this purpose, a 9 layers deep convolutional neural network (CNN) architecture is designed. With the model trained using this architecture, 85% accuracy was obtained in the classification performed on the test data.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960697,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960697,cerebral vessel classification;convolutional neural networks;deep learning;magnetic resonance angiography (MRA),Biological neural networks;Biomedical imaging;Brain modeling;Dogs;Image segmentation;Magnetic resonance;Nanoelectromechanical systems,biomedical MRI;diseases;feature extraction;feedforward neural nets;image classification;medical image processing;object detection,CNN architecture;aneurysm detection;arteriovenous malformation detection;brain MRA images;brain magnetic resonance angiography image analysis;cerebral vessel classification;cubic image part classification;deep convolutional neural network architecture;disease detection;vein structure;vessel structure extraction,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
132,Auto-context Convolutional Neural Network (Auto-Net) for Brain Extraction in Magnetic Resonance Imaging,S. S. M. Salehi; D. Erdogmus; A. Gholipour,"Electrical and Computer Engineering Department, Northeastern University, Boston, MA, 02115 and Radiology Department, Boston Children&#x2019;s Hospital and Harvard Medical School, Boston MA 02115.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. State-of-the-art brain extraction techniques rely heavily on the accuracy of alignment or registration between brain atlases and query brain anatomy, and/or make assumptions about the image geometry; therefore have limited success when these assumptions do not hold or image registration fails. With the aim of designing an accurate, learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. We consider two different architectures: 1) a voxelwise approach based on three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) that implicitly learn 3D image information without the need for computationally expensive 3D convolutions, and 2) a fully convolutional network based on the U-net architecture. Posterior probability maps generated by the networks are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain to extract it from non-brain tissue. The brain extraction results we have obtained from our CNNs are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.73% and 97.62%, respectively. Significant improvement was achieved via our auto-context algorithm. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fe- al brain magnetic resonance imaging (MRI) datasets. In this application our voxelwise auto-context CNN performed much better than the other methods (Dice coefficient: 95.97%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Through training, our method can provide accurate brain extraction in challenging applications. This in-turn may reduce the problems associated with image registration in segmentation tasks.",0278-0062;02780062,,10.1109/TMI.2017.2721362,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961201,Auto-Context;Brain extraction;CNN;Convolutional neural network;MRI;U-net;Whole brain segmentation,Computer architecture;Context;Feature extraction;Image segmentation;Magnetic resonance imaging;Three-dimensional displays;Two dimensional displays,,,,,,,,,20170628.0,,,IEEE,IEEE Early Access Articles
133,Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction,R. Li; T. Zeng; H. Peng; S. Ji,"School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA",IEEE Transactions on Medical Imaging,20170628.0,2017,36,7.0,1533,1541,"Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.",0278-0062;02780062,,10.1109/TMI.2017.2679713,10.13039/100000001 - National Science Foundation; 10.13039/100007588 - Washington State University; 10.13039/100009980 - Old Dominion University; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874113,BigNeuron;Deep learning;image denoising;image segmentation;neuron reconstruction,Convolution;Image reconstruction;Image segmentation;Microscopy;Morphology;Neurons;Three-dimensional displays,biomedical optical imaging;brain;image denoising;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optical microscopy,3-D convolutional neural networks;3-D microscopy images;3-D neuron reconstruction;3-D neuron structure;CNN architecture;brain anatomy;brain wiring;deep learning segmentation;digital reconstruction;digital tracing;discontinued segments;image segmentation methods;neurite patterns;neuronal microscopy images;neuronal voxels;noise removal;optical microscopy images;organisms;preprocessing step;reconstruction algorithms;reversing engineering;tracing performance;volumetric images;voxel-wise segmentation maps,,,,,,,20170308.0,July 2017,,IEEE,IEEE Journals & Magazines
134,Classifying histopathology whole-slides using fusion of decisions from deep convolutional network on a collection of random multi-views at multi-magnification,K. Das; S. P. K. Karri; A. Guha Roy; J. Chatterjee; D. Sheet,"Department of Electrical Engineering, IIT Kharagpur, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1024,1027,"Histopathology forms the gold standard for confirmed diagnosis of a suspicious hyperplasia being benign or malignant and for its sub-typing. While techniques like whole-slide imaging have enabled computer assisted analysis for exhaustive reporting of the tissue section, it has also given rise to the big-data deluge and the time complexity associated with processing GBs of image data acquired over multiple magnifications. Since preliminary screening of a slide into benign or malignant carried out on the fly during the digitization process can reduce a Pathologist's work load, to devote more time for detailed analysis, slide screening has to be performed on the fly with high sensitivity. We propose a deep convolutional neural network (CNN) based solution, where we analyse images from random number of regions of the tissue section at multiple magnifications without any necessity of view correspondence across magnifications. Further a majority voting based approach is used for slide level diagnosis, i.e., the class posteriori estimate of each views at a particular magnification is obtained from the magnification specific CNN, and subsequently posteriori estimate across random multi-views at multi-magnification are voting filtered to provide a slide level diagnosis. We have experimentally evaluated performance using a patient level 5-folded cross-validation with 58 malignant and 24 benign cases of breast tumors to obtain average accuracy of 94.67 ± 14.60%, sensitivity of 96.00 ± 8.94%, specificity of 92.00 ± 17.85% and F-score of 96.24 ± 5.29% while processing each view in ≈ 10 ms.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950690,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950690,Convolutional neural network;histopathology image analysis;multi-scale analysis;multi-view analysis;whole slide imaging,Breast;Cancer;Image analysis;Neural networks;Sensitivity;Standards;Support vector machines,cancer;convolution;image classification;medical image processing;neural nets;tumours,big-data deluge;breast tumors;computer assisted analysis;deep convolutional neural network based solution;histopathology whole-slide classification;suspicious hyperplasia diagnosis;tissue section,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
135,Deep residual learning for compressed sensing MRI,D. Lee; J. Yoo; J. C. Ye,"Bio Imaging and Signal Processing Lab., Dep. of Bio and Brain Engineering, KAIST, South Korea",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,15,18,"Compressed sensing (CS) enables significant reduction of MR acquisition time with performance guarantee. However, computational complexity of CS is usually expensive. To address this, here we propose a novel deep residual learning algorithm to reconstruct MR images from sparsely sampled k-space data. In particular, based on the observation that coherent aliasing artifacts from downsampled data has topologically simpler structure than the original image data, we formulate a CS problem as a residual regression problem and propose a deep convolutional neural network (CNN) to learn the aliasing artifacts. Experimental results using single channel and multi channel MR data demonstrate that the proposed deep residual learning outperforms the existing CS and parallel imaging algorithms. Moreover, the computational time is faster in several orders of magnitude.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950457,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950457,CNN;Compressed sensing MRI;deep learning;residual learning,Complexity theory;Image reconstruction;Machine learning;Magnetic resonance imaging;Manifolds;Topology,biomedical MRI;compressed sensing;data acquisition;image reconstruction;learning (artificial intelligence);medical image processing;neural nets;regression analysis,MR acquisition time;MR image reconstruction;compressed sensing MRI;deep convolutional neural network;deep residual learning algorithm;residual regression problem;sparsely sampled k-space data,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
136,Hybrid dermoscopy image classification framework based on deep convolutional neural network and Fisher vector,Z. Yu; D. Ni; S. Chen; J. Qin; S. Li; T. Wang; B. Lei,"School of Biomedical Engineering, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,301,304,"Dermoscopy image is usually used in early diagnosis of malignant melanoma. The diagnosis accuracy by visual inspection is highly relied on the dermatologist's clinical experience. Due to the inaccuracy, subjectivity, and poor reproducibility of human judgement, an automatic recognition algorithm of dermoscopy image is highly desired. In this work, we present a hybrid classification framework for dermoscopy image assessment by combining deep convolutional neural network (CNN), Fisher vector (FV) and support vector machine (SVM). Specifically, the deep representations of subimages at various locations of a rescaled dermoscopy image are first extracted via a natural image dataset pre-trained on CNN. Then we adopt an orderless visual statistics based FV encoding methods to aggregate these features to build more invariant representations. Finally, the FV encoded representations are classified for diagnosis using a linear SVM. Compared with traditional low-level visual features based recognition approaches, our scheme is simpler and requires no complex preprocessing. Furthermore, the orderless representations are less sensitive to geometric deformation. We evaluate our proposed method on the ISBI 2016 Skin lesion challenge dataset and promising results are obtained. Also, we achieve consistent improvement in accuracy even without fine-tuning the CNN.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950524,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950524,Classification;Deep convolutional neural network;Dermoscopy image;Fisher vector,Feature extraction;Image coding;Lesions;Malignant tumors;Skin;Support vector machines;Visualization,image classification;medical image processing;neural nets;optical microscopy;skin;support vector machines,Fisher vector;deep convolutional neural network;dermoscopy image assessment;hybrid classification framework;skin lesion;support vector machine,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
137,Automated vesicle fusion detection using Convolutional Neural Networks,H. Li; Z. Yin; Y. Xu,"Department of Computer Science, Missouri University of Science and Technology, Rolla, 65401, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,183,187,"Quantitative analysis of vesicle-plasma membrane fusion events in the fluorescence microscopy, has been proven to be important in the vesicle exocytosis study. In this paper, we present a framework to automatically detect fusion events. First, an iterative searching algorithm is developed to extract image patch sequences containing potential events. Then, we propose an event image to integrate the critical image patches of a candidate event into a single-image joint representation as the input to Convolutional Neural Networks (CNNs). According to the duration of candidate events, we design three CNN architectures to automatically learn features for the fusion event classification. Compared on 9 challenging datasets, our proposed method showed very competitive performance and outperformed two state-of-the-arts.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950497,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950497,Vesicle exocytosis;convolutional neural networks;fusion event identification,Biomembranes;Computer architecture;Correlation;Image sequences;Microprocessors;Neural networks;Shape,biomedical optical imaging;biomembranes;cellular biophysics;fluorescence;image fusion;image sequences;iterative methods;medical image processing;neural nets;optical microscopes,CNN architecture;automated vesicle fusion detection;convolutional neural networks;fluorescence microscopy;fusion event classification;image patch sequences;image patches;iterative searching algorithm;single-image joint representation;vesicle exocytosis study;vesicle-plasma membrane fusion event,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
138,A novel hybrid approach for severity assessment of Diabetic Retinopathy in colour fundus images,P. Roy; R. Tennakoon; K. Cao; S. Sedai; D. Mahapatra; S. Maetschke; R. Garnavi,"IBM Research - Australia, Melbourne, VIC, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1078,1082,"Diabetic Retinopathy (DR) is one of the leading causes of blindness worldwide. Detecting DR and grading its severity is essential for disease treatment. Convolutional neural networks (CNNs) have achieved state-of-the-art performance in many different visual classification tasks. In this paper, we propose to combine CNNs with dictionary based approaches, which incorporates pathology specific image representation into the learning framework, for improved DR severity classification. Specifically, we construct discriminative and generative pathology histograms and combine them with feature representations extracted from fully connected CNN layers. Our experimental results indicate that the proposed method shows improvement in quadratic kappa score (κ<sup>2</sup> = 0.86) compared to the state-of-the-art CNN based method (κ<sup>2</sup> = 0.81).",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950703,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950703,Convolution Neural Network;Fisher Vector;Mixture of Gaussians;Principle Component Analysis;Random Forest,Dictionaries;Feature extraction;Histograms;Pathology;Radio frequency;Retina;Training,diseases;eye;feature extraction;image classification;image representation;medical image processing;neural nets,CNN;DR severity classification;blindness;colour fundus images;convolutional neural networks;diabetic retinopathy;dictionary based approaches;disease treatment;feature extraction;generative pathology histogram;quadratic kappa score;specific image representation;visual classification,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
139,Lung nodule segmentation using deep learned prior based graph cut,S. Mukherjee; X. Huang; R. R. Bhagalia,"GE Global Research, Bangalore, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1205,1208,"We propose an automated framework for lung nodule segmentation from pulmonary CT scan using graph cut with a deep learned prior. The segmentation problem is formulated as a hybrid cost function minimization task, which combines a domain specific data term with a deep learned probability map. The proposed segmentation framework embodies the robustness of deep learning in object localization, while retaining the hallmark of traditional segmentation models in addressing the morphological intricacies of elaborate objects. The proposed solution offers more than 20% performance improvement over a contemporary data driven model, and also outperforms traditional graph cuts especially in situations where model initialization is slightly inaccurate.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950733,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950733,CNN;CT;graph cuts;segmentation,Cost function;Image segmentation;Lungs;Neural networks;Robustness;Solids;Two dimensional displays,computerised tomography;image segmentation;learning (artificial intelligence);lung;medical image processing;physiological models;probability,contemporary data driven model;deep learned prior based graph cut;deep learned probability map;lung nodule segmentation;pulmonary CT scan,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
140,M-net: A Convolutional Neural Network for deep brain structure segmentation,R. Mehta; J. Sivaswamy,"Center for Visual Information Technology (CVIT), IIIT-Hyderabad, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,437,440,"In this paper, we propose an end-to-end trainable Convolutional Neural Network (CNN) architecture called the M-net, for segmenting deep (human) brain structures from Magnetic Resonance Images (MRI). A novel scheme is used to learn to combine and represent 3D context information of a given slice in a 2D slice. Consequently, the M-net utilizes only 2D convolution though it operates on 3D data, which makes M-net memory efficient. The segmentation method is evaluated on two publicly available datasets and is compared against publicly available model based segmentation algorithms as well as other classification based algorithms such as Random Forrest and 2D CNN based approaches. Experiment results show that the M-net outperforms all these methods in terms of dice coefficient and is at least 3 times faster than other methods in segmenting a new volume which is attractive for clinical use.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950555,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950555,Convolutional Neural Networks;Deep Brain Structures;Magnetic Resonance Images;Segmentation,Brain;Convolution;Image segmentation;Magnetic resonance imaging;Three-dimensional displays;Training;Two dimensional displays,biomedical MRI;brain;convolution;image segmentation;medical image processing;neural nets,2D convolution;M-net;convolutional neural network;deep brain structure segmentation;magnetic resonance images,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
141,Deep learning model based breast cancer histopathological image classification,Benzheng Wei; Zhongyi Han; Xueying He; Yilong Yin,"College of Science and Technology, Shandong University of Traditional Chinese Medicine, Jinan, China",2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),20170619.0,2017,,,348,353,"The automatic and precision classification for breast cancer histopathological image has a great significance in clinical application. However, the existing analysis approaches are difficult to addressing the breast cancer classification problem because the feature subtle differences of inter-class histopathological image and the classification accuracy still hard to meet the clinical application. Recent advancements in data-driven sharing processing and multi-level hierarchical feature learning have made available considerable chance to dope out a solution to this problem. To address the challenging problem, we propose a novel breast cancer histopathological image classification method based on deep convolutional neural networks, named as BiCNN model, to address the two-class breast cancer classification on the pathological image. This deep learning model considers class and sub-class labels of breast cancer as prior knowledge, which can restrain the distance of features of different breast cancer pathological images. In addition, an advanced data augmented method is proposed to fit tolerance whole slide image recognition, which can full reserve image edge feature of cancerization region. The transfer learning and fine-tuning method are adopted as an optimal training strategy to improve breast cancer histopathological image classification accuracy. The experiment results show that the proposed method leads to a higher classification accuracy (up to 97%) and displays good robustness and generalization, which provides efficient tools for breast cancer clinical diagnosis.",,CD:978-1-5090-4497-9; Electronic:978-1-5090-4499-3; POD:978-1-5090-4500-6; Paper:978-1-5090-4498-6,10.1109/ICCCBDA.2017.7951937,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7951937,CNN;breast cancer;classification;deep learning;histopathological image;massive image data,Cancer;Feature extraction;Image color analysis;Image recognition;Image reconstruction;Neurons;Robustness,cancer;convolution;image classification;learning (artificial intelligence);medical image processing;neural nets,BiCNN model;advanced data augmented method;breast cancer clinical diagnosis;breast cancer histopathological image classification method;cancerization region;class labels;classification accuracy;data-driven sharing processing;deep convolutional neural networks;deep learning model;fine-tuning method;fit tolerance whole slide image recognition;image edge feature;multilevel hierarchical feature learning;optimal training strategy;subclass labels;transfer learning,,,,,,,,28-30 April 2017,,IEEE,IEEE Conference Publications
142,Exploring texture Transfer Learning for Colonic Polyp Classification via Convolutional Neural Networks,E. Ribeiro; M. Häfner; G. Wimmer; T. Tamaki; J. J. W. Tischendorf; S. Yoshida; S. Tanaka; A. Uhl,"University of Salzburg - Department of Computer Sciences, AT",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1044,1048,"This work addresses Transfer Learning via Convolutional Neural Networks (CNN's) for the automated classification of colonic polyps in eight HD-endoscopic image databases acquired using different modalities. For this purpose, we explore if the architecture, the training approach, the number of classes, the number of images as well as the nature of the images in the training phase can influence the results. The experiments show that when the number of classes and the nature of the images are similar to the target database, the results are improved. Also, the better results obtained by the transfer learning compared to the most used features in the literature suggest that features learned by CNN's can be highly relevant for automated classification of colonic polyps.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950695,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950695,Colonic Polyp Classification;Convolutional Neural Networks;Deep Learning;Texture Transfer Learning,Biomedical imaging;Colonic polyps;Computers;Feature extraction;Image databases;Training,biomedical optical imaging;endoscopes;image classification;learning (artificial intelligence);medical image processing;neural nets,HD-endoscopic image databases;colonic polyp classification;convolutional neural networks;deep learning;transfer learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
143,Modeling Task fMRI Data via Deep Convolutional Autoencoder,H. Huang; X. Hu; Y. Zhao; M. Makkie; Q. Dong; S. Zhao; L. Guo; T. Liu,"School of Automation, Northwestern Polytechnical University, Xi&#x2019;an, 710072, China.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"Task-based fMRI (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, Independent Component Analysis (ICA) and Sparse Dictionary Learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this study, we developed a new neural network structure based on CNN, called Deep Convolutional Auto-Encoder (DCAE), in order to take the advantages of both data-driven approach and CNN’s hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project (HCP) tfMRI datasets, and promising results are achieved.",0278-0062;02780062,,10.1109/TMI.2017.2715285,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949140,CNN;Task fMRI;deep learning;unsupervised,Brain modeling;Convolution;Data models;Decoding;Hidden Markov models;Machine learning;Time series analysis,,,,,,,,,20170615.0,,,IEEE,IEEE Early Access Articles
144,Identifying Carotid Plaque Composition in MRI with Convolutional Neural Networks,Y. Dong; Y. Pan; X. Zhao; R. Li; C. Yuan; W. Xu,"Inst. for Interdiscipl. Inf. Sci., Tsinghua Univ., Beijing, China",2017 IEEE International Conference on Smart Computing (SMARTCOMP),20170615.0,2017,,,1,8,"Carotid plaques may cause strokes. The composition of the plaque helps assessing the risk. Magnetic resonance imaging (MRI) is a powerful technology for analyzing the composition. It is both tedious and error-prone for a human radiologist to review such images. Traditional computer-aided diagnosis tools use manually crafted features that lack both generality and accuracy. We propose a novel approach using Deep convolutional neural networks (CNN) to classify these plaque tissues. In order to accommodate the multi-contrast MRI images, we modify state-of-the-art CNN models to support different number of input channels, and also adapt the models to do pixel- wise predictions. On a dataset with 1,098 human subjects, we show that we achieve significantly better accuracy than previous models. Our result also indicates interesting relations between contrast weightings and tissue types.",,Electronic:978-1-5090-6517-2; POD:978-1-5090-6518-9,10.1109/SMARTCOMP.2017.7947015,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947015,,Adaptation models;Atherosclerosis;Biomedical imaging;Hemorrhaging;Magnetic resonance imaging;Neural networks,biomedical MRI;feedforward neural nets;medical image processing,CNN;carotid plaque composition;contrast weightings;deep convolutional neural networks;human radiologist;magnetic resonance imaging;multicontrast MRI images;plaque tissues;tissue types,,,,,,,,29-31 May 2017,,IEEE,IEEE Conference Publications
145,Deep Convolutional Neural Networks and Learning ECG Features for Screening Paroxysmal Atrial Fibrillation Patients,B. Pourbabaee; M. J. Roshtkhari; K. Khorasani,"Department of Electrical and Computer Engineering, Concordia University, Montreal, QC H3G1M8, Canada.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",,2017,PP,99.0,1,10,"In this paper, a novel computationally intelligent-based electrocardiogram (ECG) signal classification methodology using a deep learning (DL) machine is developed. The focus is on patient screening and identifying patients with paroxysmal atrial fibrillation (PAF), which represents a life threatening cardiac arrhythmia. The proposed approach operates with a large volume of raw ECG time-series data as inputs to a deep convolutional neural networks (CNN). It autonomously learns representative and key features of the PAF to be used by a classification module. The features are therefore learned directly from the large time domain ECG signals by using a CNN with one fully connected layer. The learned features can effectively replace the traditional ad hoc and time-consuming user's hand-crafted features. Our experimental results verify and validate the effectiveness and capabilities of the learned features for PAF patient screening. The main advantages of our proposed approach are to simplify the feature extraction process corresponding to different cardiac arrhythmias and to remove the need for using a human expert to define appropriate and critical features working with a large time-series data set. The extensive simulations and case studies conducted indicate that combining the learned features with other classifiers will significantly improve the performance of the patient screening system as compared to an end-to-end CNN classifier. The effectiveness and capabilities of our proposed ECG DL classification machine is demonstrated and quantitative comparisons with several conventional machine learning classifiers are also provided.",2168-2216;21682216,,10.1109/TSMC.2017.2705582,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937819,Biomedical monitoring;deep convolution neural network;electrocardiogram (ECG);feature extraction;neural network architecture;paroxysmal atrial fibrillation (PAF),Convolution;Electrocardiography;Feature extraction;Hidden Markov models;Medical services;Monitoring;Neural networks,,,,,,,,,20170601.0,,,IEEE,IEEE Early Access Articles
146,Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning,W. Xue; A. Islam; M. Bhaduri; S. Li,"Department of Medical Imaging, Western University, London, ON N6A 3K7, Canada and also with Digital Imaging Group of London, London, ON N6A 3K7, Canada.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.440.71mm) and areas of cavity and myocardium (204133mm2). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment",0278-0062;02780062,,10.1109/TMI.2017.2709251,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934404,cardiac MR;deep convolution autoencoder;direct estimation;joint learning;multitype cardiac indices,Convolution;Estimation;Feature extraction;Image representation;Image segmentation;Myocardium;Volume measurement,,,,,,,,,20170526.0,,,IEEE,IEEE Early Access Articles
147,Single-Trial Classification of Event-Related Potentials in Rapid Serial Visual Presentation Tasks Using Supervised Spatial Filtering,H. Cecotti; M. P. Eckstein; B. Giesbrecht,"Department of Psychological and Brain Sciences, Institute for Collaborative Biotechnologies, University of California, Santa Barbara, CA, USA",IEEE Transactions on Neural Networks and Learning Systems,20170520.0,2014,25,11.0,2030,2042,"Accurate detection of single-trial event-related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques. Supervised spatial filtering methods that enhance the discriminative information in EEG data are commonly used to improve single-trial ERP detection. We propose a convolutional neural network (CNN) with a layer dedicated to spatial filtering for the detection of ERPs and with training based on the maximization of the area under the receiver operating characteristic curve (AUC). The CNN is compared with three common classifiers: 1) Bayesian linear discriminant analysis; 2) multilayer perceptron (MLP); and 3) support vector machines. Prior to classification, the data were spatially filtered with xDAWN (for the maximization of the signal-to-signal-plus-noise ratio), common spatial pattern, or not spatially filtered. The 12 analytical techniques were tested on EEG data recorded in three rapid serial visual presentation experiments that required the observer to discriminate rare target stimuli from frequent nontarget stimuli. Classification performance discriminating targets from nontargets depended on both the spatial filtering method and the classifier. In addition, the nonlinear classifier MLP outperformed the linear methods. Finally, training based AUC maximization provided better performance than training based on the minimization of the mean square error. The results support the conclusion that the choice of the systems architecture is critical and both spatial filtering and classification must be considered together.",2162-237X;2162237X,,10.1109/TNNLS.2014.2302898,Institute for Collaborative Biotechnologies through the U.S. Army Research Office; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737255,Brain–computer interface (BCI);Brain-computer interface (BCI);common spatial patterns (CSP);convolution;electroencephalogram (EEG);neural networks;rapid serial visual presentation (RSVP);spatial filters;spatial filters.,Biological neural networks;Convolution;Electroencephalography;Neurons;Sensors;Training;Visualization,Bayes methods;electroencephalography;learning (artificial intelligence);medical signal detection;medical signal processing;multilayer perceptrons;signal classification;spatial filters;support vector machines;visual evoked potentials,Bayesian linear discriminant analysis;CNN;EEG data;area maximization;convolutional neural network;discriminative information;frequent nontarget stimuli;linear methods;machine learning techniques;mean square error minimization;multilayer perceptron;nonlinear classifier MLP;rapid serial visual presentation;rapid serial visual presentation tasks;receiver operating characteristic curve;signal processing;signal-to-signal-plus-noise ratio;single-trial ERP detection;single-trial classification;single-trial event-related potentials;spatial pattern;supervised spatial filtering methods;support vector machines;target stimuli;training based AUC maximization;xDAWN,"0;Adolescent;Adult;Algorithms;Area Under Curve;Brain Mapping;Electroencephalography;Evoked Potentials;Female;Filtration;Humans;Male;Neural Networks (Computer);Photic Stimulation;ROC Curve;Reaction Time;Signal Processing, Computer-Assisted;Support Vector Machine;Visual Perception;Young Adult",12.0,,61.0,,,20140211.0,Nov. 2014,,IEEE,IEEE Journals & Magazines
148,ECG Monitoring System Integrated With IR-UWB Radar Based on CNN,W. Yin; X. Yang; L. Zhang; E. Oki,"Beijing University of Posts and Telecommunications, Beijing, China",IEEE Access,20170520.0,2016,4,,6344,6351,"In the demand for protecting the increasing aged groups from heart attacks, the improvement of the mobile electrocardiogram (ECG) monitoring systems becomes significant. The limitations of the arrhythmia classification in these systems are the lack of ability to cope with motion state and the low accuracy in new users' data. This paper proposes a system which applies the impulse radio ultra wideband radar data as additional information to assist the arrhythmia classification of ECG recordings in the slight motion state. Besides, this proposed system employs a cascade convolutional neural network to achieve an integrated analysis of ECG recordings and radar data. The experiments are implemented in the Caffe platform and the result reaches an accuracy of 88.89% in the slight motion state. It turns out that this proposed system keeps a stable accuracy of classification for normal and abnormal heartbeats in the slight motion state.",2169-3536;21693536,,10.1109/ACCESS.2016.2608777,111 Project; EU FP7IRSES Mobile Cloud Project; 10.13039/501100001809 - National Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576640,Electrocardiography;neural networks;ultra wideband radar,Convolution;Electrocardiography;Feature extraction;Heart beat;Kernel;Monitoring;Radar,bioelectric potentials;electrocardiography;medical disorders;medical signal processing;neurophysiology;signal classification;ultra wideband radar,CNN;Caffe platform;ECG recordings;IR-UWB radar data;abnormal heartbeats;arrhythmia classification;cascade convolutional neural network;heart attacks;impulse radio ultra wideband radar data;mobile ECG monitoring system;mobile electrocardiogram monitoring systems,,,,,,,20160926.0,2016,,IEEE,IEEE Journals & Magazines
149,Computer diagnostic tools based on biomedical image analysis,O. Berezsky; O. Pitsun; S. Verbovyy; T. Datsko; A. Bodnar,"Computer Engineering Department, Ternopil National Economic University, UKRAINE, Ternopil, 11 Lvivska str.",2017 14th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM),20170504.0,2017,,,388,391,"In this paper, the authors investigated the main types of mammary dysplasia. In order to classify biomedical images, the researchers developed a basic model of convolutional neural network (CNN). Input parameters of the neural network to classify cytological and histological images were thoroughly researched and selected.",,Electronic:978-1-5090-5045-1; POD:978-1-5090-5046-8,10.1109/CADSM.2017.7916157,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916157,convolutional neural network;cytological and histological images;mammary dysplasia,Biological neural networks;Breast;Connective tissue;Diseases;Ducts;Image classification;Kernel,cellular biophysics;diseases;feedforward neural nets;image classification;medical image processing,Input parameters;biomedical image analysis;biomedical image classification;computer diagnostic tools;convolutional neural network;cytological image classification;histological image classification;mammary dysplasia,,,,,,,,21-25 Feb. 2017,,IEEE,IEEE Conference Publications
150,Skin disease classification versus skin lesion characterization: Achieving robust diagnosis using multi-label deep neural networks,Haofu Liao; Yuncheng Li; Jiebo Luo,"Department of Computer Science, University of Rochester, New York 14627, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,355,360,"In this study, we investigate what a practically useful approach is in order to achieve robust skin disease diagnosis. A direct approach is to target the ground truth diagnosis labels, while an alternative approach instead focuses on determining skin lesion characteristics that are more visually consistent and discernible. We argue that, for computer aided skin disease diagnosis, it is both more realistic and more useful that lesion type tags should be considered as the target of an automated diagnosis system such that the system can first achieve a high accuracy in describing skin lesions, and in turn facilitate disease diagnosis using lesion characteristics in conjunction with other evidences. To further meet such an objective, we employ convolutional neutral networks (CNNs) for both the disease-targeted and lesion-targeted classifications. We have collected a large-scale and diverse dataset of 75,665 skin disease images from six publicly available dermatology atlantes. Then we train and compare both disease-targeted and lesion-targeted classifiers, respectively. For disease-targeted classification, only 27.6% top-1 accuracy and 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of 0.42. In contrast, for lesion-targeted classification, we can achieve a much higher mAP of 0.70.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899659,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899659,convolutional neural networks;skin disease classification;skin lesion characterization,Dermatology;Diseases;Lesions;Malignant tumors;Skin;Training;Visualization,diseases;medical image processing;neural nets;skin,CNN;automated diagnosis system;computer aided skin disease diagnosis;convolutional neutral networks;disease-targeted classification;ground truth diagnosis labels;lesion-targeted classifications;lesion-targeted classifiers;mean average precision;multilabel deep neural networks;publicly available dermatology atlantes;robust diagnosis;robust skin disease diagnosis;skin disease classification;skin lesion characteristics;skin lesion characterization,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
151,Severity grading of psoriatic plaques using deep CNN based multi-task learning,A. Pal; A. Chaturvedi; U. Garain; A. Chandra; R. Chatterjee,"CVPR Unit, Indian Statistical Unit, Kolkata 700108, West Bengal, India",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,1478,1483,"This paper addresses the problem of automatic machine analysis based severity scoring of psoriasis skin disease. Three different disease parameters namely, erythema, scaling and induration are considered for such severity grading. Given an image containing a psoriatic plaque the task is to predict severity scores for all the three parameters. This paper presents a novel deep CNN based architecture for achieving the task. Apart from viewing this task as three different single task learning (STL) problems (i.e. three different classification problems), a new multi-task learning (MTL) is also presented where the three classification tasks are treated as interdependent and thereby the neural net is trained accordingly. A new annotated dataset consisting of seven hundred and seven (707) images has been constructed on which the performance of the severity scoring algorithms have been reported. Several competing baselines are considered to compare the performance of STL and MTL approaches. Experimental result shows that the deep CNN based architectures (both the STL and MTL) achieve promising performances, MTL producing slightly superior results to that of STL.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899846,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899846,,Computer architecture;Convolution;Diseases;Drugs;Estimation;Kernel;Skin,diseases;image classification;learning (artificial intelligence);medical image processing;neural net architecture,MTL;automatic machine analysis;deep CNN based architecture;deep CNN based multitask learning;disease parameters;erythema;image classification tasks;induration;neural net;psoriasis skin disease;psoriatic plaque severity grading;scaling;severity scoring algorithms;single task learning problem,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
152,Deep learning for magnification independent breast cancer histopathology image classification,N. Bayramoglu; J. Kannala; J. Heikkilä,"Center for Machine Vision and Signal Analysis, University of Oulu, Finland",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,2440,2445,"Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7900002,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900002,,Breast cancer;Databases;Microscopy;Pathology;Training;Training data,cancer;computer vision;image classification;learning (artificial intelligence);medical image processing;neural nets,BreaKHis dataset;breast tissues;computer vision;convolutional neural networks;deep learning;diagnostic pathology workflow;digital imaging techniques;image classification;machine learning;magnification independent breast cancer histopathology image classification;microscopic analysis;multitask CNN;single task CNN,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
153,Skin lesion segmentation in clinical images using deep learning,M. H. Jafari; N. Karimi; E. Nasr-Esfahani; S. Samavi; S. M. R. Soroushmehr; K. Ward; K. Najarian,"Department of Electrical and Computer Engineering, Isfahan University of Technology, 84156-83111 Iran",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,337,342,"Melanoma is the most aggressive form of skin cancer and is on rise. There exists a research trend for computerized analysis of suspicious skin lesions for malignancy using images captured by digital cameras. Analysis of these images is usually challenging due to existence of disturbing factors such as illumination variations and light reflections from skin surface. One important stage in diagnosis of melanoma is segmentation of lesion region from normal skin. In this paper, a method for accurate extraction of lesion region is proposed that is based on deep learning approaches. The input image, after being preprocessed to reduce noisy artifacts, is applied to a deep convolutional neural network (CNN). The CNN combines local and global contextual information and outputs a label for each pixel, producing a segmentation mask that shows the lesion region. This mask will be further refined by some post processing operations. The experimental results show that our proposed method can outperform the existing state-of-the-art algorithms in terms of segmentation accuracy.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899656,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899656,Melanoma;convolutional neural network;deep learning;medical image segmentation;skin cancer,Feature extraction;Image segmentation;Lesions;Lighting;Machine learning;Malignant tumors;Skin,cancer;convolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets,CNN;clinical images;computerized analysis;convolutional neural network;deep learning approaches;digital cameras;global contextual information;illumination variations;lesion region extraction;light reflections;local contextual information;melanoma;noisy artifacts reduction;normal skin;post processing operations;segmentation accuracy;segmentation mask;skin cancer;skin lesion segmentation,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
154,Quantifying radiographic knee osteoarthritis severity using deep convolutional neural networks,J. Antony; K. McGuinness; N. E. O'Connor; K. Moran,"Insight Centre for Data Analytics, Dublin City University, Ireland",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,1195,1200,"This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren & Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899799,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899799,Convolutional neural network;KL grades;Knee osteoarthritis;classification;regression;wndchrm,Feature extraction;Indexes;Neural networks;Osteoarthritis;Radiography;Support vector machines;Training,X-ray imaging;image classification;medical image processing;neural nets;radiography,CNN;ImageNet;KL grades;OAI;Osteoarthritis Initiative;X-ray images;automatic knee OA severity predictions;continuous distance-based evaluation;deep convolutional neural network models;knee OA images;mean squared error;radiograph images;radiographic knee osteoarthritis severity;radiographs;regression problem,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
155,Optic Disc Detection Using Fine Tuned Convolutional Neural Networks,F. Calimeri; A. Marzullo; C. Stamile; G. Terracina,"Dept. of Math. & Comput. Sci., Univ. of Calabria, Rende, Italy",2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),20170424.0,2016,,,69,75,"The detection of the Optic Disc (OD) is an significant step in retinal fundus images analysis, it allows to extract relevant information that proved to be useful for the prevention of several pathologies, such as glaucoma, hypertension, diabetes and other cardiovascular diseases, which manifest their effects in the retina. In this work we present a supervised method for automatically detecting the position of the Optic Disc in retinal fundus digital images, the goal has been achieved by means of a proper reuse of previous knowledge from a pre-trained Convolutional Neural Network (CNN), already able to detect faces in an image. Experimental analyses showed high level of accuracy in the detection of the optic disc on the DRIVE, STARE and DRIONS databases.",,Electronic:978-1-5090-5698-9; POD:978-1-5090-5699-6,10.1109/SITIS.2016.20,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907447,Convolutional Neural Networks;Fine Tuning;Fundus Image;Optic Disc Detection;Transfer Learning,Biomedical optical imaging;Feature extraction;Neural networks;Optical computing;Optical fiber networks;Optical imaging;Retina,feature extraction;medical image processing;neural nets;object detection,CNN;DRIONS database;DRIVE database;OD;STARE database;fine tuned convolutional neural networks;information extraction;optic disc detection;pathologies prevention;retinal fundus image analysis,,,,,,,,Nov. 28 2016-Dec. 1 2016,,IEEE,IEEE Conference Publications
156,Hybrid deep learning for Reflectance Confocal Microscopy skin images,P. Kaur; K. J. Dana; G. O. Cula; M. C. Mack,"Department of Electrical and Computer Engineering, Rutgers University, NJ, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,1466,1471,"Reflectance Confocal Microscopy (RCM) is used for evaluation of human skin disorders and the effects of skin treatments by imaging the skin layers at different depths. Traditionally, clinical experts manually categorize the images captured into different skin layers. This time-consuming labeling task impedes the convenient analysis of skin image datasets. In recent automated image recognition tasks, deep learning with convolutional neural nets (CNN) has achieved remarkable results. However in many clinical settings, training data is often limited and insufficient for CNN training. For recognition of RCM skin images, we demonstrate that a CNN trained on a moderate size dataset leads to low accuracy. We introduce a hybrid deep learning approach which uses traditional texton-based feature vectors as input to train a deep neural network. This hybrid method uses fixed filters in the input layer instead of tuned filters, yet superior performance is achieved. Our dataset consists of 1500 images from 15 RCM stacks belonging to six different categories of skin layers. We show that our hybrid deep learning approach performs with a test accuracy of 82% compared with 51% for CNN. We also compare the results with additional proposed methods for RCM image recognition and show improved accuracy.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899844,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899844,,Epidermis;Histograms;Image recognition;Libraries;Machine learning;Neural networks,convolution;data analysis;filtering theory;image recognition;learning (artificial intelligence);medical image processing;microscopy;vectors;visual databases,CNN training;RCM;automated image recognition tasks;clinical experts;convolutional neural nets;fixed filters;human skin disorders;hybrid deep learning approach;reflectance confocal microscopy skin images;skin image datasets;skin layers;traditional texton-based feature vectors,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
157,HEp-2 specimen classification via deep CNNs and pattern histogram,Hongwei Li; Hao Huang; W. S. Zheng; Xiaohua Xie; J. Zhang,"School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,2145,2149,"Automatic classification of Human Epithelial Type-2 (HEp-2) specimen patterns is an important yet challenging problem in medical image analysis. Most prior works have primarily focused on cells images classification problem which is one of the early essential steps in the system pipeline, while less attention has been paid to the classification of whole-specimen ones. In this work, a specimen pattern recognition system combining convolutional neural networks (CNNs) and pattern histogram was proposed. The pattern histograms were obtained based on the prediction of each single cell inside the specimens. Two strategies were designed to predicted the pattern of a whole specimen: 1) the most dominant cell pattern in pattern histogram was represented as the specimen pattern, 2) the pattern histograms were employed as bags of patterns and then were trained and predicted separately by a SVM classifier. Experimental results show that the proposed system is effective and achieves high classification accuracy on public benchmark datasets. We further evaluate the robustness of the proposed framework by testing trained CNNs on another different dataset, demonstrating that the system is robust to inter-lab data.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899953,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899953,,Convolution;Feature extraction;Histograms;Image segmentation;Neural networks;Support vector machines;Training,biology computing;feedforward neural nets;image classification;support vector machines,HEp-2 specimen classification;SVM classifier;cells images classification problem;convolutional neural networks;deep CNN;human epithelial type-2 specimen patterns;interlab data;medical image analysis;pattern histogram;pattern histograms;specimen pattern recognition system;system pipeline,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
158,Deep convolutional neural network based HEp-2 cell classification,Xi Jia; Linlin Shen; Xiande Zhou; Shiqi Yu,"Computer Vision Institute, College of Computer Science and Software Engineering, Shenzhen University, China",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,77,80,"As different staining patterns of HEp-2 cells indicate different diseases, the classification of Indirect Immune Fluorescence (IIF) images on Human Epithelial-2 (HEp-2) cell is important for clinical applications. Different from traditional pattern recognition techniques, we use CNN to extract more high-level features for cell images classification. Compared to the existing CNN based HEp-2 classification methods, we proposed a network with deeper architecture. A class-balanced approach is also proposed to augment the HEp-2 cell dataset for network training. The proposed framework achieves an average class accuracy of 79.29% on ICPR 2012 HEp-2 dataset and a mean class accuracy of 98.26% on ICPR 2016 HEp-2 training set.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899611,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899611,CNN;Hep-2;class-balanced;classification,Computer architecture;Feature extraction;Immune system;Microprocessors;Pattern recognition;Testing;Training,cellular biophysics;diseases;feature extraction;image classification;medical image processing;neural net architecture,CNN;HEp-2 cell classification;IIF cell image classification;class-balanced approach;clinical applications;deep convolutional neural network;diseases;high-level feature extraction;human epithelial-2 cell;indirect immune fluorescence image classification;network training;pattern recognition techniques;staining patterns,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
159,A temporal deep learning approach for MR perfusion parameter estimation in stroke,K. C. Ho; F. Scalzo; K. V. Sarma; S. El-Saden; C. W. Arnold,"Medical Imaging Informatics Group, Department of Radiological Sciences, University of California Los Angeles, 90024, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,1315,1320,"Perfusion magnetic resonance (MR) images are often used in the assessment of acute ischemic stroke to distinguish between salvageable tissue and infarcted core. Deconvolution methods such as singular value decomposition have been used to approximate model-based perfusion parameters from these images. However, studies have shown that these existing deconvolution algorithms can introduce distortions that may negatively influence the utility of these parameter maps. There is limited previous work on utilizing machine learning algorithms to estimate perfusion parameters. In this work, we present a novel bi-input convolutional neural network (bi-CNN) to approximate four perfusion parameters without using an explicit deconvolution method. These bi-CNNs produced good approximations for all four parameters, with relative average root-mean-square errors (ARMSEs) ≤ 5% of the maximum values. We further demonstrate the utility of the estimated perfusion maps for quantifying the salvageable tissue volume in stroke, with more than 80% agreement with the ground truth. These results show that deep learning techniques are a promising tool for perfusion parameter estimation without requiring a standard deconvolution process.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899819,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899819,,Biological neural networks;Biological tissues;Convolution;Deconvolution;Estimation;Imaging;Parameter estimation,approximation theory;biological tissues;biomedical MRI;deconvolution;feedforward neural nets;haemorheology;learning (artificial intelligence);medical signal processing;parameter estimation;singular value decomposition,MR perfusion parameter estimation;acute ischemic stroke assessment;bi-CNN;bi-input convolutional neural network;infarcted core;model-based perfusion parameters;perfusion magnetic resonance images;perfusion parameter approximation;relative average root-mean-square errors;salvageable tissue volume;singular value decomposition;temporal deep learning,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
160,Low quality dermal image classification using transfer learning,M. S. Elmahdy; S. S. Abdeldayem; I. A. Yassine,"Systems and Biomedical Department, Faculty of Engineering, Cairo University. Giza, Egypt",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413.0,2017,,,373,376,"In this study, we investigate three class skin lesion classification problem of a low quality and small size dataset using transfer learning using AlexNet deep Convolutional Neural Network (CNN). Our approach involves modifying the pre-trained AlexNet model; through replacing the decision layer to be compatible with our three class problem. In addition, we propose adding two dropout layers to overcome the over fitting problem. The fine tuning process of the complete network, based on stochastic gradient descent, is performed using skin lesion dataset. Furthermore, we investigated augmenting the original dataset through three flipping directions and sixteen rotation angles processes using a new methodology. The proposed algorithm has been compared with a hand crafted features, based on Local Binary Pattern (LBP) representation followed by Support Vector Machine (SVM) classifier. Increasing the dataset size has dramatically boosted the performance of classifiers achieving accuracy of 98.67% for the modified AlexNet compared to 96.8% using the LBP based system.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897283,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897283,,Diseases;Feature extraction;Lesions;Neural networks;Skin;Support vector machines;Tuning,biomedical optical imaging;convolutional codes;image classification;medical image processing;neural nets;skin;support vector machines,AlexNet deep convolutional neural network;CNN;LBP;SVM;class skin lesion classification;local binary pattern representation;low quality dermal image classification;stochastic gradient descent;support vector machine classifier;transfer learning,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
161,Augmenting data when training a CNN for retinal vessel segmentation: How to warp?,A. Oliveira; S. Pereira; C. A. Silva,"CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal",2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG),20170330.0,2017,,,1,4,"The retinal vascular condition is a trustworthy biomarker of several ophthalmologic and cardiovascular diseases, so automatic vessel segmentation is a crucial step to diagnose and monitor these problems. Deep Learning models have recently revolutionized the state-of-the-art in several fields, since they can learn features with multiple levels of abstraction from the data itself. However, these methods can easily fall into overfitting, since a huge number of parameters must be learned. Having bigger datasets may act as regularization and lead to better models. Yet, acquiring and manually annotating images, especially in the medical field, can be a long and costly procedure. Hence, when using regular datasets, people heavily need to apply artificial data augmentation. In this work, we use a fully convolutional neural network capable of reaching the state-of-the-art. Also, we investigate the benefits of augmenting data with new samples created by warping retinal fundus images with nonlinear transformations. Our results hint that may be possible to halve the amount of data, while maintaining the same performance.",,Electronic:978-1-5090-4801-4; POD:978-1-5090-4802-1,10.1109/ENBENG.2017.7889443,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889443,Convolutional neural network;Data augmentation;Retinal blood vessel segmentation,Data mining;Image segmentation;Neural networks;Retinal vessels;Training;Two dimensional displays,biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets;vision defects,CNN;artificial data augmentation;automatic vessel segmentation;biomarker;cardiovascular diseases;deep Learning models;fully convolutional neural network;medical field;nonlinear transformations;ophthalmologic diseases;regular datasets;retinal fundus images;retinal vascular condition;retinal vessel segmentation,,,,,,,,16-18 Feb. 2017,,IEEE,IEEE Conference Publications
162,A Digital Pathology application for whole-slide histopathology image analysis based on genetic algorithm and Convolutional Networks,M. Puerto; T. Vargas; A. Cruz-Roa,"GITECX Research Group, University of Los Llanos, Colombia",2016 IEEE Latin American Conference on Computational Intelligence (LA-CCI),20170327.0,2016,,,1,7,"The last decade Digital Pathology is coming as a relevant and promising area for cancer research and clinical practice thanks to two main trends, 1) the availability of whole slide scanners for complete pathology slide digitalization, and 2) the development of several computational method for histopathology image analysis. However, there are very few works addressed to analyze the whole-slide digitized images (WSI) because their large resolution (e.g. 80,000 × 80,000 pixels at 40× magnification) resulting in huge computational cost for automatic analysis. This paper presents an application design of a meta-heuristic optimization method based on a genetic algorithm (GA) for exploration and exploitation of regions of interest for diagnosis in a WSI in combination with a Convolutional Neural Network (CNN) trained in previous works [10], [11]. The preliminary results show that presented solution scales in computing time given the initial number of samples (initial population). The developed application in Java including the GA method for WSI analysis could be used for diagnosis support by pathologists thanks of its usability and visual interpretability through a probability map of the invasive tumor regions in the WSI.",,Electronic:978-1-5090-5105-2; POD:978-1-5090-5106-9; USB:978-1-5090-5104-5,10.1109/LA-CCI.2016.7885738,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885738,Adaptive Sampling;Convolutional Neural Network;Digital pathology;Genetic Algorithm;Whole-Slide Imaging,Biological cells;Cancer;Genetic algorithms;Pathology;Sociology;Statistics;Tumors,cancer;genetic algorithms;learning (artificial intelligence);medical image processing;neural nets;tumours,CNN training;Java;WSI analysis;cancer research;clinical practice;complete pathology slide digitalization;convolutional neural network;diagnosis;digital pathology application;genetic algorithm;invasive tumor region;metaheuristic optimization method;probability map;visual interpretability;whole slide scanner;whole-slide digitized image;whole-slide histopathology image analysis,,,,,,,,2-4 Nov. 2016,,IEEE,IEEE Conference Publications
163,Multimodal learning using convolution neural network and Sparse Autoencoder,Tien Duong Vu; Hyung-Jeong Yang; V. Q. Nguyen; A-Ran Oh; Mi-Sun Kim,"Department of Electronics and Computer Engineering, Chonnam National University, Gwangju, South Korea",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320.0,2017,,,309,312,"In the last decade, pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease (AD) have been the subject of extensive research. Deep learning has recently been a great interest in AD classification. Previous works had done almost on single modality dataset, such as Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET), shown high performances. However, identifying the distinctions between Alzheimer's brain data and healthy brain data in older adults (age > 75) is challenging due to highly similar brain patterns and image intensities. The corporation of multimodalities can solve this issue since it discovers and uses the further complementary of hidden biomarkers from other modalities instead of only one, which itself cannot provide. We therefore propose a deep learning method on fusion multimodalities. In details, our approach includes Sparse Autoencoder (SAE) and convolution neural network (CNN) train and test on combined PET-MRI data to diagnose the disease status of a patient. We focus on advantages of multimodalities to help providing complementary information than only one, lead to improve classification accuracy. We conducted experiments in a dataset of 1272 scans from ADNI study, the proposed method can achieve a classification accuracy of 90% between AD patients and healthy controls, demonstrate the improvement than using only one modality.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881683,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881683,Alzheimer's disease;MRI;PET;autoencoder;convolutional neural network;deep learning,Biological neural networks;Convolution;Feature extraction;Magnetic resonance imaging;Positron emission tomography;Support vector machines;Three-dimensional displays,biomedical MRI;brain;convolution;diseases;image classification;image coding;image fusion;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography,Alzheimer disease diagnosis;convolution neural network;deep learning method;image fusion multimodalities;magnetic resonance imaging;multimodal learning method;neuroimaging data;pattern recognition methods;positron emission tomography;sparse autoencoder,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
164,Simultaneous reconstruction and restoration of sparsely sampled optical coherence tomography image through learning separable filters for deep architectures,S. P. K. Karri; N. Garai; D. Nawn; S. Ghosh; D. Chakraborty; J. Chatterjee,"IIT Kharagpur, Kharagpur, India 721302",2016 IEEE Students&#8217; Technology Symposium (TechSym),20170309.0,2016,,,52,55,Spectral domain optical coherence tomography (SD-OCT) is widely employed across ophthalmology practices for visual investigation of live tissues. The involuntary movements of subjects frequently infuse motion artifacts to SD-OCT images. Sub-sampling of signals is introduced in imaging protocol to avoid such artifacts which causes fall in spatial resolution and peak signal to noise ratio (PSNR). Sparse coding (SC) is opted for restoration and rectification of complete signals from sparse samples through constructing complete and sparse space dictionaries independently. Convolutional neural networks (CNN) can be casted as SC for jointly learning dictionaries resulting less number of CNN filters (equivalence of SC dictionaries) to be trained. The proposed approach extends the separable filters to CNN through architectural constrain. This results in a parallel architecture and reduced number of parameters without compromising on performance. The approach scaled down trainable parameters by 46% with a trade-off of 0.108 PSNR during training and 0.107 PSNR during testing in comparison to conventional CNN.,,Electronic:978-1-5090-5163-2; POD:978-1-5090-5164-9,10.1109/TechSym.2016.7872654,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872654,convolutional neural network;denoising;image restoration;optical coherence tomography;separable filters;sparse coding,Convolution;Dictionaries;High definition video;Image restoration;Optical coherence tomography;Testing;Training,biological tissues;image restoration;medical image processing;neural nets;optical tomography,SD-OCT images;convolutional neural networks;deep architectures;learning separable filters;live tissues;ophthalmology;simultaneous image reconstruction;simultaneous image restoration;sparse coding;sparsely sampled optical coherence tomography image;spatial resolution;spectral domain optical coherence tomography,,,,,,,,Sept. 30 2016-Oct. 2 2016,,IEEE,IEEE Conference Publications
165,HEp-2 Cell Image Classification With Deep Convolutional Neural Networks,Z. Gao; L. Wang; L. Zhou; J. Zhang,"School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia",IEEE Journal of Biomedical and Health Informatics,20170303.0,2017,21,2.0,416,428,"Efficient Human Epithelial-2 cell image classification can facilitate the diagnosis of many autoimmune diseases. This paper proposes an automatic framework for this classification task, by utilizing the deep convolutional neural networks (CNNs) which have recently attracted intensive attention in visual recognition. In addition to describing the proposed classification framework, this paper elaborates several interesting observations and findings obtained by our investigation. They include the important factors that impact network design and training, the role of rotation-based data augmentation for cell images, the effectiveness of cell image masks for classification, and the adaptability of the CNN-based classification system across different datasets. Extensive experimental study is conducted to verify the above findings and compares the proposed framework with the well-established image classification models in the literature. The results on benchmark datasets demonstrate that 1) the proposed framework can effectively outperform existing models by properly applying data augmentation, 2) our CNN-based framework has excellent adaptability across different datasets, which is highly desirable for cell image classification under varying laboratory settings. Our system is ranked high in the cell image classification competition hosted by ICPR 2014.",2168-2194;21682194,,10.1109/JBHI.2016.2526603,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400923,Deep convolutional neural networks;indirect immunofluorescence (IIF);staining patterns classification,Brain models;Computer architecture;Feature extraction;Informatics;Microprocessors;Visualization,biomedical optical imaging;cancer;cellular biophysics;fluorescence;image classification;medical image processing;neural nets,CNN-based classification system;HEp-2 cell image classification;ICPR 2014;autoimmune disease diagnosis;benchmark datasets;cell image classification competition;cell image masks;data augmentation;deep convolutional neural networks;human epithelial-2 cell image classification task;impact network design;impact network training;rotation-based data augmentation;visual recognition,,,,,,,20160208.0,March 2017,,IEEE,IEEE Journals & Magazines
166,Subject-specific detection of ventricular tachycardia using convolutional neural networks,B. S. Chandra; C. S. Sastry; S. Jana,"Indian Institute of Technology, Hyderabad, Telangana, India",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,53,56,"Onset of ventricular tachycardia (VT) is clinically significant, including as a trigger to defibrillator implants. In this paper, we propose a reliable technique to detect such onset using convolutional neural networks (CNNs). The proposed CNN adds convolution and pooling layers below the input layer and above the hidden and output layers of usual neural network (NN). Such layers would learn suitable linear features from training data, while eliminating the need to extract the traditionally used adhoc features. Employing such subject-specific features, we reported the performance of the proposed classifier using Creighton University ventricular tachyarrhythmia database (CUVT). In particular, we achieved mean (± standard deviation) performance of 95.6 (± 00.6) using subject-specific evaluation scheme over 100 random independent iterations.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868677,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868677,,Convolution;Databases;Electrocardiography;Feature extraction;Neural networks;Standards;Training,electrocardiography;medical disorders;medical signal processing;neural nets;pattern classification;prosthetics,CUVT;Creighton University ventricular tachyarrhythmia database;classifier;convolutional neural network;defibrillator implant;ventricular tachycardia detection,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
167,Classification of heart sound recordings using convolution neural network,H. Ryu; J. Park; H. Shin,"Industrial and Systems Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,1153,1156,"Aims: This study proposes a cardiac diagnostic model using convolution neural network (CNN). This model can predict whether a heart sound recording is normal or not by classifying phonocardiograms (PCGs) from both clinical and non-clinical environments - in accordance with the “2016 Physionet/CinCChallenge”. Methods: Heart sound recordings in the training data set are filtered by using Windowed-sinc Hamming filter algorithm to remove signals regarded as noise. The filtered recordings are then scaled and segmented. Using the filtered and segmented recordings, CNN is trained to extract features and construct a classification function. The CNN is trained by back propagation algorithm with stochastic gradient descent and mini-batch learning. To classify one sound recording, the signal should be filtered and segmented. Each segment of the signal is then classified by the trained CNN model. The model assigns each segment signal a relative probability between normal and abnormal labels. By accumulating these relative probability values for all the segmented signals, one can reliably and robustly determine whether the target signal is normal or abnormal. Results: The proposed model achieved an overall score of 79.5 with a sensitivity of 70.8 and a specificity of 88.2.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868952,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868952,,Classification algorithms;Convolution;Feature extraction;Filtering;Filtering algorithms;Heart;Hidden Markov models,bioacoustics;biomedical ultrasonics;cardiology;feature extraction;filters;medical signal processing;neural nets;signal classification;signal denoising,Windowed-sinc Hamming filter algorithm;back propagation algorithm;cardiac diagnostic model;convolution neural network;feature extraction;heart sound recording classification;phonocardiogram classification;signal classification;signal removal;signal segmentation,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
168,Heart sound classification using deep structured features,M. Tschannen; T. Kramer; G. Marti; M. Heinzmann; T. Wiatowski,"Dept. IT & EE, ETH Zurich, Switzerland",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,565,568,"We present a novel machine learning-based method for heart sound classification which we submitted to the PhysioNet/CinC Challenge 2016. Our method relies on a robust feature representation - generated by a wavelet-based deep convolutional neural network (CNN) - of each cardiac cycle in the test recording, and support vector machine classification. In addition to the CNN-based features, our method incorporates physiological and spectral features to summarize the characteristics of the entire test recording. The proposed method obtained a score, sensitivity, and specificity of 0.812, 0.848, and 0.776, respectively, on the hidden challenge testing set.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868805,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868805,,Convolution;Feature extraction;Heart;Robustness;Sensitivity;Support vector machines;Wavelet transforms,bioacoustics;biomedical ultrasonics;cardiology;learning (artificial intelligence);medical signal processing;neural nets;signal classification;support vector machines,cardiac cycle;deep structured feature;feature representation;heart sound classification;machine learning method;support vector machine classification;wavelet-based deep convolutional neural network,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
169,Ensemble of feature-based and deep learning-based classifiers for detection of abnormal heart sounds,C. Potes; S. Parvaneh; A. Rahman; B. Conroy,"Philips Research North America, Acute Care Solutions, Cambridge, MA, USA",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,621,624,"The goal of the 2016 PhysioNet/CinC Challenge is the development of an algorithm to classify normal/abnormal heart sounds. A total of 124 time-frequency features were extracted from the phonocardiogram (PCG) and input to a variant of the AdaBoost classifier. A second classifier using convolutional neural network (CNN) was trained using PCGs cardiac cycles decomposed into four frequency bands. The final decision rule to classify normal/abnormal heart sounds was based on an ensemble of classifiers combining the outputs of AdaBoost and the CNN. The algorithm was trained on a training dataset (normal= 2575, abnormal= 665) and evaluated on a blind test dataset. Our classifier ensemble approach obtained the highest score of the competition with a sensitivity, specificity, and overall score of 0.9424, 0.7781, and 0.8602, respectively.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868819,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868819,,Databases;Heart beat;Mel frequency cepstral coefficient;Phonocardiography;Sensitivity;Training,feature extraction;learning (artificial intelligence);medical signal processing;neural nets;pattern classification;phonocardiography;signal classification;time-frequency analysis,AdaBoost classifier;PCG cardiac cycle;abnormal heart sound detection;convolutional neural network;deep learning-based classifier;feature extraction;feature-based classifier;normal-abnormal heart sound classification;phonocardiogram;time-frequency analysis,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
170,Adaptive Estimation of Active Contour Parameters Using Convolutional Neural Networks and Texture Analysis,A. Hoogi; A. Subramaniam; R. Veerapaneni; D. L. Rubin,"Departments of Biomedical Data Science, Radiology, and Medicine (Biomedical Informatics Research), Stanford University, Stanford, CA, USA",IEEE Transactions on Medical Imaging,20170301.0,2017,36,3.0,781,791,"In this paper, we propose a generalization of the level set segmentation approach by supplying a novel method for adaptive estimation of active contour parameters. The presented segmentation method is fully automatic once the lesion has been detected. First, the location of the level set contour relative to the lesion is estimated using a convolutional neural network (CNN). The CNN has two convolutional layers for feature extraction, which lead into dense layers for classification. Second, the output CNN probabilities are then used to adaptively calculate the parameters of the active contour functional during the segmentation process. Finally, the adaptive window size surrounding each contour point is re-estimated by an iterative process that considers lesion size and spatial texture. We demonstrate the capabilities of our method on a dataset of 164 MRI and 112 CT images of liver lesions that includes low contrast and heterogeneous lesions as well as noisy images. To illustrate the strength of our method, we evaluated it against state of the art CNN-based and active contour techniques. For all cases, our method, as assessed by Dice similarity coefficients, performed significantly better than currently available methods. An average Dice improvement of 0.27 was found across the entire dataset over all comparisons. We also analyzed two challenging subsets of lesions and obtained a significant Dice improvement of 0.24 with our method (p <;0.001, Wilcoxon).",0278-0062;02780062,,10.1109/TMI.2016.2628084,"10.13039/100000054 - National Cancer Institute, National Institutes of Health; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742396,Active contours;adaptive parameters;convolutional neural network;image segmentation,Active contours;Adaptation models;Adaptive estimation;Image segmentation;Lesions;Level set;Neural networks,biomedical MRI;computerised tomography;feature extraction;image classification;image segmentation;image texture;iterative methods;liver;medical image processing;neural nets;probability,CNN-based techniques;CT images;Dice similarity coefficients;MRI images;active contour functional;active contour parameters;active contour techniques;adaptive estimation;adaptive window size;average Dice improvement;contour point;convolutional neural networks;feature extraction;heterogeneous lesions;image classification;iterative process;lesion size;level set contour location;level set segmentation;liver lesions;low contrast lesions;noisy images;output CNN probability;segmentation method;spatial texture;texture analysis,,,,,,,20161111.0,March 2017,,IEEE,IEEE Journals & Magazines
171,Prognostic Analysis of Polypoidal Choroidal Vasculopathy Using an Image-Based Approach,Y. M. Chen; W. Y. Lin; C. L. Tsai,"Dept. of Comput. Sci. & Inf. Eng., Nat. Chung Cheng Univ., Chiayi, Taiwan",2016 International Computer Symposium (ICS),20170220.0,2016,,,406,409,"In this paper, we firstly propose to perform prognostic analysis of polypoidal choroidal vasculopathy (PCV) using indocyanine green angiography (ICGA) sequence. Our goal is to develop a computer-aided diagnostic system which can predict the likely treatment outcome of patients with PCV based on their before-treatment ICGA sequences. In order to create a prognostic model for PCV, we utilize both the before-treatment and the aftertreatment ICGA sequences collected in the EVEREST study. By comparing the before-treatment and the after-treatment PCV region in ICGA sequences, we can generate positive and negative samples for training our prognostic model. Here, we design an 8-layer convolution neural network (CNN) and use it to serve as the prognostic model. We have conducted experiments using 17 patients cases. In particular, we perform leave-one-out cross validation so that each patient can be utilized as testing case once. Our proposed method achieves promising results on the EVEREST dataset.",,Electronic:978-1-5090-3438-3; POD:978-1-5090-3439-0,10.1109/ICS.2016.0088,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858510,indocyanine green angiography;polypoidal choroidal vasculopathy;prognostic analysis,Computers;Sensitivity,biomedical MRI;medical image processing;neural nets,CNN;ICGA sequences;PCV;after-treatment PCV region;before-treatment PCV region;computer-aided diagnostic system;convolution neural network;image-based approach;indocyanine green angiography sequence;leave-one-out cross validation;polypoidal choroidal vasculopathy;prognostic analysis,,,,,,,,15-17 Dec. 2016,,IEEE,IEEE Conference Publications
172,Brain tumor image segmentation based on convolution neural network,R. Lang; L. Zhao; K. Jia,"Beijing Laboratory of Advanced Information Networks & College of Information and Communication, Beijing University of Technology, China","2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",20170216.0,2016,,,1402,1406,"Automatic segmentation and early diagnosis of brain tumor is a challenging problem in computer vision and it can provide possibility for pre-operative planning, and solve the problem such as low accurateness and time-consuming in traditional manual segmentation. Under the mentioned problems above, this paper put forward a new method: Based on traditional convolutional neural networks (CNNs), a new architecture model is proposed for automatic brain tumor segmentation, which combines multi-modality images. The newly designed CNNs model automatically learns useful features from multi-modality images to combine multi-modality information. Experiment results show that the proposed model is more accurate than traditional methods and can provide reliable information for clinic treatments.",,Electronic:978-1-5090-3710-0; POD:978-1-5090-3711-7; USB:978-1-5090-3709-4,10.1109/CISP-BMEI.2016.7852936,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852936,CNNs;brain tumor segmentation;multi-modality tumor image,Cancer;Computer architecture;Convolution;Feature extraction;Image segmentation;Tumors;Two dimensional displays,brain;computer vision;feature extraction;feedforward neural nets;image segmentation;medical image processing;tumours,CNN;automatic brain tumor image segmentation;brain tumor early diagnosis;clinic treatments;computer vision;convolution neural network;multimodality images;multimodality information;preoperative planning,,,,,,,,15-17 Oct. 2016,,IEEE,IEEE Conference Publications
173,Mitosis detection using convolutional neural network based features,A. Albayrak; G. Bilgin,"Department of Computer Engineering, Signal and Image Processing Lab. (SIMPLAB), Yildiz Technical University, 34220 Istanbul, Turkey",2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI),20170209.0,2016,,,335,340,"Breast cancer is the second leading cause of cancer death in women according to World Health Organization (WHO). Development of computer aided diagnostic (CAD) systems has great importance as a secondary reader systems for a correct diagnosis and treatment process. In this paper, a deep learning based feature extraction method by convolutional neural network (CNN) is proposed for automated mitosis detection for cancer diagnosis and grading by histopathological images. The proposed framework is tested on the MITOS data set provided for a contest on mitosis detection in breast cancer histological images released for research purposes in International Conference on Pattern Recognition (ICPR'2014). By using provided histopathological images, cellular structures are initially found by combined clustering based segmentation and blob analysis after preprocessing step. Then, obtained cellular image patches are cropped automatically from the histopathological images for feature extraction stage. CNN, which is a prominent deep learning method on image processing tasks, is utilized for extracting discriminative features. Due to the high dimensional output of the CNN, combination of PCA and LDA dimension reduction methods are performed respectively for regularization and dimension reduction process. Afterwards, a robust kernel based classifier, support vector machine (SVM), is used for final classification of mitotic and non-mitotic cells. The test results on MITOS data set prove that the proposed framework achieved promising results for mitosis detection on histopathological images.",,Electronic:978-1-5090-3909-8; POD:978-1-5090-3910-4; USB:978-1-5090-3908-1,10.1109/CINTI.2016.7846429,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846429,,Breast cancer;Clustering algorithms;Feature extraction;Image segmentation;Kernel;Neural networks,cancer;feature extraction;image segmentation;medical image processing;neural nets;pattern clustering;principal component analysis;support vector machines,CAD systems;CNN;International Conference on Pattern Recognition;LDA dimension reduction methods;MITOS data set;PCA;SVM;WHO;World Health Organization;automated mitosis detection;blob analysis;breast cancer histological images;cancer death;cancer diagnosis;cellular image patches;cellular structures;clustering based segmentation;computer aided diagnostic systems;convolutional neural network;deep learning based feature extraction;deep learning method;dimension reduction process;histopathological images;image processing tasks;nonmitotic cells;regularization;robust kernel based classifier;secondary reader systems;support vector machine,,,,,,,,17-19 Nov. 2016,,IEEE,IEEE Conference Publications
174,Automatic segmentation of the left atrium from MR images via semantic information,Chunhua Deng; Xiaolong Zhang,"College of Computer Science and Technology Wuhan University of Science and Technology, 430065, China","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",20170209.0,2016,,,3312,3316,"Magnetic resonance imaging (MRI) can aid in assessing post-ablation scar formation. Automatic segmentation of left atrium (LA) offers great benefits for an accurate statistical assessment of LA region. However, how to robustly segment LA is still remaining as a challenging task for its high anatomical variability. In this paper, a robust segmentation method that exploits semantic information from different parts is proposed. The semantic correlation is exploited by the K Nearest Neighbor (KNN) search from corpus images with Convolutional Neural Network (CNN) features, which can be regarded as our main contribution. We propose a graph model to fuse semantic cues and eliminate accidental factors. Meanwhile, to optimize segmentation results, a super pixel voting method is also proposed. Experiments on public datasets of MRI image demonstrate the validity and accuracy of our semantic segmentation.",,Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2,10.1109/SMC.2016.7844745,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844745,Convolutional Neural Network (CNN);K Nearest Neighbor (KNN);Magnetic resonance images;Semantic segmentation;left atrium,Conferences;Cybernetics;Heart;Image segmentation;Magnetic resonance imaging;Semantics;Three-dimensional displays,biomedical MRI;graph theory;image segmentation;medical image processing;neural nets;search problems;statistical analysis,CNN;K nearest neighbor search;KNN search;MRI;automatic segmentation;convolutional neural network;corpus images;graph model;high anatomical variability;left atrium;magnetic resonance imaging;semantic information;statistical assessment;super pixel voting method,,,,,,,,9-12 Oct. 2016,,IEEE,IEEE Conference Publications
175,DemNet: A Convolutional Neural Network for the detection of Alzheimer's Disease and Mild Cognitive Impairment,C. D. Billones; O. J. L. D. Demetria; D. E. D. Hostallero; P. C. Naval,"Computer Vision & Machine Intelligence Group, Department of Computer Science, College of Engineering, University of the Philippines-Diliman, Philippines",2016 IEEE Region 10 Conference (TENCON),20170209.0,2016,,,3724,3727,"The early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the diagnosis of AD and MCI using structural Magnetic Resonance Imaging (MRI) scans. In this paper, we propose the use of a Convolutional Neural Network (CNN) in the detection of AD and MCI. In particular, we modified the 16-layered VGGNet for the 3-way classification of AD, MCI and Healthy Controls (HC) on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset achieving an overall accuracy of 91.85% and outperforming several classifiers from other studies.",,Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1,10.1109/TENCON.2016.7848755,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848755,,Alzheimer's disease;Biological neural networks;Feature extraction;Kernel;Magnetic resonance imaging;Neurons,biomedical MRI;convolution;data analysis;medical image processing;neural nets,16-layered VGGNet;3-way classification;AD;ADNI dataset;Alzheimer disease neuroimaging initiative dataset;CNN;DemNet;HC;MCI;MRI scans;convolutional neural network;healthy controls;magnetic resonance imaging scans;mild cognitive impairment detection;prodromal form;structural magnetic resonance imaging,,,,,,,,22-25 Nov. 2016,,IEEE,IEEE Conference Publications
176,DeepCut: Object Segmentation From Bounding Box Annotations Using Convolutional Neural Networks,M. Rajchl; M. C. H. Lee; O. Oktay; K. Kamnitsas; J. Passerat-Palmbach; W. Bai; M. Damodaram; M. A. Rutherford; J. V. Hajnal; B. Kainz; D. Rueckert,"Department of Computing, Imperial College London, SW7 2AZ, London, U.K",IEEE Transactions on Medical Imaging,20170201.0,2017,36,2.0,674,683,"In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled weak annotations, in our case bounding boxes. It extends the approach of the well-known GrabCut[1] method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a naïve approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy.",0278-0062;02780062,,10.1109/TMI.2016.2621185,Developing Human Connectome Project; iFIND project; 10.13039/501100000266 - Wellcome Trust and EPSRC IEH; 10.13039/501100000272 - National Institute for Health Research (NIHR) Biomedical Research Centre based at Guy¿s and St Thomas¿ NHS Foundation Trust and King¿s College London; 10.13039/501100000781 - ERC; 10.13039/501100000781 - Synergy Grant by the European Research Council (ERC); 10.13039/501100004963 - European Union¿s Seventh Framework Programme; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7739993,Bounding box;DeepCut;convolutional neural networks;image segmentation;machine learning;weak annotations,Biological neural networks;Computational modeling;Image segmentation;Imaging;Object segmentation;Optimization;Training,biomedical MRI;brain;image segmentation;learning (artificial intelligence);lung;medical image processing;neural nets,DeepCut;GrabCut;bounding box annotations;brain segmentation;convolutional neural networks;densely connected conditional random field;fetal magnetic resonance dataset;lung segmentation;machine learning;pixelwise object segmentation,,,,,,,20161109.0,Feb. 2017,,IEEE,IEEE Journals & Magazines
177,Convolutional Neural Network for Retinal Blood Vessel Segmentation,Z. Yao; Z. Zhang; L. Q. Xu,"Beijing Adv. innovation center for future internet Technol., Beijing Univ. of Technol., Beijing, China",2016 9th International Symposium on Computational Intelligence and Design (ISCID),20170126.0,2016,1,,406,409,"This paper proposes a CNN (Convolutional neural network) based blood vessel segmentation algorithm. Each pixel with its neighbors of the fundus image is checked by the CNN. The preliminary segmentation results of fundus images were refined by a two stages binarization and a morphological operation successively. The algorithm was tested on DRIVE dataset. While the specificity is 0.9603, sensitivity is 0.7731, which is very close to that of manual annotation. The sensitivity is 2% better than the ones found in current studies. The CNN based algorithm improves the segmentation of blood vessels performance significantly.",,Electronic:978-1-5090-3558-8; POD:978-1-5090-3559-5,10.1109/ISCID.2016.1100,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830374,CNN;Two Stages Binarization;fundus image;vessel segmentation,Computational intelligence;Decision support systems;Handheld computers;Image segmentation;Sensitivity,blood vessels;image segmentation;medical image processing;neural nets,CNN;DRIVE dataset;blood vessel performance segmentation improvement;convolutional neural network;fundus image;morphological operation;retinal blood vessel segmentation algorithm,,,,,,,,10-11 Dec. 2016,,IEEE,IEEE Conference Publications
178,Cancer Cells Detection in Phase-Contrast Microscopy Images Based on Faster R-CNN,J. Zhang; H. Hu; S. Chen; Y. Huang; Q. Guan,"Coll. of Comput. Sci. & Technol., Zhejiang Univ. of Technol., Hangzhou, China",2016 9th International Symposium on Computational Intelligence and Design (ISCID),20170126.0,2016,1,,363,367,"In biology and medicine research, detection and identification of cancer cells plays an essential role to further analysis of cell properties and developing new drugs experiments. However, owing to the adhesion among cells and great changes in morphology, it is a very challenging task to detect and locate the cells accurately, especially for the cells adhesion area. In this work, a deep detector for cells based on the framework of Faster R-CNN is proposed, and based on this, a Circle Scanning Algorithm (CSA) is presented for the redetection of adhesion cells. And then a series of experiments are achieved. The results show that the proposed deep detector can detect and identify all separate individual cells in an image, and that the hybrid method by combining Faster R-CNN with the proposed CSA can effectively detect and identify the adhesion cells under the conditions of the limited samples of adhesion cells.",,Electronic:978-1-5090-3558-8; POD:978-1-5090-3559-5,10.1109/ISCID.2016.1090,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830364,CSA;Cell detector;Faster R-CNN;RPN,Adhesives;Cancer;Cells (biology);Detectors;Morphology;Proposals;Training,cancer;cellular biophysics;feedforward neural nets;image classification;medical image processing,CSA;Faster-R-CNN;adhesion cell redetection;cancer cell detection;cancer cell identification;circle scanning algorithm;deep detector;hybrid method;phase-contrast microscopy images,,,,,,,,10-11 Dec. 2016,,IEEE,IEEE Conference Publications
179,CNN transfer learning for the automated diagnosis of celiac disease,G. Wimmer; A. Vécsei; A. Uhl,"University of Salzburg, Department of Computer Sciences, Salzburg, Austria","2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)",20170119.0,2016,,,1,6,"In this work, four well known convolutional neural networks (CNNs) that were pretrained on the ImageNet database are applied for the computer assisted diagnosis of celiac disease based on endoscopic images of the duodenum. The images are classified using three different transfer learning strategies and a experimental setup specifically adapted for the classification of endoscopic imagery. The CNNs are either used as fixed feature extractors without any fine-tuning to our endoscopic celiac disease image database or they are fine-tuned by training either all layers of the CNN or by fine-tuning only the fully connected layers. Classification is performed by the CNN SoftMax classifier as well as linear support vector machines. The CNN results are compared with the results of four state-of-the-art image representations. We will show that fine-tuning all the layers of the nets achieves the best results and outperforms the comparison approaches.",,Electronic:978-1-4673-8910-5; POD:978-1-4673-8911-2,10.1109/IPTA.2016.7821020,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821020,Automated diagnois;Celiac Disease;Convolutional neural networks;Endoscopy;Transfer learning,Databases;Diseases;Endoscopes;Feature extraction;Image representation;Support vector machines;Training,biomedical optical imaging;diseases;endoscopes;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets;support vector machines,CNN SoftMax classifier;CNN transfer learning;ImageNet database;automated celiac disease diagnosis;computer assisted diagnosis;convolutional neural networks;duodenum;endoscopic celiac disease image database;endoscopic imagery;fine-tuning;fixed feature extractors;fully connected layers;image classification;image representation;linear support vector machines,,,,,,,,12-15 Dec. 2016,,IEEE,IEEE Conference Publications
180,Emotion recognition from multi-channel EEG data through Convolutional Recurrent Neural Network,Xiang Li; Dawei Song; Peng Zhang; Guangliang Yu; Yuexian Hou; Bin Hu,"Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,352,359,"Automatic emotion recognition based on multi-channel neurophysiological signals, as a challenging pattern recognition task, is becoming an important computer-aided method for emotional disorder diagnoses in neurology and psychiatry. Traditional approaches require designing and extracting a range of features from single or multiple channel signals based on extensive domain knowledge. This may be an obstacle for non-domain experts. Moreover, traditional feature fusion method can not fully utilize correlation information between different channels. In this paper, we propose a preprocessing method that encapsulates the multi-channel neurophysiological signals into grid-like frames through wavelet and scalogram transform. We further design a hybrid deep learning model that combines the `Convolutional Neural Network (CNN)' and `Recurrent Neural Network (RNN)', for extracting task-related features, mining inter-channel correlation and incorporating contextual information from those frames. Experiments are carried out, in a trial-level emotion recognition task, on the DEAP benchmarking dataset. Our results demonstrate the effectiveness of the proposed methods, with respect to the emotional dimensions of Valence and Arousal.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822545,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822545,CNN;EEG;LSTM;emotion recognition;physiological signal,Continuous wavelet transforms;Correlation;Electroencephalography;Emotion recognition;Feature extraction,data mining;electroencephalography;emotion recognition;feature extraction;medical disorders;medical signal processing;neurophysiology;wavelet transforms,CNN;DEAP benchmarking dataset;RNN;automatic emotion recognition;computer-aided method;contextual information;convolutional neural network;convolutional recurrent neural network;emotional dimensions;emotional disorder diagnoses;extensive domain knowledge;grid-like frames;hybrid deep learning model;interchannel correlation mining;multichannel EEG data;multichannel neurophysiological signals;multiple channel signals;neurology;nondomain experts;pattern recognition task;psychiatry;recurrent neural network;scalogram transform;single channel signals;task-related feature extraction;trial-level emotion recognition task;valence-and-arousal;wavelet transform,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
181,ML-CNN: A novel deep learning based disease named entity recognition architecture,Zhehuan Zhao; Zhihao Yang; Ling Luo; Yin Zhang; Lei Wang; Hongfei Lin; Jian Wang,"College of Computer Science and Technology, Dalian University of Technology, China, 116023",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,794,794,"In this paper, we present a deep learning based disease named entity recognition architecture. First, the word-level embedding, character-level embedding and lexicon feature embedding are concatenated as input. Then multiple convolutional layers are stacked over the input to extract useful features automatically. Finally, multiple label strategy, which is firstly introduced, is applied to the output layer to capture the correlation information between neighboring labels. Experimental results on both NCBI and CDR corpora show that ML-CNN can achieve the state-of-the-art performance.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822625,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822625,convolutional neural network;deep learning;disease;multiple label strategy;named entity recognition,Computational linguistics;Computer architecture;Context;Correlation;Diseases;Feature extraction;Machine learning,diseases;feature extraction;learning (artificial intelligence);medical computing;neural nets,CDR corpora;ML-CNN;NCBI corpora;character-level embedding;deep learning method;disease named entity recognition architecture;feature extraction;lexicon feature embedding;multiple label convolutional neural network;word-level embedding,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
182,CNN-based image analysis for malaria diagnosis,Z. Liang; A. Powell; I. Ersoy; M. Poostchi; K. Silamut; K. Palaniappan; P. Guo; M. A. Hossain; A. Sameer; R. J. Maude; J. X. Huang; S. Jaeger; G. Thoma,"School of Information Technology, York University, Toronto, ON, M3J1P3, Canada",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,493,496,"Malaria is a major global health threat. The standard way of diagnosing malaria is by visually examining blood smears for parasite-infected red blood cells under the microscope by qualified technicians. This method is inefficient and the diagnosis depends on the experience and the knowledge of the person doing the examination. Automatic image recognition technologies based on machine learning have been applied to malaria blood smears for diagnosis before. However, the practical performance has not been sufficient so far. This study proposes a new and robust machine learning model based on a convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. In a ten-fold cross-validation based on 27,578 single cell images, the average accuracy of our new 16-layer CNN model is 97.37%. A transfer learning model only achieves 91.99% on the same images. The CNN model shows superiority over the transfer learning model in all performance indicators such as sensitivity (96.99% vs 89.00%), specificity (97.75% vs 94.98%), precision (97.73% vs 95.12%), F1 score (97.36% vs 90.24%), and Matthews correlation coefficient (94.75% vs 85.25%).",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822567,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822567,computer-aided diagnosis;convolutional neural network;deep learning;machine learning;malaria,Blood;Data models;Diseases;Machine learning;Mathematical model;Microscopy;Training,blood;cellular biophysics;diseases;image recognition;learning (artificial intelligence);medical image processing;neural nets,CNN-based image analysis;Matthews correlation coefficient;automatic image recognition technology;convolutional neural network;machine learning model;malaria diagnosis;parasite-infected red blood cell;single cell classification;single cell images,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
183,Cardiac left ventricular volumes prediction method based on atlas location and deep learning,G. Luo; S. Dong; K. Wang; H. Zhang,"School of Computer Science and Technology, Harbin Institute of Technology, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,1604,1610,"In this paper, we proposed a novel left ventricular volumes prediction method. This method is a cascade architecture which is based on multi-scale LV atlas location and deep convolutional neural networks (CNN). Firstly, we adopted LV atlas mapping method to achieve accurate location of LV region in cardiac magnetic resonance (CMR) images. And then, the CNN were used to train an end-to-end LV volumes prediction model to achieve the direct prediction. What's more, the large number of CMR images data (1140 subjects, more than 1026000 images) make the proposed deep CNN have relatively better feature representation and robust prediction ability. The experiment results on the large-scale CMR datasets prove that the proposed method has higher accuracy than the state-of-the-art prediction methods in terms of the end-diastole volumes (EDV), the end-systole volumes (ESV), and the ejection fraction (EF). Besides, we make the proposed method open accessible to public for wide application in other biomedical image processing fields.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822759,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822759,CMR images;LV atlas;deep convolutional neural networks;volumes prediction,Fitting;Image segmentation;Kernel;Manuals,biomedical MRI;cardiology;convolution;feature extraction;image representation;learning (artificial intelligence);medical image processing;neural nets,biomedical image processing fields;cardiac left ventricular volume prediction method;cardiac magnetic resonance images;deep convolutional neural networks;deep learning;end-diastole volumes;end-systole volumes;feature representation;multiscale LV atlas mapping method,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
184,Oriented tooth localization for periapical dental X-ray images via convolutional neural network,H. Eun; C. Kim,"School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST)",2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),20170119.0,2016,,,1,7,"In studies using dental X-ray images, tooth localization is essential to produce accurate results. In this paper, we propose a tooth localization method that tightly localize diverse teeth in periapical dental X-ray images. Oriented tooth proposals are generated by using teeth separation lines (TSLs) and a tooth top line, which are reliable and tight to teeth. To classify each tooth proposal into either a tooth or a non-tooth, we utilize a convolutional neural network (CNN). Our CNN model is trained with three classes, i.e., one negative and two positives, for better classification. In addition, we propose scale based non-maximum suppression by integrating scale confidence with non-maximum suppression to efficiently eliminate multiple tooth localizations.",,Electronic:978-9-8814-7682-1; POD:978-1-5090-2401-8,10.1109/APSIPA.2016.7820720,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820720,,Dentistry;Image segmentation;Neural networks;Proposals;Teeth;Training;X-ray imaging,dentistry;medical image processing;neural nets,CNN model;convolutional neural network;oriented tooth localization;periapical dental X-ray images;teeth separation lines;tooth localization method,,,,,,,,13-16 Dec. 2016,,IEEE,IEEE Conference Publications
185,A Novel Deep Model for Biopsy Image Grading,G. Zhang; Z. H. Liang; H. D. Lai; Y. Y. Lin; D. Lin; Z. P. Li,"Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China",2016 International Conference on Information System and Artificial Intelligence (ISAI),20170116.0,2016,,,323,326,"We propose in this paper a deep learning model based on convolutional neural network (CNN) for biopsy image grading. The model outputs a vector of scores indicating presence or severity of the target histopathological characteristics. Within the model, we first design a 7-layer CNN for feature representation and high level concept extraction. Each biopsy image is expressed as a feature vector through our CNN processor. We then place a sigmoid function into the output layer so as to generate a score for each target characteristic. The proposed model is evaluated on a benchmark dataset and a real biopsy image dataset to show its effectiveness.",,Electronic:978-1-5090-1585-6; POD:978-1-5090-1586-3,10.1109/ISAI.2016.0075,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816728,biopsy image grading;convolutional neural network;deep learning;histopathological image analysis;sigmoid function,Biological system modeling;Biopsy;Computational modeling;Feature extraction;Machine learning;Solid modeling;Training,feature extraction;learning (artificial intelligence);medical image processing;neural nets,CNN;biopsy image grading;convolutional neural network;deep learning model;feature representation;high level concept extraction;histopathological characteristics;sigmoid function,,,,,,,,24-26 June 2016,,IEEE,IEEE Conference Publications
186,Recognition of persisting emotional valence from EEG using convolutional neural networks,M. Yanagimoto; C. Sugimoto,"Graduate School of Engineering, Yokohama National University, Yokohama, Japan",2016 IEEE 9th International Workshop on Computational Intelligence and Applications (IWCIA),20170105.0,2016,,,27,32,"Recently there has been considerable interest in EEG-based emotion recognition (EEG-ER), which is one of the utilization of BCI. However, it is not easy to realize the EEG-ER system which can recognize emotions with high accuracy because of the tendency for important information in EEG signals to be concealed by noises. Deep learning is the golden tool to grasp the features concealed in EEG data and enable highly accurate EEG-ER because deep neural networks (DNNs) may have higher recognition capability than humans'. The publicly available dataset named DEAP, which is for emotion analysis using EEG, was used in the experiment. The CNN and a conventional model used for comparison are evaluated by the tests according to 11-fold cross validation scheme. EEG raw data obtained from 16 electrodes without general preprocesses were used as input data. The models classify and recognize EEG signals according to the emotional states ""positive"" or ""negative"" which were caused by watching music videos. The results show that the more training data are, the much higher the accuracies of CNNs are (by over 20%). It also suggests that the increased training data need not to belong to the same person's EEG data as the test data so as to get the CNN recognizing emotions accurately. The results indicate that there are not only the considerable amount of the interpersonal difference but also commonality of EEG properties.",,Electronic:978-1-5090-2775-0; POD:978-1-5090-2776-7,10.1109/IWCIA.2016.7805744,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805744,BCI;Convolutional neural networks;Deep learning;EEG;Emotion recognition;Interpersonal difference/commonality,Brain modeling;Convolution;Data models;Electroencephalography;Emotion recognition;Feature extraction;Videos,brain-computer interfaces;convolution;electroencephalography;emotion recognition;learning (artificial intelligence);medical signal processing;neural nets;signal classification,BCI;DEAP dataset;EEG signal classification;EEG-ER system;EEG-based emotion recognition;convolutional neural networks;deep learning;deep neural networks;emotion analysis;persisting emotional valence recognition,,,,,,,,5-5 Nov. 2016,,IEEE,IEEE Conference Publications
187,Deep convolutional encoder-decoder for myelin and axon segmentation,R. Mesbah; B. McCane; S. Mills,"Department of Computer Science, University of Otago, New Zealand",2016 International Conference on Image and Vision Computing New Zealand (IVCNZ),20170105.0,2016,,,1,6,"We propose a fully automatic method for segmenting myelin and axon from microscopy images of excised mouse spinal cord based on Convolutional Neural Networks (CNNs) and Deep Convolutional Encoder-Decoder. We compare a two-class CNN, multi-class CNN, and multi-class deep convolutional encoder-decoder with traditional methods. The CNN method gives a pixel-wise accuracy of 79.7% whereas an Active Contour method gives 59.4%. The encoder-decoder shows better performance with 82.3% and noticeably shorter classification time than CNN methods.",,Electronic:978-1-5090-2748-4; POD:978-1-5090-2749-1; USB:978-1-5090-2747-7,10.1109/IVCNZ.2016.7804455,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804455,,Axons;Computer architecture;Convolution;Decoding;Image segmentation;Microscopy;Training,feedforward neural nets;image classification;image segmentation;medical image processing,active contour method;axon segmentation;classification time;convolutional neural networks;excised mouse spinal cord;medical image processing;microscopy images;multiclass CNN;multiclass deep convolutional encoder-decoder;myelin segmentation;pixel-wise accuracy;two-class CNN,,,,,,,,21-22 Nov. 2016,,IEEE,IEEE Conference Publications
188,Diagnosis of Parkinson's disease from continuous speech using deep convolutional networks without manual selection of features,A. Frid; A. Kantor; D. Svechin; L. M. Manevitz,"Department of Computer Science and Neurocomputation Lab., University of Haifa, Haifa, Israel",2016 IEEE International Conference on the Science of Electrical Engineering (ICSEE),20170105.0,2016,,,1,4,"Parkinson's Disease (PD) is a relatively common neurodegenerative disabling disease. It affects central nervous system with profound effect on the motor system. The most common symptoms include slowness, rigidity and tremor during motion. It has been suggested that the vocal cords are among the first one to be affected and thus the speech is affected at very early stage of the disease and continues to deteriorate as the disease progress. In this work, we focus on automating the process of diagnosis from continuous native speech by removing the necessity of a trained personal from the diagnosis process. This is done by using an adaptation of Convolutional Neural Network (CNN) architecture for one-dimensional signal processing (i.e. raw speech signal) on a relatively small training set. This is a continuation to previous works where we showed (i) that this task can be achieved by using manually-extracted features of the speech (such as formants and their ratios) and (ii) by using an automatic process of auditory features extraction, where the features were selected by signal processing specialist.",,Electronic:978-1-5090-2152-9; POD:978-1-5090-2153-6,10.1109/ICSEE.2016.7806118,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806118,Convolutional Neural Networks (CNN);Machine Learning;Parkinson's Disease;Speech Analysis,Biological neural networks;Convolution;Diseases;Feature extraction;Neurons;Speech;Training,auditory evoked potentials;convolution;diseases;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;speech processing,Parkinson disease diagnosis;auditory feature extraction;central nervous system;continuous native speech;convolutional neural network architecture;deep convolutional networks;manually-extracted features;motor system;neurodegenerative disabling disease;one-dimensional signal processing;vocal cords,,,,,,,,16-18 Nov. 2016,,IEEE,IEEE Conference Publications
189,Mental Disease Feature Extraction with MRI by 3D Convolutional Neural Network with Multi-channel Input,L. Cao; Z. Liu; X. He; Y. Cao; K. Li,"Sch. of Inf. Sci. & Eng., Shandong Univ., Jinan, China",2016 IEEE International Conference on Smart Cloud (SmartCloud),20161226.0,2016,,,224,227,"Magnetic resonance imaging (MRI) plays an important role in early diagnosis, which can accurately capture the disease variations of the anatomical brain structure. We propose a novel method for improving feature extraction performance from magnetic resonance images (MRI). This study presents a combination of multi-channel input and 3D convolutional neural network architecture which can reduce the feature dimensionality. Multi-channel input scheme is devised to apply prior knowledge on MRI original inputs in order to overcame possibly uncertainty and unsteadiness on the final features. While, the 3D-CNN model can simultaneously extract features from spatial and temporal dimensions for purpose of capturing the variations of constructive information.",,Electronic:978-1-5090-5263-9; POD:978-1-5090-5264-6,10.1109/SmartCloud.2016.38,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796178,3D convolutional neural network;MRI;Multi-channel input;feature extraction,Convolution;Feature extraction;Kernel;Magnetic resonance imaging;Three-dimensional displays;Wavelet transforms,biomedical MRI;brain;convolution;diseases;feature extraction;medical image processing;neural nets;patient diagnosis;stereo image processing,3D convolutional neural network;MRI;anatomical brain structure;diagnosis;magnetic resonance imaging;mental disease feature extraction;multichannel input,,,,,,,,18-20 Nov. 2016,,IEEE,IEEE Conference Publications
190,Image Registration for Placenta Reconstruction,F. Gaisser; P. P. Jonker; T. Chiba,"Dept. of Biomech. Eng., Delft Univ. of Technol., Delft, Netherlands",2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20161219.0,2016,,,473,480,"In this paper we introduce a method to handle the challenges posed by image registration for placenta reconstruction from fetoscopic video as used in the treatment of Twinto-Twin Transfusion Syndrome (TTTS). Panorama reconstruction of the placenta greatly supports the surgeon in obtaining a complete view of the placenta to localize vascular anastomoses. The found shunts can subsequently be blocked by coagulation in the correct order. By using similarity learning in training a Convolutional Neural Network we created a novel feature extraction method, allowing robust matching of keypoints for image registration and therefore taking the most critical step in placenta reconstruction from fetoscopic video. The fetoscopic video we used for our experiments was acquired from a training simulator for TTTS surgery. We compared our method with state-of-the-art methods. The matching performance of our method is up to three times better while the mean projection error is reduced with 64% for the registered images. Our image registration method provides the ground work for a complete panorama reconstruction of the placenta.",,Electronic:978-1-5090-1437-8; POD:978-1-5090-1438-5,10.1109/CVPRW.2016.66,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789556,,Feature extraction;Image reconstruction;Image registration;Robustness;Surgery;Training;Transforms,feature extraction;image matching;image reconstruction;image registration;medical image processing;neural nets;obstetrics;video signal processing,CNN;TTTS surgery;convolutional neural network;feature extraction method;fetoscopic video;image registration;keypoint matching;mean projection error;placenta panorama reconstruction;similarity learning;twin-to-twin transfusion syndrome;vascular anastomoses,,,,,,,,June 26 2016-July 1 2016,,IEEE,IEEE Conference Publications
191,Neuron Segmentation Based on CNN with Semi-Supervised Regularization,K. Xu; H. Su; J. Zhu; J. S. Guan; B. Zhang,"Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China",2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20161219.0,2016,,,1324,1332,"Neuron segmentation in two-photon microscopy images is a critical step to investigate neural network activities in vivo. However, it still remains as a challenging problem due to the image qualities, which largely results from the non-linear imaging mechanism and 3D imaging diffusion. To address these issues, we proposed a novel framework by incorporating the convolutional neural network (CNN) with a semi-supervised regularization term, which reduces the human efforts in labeling without sacrificing the performance. Specifically, we generate a putative label for each unlabeled sample regularized with a graph-smooth term, which are used as if they were true labels. A CNN model is therefore trained in a supervised fashion with labeled and unlabeled data simultaneously, which is used to detect neuron regions in 2D images. Afterwards, neuron segmentation in a 3D volume is conducted by associating the corresponding neuron regions in each image. Experiments on real-world datasets demonstrate that our approach outperforms neuron segmentation based on the graph-based semisupervised learning, the supervised CNN and variants of the semi-supervised CNN.",,Electronic:978-1-5090-1437-8; POD:978-1-5090-1438-5,10.1109/CVPRW.2016.167,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789657,,Biological neural networks;Image segmentation;Microscopy;Neurons;Three-dimensional displays;Two dimensional displays,feedforward neural nets;graph theory;image segmentation;learning (artificial intelligence);medical image processing;neurophysiology;two-photon spectroscopy,3D imaging diffusion;3D volume;convolutional neural network;graph-based semisupervised learning;graph-smooth term;image qualities;neural network activities;neuron region detection;neuron segmentation;nonlinear imaging mechanism;semisupervised regularization;supervised CNN model;two-photon microscopy images,,,,,,,,June 26 2016-July 1 2016,,IEEE,IEEE Conference Publications
192,Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification,L. Hou; D. Samaras; T. M. Kurc; Y. Gao; J. E. Davis; J. H. Saltz,,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212.0,2016,,,2424,2433,"Convolutional Neural Networks (CNN) are state-of-theart models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.266,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780635,,Cancer;Image resolution;Neural networks;Predictive models;Robustness;Training;Visualization,cancer;image classification;learning (artificial intelligence);medical image processing;neural nets;optimisation,CNN training;EM based method;WSI;cancer subtype differentiation;cellular-level visual features;decision fusion model training;discriminative patch automatic location;expectation-maximization based method;glioma classification;image patch scale;nonsmall-cell lung carcinoma;patch spatial relationships;patch-based CNN;patch-based convolutional neural network;patch-level CNN;patch-level classifier;patch-level prediction aggregation;whole slide tissue image classification,,,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
193,Cellular neural network processing of CEUS examination. A pilot study,C. Botoca; M. Botoca,"Communications Department, Politehnica University of Timi&#x015F;oara, Romania",2016 12th IEEE International Symposium on Electronics and Telecommunications (ISETC),20161212.0,2016,,,309,312,"A new method for a differential diagnosis of focal liver lesions (FLL), using cellular neural networks (CNN), intended to develop a computer assisted diagnosis (CAD) procedure, is presented. Two types of lesions with a known typical behavior were selected for the CNN image processing. The database consisted of 20 cases of video recordings of contrast enhancement ultrasound (CEUS) examination. The images processed by the CNN algorithm were selected by ultrasound experts. The images resulted from the CNN processing were accurate and characteristic enough to make possible an instant differential diagnosis between hemangioma and hepatocellular carcinoma.",,Electronic:978-1-5090-3748-3; POD:978-1-5090-3749-0,10.1109/ISETC.2016.7781119,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7781119,cellular neural networks;computer assisted diagnosis hemangioma;contrast enhancement ultrasound;hepatocellular carcinoma;image processing,Biomedical imaging;Cellular neural networks;Computers;Image processing;Lesions;Portals;Ultrasonic imaging,biomedical ultrasonics;image enhancement;medical image processing;neural nets,CAD;CEUS examination;CNN image processing;FLL;cellular neural network processing;computer assisted diagnosis;contrast enhancement ultrasound examination;focal liver lesions;hemangioma carcinoma;hepatocellular carcinoma;video recordings,,,,,,,,27-28 Oct. 2016,,IEEE,IEEE Conference Publications
194,Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation,H. C. Shin; K. Roberts; L. Lu; D. Demner-Fushman; J. Yao; R. M. Summers,"Imaging Biomarkers & Comput. Aided Diagnosis Lab., Nat. Inst. of Health, Bethesda, MD, USA",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212.0,2016,,,2497,2506,"Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normalvs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.274,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780643,,Biomedical imaging;Context;Diseases;Radiology;Recurrent neural networks;Training;X-rays,X-rays;convolution;diagnostic radiography;diseases;image annotation;medical image processing;radiology;recurrent neural nets,CNNs;automated image annotation;chest X-rays;convolutional neural networks;deep learning model;image caption datasets;natural images;normalvs-diseased cases bias;radiology dataset;recurrent neural cascade model,,,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
195,Automating Carotid Intima-Media Thickness Video Interpretation with Convolutional Neural Networks,J. Y. Shin; N. Tajbakhsh; R. T. Hurst; C. B. Kendall; J. Liang,"Dept. of Biomed. Inf., Arizona State Univ., Scottsdale, AZ, USA",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212.0,2016,,,2526,2535,"Cardiovascular disease (CVD) is the leading cause of mortality yet largely preventable, but the key to prevention is to identify at-risk individuals before adverse events. For predicting individual CVD risk, carotid intima-media thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable, offering several advantages over CT coronary artery calcium score. However, each CIMT examination includes several ultrasound videos, and interpreting each of these CIMT videos involves three operations: (1) select three end-diastolic ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI) in each selected frame, and (3) trace the lumen-intima interface and the media-adventitia interface in each ROI to measure CIMT. These operations are tedious, laborious, and time consuming, a serious limitation that hinders the widespread utilization of CIMT in clinical practice. To overcome this limitation, this paper presents a new system to automate CIMT video interpretation. Our extensive experiments demonstrate that the suggested system performs reliably. The reliable performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.277,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780646,,Carotid arteries;Electrocardiography;Image restoration;Neural networks;Reliability;Training;Ultrasonic imaging,biomedical ultrasonics;cardiovascular system;diseases;image representation;medical image processing;neural nets;video signal processing,CIMT examination;CIMT video interpretation;CNN outputs;CVD risk;ROI;at-risk individuals;cardiovascular disease;carotid intima-media thickness video interpretation automation;clinical practice;convolutional neural networks;end-diastolic ultrasound frames;informative image representation;lumen-intima interface;media-adventitia interface;noninvasive ultrasound method;ultrasound videos,,,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
196,Automatic histopathology image analysis with CNNs,L. Hou; K. Singh; D. Samaras; T. M. Kurc; Y. Gao; R. J. Seidman; J. H. Saltz,"Dept of Computer Science, Stony Brook University",2016 New York Scientific Data Summit (NYSDS),20161121.0,2016,,,1,6,"We define Pathomics as the process of high throughput generation, interrogation, and mining of quantitative features from high-resolution histopathology tissue images. Analysis and mining of large volumes of imaging features has great potential to enhance our understanding of tumors. The basic Pathomics workflow consists of several steps: segmentation of tissue images to delineate the boundaries of nuclei, cells, and other structures; computation of size, shape, intensity, and texture features for each segmented object; classification of images and patients based on imaging features; and correlation of classification results with genomic signatures and clinical outcome. Executing a Pathomics workflow on a dataset of thousands of very high resolution (gigapixels) and heterogeneous histopathology images is a computationally challenging problem. In this paper, we use Convolutional Neural Networks (CNN) for automatic recognition of nuclear morphological attributes in histopathology images of glioma, the most common malignant brain tumor. We constructed a comprehensive multi-label dataset of glioma nuclei and applied two CNN based methods on this dataset. Both methods perform well recognizing some but not all morphological attributes and are complementary with each other.",,Electronic:978-1-4673-9051-4; POD:978-1-4673-9052-1,10.1109/NYSDS.2016.7747812,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747812,Convolutional Neural Network;Nucleus Classification;Pathomics,Feature extraction;Image recognition;Image reconstruction;Image segmentation;Neurons;Support vector machines;Training,biological tissues;brain;cellular biophysics;data mining;genomics;image classification;image resolution;image segmentation;image texture;medical image processing;neural nets;tumours,CNN;Pathomics workflow;automatic histopathology image analysis;automatic nuclear morphological attribute recognition;cell boundary delineation;convolutional neural networks;genomic signatures;glioma nuclei;heterogeneous histopathology images;high-resolution histopathology tissue images;image classification;imaging feature analysis;imaging feature mining;malignant brain tumor;nuclei boundary delineation;texture features;tissue image segmentation,,,,,,,,14-17 Aug. 2016,,IEEE,IEEE Conference Publications
197,Customizing CNNs for blood vessel segmentation from fundus images,S. K. Vengalil; N. Sinha; S. S. S. Kruthiventi; R. V. Babu,"International Institute of Information Technology, Bangalore, India",2016 International Conference on Signal Processing and Communications (SPCOM),20161117.0,2016,,,1,4,"For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 × 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.",,Electronic:978-1-5090-1746-1; POD:978-1-5090-1747-8,10.1109/SPCOM.2016.7746702,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746702,,Biomedical imaging;Blood vessels;Image resolution;Image segmentation;Pathology;Testing;Training,blood vessels;convolution;diseases;image colour analysis;image resolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets,CNN;DRIVE database;HRF databases;ROC curve;automatic screening;blood vessel network;blood vessel segmentation;convolutional neural network;deep learning framework;eye diseases;fundal images;fundus images;gray scale image;image resolution;semantic image segmentation,,,,,,,,12-15 June 2016,,IEEE,IEEE Conference Publications
198,Deep convolutional neural networks for classification of mild cognitive impaired and Alzheimer's disease patients from scalp EEG recordings,F. C. Morabito; M. Campolo; C. Ieracitano; J. M. Ebadi; L. Bonanno; A. Bramanti; S. Desalvo; N. Mammone; P. Bramanti,"DICEAM Department, Mediterranean University of Reggio Calabria, Via Graziella Feo di Vito, 89060 Reggio Calabria, Italy",2016 IEEE 2nd International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI),20161114.0,2016,,,1,6,"In spite of the worldwide financial and research efforts made, the pathophysiological mechanism at the basis of Alzheimer's disease (AD) is still poorly understood. Previous studies using electroencephalography (EEG) have focused on the slowing of oscillatory brain rhythms, coupled with complexity reduction of the corresponding time-series and their enhanced compressibility. These analyses have been typically carried out on single channels. However, limited investigations have focused on the possibility yielded by computational intelligence methodologies and novel machine learning approaches applied to multichannel schemes. The study at screening level on EEG recordings of subjects at risk could be useful to highlight the emergence of underlying AD progression (or at least support any further clinical investigation). In this work, the representational power of Deep Learning on Convolutional Neural Networks (CNN) is exploited to generate suitable sets of features that are then able to classify EEG patterns of AD from a prodromal version of dementia (Mild Cognitive Impairment, MCI) and from age-matched Healthy Controls (HC). The processing system here used enforces a series of convolutional-subsampling layers in order to derive a multivariate assembly of latent, novel patterns, finally used to categorize sets of EEG from different classes of subjects. The final processor here proposed is able to reach an averaged 80% of correct classification with good performance on both sensitivity and specificity by using a Multilayered Feedforward Perceptron (MLP) of the standard type as a final block of the procedure.",,Electronic:978-1-5090-1131-5; POD:978-1-5090-1132-2,10.1109/RTSI.2016.7740576,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740576,Alzheimer's disease;Convolutional Neural Networks;Deep Learning;Mild Cognitive Impairment;Scalp EEG,Databases;Diseases;Electroencephalography;Feature extraction;Scalp;Standards;Time-frequency analysis,diseases;electroencephalography;learning (artificial intelligence);medical signal processing;neural nets,AD progression;Alzheimer's disease patients;EEG classification;computational intelligence methodologies;convolutional-subsampling layers;deep convolutional neural networks;electroencephalography;healthy controls;machine learning approaches;mild cognitive impairment;multilayered feedforward perceptron;oscillatory brain rhythms;pathophysiological mechanism;scalp EEG recordings,,,,,,,,7-9 Sept. 2016,,IEEE,IEEE Conference Publications
199,Improving convolutional neural network using accelerated proximal gradient method for epilepsy diagnosis,D. Li; G. Wang; T. Song; Q. Jin,"Department of Automation, Beijing University of Chemical Technology, Beijing, China",2016 UKACC 11th International Conference on Control (CONTROL),20161110.0,2016,,,1,6,"The task of epilepsy diagnosing in medicine by classification of electroencephao-graph (EEG) signals is considered. Since an EEG signal has a large number of dimensions as an input sample vector, many previous classification methods have been proposed as hybrid frameworks, which are structurally complex and computationally expensive. In this paper, convolutional neural network (CNN) is used to realize feature extraction and classification simultaneously. The scheme of CNN is adopted to overcome the curse of dimensionality. Meanwhile, the accelerated proximal gradient method is used to increase the training ratio. Experimental results show that the proposed method achieves ideal accuracy of epilepsy diagnosis and converges faster than CNNs based on traditional gradient back propagation.",,Electronic:978-1-4673-9891-6; POD:978-1-4673-9892-3,10.1109/CONTROL.2016.7737620,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737620,classification;convolutional neural network;epilepsy;feature extraction;proximal gradient,Acceleration;Biological neural networks;Classification algorithms;Convolution;Electroencephalography;Epilepsy;Feature extraction,electroencephalography;feature extraction;gradient methods;medical signal processing;neural nets;patient diagnosis;signal classification,CNN;EEG signal classification;accelerated proximal gradient method;convolutional neural network;curse of dimensionality;electroencephalograph signal;epilepsy diagnosis;feature extraction;input sample vector;training ratio,,,,,,,,Aug. 31 2016-Sept. 2 2016,,IEEE,IEEE Conference Publications
200,An Automated ECG Beat Classification System Using Convolutional Neural Networks,M. Zubair; J. Kim; C. Yoon,"Inf. & Commun. Network Dept., Korea Univ. of Sci. & Technol., Daejeon, South Korea",2016 6th International Conference on IT Convergence and Security (ICITCS),20161110.0,2016,,,1,5,"Classification of Electrocardiogram (ECG) plays an important role in clinical diagnosis of cardiac diseases. In this paper, we introduce an ECG beat classification system using convolutional neural networks (CNNs). The proposed model integrates two main parts, feature extraction and classification, of ECG pattern recognition system. This model automatically learns a suitable feature representation from raw ECG data and thus negates the need of hand-crafted features. By using a small and patient-specific training data, the proposed classification system efficiently classified ECG beats into five different classes recommended by Association for Advancement of Medical Instrumentation (AAMI). ECG signal from 44 recordings of the MIT-BIH database are used to evaluate the classification performance and the results demonstrate that the proposed approach achieves a significant classification accuracy and superior computational efficiency than most of the state-of-the-art methods for ECG signal classification.",,Electronic:978-1-5090-3765-0; POD:978-1-5090-3766-7,10.1109/ICITCS.2016.7740310,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740310,,Classification algorithms;Convolution;Databases;Electrocardiography;Feature extraction;Heart rate variability;Neural networks,convolution;diseases;electrocardiography;feature extraction;medical signal processing;neural nets;patient diagnosis;signal classification,CNN;ECG beat classification system;cardiac disease;clinical diagnosis;convolutional neural network;electrocardiogram signal;feature classification;feature extraction,,,,,,,,26-26 Sept. 2016,,IEEE,IEEE Conference Publications
201,Breast cancer histopathological image classification using Convolutional Neural Networks,F. A. Spanhol; L. S. Oliveira; C. Petitjean; L. Heutte,"Federal University of Technology - Parana, Toledo, Brazil",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,2560,2567,"The performance of most conventional classification systems relies on appropriate data representation and much of the efforts are dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful features. On the other hand, deep learning can extract and organize the discriminative information from the data, not requiring the design of feature extractors by a domain expert. Convolutional Neural Networks (CNNs) are a particular type of deep, feedforward network that have gained attention from research community and industry, achieving empirical successes in tasks such as speech recognition, signal processing, object recognition, natural language processing and transfer learning. In this paper, we conduct some preliminary experiments using the deep learning approach to classify breast cancer histopathological images from BreaKHis, a publicly dataset available at http://web.inf.ufpr.br/vri/breast-cancer-database. We propose a method based on the extraction of image patches for training the CNN and the combination of these patches for final classification. This method aims to allow using the high-resolution histopathological images from BreaKHis as input to existing CNN, avoiding adaptations of the model that can lead to a more complex and computationally costly architecture. The CNN performance is better when compared to previously reported results obtained by other machine learning models trained with hand-crafted textural descriptors. Finally, we also investigate the combination of different CNNs using simple fusion rules, achieving some improvement in recognition rates.",,,10.1109/IJCNN.2016.7727519,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727519,,Neural networks,cancer;image classification;medical image processing;neural nets,BreaKHis;CNN performance;breast cancer histopathological image classification;breast cancer histopathological images;convolutional neural networks;data representation;deep learning;discriminative information;domain expert;feature extractors;feedforward network;fusion rules;hand-crafted textural descriptors;high-resolution histopathological images;image patches;machine learning models;natural language processing;object recognition;recognition rates;research community;signal processing;speech recognition;time-consuming process;transfer learning,,1.0,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
202,Classification of mammogram images by using CNN classifier,K. Sharma; B. Preet,"Dept. of Electronics and Communication, Chandigarh University, Gharuan, Mohali, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",20161103.0,2016,,,2743,2749,"Classification of the breast tissues into the benign and malignant classes is a difficult assignment. The experimental results are takes 40 input images from DDSM dataset. We extract the GLCM, GLDM and Geometrical features from the mammogram images. In this paper we apply Convolution Neural Network as a classifier on the mammogram images to enhance the accuracy rate of CAD. Performance of the different classifiers is measured on receiver operating characteristic. In training stage, overall classification accuracy of 73%, with 71.5% sensitivity and 73.5% specificity for dense tissue is achieved by our proposed method along with it, accuracy of 79.23%, 73.25% sensitivity and 74.5% specificity is achieved for fatty tissue. Convolution neural network classifier is used to boost the classification performance. This classifier performs better than previous classifiers in that it shows more accuracy than the other classifiers, the misclassification rate of normal mammograms as abnormal. This approach performs good on overlapping problem. This method is different from all other approaches, which are used to identify normal mammograms by detecting cancers. Overlapped tissues are also detected by this using this classifier.",,,10.1109/ICACCI.2016.7732477,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732477,Computer-aided diagnosis (CADx);Convolution neural network (CNN);Logistic regression (LR);mammography;microcalcifications,Cancer;Digital filters;Image edge detection;Image segmentation;Mammography;Training;Wiener filters,CAD;biological tissues;geometry;image classification;mammography;medical image processing;neural nets,CAD;CNN classifier;DDSM dataset;GLCM;GLDM;breast tissues;convolution neural network;dense tissue;geometrical features;mammogram image classification,,,,,,,,21-24 Sept. 2016,,IEEE,IEEE Conference Publications
203,Using Convolutional Neural Network for edge detection in musculoskeletal ultrasound images,S. I. Jabbar; C. R. Day; N. Heinz; E. K. Chadwick,"Biomedical Engineering, Institute for Science and Technology in Medicine, Keele University, UK",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,4619,4626,"Fast and accurate segmentation of musculoskeletal ultrasound images is an on-going challenge. Two principal factors make this task difficult: firstly, the presence of speckle noise arising from the interference that accompanies all coherent imaging approaches; secondly, the sometimes subtle interaction between musculoskeletal components that leads to non-uniformity of the image intensity. Our work presents an investigation of the potential of Convolutional Neural Networks (CNNs) to address both of these problems. CNNs are an effective tool that has previously been used in image processing of several biomedical imaging modalities. However, there is little published material addressing the processing of musculoskeletal ultrasound images. In our work we explore the effectiveness of CNNs when trained to act as a pre-segmentation pixel classifier that determines whether a pixel is an edge or non-edge pixel. Our CNNs are trained using two different ground truth interpretations. The first one uses an automatic Canny edge detector to provide the ground truth image; the second ground truth was obtained using the same image marked-up by an expert anatomist. In this initial study the CNNs have been trained using half of the prepared data from one image, using the other half for testing; validation was also carried out using three unseen ultrasound images. CNN performance was assessed using Mathew's Correlation Coefficient, Sensitivity, Specificity and Accuracy. The results show that CNN performance when using expert ground truth image is better than using Canny ground truth image. Our technique is promising and has the potential to speed-up the image processing pipeline using appropriately trained CNNs.",,,10.1109/IJCNN.2016.7727805,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727805,Convolutional Neural Networks;Musculoskeletal model;Segmentation;Ultrasound image,Biological system modeling;Biomedical imaging;Image edge detection;Image segmentation;Kernel;Musculoskeletal system;Ultrasonic variables measurement,biomedical ultrasonics;bone;edge detection;image classification;image resolution;image segmentation;learning (artificial intelligence);medical image processing;muscle;neural nets;ultrasonic imaging,CNN performance;Mathew's correlation coefficient;automatic Canny edge detector;biomedical imaging modalities;convolutional neural network;edge pixel;expert ground truth image;ground truth images;ground truth interpretations;image intensity;image processing;image processing pipeline;musculoskeletal components;musculoskeletal ultrasound image segmentation;nonedge pixel;presegmentation pixel classifier;speckle noise,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
204,Feature leaning with deep Convolutional Neural Networks for screening patients with paroxysmal atrial fibrillation,B. Pourbabaee; M. J. Roshtkhari; K. Khorasani,"Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,5057,5064,"In this paper, a novel electrocardiogram (ECG) signal classification and patient screening method is developed. The focus is on identifying patients with paroxysmal atrial fibrillation (PAF) which is a life threatening cardiac arrhythmia. The proposed approach uses the raw ECG signal as the input and automatically learns the representative features for PAF to be used by a classification mechanism. The features are learned directly from the time domain ECG signals by using a Convolutional Neural Network (CNN) with one fully connected layer. The learned features can replace the hand-crafted features and our experimental results indicate the effectiveness of the learned features in patient screening. The experimental results indicate that combining the learned features with other classifiers will improve the performance of the patient screening system as compared to an End-to-End convolutional neural network classifier. The major characteristics of the proposed approach are to simplify the process of feature extraction for different cardiac arrhythmias and to remove the need for using a human expert to specify the appropriate features. The effectiveness of the proposed ECG classification method is demonstrated through performing extensive simulation studies.",,,10.1109/IJCNN.2016.7727866,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727866,,Biological neural networks;Convolution;Electrocardiography;Feature extraction;Hidden Markov models;Learning systems,convolution;diseases;electrocardiography;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;signal classification;signal representation;time-domain analysis,CNN;ECG signal classification;PAF;deep convolutional neural networks;electrocardiogram;feature extraction;feature leaning;life threatening cardiac arrhythmia;paroxysmal atrial fibrillation;patients screening;representative features;time domain ECG signals,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
205,Vessel extraction in X-ray angiograms using deep learning,E. Nasr-Esfahani; S. Samavi; N. Karimi; S. M. R. Soroushmehr; K. Ward; M. H. Jafari; B. Felfeliyan; B. Nallamothu; K. Najarian,"Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,643,646,"Coronary artery disease (CAD) is the most common type of heart disease which is the leading cause of death all over the world. X-ray angiography is currently the gold standard imaging technique for CAD diagnosis. These images usually suffer from low quality and presence of noise. Therefore, vessel enhancement and vessel segmentation play important roles in CAD diagnosis. In this paper a deep learning approach using convolutional neural networks (CNN) is proposed for detecting vessel regions in angiography images. Initially, an input angiogram is preprocessed to enhance its contrast. Afterward, the image is evaluated using patches of pixels and the network determines the vessel and background regions. A set of 1,040,000 patches is used in order to train the deep CNN. Experimental results on angiography images of a dataset show that our proposed method has a superior performance in extraction of vessel regions.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590784,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590784,Angiography;convolutional neural networks;deep learning;vessel segmentation,Angiography;Arteries;Feature extraction;Filter banks;Image segmentation;Training;Transforms,blood vessels;diagnostic radiography;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets;noise,CAD diagnosis;X-ray angiography;convolutional neural networks;coronary artery disease;death;deep learning;heart disease;noise;vessel enhancement;vessel extraction;vessel segmentation,,1.0,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
206,Thorax disease diagnosis using deep convolutional neural network,J. Chen; X. Qi; O. Tervonen; O. Silvén; G. Zhao; M. Pietikäinen,"University of Oulu, Finland",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,2287,2290,"Computer aided diagnosis (CAD) is an important issue, which can significantly improve the efficiency of doctors. In this paper, we propose a deep convolutional neural network (CNN) based method for thorax disease diagnosis. We firstly align the images by matching the interest points between the images, and then enlarge the dataset by using Gaussian scale space theory. After that we use the enlarged dataset to train a deep CNN model and apply the obtained model for the diagnosis of new test data. Our experimental results show our method achieves very promising results.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7591186,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591186,,Diseases;Filtering theory;Machine learning;Radiography;Solid modeling;Thorax;Training,Gaussian processes;diagnostic radiography;diseases;image matching;medical image processing;neural nets;radiology,CAD;Gaussian scale space theory;computer aided diagnosis;deep CNN model;deep convolutional neural network;image alignment;image matching;radiograph;radiology;thorax disease diagnosis,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
207,Melanoma detection by analysis of clinical images using convolutional neural network,E. Nasr-Esfahani; S. Samavi; N. Karimi; S. M. R. Soroushmehr; M. H. Jafari; K. Ward; K. Najarian,"Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,1373,1376,"Melanoma, most threatening type of skin cancer, is on the rise. In this paper an implementation of a deep-learning system on a computer server, equipped with graphic processing unit (GPU), is proposed for detection of melanoma lesions. Clinical (non-dermoscopic) images are used in the proposed system, which could assist a dermatologist in early diagnosis of this type of skin cancer. In the proposed system, input clinical images, which could contain illumination and noise effects, are preprocessed in order to reduce such artifacts. Afterward, the enhanced images are fed to a pre-trained convolutional neural network (CNN) which is a member of deep learning models. The CNN classifier, which is trained by large number of training samples, distinguishes between melanoma and benign cases. Experimental results show that the proposed method is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590963,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590963,,Computers;Feature extraction;Lesions;Lighting;Machine learning;Malignant tumors;Training,biomedical optical imaging;cancer;graphics processing units;image classification;image denoising;learning (artificial intelligence);medical image processing;neural nets;skin,CNN classifier;artifact reduction;clinical image analysis;convolutional neural network;deep-learning system;diagnostic accuracy;early skin cancer diagnosis;graphic processing unit;illumination effect;input clinical image;melanoma detection;noise effect;nondermoscopic image,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
208,Automatic Lumbar Vertebrae Detection Based on Feature Fusion Deep Learning for Partial Occluded C-arm X-ray Images,Y. Li; W. Liang; Y. Zhang; H. An; J. Tan,"Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,647,650,"Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590785,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590785,,Computational modeling;Feature extraction;Kernel;Machine learning;Pathology;Surgery;X-ray imaging,computational complexity;computerised tomography;image fusion;image segmentation;image texture;learning (artificial intelligence);medical image processing;neural nets;prosthetics;surgery,CNN;DRR;FFDL;Gabor kernel;IG-MISS;ROI;abnormal pathological condition;automatic lumbar vertebrae detection;automatic segmentation;computational complexity;convolutional neural network model;feature fusion deep learning model;image-guided minimally invasive spine surgery;imaging angle;lumbar vertebrae X-ray images;lumbar vertebrae contour;lumbar vertebrae texture;partial occluded C-arm X-ray images;sobel kernel;surgical implants;training data,,1.0,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
209,A deep symmetry convnet for stroke lesion segmentation,Y. Wang; A. K. Katsaggelos; X. Wang; T. B. Parrish,"Northwestern University, Department of EECS, Evanston, IL, USA",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,111,115,"Stroke is one of the leading causes of death and disability. Clinically, to establish stroke patient prognosis, an accurate delineation of brain lesion is essential, which is time consuming and prone to subjective errors. In this paper, we propose a novel method call Deep Lesion Symmetry ConvNet to automatically segment chronic stroke lesions using MRI. An 8-layer 3D convolutional neural network is constructed to handle the MRI voxels. An additional CNN stream using the corresponding symmetric MRI voxels is combined, leading to a significant improvement in system performance. The high average dice coefficient achieved on our dataset based on data collected from three research labs demonstrates the effectiveness of our method.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532329,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532329,Brain Quasi-symmetry;Deep Learning;Image Segmentation;MRI;Stroke,Biological neural networks;Brain modeling;Image segmentation;Lesions;Magnetic resonance imaging;Pipelines;Three-dimensional displays,biomedical MRI;brain;image segmentation;medical image processing;neural nets;patient treatment,3D convolutional neural network;MRI voxels;brain lesion;death;deep symmetry ConvNet;disability;stroke lesion segmentation;stroke patient prognosis,,,,13.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
210,Alzheimer's disease diagnostics by adaptation of 3D convolutional network,E. Hosseini-Asl; R. Keynton; A. El-Baz,"Electrical and Computer Engineering Department, University of Louisville, Louisville, KY, USA",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,126,130,"Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532332,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532332,3D convolutional neural network;Alzheimer's disease;autoencoder;brain MRI;deep learning,Convolution;Feature extraction;Hippocampus;Magnetic resonance imaging;Positron emission tomography;Three-dimensional displays;Training,biomedical MRI;brain;diseases;feature extraction;generalisation (artificial intelligence);image classification;learning (artificial intelligence);medical image processing;neural nets;neurophysiology,3D convolutional autoencoder;3D-CNN;AD biomarkers;ADNI dataset;Alzheimer's disease diagnostics;Alzheimer's disease progress prevention;Alzheimer's disease treatment;CADDementia MRI dataset;anatomical brain structures;anatomical shape variation;brain image;brain volume;cortical thickness;deep 3D convolutional neural network;early diagnosis;feature classification;feature extraction;feature generalization;generic feature learning;hippocampus shape;structural brain MRI scan;ventricles size,,,,38.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
211,Improving Tuberculosis Diagnostics Using Deep Learning and Mobile Health Technologies among Resource-Poor and Marginalized Communities,Y. Cao; C. Liu; B. Liu; M. J. Brunette; N. Zhang; T. Sun; P. Zhang; J. Peinado; E. S. Garavito; L. L. Garcia; W. H. Curioso,"Dept. of Comput. Sci., Univ. of Massachusetts-Lowell, Lowell, MA, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20160818.0,2016,,,274,281,"Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Perú, a densely occupied urban community and high-burden TB.",,Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2,10.1109/CHASE.2016.18,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545842,Perú;deep convolutional neural networks;deep learning;diagnosis;mHealth;mobile computing;tuberculosis,Diagnostic radiography;Image databases;Mobile communication;Mobile computing;Mobile handsets;X-ray imaging,diagnostic radiography;diseases;epidemics;image classification;learning (artificial intelligence);medical image processing;mobile computing;neural nets;patient diagnosis;visual databases,African;CNN-based models;Deep Learning;Mobile Health Technologies;South-East Asia;TB diagnosis delay;Tuberculosis Diagnostics;Western Pacific;X-ray image database;automated TB screening;chronic infectious disease;deep convolutional neural network-based models;disease transmission;healthcare;image classification;machine learning;mobile computing;tuberculosis epidemic,,,,,,,,27-29 June 2016,,IEEE,IEEE Conference Publications
212,Human Pulse Recognition Based on Convolutional Neural Networks,S. R. Zhang; Q. F. Sun,"Coll. of Commun. & Inf. Eng., Xi'an Univ. of Sci. & Technol., Xi'an, China","2016 International Symposium on Computer, Consumer and Control (IS3C)",20160818.0,2016,,,366,369,"Human pulse recognition is an important part of the objective study of Traditional Chinese Medicine (TCM). In the current human pulse recognition methods, there are many feature extraction algorithms but many are complex and redundancy exists in the features selections. This paper focused on the typical convolutional neural network (CNN), and designed a 9-layer CNN which can be used to human wrist pulse signal classification. Feature extraction process is not necessary in the proposed method so the computing and the complexity are reduced. Experimental results show that the recognition rate can be 93.49%, which further verified the feasibility of our method.",,Electronic:978-1-5090-3071-2; POD:978-1-5090-3072-9,10.1109/IS3C.2016.101,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545211,Back propagation;convolutional neural network;traditional Chinese medicine;wrist pulse signal,Convolution;Cost function;Feature extraction;Kernel;Neural networks;Training;Wrist,computational complexity;medical signal processing;medicine;neural nets;signal classification,9-layer CNN;TCM;computational complexity reduction;convolutional neural networks;human pulse recognition methods;human wrist pulse signal classification;traditional Chinese medicine,,,,,,,,4-6 July 2016,,IEEE,IEEE Conference Publications
213,Application of deep learning for recognizing infant cries,C. Y. Chang; J. J. Li,"National Yunlin University of Science & Technology, Taiwan",2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW),20160728.0,2016,,,1,2,"Crying is a way which infants express their needs to their parents. In general, parents often feel worried and anxious when infant crying. For realizing the reason of baby crying, this paper presents an automatic infant crying recognition method. Crying is convert to spectrogram. A convolutional neural networks (CNN) based deep learning is then adopted to train and classify the crying into three categories including hungry, pain, and sleepy. Experimental results shows that the proposed method achieves high classification accuracy.",,Electronic:978-1-5090-2073-7; POD:978-1-5090-2074-4,10.1109/ICCE-TW.2016.7520947,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7520947,,Biological neural networks;Convolution;Machine learning;Neurons;Pain;Spectrogram;Speech recognition,audio signal processing;convolution;learning (artificial intelligence);medical signal processing;neural nets;paediatrics,CNN based deep learning;automatic infant crying recognition method;convolutional neural networks;spectrogram,,,,,,,,27-29 May 2016,,IEEE,IEEE Conference Publications
214,Prediction of visual attention with Deep CNN for studies of neurodegenerative diseases,S. Chaabouni; F. Tison; J. Benois-Pineau; C. Ben Amar,"LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, 33405 Talence Cedex, France",2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI),20160630.0,2016,,,1,6,"As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video.",,Electronic:978-1-4673-8695-1; POD:978-1-4673-8696-8,10.1109/CBMI.2016.7500243,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500243,,Degradation;Diseases;Measurement;Predictive models;Sociology;Statistics;Visualization,diseases;medical image processing;video signal processing,automatic prediction model;deep CNN;deep learning architecture;dementia;gaze records;neurodegenerative diseases;visual attention prediction,,,,21.0,,,,15-17 June 2016,,IEEE,IEEE Conference Publications
215,New Signal Processing Methods for the Development of Seizure Warning Devices in Epilepsy,V. Senger; R. Tetzlaff,"Chair of Fundamentals of Electrical Engineering, Faculty of Electrical and Computer Engineering, Technische Universit&#228;t Dresden, Dresden, Germany",IEEE Transactions on Circuits and Systems I: Regular Papers,20160628.0,2016,63,5.0,609,616,"The seizure prediction problem has been addressed by many researchers from very different fields for more than three decades. The vision of an implantable seizure prediction device may become reality soon: the first clinical study of such a device has been realized very recently and other realizations are not far behind. Cellular Nonlinear Networks (CNN) were firstly introduced by Chua and Yang in 1988 and later extended to an inherently parallel processing framework called the CNN Universal Machine (CNN-UM). This framework combines high computational power with low power consumption and miniaturized design-making it a very promising basis for the realization of a seizure warning device. In this contribution, we compare the seizure prediction performance of an eigenvalue based PCA-preprocessing followed by a nonlinear CNN signal prediction to the performance of a linear signal prediction approach followed by a level-crossing behavior analysis as well as to the performance of a combination of the two methods.",1549-8328;15498328,,10.1109/TCSI.2016.2553278,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464898,Cellular Nonlinear Networks;epilepsy;level-crossing analysis;multivariate signal processing;seizure prediction;statistical analysis,Eigenvalues and eigenfunctions;Electroencephalography;Epilepsy;Performance evaluation;Principal component analysis;Signal processing;Signal processing algorithms,diseases;medical signal processing;principal component analysis,CNN universal machine;PCA-preprocessing;cellular nonlinear networks;epilepsy;implantable seizure prediction device;level-crossing behavior analysis;nonlinear CNN signal prediction;seizure prediction problem;seizure warning devices;signal processing methods,,,,20.0,,,20160504.0,May 2016,,IEEE,IEEE Journals & Magazines
216,Feature extraction for histopathological images using Convolutional Neural Network,N. Hatipoglu; G. Bilgin,"Bilgisayar Teknolojileri B&#246;l&#252;m&#252;, Trakya &#220;niversitesi, 22030 Edirne, Turkiye",2016 24th Signal Processing and Communication Application Conference (SIU),20160623.0,2016,,,645,648,"In this study, it is intended to increase the classification accuracy results of histopathalogical images by evaluating spatial relations. As a first step, Convolutional Neural Network (CNN) based features are extracted in the original RGB color space of digital histopathalogical images. Training data sets are formed by selecting equal number of different cellular and extra-cellular structures in spatial domain from the images. Classification models of each training data set are obtained by utilizing CNN (as a supervised classifier), Support Vector Machine (SVM) and Random Forest (RF) methods. Visual classification maps and output tables which are obtained from supervised training methods are presented for comparison purpose in the experimental results section.",,Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5,10.1109/SIU.2016.7495823,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495823,Histopathologic images;classification;convolutional neural network;feature extraction;spatial relations,Biomedical imaging;Feature extraction;Image analysis;Image color analysis;Neural networks;Support vector machines;Training data,cellular biophysics;feature extraction;feedforward neural nets;image classification;image colour analysis;learning (artificial intelligence);medical image processing;support vector machines,CNN;RGB color space;SVM;cellular structures;classification models;convolutional neural network;digital histopathalogical images;extra-cellular structures;feature extraction;output tables;random forest methods;supervised classifier;supervised training methods;support vector machine;training data sets;visual classification maps,,,,,,,,16-19 May 2016,,IEEE,IEEE Conference Publications
217,Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation,M. Ghafoorian; N. Karssemeijer; T. Heskes; I. W. M. van Uder; F. E. de Leeuw; E. Marchiori; B. van Ginneken; B. Platel,"Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, the Netherlands",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1414,1417,"Convolutional neural networks (CNN) have been widely used for visual recognition tasks including semantic segmentation of images. While the existing methods consider uniformly sampled single-or multi-scale patches from the neighborhood of each voxel, this approach might be sub-optimal as it captures and processes unnecessary details far away from the center of the patch. We instead propose to train CNNs with non-uniformly sampled patches that allow a wider extent for the sampled patches. This results in more captured contextual information, which is in particular of interest for biomedical image analysis, where the anatomical location of imaging features are often crucial. We evaluate and compare this strategy for white matter hyperintensity segmentation on a test set of 46 MRI scans. We show that the proposed method not only outperforms identical CNNs with uniform patches of the same size (0.780 Dice coefficient compared to 0.736), but also gets very close to the performance of an independent human expert (0.796 Dice coefficient).",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493532,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493532,convolutional neural network;deep learning;non-uniform patch;white matter hyperintensity,Biological neural networks;Biomedical imaging;Convolution;Image segmentation;Sampling methods;Training;Visualization,biomedical MRI;brain;feature extraction;image sampling;image segmentation;medical image processing;neural nets;neurophysiology,Dice coefficient;MRI scans;anatomical location;biomedical image analysis;captured contextual information;deep convolutional neural networks;identical CNNs;imaging features;independent human expert;nonuniform patch sampling;sampled multiscale patches;sampled single-scale patches;semantic image segmentation;visual recognition tasks;white matter hyperintensity segmentation,,1.0,,9.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
218,Structure-based assessment of cancerous mitochondria using deep networks,M. Mishra; S. Schmitt; L. Wang; M. K. Strasser; C. Marr; N. Navab; H. Zischka; T. Peng,"Computer Aided Medical Procedures (CAMP), Technische Universitaet Muenchen, Germany",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,545,548,"Mitochondrial functions are essential for cell survival. Pathologic situations, e.g. cancer, can impair mitochondrial function which is frequently reflected by an altered morphology. So far, feature description of mitochondrial structure in cancer remains largely qualitative. In this study, we propose a learning-based approach to quantitatively assess the structure of mitochondria isolated from liver tumor cell lines using convolutional neural network (CNN). Besides achieving a high classification accuracy on isolated mitochondria from healthy tissue and different tumor cell lines which the CNN model was trained on, CNN is also able to classify unseen tumor cell lines, which suggests its superior capability to capture the intrinsic structural transition from healthy to tumor mitochondria.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493327,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493327,Mitochondria;convolutional neural network;deep learning;electron microscopy,Indexes;Liver;Standards;Support vector machines;Training;Tumors,cancer;cellular biophysics;learning (artificial intelligence);liver;medical image processing;microorganisms;neurophysiology;tumours,CNN model;altered morphology;cancer;cancerous mitochondria;cell survival;convolutional neural network;deep networks;healthy tumor mitochondria;high classification accuracy;learning-based approach;liver tumor cell lines;mitochondria isolated structure;mitochondrial functions;mitochondrial structure;pathologic situations;structural transition;structure-based assessment;tumor cell lines,,,,10.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
219,Gland segmentation in colon histology images using hand-crafted features and convolutional neural networks,W. Li; S. Manivannan; S. Akbar; J. Zhang; E. Trucco; S. J. McKenna,"CVIP, School of Science and Engineering, University of Dundee, Dundee, UK",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1405,1408,"We investigate glandular structure segmentation in colon histology images as a window-based classification problem. We compare and combine methods based on fine-tuned convolutional neural networks (CNN) and hand-crafted features with support vector machines (HC-SVM). On 85 images of H&E-stained tissue, we find that fine-tuned CNN outperforms HC-SVM in gland segmentation measured by pixel-wise Jaccard and Dice indices. For HC-SVM we further observe that training a second-level window classifier on the posterior probabilities - as an output refinement - can substantially improve the segmentation performance. The final performance of HC-SVM with refinement is comparable to that of CNN. Furthermore, we show that by combining and refining the posterior probability outputs of CNN and HC-SVM together, a further performance boost is obtained.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493530,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493530,Convolutional neural network;Gland segmentation;Histology image analysis,Dictionaries;Feature extraction;Glands;Image segmentation;Neural networks;Support vector machines;Training,biological organs;biological tissues;feature extraction;image classification;image segmentation;medical image processing;neural nets;probability;support vector machines,H&E-stained tissue;colon histology images;fine-tuned convolutional neural networks;glandular structure segmentation;hand-crafted features;pixel-wise Dice indices;pixel-wise Jaccard indices;posterior probabilities;second-level window classifier;support vector machines;window-based classification problem,,3.0,,17.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
220,X-ray image classification using domain transferred convolutional neural networks and local sparse spatial pyramid,E. Ahn; A. Kumar; J. Kim; C. Li; D. Feng; M. Fulham,"School of Information Technologies, University of Sydney, Australia",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,855,858,"The classification of medical images is a critical step for imaging-based clinical decision support systems. Existing classification methods for X-ray images, however, generally represent the image using only local texture or generic image features (e.g. color or shape) derived from predefined feature spaces. This limits the ability to quantify the image characteristics using general data-derived features learned from image datasets. In this study we present a new algorithm to improve the performance of X-ray image classification, where we propose a late-fusion of domain transferred convolutional neural networks (DT-CNNs) with sparse spatial pyramid (SSP) features derived from a local image dictionary. Our method is robust as it exploits the rich generic information provided by the DT-CNNs and uses the specific local features and characteristics inherent in the X-ray images. Our method was evaluated on a public dataset of X-ray images and was compared to several state-of-the-art approaches. Experimental results show that our method was the most accurate for classification.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493400,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493400,X-ray;classification;convolutional neural network;sparse coding;transfer learning,Biomedical imaging;Dictionaries;Encoding;Feature extraction;Image classification;Support vector machines;X-ray imaging,decision support systems;diagnostic radiography;image classification;image fusion;image representation;medical image processing;neurophysiology,DT-CNN;X-ray image classification;data-derived features;domain transferred convolutional neural networks;feature spaces;generic image features;generic information;image datasets;image representation;imaging-based clinical decision support systems;local image dictionary;local sparse spatial pyramid;local texture;medical image classification;public dataset;sparse spatial pyramid features,,1.0,,18.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
221,Sub-cortical brain structure segmentation using F-CNN'S,M. Shakeri; S. Tsogkas; E. Ferrante; S. Lippe; S. Kadoury; N. Paragios; I. Kokkinos,Polytechnique Montreal,2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,269,272,"In this paper we propose a deep learning approach for segmenting sub-cortical structures of the human brain in Magnetic Resonance (MR) image data. We draw inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN) architecture for semantic segmentation of objects in natural images, and adapt it to our task. Unlike previous CNN-based methods that operate on image patches, our model is applied on a full blown 2D image, without any alignment or registration steps at testing time. We further improve segmentation results by interpreting the CNN output as potentials of a Markov Random Field (MRF), whose topology corresponds to a volumetric grid. Alpha-expansion is used to perform approximate inference imposing spatial volumetric homogeneity to the CNN priors. We compare the performance of the proposed pipeline with a similar system using Random Forest-based priors, as well as state-of-art segmentation algorithms, and show promising results on two different brain MRI datasets.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493261,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493261,Convolutional neural networks;Magnetic Resonance Imaging;Markov Random Fields;semantic segmentation;sub-cortical structures,Brain;Image segmentation;Magnetic resonance imaging;Markov random fields;Semantics;Three-dimensional displays;Training,Markov processes;biomedical MRI;brain;image segmentation;medical image processing;neural nets,F-CNN architecture;Markov random field;brain MRI dataset;deep learning approach;fully-convolutional neural network;human brain;image patch;image registration;image segmentation;magnetic resonance image data;random forest-based prior;semantic segmentation;subcortical brain structure segmentation,,,,12.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
222,Handcrafted features with convolutional neural networks for detection of tumor cells in histology images,M. N. Kashif; S. E. A. Raza; K. Sirinukunwattana; M. Arif; N. Rajpoot,"Department of Electrical Engineering, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1029,1032,"Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493441,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493441,Convolutional Neural Network;Digital Pathology;Scattering Transform;Tumor Nuclei Detection,Feature extraction;Image color analysis;Neural networks;Scattering;Standards;Transforms;Tumors,cellular biophysics;feature extraction;medical image processing;neural nets;tumours,cancer histology images;handcrafted features;nonlinear invariant texture features;scattering transform;spatially constrained convolutional neural networks;tumor cell detection,,,,13.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
223,"An automatic breast cancer grading method in histopathological images based on pixel-, object-, and semantic-level features",J. Cao; Z. Qin; J. Jing; J. Chen; T. Wan,"Intelligent Computing & Machine Learning Lab, School of ASEE, Beihang University, China",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1151,1154,"We present an automatic breast cancer grading method in histopathological images based on the computer extracted pixel-, object-, and semantic-level features derived from convolutional neural networks (CNN). The multiple level features allow not only characterization of nuclear polymorphism, but also extraction of structural and interpretable information within the images. In this study, a hybrid level set based segmentation method was used to segment nuclei from the images. A quantile normalization approach was utilized to improve image color consistency. The semantic level features are extracted by a CNN approach, which describe the proportions of nuclei belonging to the different grades, in conjunction with pixel-level (texture) and object-level (structure) features, to form an integrated set of attributes. A support vector machine classifier was trained to discriminate the breast cancer between low, intermediate, and high grades. The results demonstrated that our method achieved accuracy of 0.90 (low vs. high), and 0.74 (low vs. intermediate), and 0.76 (intermediate vs. high), suggesting that the present method could play a fundamental role in developing a computer-aided breast cancer grading system.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493470,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493470,breast cancer grading;convolutional neural networks;histopathology;multi-level features,Breast cancer;Feature extraction;Image color analysis;Image segmentation;Neurons;Support vector machines,,,,1.0,,9.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
224,Real-time 2D/3D registration via CNN regression,S. Miao; Z. J. Wang; Y. Zheng; R. Liao,"Electrical and Computer Engineering, University of British Columbia, Canada",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1430,1434,"In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493536,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493536,2-D/3-D Registration;Convolutional Neural Network;Deep Learning;Image Guided Intervention,Biomedical imaging;Feature extraction;Neural networks;Real-time systems;Solid modeling;Training;X-ray imaging,,,,,,18.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
225,Epileptiform spike detection via convolutional neural networks,A. R. Johansen; J. Jin; T. Maszczyk; J. Dauwels; S. S. Cash; M. B. Westover,"Technical University of Denmark, DTU Compute, Lyngby, Denmark","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519.0,2016,,,754,758,"The EEG of epileptic patients often contains sharp waveforms called ""spikes"", occurring between seizures. Detecting such spikes is crucial for diagnosing epilepsy. In this paper, we develop a convolutional neural network (CNN) for detecting spikes in EEG of epileptic patients in an automated fashion. The CNN has a convolutional architecture with filters of various sizes applied to the input layer, leaky ReLUs as activation functions, and a sigmoid output layer. Balanced mini-batches were applied to handle the imbalance in the data set. Leave-one-patient-out cross-validation was carried out to test the CNN and benchmark models on EEG data of five epilepsy patients. We achieved 0.947 AUC for the CNN, while the best performing benchmark model, Support Vector Machines with Gaussian kernel, achieved an AUC of 0.912.",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7471776,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471776,Convolutional neural network;Deep learning;EEG;Epilepsy;Spike detection,Benchmark testing;Biological neural networks;Brain models;Computational modeling;Electroencephalography;Epilepsy,electroencephalography;medical signal processing;neural nets,CNN;EEG;convolutional neural networks;electroencephalography;epileptic patients;epileptiform spike detection;leave-one-patient-out cross-validation;support vector machines,,,,29.0,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
226,AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images,S. Albarqouni; C. Baur; F. Achilles; V. Belagiannis; S. Demirci; N. Navab,"Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany",IEEE Transactions on Medical Imaging,20160502.0,2016,35,5.0,1313,1321,"The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.",0278-0062;02780062,,10.1109/TMI.2016.2528120,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405343,Aggregation;crowdsourcing;deep learning;gamification;online learning,Biomedical imaging;Computational modeling;Crowdsourcing;Data models;Machine learning;Noise measurement;Robustness,biological organs;cancer;data aggregation;image classification;image denoising;learning (artificial intelligence);medical image processing,biomedical image database;breast cancer histology imaging;conventional machine-learning methods;convolutional neural network;crowd annotation datasets;crowd flower API;crowd sourcing layer;data aggregation;deep CNN learning;ground-truth data;image annotation tasks;learning annotation models;learning process;mitosis detection;noisy annotations;self-implemented web-platform,,12.0,,38.0,,,20160211.0,May 2016,,IEEE,IEEE Journals & Magazines
227,Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images,S. Pereira; A. Pinto; V. Alves; C. A. Silva,"CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1240,1251,"Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 ×3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.",0278-0062;02780062,,10.1109/TMI.2016.2538465,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426413,Brain tumor;brain tumor segmentation;convolutional neural networks;deep learning;glioma;magnetic resonance imaging,Brain modeling;Context;Image segmentation;Kernel;Magnetic resonance imaging;Training;Tumors,biomedical MRI;brain;cancer;image segmentation;medical image processing;neurophysiology;tumours,CNN-based segmentation methods;Dice similarity coefficient metrics;MRI images;automatic segmentation;automatic segmentation methods;brain tumor segmentation;clinical practice;convolutional neural networks;data augmentation;gliomas;imaging technique;intensity normalization;kernels;magnetic resonance imaging;manual segmentation;on-site BRATS 2015 Challenge;oncological patients;online evaluation platform;precise quantitative measurements;preprocessing step;quality-of-life;reliable segmentation methods;spatial variability;structural variability,,15.0,,52.0,,,20160304.0,May 2016,,IEEE,IEEE Journals & Magazines
228,Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images,K. Sirinukunwattana; S. E. A. Raza; Y. W. Tsang; D. R. J. Snead; I. A. Cree; N. M. Rajpoot,"Department of Computer Science, University of Warwick, Coventry, UK",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1196,1206,"Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.",0278-0062;02780062,,10.1109/TMI.2016.2525803,10.13039/100008982 - Qatar National Research Fund; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399414,Convolutional neural network;deep learning;histology image analysis;nucleus detection,Cancer;Computer architecture;Feature extraction;Machine learning;Microprocessors;Shape;Tumors,biological organs;biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;probability;tumours,NEP;SC-CNN;cancerous tissue;cell nuclei classification;cell nuclei detection;cellular heterogeneity;colorectal adenocarcinoma images;dataset;eosin stain;high-probability values;highest average F1 score;histopathology images;joint detection;locality sensitive deep learning;neighboring ensemble predictor;quantitative analysis;routine colon cancer histology images;spatially constrained convolutional neural network;standard hematoxylin;tissue constituents;whole-slide images,,16.0,,38.0,,,20160204.0,May 2016,,IEEE,IEEE Journals & Magazines
229,Deep Learning for Imbalanced Multimedia Data Classification,Y. Yan; M. Chen; M. L. Shyu; S. C. Chen,"Dept. of Electr. & Comput. Eng., Univ. of Miami, Coral Gables, FL, USA",2015 IEEE International Symposium on Multimedia (ISM),20160328.0,2015,,,483,488,"Classification of imbalanced data is an important research problem as lots of real-world data sets have skewed class distributions in which the majority of data instances (examples) belong to one class and far fewer instances belong to others. While in many applications, the minority instances actually represent the concept of interest (e.g., fraud in banking operations, abnormal cell in medical data, etc.), a classifier induced from an imbalanced data set is more likely to be biased towards the majority class and show very poor classification accuracy on the minority class. Despite extensive research efforts, imbalanced data classification remains one of the most challenging problems in data mining and machine learning, especially for multimedia data. To tackle this challenge, in this paper, we propose an extended deep learning approach to achieve promising performance in classifying skewed multimedia data sets. Specifically, we investigate the integration of bootstrapping methods and a state-of-the-art deep learning approach, Convolutional Neural Networks (CNNs), with extensive empirical studies. Considering the fact that deep learning approaches such as CNNs are usually computationally expensive, we propose to feed low-level features to CNNs and prove its feasibility in achieving promising performance while saving a lot of training time. The experimental results show the effectiveness of our framework in classifying severely imbalanced data in the TRECVID data set.",,Electronic:978-1-5090-0379-2; POD:978-1-5090-0380-8; USB:978-1-5090-0378-5,10.1109/ISM.2015.126,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442383,classification;convolutional neural network (CNN);deep learning;imbalanced data;semantic indexing,Classification algorithms;Convolution;Data models;Machine learning;Multimedia communication;Neural networks;Training,convolution;data analysis;learning (artificial intelligence);multimedia computing;neural nets,CNN;TRECVID data set;banking operations;convolutional neural networks;data instances;deep learning approaches;imbalanced multimedia data classification;low-level features;real-world data sets,,4.0,,41.0,,,,14-16 Dec. 2015,,IEEE,IEEE Conference Publications
230,ABM and CNN application in ventral stream of visual system,B. Yousefi; P. Yousefi,"Department of Electrical and robotic engineering, Shahrood University Technology, Shahrood, Iran",2015 IEEE Student Symposium in Biomedical Engineering & Sciences (ISSBES),20160321.0,2015,,,87,92,"This paper addresses an investigation regarding the suitability of two different techniques, Active Basis Model (ABM) and Gabor based Convolutional Neural Network (CNN or G-ConvNets) in the mechanism for recognition of biological movement (mammalian visual system model). This method inspired by ventral streams which provide the form information. Both of these approaches contain information of the shape of human object obtained by the Gabor features. The comparison of these methods concludes advantages and drawbacks of both methods that shown CNN have advantage of recognition of the action (for CNN only walking, running and waving) however ABM basically used for object recognition task (not particularly for action recognition).",,CD-ROM:978-1-4673-7815-4; Electronic:978-1-4673-7816-1; POD:978-1-4673-7817-8,10.1109/ISSBES.2015.7435920,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435920,Gabor Convolutional Neural Network;Ventral stream;active basis model;biologically inspired model;human actionrecognition,Biological system modeling;Convolution;Gabor filters;Neurons;Shape;Training,Gabor filters;biomechanics;convolution;image recognition;medical image processing;neural nets,ABM application;CNN application;G-ConvNets;Gabor based convolutional neural network;Gabor features;active basis model;biological movement recognition;human object;mammalian visual system model;object recognition task;ventral stream,,,,25.0,,,,4-4 Nov. 2015,,IEEE,IEEE Conference Publications
231,A hybrid convolutional neural networks with extreme learning machine for WCE image classification,J. s. Yu; J. Chen; Z. Q. Xiang; Y. X. Zou,"ADSPLAB, School of ECE, Peking University, Shenzhen 518055, China",2015 IEEE International Conference on Robotics and Biomimetics (ROBIO),20160225.0,2015,,,1822,1827,"Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25% classification accuracy can be achieved in average.",,Electronic:978-1-4673-9675-2; USB:978-1-4673-9674-5,10.1109/ROBIO.2015.7419037,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419037,,Endoscopes;Esophagus;Feature extraction;Image classification;Iron;Testing;Training,diseases;endoscopes;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets,CNN-based WCE classification system;HCNN-NELM;WCE image classification system;cascaded ELM;complex digestive tract;data-driven feature extractor;deep CNN classification system;digestive organs;discriminative image features extraction;extreme learning machine;full-connection classifier;hybrid CNN;hybrid convolutional neural networks;noninvasive gastrointestinal disease examination;wireless capsule endoscopy,,1.0,,14.0,,,,6-9 Dec. 2015,,IEEE,IEEE Conference Publications
232,Deep Feature Learning with Discrimination Mechanism for Brain Tumor Segmentation and Diagnosis,L. Zhao; K. Jia,"Multimedia Inf. Process. Group, Beijing Univ. of Technol., Beijing, China",2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP),20160225.0,2015,,,306,309,"Brain tumor segmentation is one of the main challenging problems in computer vision and its early diagnosis is critical to clinics. Segmentation needs to be accurate, efficient and robust to avoid influences caused by various large and complex biases added to images. This paper proposes a multiple convolutional neural network (CNNs) framework with discrimination mechanism which is effective to achieve these goals. First of all, this paper proposes to construct different triplanar 2D CNNs architecture for 3D voxel classification, greatly reducing segmentation time. Experiment is conducted on images provided by Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized by MICCAI 2013 for both training and testing. As T1, T1-enhanced, T2 and FLAIR MRI images are utilized, multimodal features are combined. As a result, accuracy, sensitivity and specificity are comparable in comparison with manual gold standard images and better than state-of-the-art segmentation methods.",,CD-ROM:978-1-5090-0187-3; Electronic:978-1-5090-0188-0; POD:978-1-5090-0189-7,10.1109/IIH-MSP.2015.41,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415818,CNNs;brain tumor segmentation;voting strategy,Cancer;Computer architecture;Feature extraction;Image segmentation;Magnetic resonance imaging;Three-dimensional displays;Tumors,biomedical MRI;brain;computational geometry;computer vision;feedforward neural nets;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours,3D voxel classification;BRATS;FLAIR MRI image utilization;MICCAI 2013;T1 image utilization;T1-enhanced image utilization;T2 image utilization;brain tumor diagnosis;brain tumor segmentation;computer vision;deep feature learning;discrimination mechanism;multimodal brain tumor image segmentation benchmark;multiple convolutional neural network framework;segmentation time reduction;triplanar 2D CNN architecture,,,,14.0,,,,23-25 Sept. 2015,,IEEE,IEEE Conference Publications
233,Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural Networks,S. Kiranyaz; T. Ince; M. Gabbouj,"Electrical Engineering Department, College of Engineering, Doha, Qatar",IEEE Transactions on Biomedical Engineering,20160218.0,2016,63,3.0,664,675,"Goal: This paper presents a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system. Methods: An adaptive implementation of 1-D convolutional neural networks (CNNs) is inherently used to fuse the two major blocks of the ECG classification into a single learning body: feature extraction and classification. Therefore, for each patient, an individual and simple CNN will be trained by using relatively small common and patient-specific training data, and thus, such patient-specific feature extraction ability can further improve the classification performance. Since this also negates the necessity to extract hand-crafted manual features, once a dedicated CNN is trained for a particular patient, it can solely be used to classify possibly long ECG data stream in a fast and accurate manner or alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. Results: The results over the MIT-BIH arrhythmia benchmark database demonstrate that the proposed solution achieves a superior classification performance than most of the state-of-the-art methods for the detection of ventricular ectopic beats and supraventricular ectopic beats. Conclusion: Besides the speed and computational efficiency achieved, once a dedicated CNN is trained for an individual patient, it can solely be used to classify his/her long ECG records such as Holter registers in a fast and accurate manner. Significance: Due to its simple and parameter invariant nature, the proposed system is highly generic, and, thus, applicable to any ECG dataset.",0018-9294;00189294,,10.1109/TBME.2015.2468589,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202837,Convolutional Neural Networks;Convolutional neural networks (CNNs);Patient-specific ECG classification;patient-specific ECG classification;real-time heart monitoring,Databases;Electrocardiography;Feature extraction;Kernel;Monitoring;Neurons;Training,diseases;electrocardiography;feature extraction;medical signal processing;patient monitoring;signal classification,1D convolutional neural networks;ECG data stream;ECG records;Holter registers;MIT-BIH arrhythmia benchmark database;classification performance;light-weight wearable device;patient-specific electrocardiogram classification system;patient-specific electrocardiogram monitoring system;patient-specific feature extraction;patient-specific training data;real-time ECG monitoring;real-time patient-specific ECG classification system;supraventricular ectopic beats,,12.0,,27.0,,,20150814.0,March 2016,,IEEE,IEEE Journals & Magazines
234,Patient prognosis from vital sign time series: Combining convolutional neural networks with a dynamical systems approach,L. w. Lehman; M. Ghassemi; J. Snoek; S. Nemati,"Massachusetts Institute of Technology, Cambridge, USA",2015 Computing in Cardiology Conference (CinC),20160218.0,2015,,,1069,1072,"In this work, we propose a stacked switching vector-autoregressive (SVAR)-CNN architecture to model the changing dynamics in physiological time series for patient prognosis. The SVAR-layer extracts dynamical features (or modes) from the time-series, which are then fed into the CNN-layer to extract higher-level features representative of transition patterns among the dynamical modes. We evaluate our approach using 8-hours of minute-by-minute mean arterial blood pressure (BP) from over 450 patients in the MIMIC-II database. We modeled the time-series using a third-order SVAR process with 20 modes, resulting in first-level dynamical features of size 20×480 per patient. A fully connected CNN is then used to learn hierarchical features from these inputs, and to predict hospital mortality. The combined CNN/SVAR approach using BP time-series achieved a median and interquartile-range AUC of 0.74 [0.69, 0.75], significantly outperforming CNN-alone (0.54 [0.46, 0.59]), and SVAR-alone with logistic regression (0.69 [0.65, 0.72]). Our results indicate that including an SVAR layer improves the ability of CNNs to classify nonlinear and nonstationary time-series.",2325-8861;23258861,Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1,10.1109/CIC.2015.7411099,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411099,,Physiology;Prognostics and health management;Switches,autoregressive processes;blood pressure measurement;feature extraction;medical signal processing;neural nets;signal classification;time series,MIMIC-II database;SVAR;convolutional neural networks;dynamical feature extraction;dynamical modes;dynamical systems approach;logistic regression;mean arterial blood pressure;nonlinear time series classification;nonstationary time series classification;patient prognosis;physiological time series;stacked switching vector-autoregressive-CNN architecture;transition patterns;vital sign time series,,1.0,,10.0,,,,6-9 Sept. 2015,,IEEE,IEEE Conference Publications
235,Lesion detection of endoscopy images based on convolutional neural network features,R. Zhu; R. Zhang; D. Xue,"Department of Electronic Engineering and Information Science, University of Science and Technology of China",2015 8th International Congress on Image and Signal Processing (CISP),20160218.0,2015,,,372,376,"Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.",,Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2,10.1109/CISP.2015.7407907,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407907,,Endoscopes;Feature extraction;Gastrointestinal tract;Histograms;Image color analysis;Lesions;Support vector machines,cancer;endoscopes;feature extraction;image colour analysis;image texture;medical image processing;neural nets;support vector machines,CNN;SVM;color features;computer-aided lesion detection system;convolutional neural network features;detect gastrointestinal disease;endoscopy image lesion detection;gastrointestinal examination;gastrointestinal tract directly;support vector machine;texture features;trainable feature extractor,,4.0,,26.0,,,,14-16 Oct. 2015,,IEEE,IEEE Conference Publications
236,An Automatic Learning-Based Framework for Robust Nucleus Segmentation,F. Xing; Y. Xie; L. Yang,"Department of Electrical and Computer Engineering, University of Florida, Gainesville",IEEE Transactions on Medical Imaging,20160202.0,2016,35,2.0,550,566,"Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.",0278-0062;02780062,,10.1109/TMI.2015.2481436,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274740,Deep convolutional neural network;nucleus segmentation;sparse representation,Breast cancer;Computational modeling;Image color analysis;Image segmentation;Robustness;Shape;Tumors,cancer;diagnostic radiography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;probability;tumours,automated nucleus segmentation;automatic learning-based framework;automatic morphological feature computation;brain tumor;breast cancer;computer-aided image analysis;deep CNN model;deep convolutional neural network model;diseases;early detection;feature learning characteristic;high level shape prior modeling;histopathology imaging;histopathology specimens;iterative region merging approach;large-scale pathology image datasets;local repulsive deformable model;pancreatic neuroendocrine tumor;probability map;quantitative analysis;robust nucleus segmentation;robust selection-based sparse shape model;staining histopathology images;tissue,,7.0,,83.0,,,20150923.0,Feb. 2016,,IEEE,IEEE Journals & Magazines
237,Prediction of driver's drowsy and alert states from EEG signals with deep learning,M. Hajinoroozi; Z. Mao; Y. Huang,"Department of Electrical and Computer Engineering, University of Texas at San Antonio, One UTSA Circle, USA",2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),20160121.0,2015,,,493,496,"We investigate in this paper deep learning (DL) solutions for prediction of driver's cognitive states (drowsy or alert) using EEG data. We discussed the novel channel-wise convolutional neural network (CCNN) and CCNN-R which is a CCNN variation that uses Restricted Boltzmann Machine in order to replace the convolutional filter. We also consider bagging classifiers based on DL hidden units as an alternative to the conventional DL solutions. To test the performance of the proposed methods, a large EEG dataset from 3 studies of driver's fatigue that includes 70 sessions from 37 subjects is assembled. All proposed methods are tested on both raw EEG and Independent Component Analysis (ICA)-transformed data for cross-session predictions. The results show that CCNN and CCNN-R outperform deep neural networks (DNN) and convolutional neural networks (CNN) as well as other non-DL algorithms and DL with raw EEG inputs achieves better performance than ICA features.",,Electronic:978-1-4799-1963-5; POD:978-1-4799-1964-2,10.1109/CAMSAP.2015.7383844,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383844,,Backpropagation;Bagging;Convolution;Electroencephalography;Feature extraction;Machine learning;Prediction algorithms,Boltzmann machines;driver information systems;electroencephalography;independent component analysis;learning (artificial intelligence);medical signal processing,CCNN-R;EEG signal;ICA;bagging classifier;channel-wise convolutional neural network;cognitive state;deep learning;driver alert state;driver drowsy state;independent component analysis;restricted Boltzmann machine,,1.0,,18.0,,,,13-16 Dec. 2015,,IEEE,IEEE Conference Publications
238,Maximum Margin Learning of t-SPNs for Cell Classification With Filtered Input,H. Kang; C. D. Yoo; Y. Na,"School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea",IEEE Journal of Selected Topics in Signal Processing,20160121.0,2016,10,1.0,130,139,"An algorithm based on a deep probabilistic architecture referred to as tree-structured sum-product network (t-SPN) is considered for cells classification. The t-SPN is a rooted acyclic graph constructed as a tree of several sum-product networks where each network is constructed over a subset of most confusing class features. The constructed t-SPN architecture is learned by maximizing the margin which is defined to be the difference in the conditional probability between the true and the most competitive false labels. To enhance generalization, l<sub>2</sub>-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate compared to other state-of-the-art algorithms that include convolutional neural network (CNN) based algorithms. Ideal high-pass filter was more effective on the HEp-2 dataset which is based on immunofluorescence staining while the LOG was more effective on Feulgen dataset which is based on Feulgen staining.",1932-4553;19324553,,10.1109/JSTSP.2015.2502542,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332738,confusing classes;maximum margin;sub-SPNs;t-SPNs,Computer architecture;Input variables;Microprocessors;Microscopy;Signal processing;Special issues and sections,biomedical optical imaging;cellular biophysics;fluorescence;high-pass filters;image classification;learning (artificial intelligence);medical image processing;neural nets;probability,Feulgen dataset;Feulgen staining;HEp-2 dataset;Laplacian-of-Gaussian filtering;cell classification;cell feature;convolutional neural network;deep probabilistic architecture;filtered input;generic high-pass filter;immunofluorescence staining;l2-regularization;learning process;maximum margin criterion;maximum margin learning;rooted acyclic graph;t-SPN architecture;tree-structured sum-product network,,,,18.0,,,20151120.0,Feb. 2016,,IEEE,IEEE Journals & Magazines
239,Rich feature hierarchies for cell detecting under phase contrast microscopy images,F. Deng; H. Hu; S. Chen; Q. Guan; Y. Zou,"Dept. College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, 310023, P.R. China",2015 Sixth International Conference on Intelligent Control and Information Processing (ICICIP),20160121.0,2015,,,348,353,"R-CNN (region-convolutional neural network) has recently achieved very outstanding results in variety of visual detecting fields, and its function of object-proposal-generation can achieve effective training models by using as small samples as possible in the field of machine learning. In this paper, a modified R-CNN is proposed and applied to detect cells under phase contrast microscopy images by adopting multiple object-proposal-generations instead of a single one to extract candidate regions. The results show that the proposed method can obtain better performance than the traditional method by using a single object-proposal-generation.",,Electronic:978-1-4799-1717-4; POD:978-1-4799-1718-1,10.1109/ICICIP.2015.7388195,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388195,R-CNN;cell detection;region proposed approach,Computer architecture;Feature extraction;Image edge detection;Microprocessors;Microscopy;Proposals;Training,biomedical optical imaging;cellular biophysics;learning (artificial intelligence);medical image processing;neural nets;object detection;optical microscopy,cell detection;machine learning;modified R-CNN;object-proposal-generation;phase contrast microscopy image;region-convolutional neural network;rich feature hierarchy;visual detecting field,,,,22.0,,,,26-28 Nov. 2015,,IEEE,IEEE Conference Publications
240,Human Epithelial Type 2 cell classification with convolutional neural networks,N. Bayramoglu; J. Kannala; J. Heikkilä,"Center for Machine Vision Research, University of Oulu, Finland",2015 IEEE 15th International Conference on Bioinformatics and Bioengineering (BIBE),20160104.0,2015,,,1,6,"Automated cell classification in Indirect Immunofluorescence (IIF) images has potential to be an important tool in clinical practice and research. This paper presents a framework for classification of Human Epithelial Type 2 cell IIF images using convolutional neural networks (CNNs). Previuos state-of-the-art methods show classification accuracy of 75.6% on a benchmark dataset. We conduct an exploration of different strategies for enhancing, augmenting and processing training data in a CNN framework for image classification. Our proposed strategy for training data and pre-training and fine-tuning the CNN network led to a significant increase in the performance over other approaches that have been used until now. Specifically, our method achieves a 80.25% classification accuracy. Source code and models to reproduce the experiments in the paper is made publicly available.",,Electronic:978-1-4673-7983-0; POD:978-1-4673-7984-7,10.1109/BIBE.2015.7367705,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367705,,Computer architecture;Histograms;Image segmentation;Microprocessors;Neural networks;Training;Training data,biomedical optical imaging;cellular biophysics;fluorescence;image classification;image enhancement;medical image processing,CNN framework;augmenting training data;benchmark dataset;classification accuracy;clinical practice;convolutional neural networks;enhancing training data;human epithelial type 2 cell classification;image classification;indirect immunofluorescence images;processing training data,,1.0,,21.0,,,,2-4 Nov. 2015,,IEEE,IEEE Conference Publications
241,Microvascular morphological type recognition using trainable feature extractor,D. X. Xue; R. Zhang; R. S. Zhu,"Department of Electronic Engineering and Information Science, University of Science and Technology of China",2015 International Symposium on Bioelectronics and Bioinformatics (ISBB),20151203.0,2015,,,67,70,"This paper focuses on the problem of feature extraction and the classification task of microvascular morphological type to aid esophageal cancer detection. A specialized convolutional neural network (CNN) is designed to extract hierarchical features and Support Vector Machines (SVMs) are introduced to enhance the generalization ability of classifiers. Experiments are conducted on the NBI-ME dataset, achieving a recognition rate of 88.19% on patch level. The results show that the CNN-SVM model beats models of traditional features with SVM as well as the original CNN with softmax. The synthesis results indicate this system is able to assist clinical diagnose to a certain extent.",,Electronic:978-1-4673-6609-0; POD:978-1-4673-6610-6; USB:978-1-4673-6608-3,10.1109/ISBB.2015.7344925,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344925,Caffe;Convolutional Neural Network;feature learning;microvascular type classification,Convolution;Feature extraction;Image recognition;Kernel;Neural networks;Support vector machines;Training,biomedical optical imaging;blood vessels;cancer;feature extraction;image classification;medical image processing;neural nets;support vector machines,NBI-ME dataset;SVM;classification task;classifier generalization ability;clinical diagnosis;esophageal cancer detection;hierarchical feature extraction;microvascular morphological type recognition;specialized convolutional neural network;support vector machines;trainable feature extractor,,,,12.0,,,,14-17 Oct. 2015,,IEEE,IEEE Conference Publications
242,Convolutional Neural Networks for patient-specific ECG classification,S. Kiranyaz; T. Ince; R. Hamila; M. Gabbouj,"Electrical Engineering, College of Engineering, Qatar University, Qatar",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,2608,2611,"We propose a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system using an adaptive implementation of 1D Convolutional Neural Networks (CNNs) that can fuse feature extraction and classification into a unified learner. In this way, a dedicated CNN will be trained for each patient by using relatively small common and patient-specific training data and thus it can also be used to classify long ECG records such as Holter registers in a fast and accurate manner. Alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. The experimental results demonstrate that the proposed system achieves a superior classification performance for the detection of ventricular ectopic beats (VEB) and supraventricular ectopic beats (SVEB).",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318926,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318926,,Convolution;Databases;Electrocardiography;Feature extraction;Neural networks;Neurons;Training,electrocardiography;feature extraction;medical signal processing;neural nets;signal classification,1D convolutional neural network;ECG monitoring;ECG record classification;feature extraction;light-weight wearable device;patient-specific ECG classification;patient-specific electrocardiogram classification;supraventricular ectopic beat detection,,1.0,,20.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
243,A comparative study for chest radiograph image retrieval using binary texture and deep learning classification,Y. Anavi; I. Kogan; E. Gelbart; O. Geva; H. Greenspan,"Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,2940,2943,"In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the “descriptor”-based approach); the second approach (“classification”-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7319008,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319008,,Biomedical imaging;Feature extraction;Heart;Machine learning;Measurement;Pathology;Support vector machines,diagnostic radiography;image classification;image retrieval;image texture;learning (artificial intelligence);medical image processing;probability;support vector machines,CNN;SVM classifier;X-ray image retrieval;binary features;binary texture;chest pathology retrieval;chest radiograph image retrieval;classification-based approach;decision values;deep learning classification;deep learning features;descriptor-based approach;image descriptors;pair-wise classification;probability descriptor;query image;texture features,,3.0,,12.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
244,Automatic localization of the left ventricle in cardiac MRI images using deep learning,O. Emad; I. A. Yassine; A. S. Fahmy,"Center for Informatics Science, Nile University, Giza, Egypt",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,683,686,"Automatic localization of the left ventricle (LV) in cardiac MRI images is an essential step for automatic segmentation, functional analysis, and content based retrieval of cardiac images. In this paper, we introduce a new approach based on deep Convolutional Neural Network (CNN) to localize the LV in cardiac MRI in short axis views. A six-layer CNN with different kernel sizes was employed for feature extraction, followed by Softmax fully connected layer for classification. The pyramids of scales analysis was introduced in order to take account of the different sizes of the heart. A publically-available database of 33 patients was used for learning and testing. The proposed method was able it localize the LV with 98.66%, 83.91% and 99.07% for accuracy, sensitivity and specificity respectively.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318454,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318454,,Biomedical imaging;Convolution;Feature extraction;Heart;Image segmentation;Magnetic resonance imaging;Sensitivity,biomedical MRI;cardiology;feature extraction;image classification;image segmentation;medical image processing;neural nets,Softmax;automatic segmentation;cardiac MRI images;deep convolutional neural network;deep learning;feature extraction;image classification;left ventricle automatic localization,,2.0,,21.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
245,Resting State EEG-based biometrics for individual identification using convolutional neural networks,L. Ma; J. W. Minett; T. Blu; W. S. Y. Wang,"Department of Electronic Engineering, The Chinese University of Hong Kong, China",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,2848,2851,"Biometrics is a growing field, which permits identification of individuals by means of unique physical features. Electroencephalography (EEG)-based biometrics utilizes the small intra-personal differences and large inter-personal differences between individuals' brainwave patterns. In the past, such methods have used features derived from manually-designed procedures for this purpose. Another possibility is to use convolutional neural networks (CNN) to automatically extract an individual's best and most unique neural features and conduct classification, using EEG data derived from both Resting State with Open Eyes (REO) and Resting State with Closed Eyes (REC). Results indicate that this CNN-based joint-optimized EEG-based Biometric System yields a high degree of accuracy of identification (88%) for 10-class classification. Furthermore, rich inter-personal difference can be found using a very low frequency band (0-2Hz). Additionally, results suggest that the temporal portions over which subjects can be individualized is less than 200 ms.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318985,,Accuracy;Biological neural networks;Biometrics (access control);Convolution;Electroencephalography;Feature extraction;Security,biometrics (access control);electroencephalography;feature extraction;medical signal processing;neural nets;signal classification;visual evoked potentials,EEG based biometrics;brainwave patterns;convolutional neural networks;electroencephalography;individual identification;interpersonal differences;intrapersonal differences;neural feature classification;neural feature extraction;resting state EEG;resting state-closed eyes condition;resting state-open eyes condition,,,,14.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
246,Automatic detection of cell divisions (mitosis) in live-imaging microscopy images using Convolutional Neural Networks,A. Shkolyar; A. Gefen; D. Benayahu; H. Greenspan,"Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,743,746,"We propose a semi-automated pipeline for the detection of possible cell divisions in live-imaging microscopy and the classification of these mitosis candidates using a Convolutional Neural Network (CNN). We use time-lapse images of NIH3T3 scratch assay cultures, extract patches around bright candidate regions that then undergo segmentation and binarization, followed by a classification of the binary patches into either containing or not containing cell division. The classification is performed by training a Convolutional Neural Network on a specially constructed database. We show strong results of AUC = 0.91 and F-score = 0.89, competitive with state-of-the-art methods in this field.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318469,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318469,,Computer architecture;Feature extraction;Gray-scale;Microscopy;Testing;Training;Wounds,biomedical optical imaging;cellular biophysics;feature extraction;image classification;image segmentation;medical image processing;neural nets;optical microscopy,NIH3T3 scratch assay cultures;automatic cell division detection;binary patch classification;binary patch extraction;convolutional neural networks;image segmentation;live-imaging microscopy images;mitosis classification;time-lapse images,,,,10.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
247,Bacterial colony counting by Convolutional Neural Networks,A. Ferrari; S. Lombardi; A. Signoroni,"Information Engineering Dept., University of Brescia, via Branze 38, I25123 (Italy)",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,7458,7461,"Counting bacterial colonies on microbiological culture plates is a time-consuming, error-prone, nevertheless fundamental task in microbiology. Computer vision based approaches can increase the efficiency and the reliability of the process, but accurate counting is challenging, due to the high degree of variability of agglomerated colonies. In this paper, we propose a solution which adopts Convolutional Neural Networks (CNN) for counting the number of colonies contained in confluent agglomerates, that scored an overall accuracy of the 92.8% on a large challenging dataset. The proposed CNN-based technique for estimating the cardinality of colony aggregates outperforms traditional image processing approaches, becoming a promising approach to many related applications.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7320116,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320116,,Accuracy;Biological neural networks;Image segmentation;Microorganisms;Training;Transforms;Yttrium,blood;cellular biophysics;computer vision;image representation;image segmentation;medical image processing;microorganisms;neurophysiology,CNN-based technique;agglomerated colonies;agglomerates;bacterial colony counting;colony aggregates;computer vision;convolutional neural networks;microbiological culture plates;reliability;traditional image processing approaches,,2.0,,20.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
248,Interleaved text/image Deep Mining on a large-scale radiology database,H. C. Shin; Le Lu; L. Kim; A. Seff; J. Yao; R. M. Summers,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States",2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20151015.0,2015,,,1090,1099,"Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning “data-hungry” obstacle in the medical domain.",1063-6919;10636919,Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3,10.1109/CVPR.2015.7298712,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298712,,Machine learning;Medical diagnostic imaging;Radiology;Semantics;Visualization,PACS;computer vision;data mining;image retrieval;learning (artificial intelligence);medical image processing;radiology;recurrent neural nets;text analysis,3D medical volume;CNN;computer vision;data-hungry obstacle;deep convolutional neural network;document-level text;embedded vector label;extracted key image;interleaved text/image deep learning system;interleaved text/image deep mining;large-scale radiology database;latent Dirichlet allocation;national research hospital;picture archiving and communication system;radiology image;recurrent neural net language model;retrieval manner;semantic interaction;semantic label;sentence description;sentence-level text;unsupervised learning;very large-scale medical image database,,3.0,,47.0,,,,7-12 June 2015,,IEEE,IEEE Conference Publications
249,Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network,Y. Zou; L. Li; Y. Wang; J. Yu; Y. Li; W. J. Deng,"ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China",2015 IEEE International Conference on Digital Signal Processing (DSP),20150910.0,2015,,,1274,1278,"This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.",1546-1874;15461874,Electronic:978-1-4799-8058-1; POD:978-1-4799-8059-8; USB:978-1-4799-8057-4,10.1109/ICDSP.2015.7252086,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252086,deep convolutional neural network;digestive organs classification;parameter selection;wireless capsule endoscopy,Accuracy;Convolution;Endoscopes;Feature extraction;Intestines;Training;Wireless communication,biological organs;biomedical optical imaging;brightness;endoscopes;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;object recognition,DCNN-WCE-CS;complex digestive tract circumstance;deep CNN-based WCE classification system;deep convolutional neural network;digestive organ classification problem;human biological visual systems;layer-wise hierarchy models;luminance change;semantic image feature recognition;training data;wireless capsule endoscopy images,,2.0,,14.0,,,,21-24 July 2015,,IEEE,IEEE Conference Publications
250,Cellular nonlinear network-based signal prediction in epilepsy: Method comparison,V. Senger; R. Tetzlaff,"Chair of Fundamentals of Electrical Engineering, Faculty of Electrical Engineering and Information Technology, Technische Universit&#x00E4;t Dresden, Dresden, Germany",2015 IEEE International Symposium on Circuits and Systems (ISCAS),20150730.0,2015,,,397,400,"The seizure prediction problem has been addressed by many researchers from very different fields for more than three decades. The vision of an implantable seizure prediction device may become reality now: the first clinical study of such a device has been realized very recently and other realizations are not far behind. Cellular Nonlinear Networks (CNN) were firstly introduced by Chua and Yang in 1988 and later extended to an inherently parallel processing framework called the CNN Universal Machine (CNN-UM). This framework combines high computational power with low power consumption and miniaturized design - making it a very promising basis for the realization of a seizure warning device. In this contribution, we compare the seizure prediction performance of an eigenvalue based PCA-preprocessing followed by a nonlinear CNN signal prediction to the performance of a linear signal prediction approach followed by a level-crossing behavior analysis.",0271-4302;02714302,Electronic:978-1-4799-8391-9; POD:978-1-4799-8392-6,10.1109/ISCAS.2015.7168654,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168654,,Eigenvalues and eigenfunctions;Electrodes;Electroencephalography;Epilepsy;Prediction algorithms;Principal component analysis;Signal processing algorithms,diseases;eigenvalues and eigenfunctions;medical signal detection;neurophysiology;principal component analysis,CNN universal machine;cellular nonlinear network;cellular nonlinear networks;epilepsy;implantable seizure prediction device;level-crossing behavior analysis;nonlinear CNN signal prediction,,1.0,,18.0,,,,24-27 May 2015,,IEEE,IEEE Conference Publications
251,CNN in drug design — Recent developments,J. D. Wichard; M. J. Ogorzałek; C. Merkwirth,"Department of Investigational Toxicology, Bayer HealthCare, Berlin, Germany",2015 IEEE International Symposium on Circuits and Systems (ISCAS),20150730.0,2015,,,405,408,"We describe a method for construction of specific types of Neural Networks composed of structures directly linked to the structure of the molecule under consideration. Each molecule can be represented by a unique neural connectivity problem (graph) which can be programmed onto a Cellular Neural Network. The idea was to translate chemical structures like small organic molecules or peptides into a self learning environment which is CNN based. In the case of small molecules, each cell of the CNN stands for one atom of the molecule under consideration. But in contrast to the standard CNN architecture where each cell is connected to the neighboring cells, only those cells of the feature net are connected for which there also exists a chemical bond in the molecule under consideration. This implies that the feature net topology varies from molecule to molecule. In the case of peptides, the amino acids that form the building blocks of the peptide are reflected by the CNN cells wherein the amino acid sequence defines the network topology. Unlike the standard CNN used for image processing, there are no input values like the input image that are fed into the feature net. Instead, all information about the input molecule is supplied to the feature net by means of the topology. The output of several feature nets is fed into a supervisor neural network which computes the final output value. The combination of several feature nets and a supervisor networks constitutes the Molecular Graph Network (MGN). The designed networks are used for selection of molecules representing wanted properties such as activity against specific diseases, interactions with other compounds, toxicity etc. and possibly being candidates to be tested further as new drugs.",0271-4302;02714302,Electronic:978-1-4799-8391-9; POD:978-1-4799-8392-6,10.1109/ISCAS.2015.7168656,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168656,,Amino acids;Computer architecture;Microprocessors;Network topology;Peptides;Topology;Training,cellular neural nets;drugs;graph theory;medical computing;organic compounds;unsupervised learning,MGN;amino acid sequence;cellular neural network;chemical structures;drug design;feature nets;image processing;molecular graph network;network topology;neural connectivity problem;peptides;self learning environment;small organic molecules;standard CNN architecture;supervisor neural network,,0.0,,14.0,,,,24-27 May 2015,,IEEE,IEEE Conference Publications
252,Iteratively training classifiers for circulating tumor cell detection,Y. Mao; Z. Yin; J. M. Schober,Missouri University of Science and Technology,2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,190,194,"The number of Circulating Tumor Cells (CTCs) in blood provides an indication of disease progression and tumor response to chemotherapeutic agents. Hence, routine detection and enumeration of CTCs in clinical blood samples have significant applications in early cancer diagnosis and treatment monitoring. In this paper, we investigate two classifiers for image-based CTC detection: (1) Support Vector Machine (SVM) with hard-coded Histograms of Oriented Gradients (HoG) features; and (2) Convolutional Neural Network (CNN) with automatically learned features. For both classifiers, we present an effective and efficient training algorithm, by which the most representative negative samples are iteratively collected to accurately define the classification boundary between positive and negative samples. The two iteratively trained classifiers are validated on a challenging dataset with high performance.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163847,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163847,circulating tumor cells;convolutional neural network;iterative training;support vector machine,Blood;Cancer;Cells (biology);Feature extraction;Support vector machines;Training;Tumors,biomedical optical imaging;cancer;cellular biophysics;image classification;iterative methods;medical image processing;neural nets;support vector machines;tumours,CNN;HoG features;SVM;cancer diagnosis;cancer treatment monitoring;chemotherapeutic agents;circulating tumor cell detection;clinical blood samples;convolutional neural network;disease progression;hard-coded histogram-of-oriented gradients;image-based CTC detection;iterative training classification;support vector machine;tumor response,,1.0,,14.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
253,Region segmentation in histopathological breast cancer images using deep convolutional neural network,H. Su; F. Liu; Y. Xie; F. Xing; S. Meyyappan; L. Yang,J. Crayton Pruitt Family Dept. of Biomedical Engineering,2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,55,58,"Computer aided diagnosis of breast cancers often relies on automatic image analysis of histopathology images. The automatic region segmentation in breast cancer is challenging due to: i) large regional variations, and ii) high computational costs of pixel-wise segmentation. Deep convolutional neural network (CNN) is proven to be an effective method for image recognition and classification. However, it is often computationally expensive. In this paper, we propose to apply a fast scanning deep convolutional neural network (fCNN) to pixel-wise region segmentation. The fCNN removes the redundant computations in the original CNN without sacrificing its performance. In our experiment it takes only 2.3 seconds to segment an image with size 1000 × 1000. The comparison experiments show that the proposed system outperforms both the LBP feature-based and texton-based pixel-wise methods.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163815,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163815,,Breast cancer;Image segmentation;Kernel;Neural networks;Scalability;Training,cancer;image classification;image recognition;image segmentation;medical image processing;neural nets,LBP feature-based methods;automatic image analysis;breast cancers;computational costs;computer aided diagnosis;deep convolutional neural network;fCNN;fast scanning deep convolutional neural network;histopathological breast cancer images;image classification;image recognition;local binary pattern;pixel-wise segmentation;region segmentation;regional variations;texton-based pixel-wise methods,,3.0,,15.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
254,Automatic muscle perimysium annotation using deep convolutional neural network,M. Sapkota; F. Xing; H. Su; L. Yang,"Department of Electrical and Computer Engineering, University of Florida",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,205,208,"Diseased skeletal muscle expresses mononuclear cell infiltration in the regions of perimysium. Accurate annotation or segmentation of perimysium can help biologists and clinicians to determine individualized patient treatment and allow for reasonable prognostication. However, manual perimysium annotation is time consuming and prone to inter-observer variations. Meanwhile, the presence of ambiguous patterns in muscle images significantly challenge many traditional automatic annotation algorithms. In this paper, we propose an automatic perimysium annotation algorithm based on deep convolutional neural network (CNN). We formulate the automatic annotation of perimysium in muscle images as a pixel-wise classification problem, and the CNN is trained to label each image pixel with raw RGB values of the patch centered at the pixel. The algorithm is applied to 82 diseased skeletal muscle images. We have achieved an average precision of 94% on the test dataset.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163850,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163850,Perimysium annotation;convolutional neural network;muscle,Image segmentation;Kernel;Muscles;Neural networks;Noise measurement;Testing;Training,convolution;diseases;image classification;image segmentation;medical image processing;muscle;neural nets,RGB value;automatic muscle perimysium annotation algorithm;deep convolutional neural network;image pixel;manual perimysium annotation;mononuclear cell infiltration;muscle images;patient treatment;perimysium segmentation;pixel-wise classification;skeletal muscle disease,,0.0,,17.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
255,Deep learning for automatic cell detection in wide-field microscopy zebrafish images,B. Dong; L. Shao; M. Da Costa; O. Bandmann; A. F. Frangi,Centre of Computational Imaging & Simulation Technologies in Biomedicine (CISTIB),2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,772,776,"The zebrafish has become a popular experimental model organism for biomedical research. In this paper, a unique framework is proposed for automatically detecting Tyrosine Hydroxylase-containing (TH-labeled) cells in larval zebrafish brain z-stack images recorded through the wide-field microscope. In this framework, a supervised max-pooling Convolutional Neural Network (CNN) is trained to detect cell pixels in regions that are preselected by a Support Vector Machine (SVM) classifier. The results show that the proposed deep-learned method outperforms hand-crafted techniques and demonstrate its potential for automatic cell detection in wide-field microscopy z-stack zebrafish images.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163986,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163986,,Computer architecture;Histograms;Microprocessors;Microscopy;Neurons;Three-dimensional displays;Training,biomedical optical imaging;brain;cellular biophysics;convolution;enzymes;feature extraction;image classification;learning (artificial intelligence);medical image processing;molecular biophysics;neural nets;neurophysiology;optical microscopy;support vector machines,SVM classifier;automatic TH-labeled cell detection;automatic tyrosine hydroxylase-containing cell detection;biomedical research;cell pixel detection;convolutional neural network;deep learning;experimental model organism;hand-crafted technique;larval zebrafish brain z-stack image recording;region preselection;supervised max-pooling CNN training;support vector machine;wide-field microscopy,,3.0,,26.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
256,A Reliable Distributed Convolutional Neural Network for Biology Image Segmentation,X. Zhang; G. Tan; M. Chen,,"2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",20150709.0,2015,,,777,780,"Many modern advanced biology experiments are carried on by Electron Microscope(EM) image analysis. Segmentation is one of the most important and complex steps in the process of image analysis. Previous ISBI contest results and related research show that Convolution Neural Network(CNN)has high classification accuracy in EM image segmentation. Besides it eliminates the pain of extracting complex features which's indispensable for traditional classification algorithms. However CNN's extremely time-consuming and fault vulnerability due to long time execution prevent it from being widely used in practice. In this paper, we try to address these problems by providing reliable high performance CNN framework for medial image segmentation. Our CNN has light weighted user level checkpoint, which costs seconds when doing one checkpoint and restart. On the fact of lacking in platform diversity in current parallel CNN framework, our CNN system tries to make it general by providing distributed cross-platform parallelism implementation. Currently we have integrated Theano's GPU implementation in our CNNsystem, and we explore parallelism potential on multi-core CPUs and many-core Intel Phi by testing performance of main kernel functions of CNN. In the future, we will integrate implementation son other two platforms into our CNN framework.",,Electronic:978-1-4799-8006-2; POD:978-1-4799-8007-9,10.1109/CCGrid.2015.108,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152555,Convolutional Neural Network;Distributed system;Faula tolerant;GPU;Image segmentation;Intel Phi,Graphics processing units;Image segmentation;Microscopy;Microwave integrated circuits;Parallel processing;Reliability;Training,electron microscopes;feature extraction;image classification;image segmentation;medical image processing;neural nets,EM image segmentation;advanced biology experiments;biology image segmentation;classification algorithms;complex feature extraction;distributed cross-platform parallelism;electron microscope image analysis;fault vulnerability;high performance CNN framework;image processing analysis;integrated Theano GPU;kernel functions;lightweighted user level checkpoint;many-core Intel Phi;medial image segmentation;parallel CNN framework;reliable distributed convolutional neural network,,0.0,,16.0,,,,4-7 May 2015,,IEEE,IEEE Conference Publications
257,Morphological process based segmentation for the detection of exudates from the retinal images of diabetic patients,Mahendran G.; Dhanasekaran R.; Narmadha Devi K. N.,"Syed Ammal Engineering College, Ramanathapuram, TamilNadu. India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies",20150126.0,2014,,,1466,1470,"Diabetic Retinopathy is an ocular systemic disease caused by complication of diabetes. It is a major cause of blindness in both middle and advanced age group. Earlier recognition of diabetic retinopathy shields understanding from visual impairment. The heading side effect of this difficulty seeing is the exudates. Exudates are the melted watery grasping solutes, proteins, cells, or cell garbage spilled from the harmed veins into near by tissues or on tissue surfaces in the retina. The spillage of these proteins or lipids causes vision misfortune to the patients. Distinguishing the exudates ahead of time can protect the diabetic patients from difficulty seeing. Ophthalmologists use widening system to identify the exudates. But it causes the irritation to the patients' eyes. This paper focuses on an automated method which detects the diabetic retinopathy through identifying exudates by Morphological process in colour fundus retinal images and then segregates the severity of the lesions. The severity level of the disease was achieved by Cascade Neural Network (CNN) classifier.",,CD-ROM:978-1-4799-3913-8; Electronic:978-1-4799-3914-5; POD:978-1-4799-3915-2,10.1109/ICACCCT.2014.7019345,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019345,Cascade Neural Network;Diabetic Retinopathy;Dilation;Erosion;Exudates,Biomedical imaging;Character recognition;Image edge detection;Image segmentation;Lesions;Proteins;Retinopathy,diseases;image segmentation;medical image processing;neural nets,cascade neural network classifier;colour fundus retinal images;diabetic patients;diabetic retinopathy;morphological process based segmentation;ocular systemic disease;retinal images,,2.0,,17.0,,,,8-10 May 2014,,IEEE,IEEE Conference Publications
258,An approach for chest tube detection in chest radiographs,C. A. Mercan; M. S. Celebi,"Inf. Inst., Istanbul Tech. Univ. (ITU), Istanbul, Turkey",IET Image Processing,20140210.0,2014,8,2.0,122,129,"It is known that overlapping tissues cause highly complex projections in chest radiographs. In addition, artificial objects, such as catheters, chest tubes and pacemakers can appear on these radiographs. It is important that the anomaly detection algorithms are not confused by these objects. To achieve this goal, the authors propose an approach to train a convolutional neural network (CNN) to detect chest tubes present on radiographs. To detect the chest tube skeleton as the final output in a better manner, non-uniform rational B-spline curves are used to automatically fit with the CNN output. This is the first study conducted to automatically detect artificial objects in the lung region of chest radiographs. Other automatic detection schemes work on the mediastinum. The authors evaluated the performance of the model using a pixel-based receiver operating characteristic (ROC) analysis. Each true positive, true negative, false positive and false negative pixel is counted and used for calculating average accuracy, sensitivity and specificity percentages. The results were 99.99% accuracy, 59% sensitivity and 99.99% specificity. Therefore they obtained promising results on the detection of artificial objects.",1751-9659;17519659,,10.1049/iet-ipr.2013.0239,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733843,,,diagnostic radiography;learning (artificial intelligence);lung;medical image processing;object detection;sensitivity analysis;splines (mathematics),anomaly detection algorithm;artificial object detection;automatic detection scheme;catheter;chest radiograph;chest tube skeleton detection;convolutional neural network;false negative pixel;false positive pixel;lung region;mediastinum;nonuniform rational B-spline curve;pacemaker;pixel-based ROC analysis;tissue;true negative pixel;true positive pixel,,0.0,,,,,,February 2014,,IET,IET Journals & Magazines
259,Probabilistic visual search for masses within mammography images using deep learning,M. G. Ertosun; D. L. Rubin,"Department of Radiology, Stanford School of Medicine, CA USA",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,1310,1315,"We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85% accuracy for the task of identifying images that contain a mass, and we were able to localize 85% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359868,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359868,Breast Cancer;CAD;Classification;Deep Learning;Detection;Mammography;Visual Search,Breast;Engines;Informatics;Mammography;Visualization,biomedical engineering;learning (artificial intelligence);mammography,automated search;classification engine;deep learning classifier;deep learning network;deep learning-based visual search system;localization engine;mammography images;mass localization,,,,21.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
260,Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation,H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA",IEEE Transactions on Medical Imaging,20160503.0,2016,35,5.0,1170,1181,"Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.",0278-0062;02780062,,10.1109/TMI.2015.2482920,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156,Computer aided diagnosis;artificial neural networks;computed tomography;deep learning;machine learning;medical diagnostic imaging;multi-layer neural network;object detection,Colonic polyps;Computed tomography;Feature extraction;Lymph nodes;Three-dimensional displays;Training,computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability,classification probability;colonic polyp detection;computed tomography;computer-aided detection;deep convolutional neural network classifier training;false positives;lymph node detection;medical imaging;random rotations;random translations;random view aggregation;scale transformations;sclerotic metastasis detection;two-tiered coarse-to-fine cascade framework,,11.0,,60.0,,,20150928.0,May 2016,,IEEE,IEEE Journals & Magazines
261,DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks,N. Pezzotti; T. Höllt; J. v. Gemert; B. P. F. Lelieveldt; E. Eisemann; A. Vilanova,"Intelligent Systems department, Delft University of Technology, Delft, the Netherlands",IEEE Transactions on Visualization and Computer Graphics,,2017,PP,99.0,1,1,"Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.",1077-2626;10772626,,10.1109/TVCG.2017.2744358,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019872,Progressive visual analytics;deep neural networks;machine learning,Kernel;Layout;Neural networks;Neurons;Three-dimensional displays;Training;Visual analytics,,,,,,,,,20170829.0,,,IEEE,IEEE Early Access Articles
262,Efficient Training of Convolutional Deep Belief Networks in the Frequency Domain for Application to High-Resolution 2D and 3D Images,T. Brosch; R. Tam,"MS/MRI Research Group, Vancouver, BC V6T 2B5, Canada, and Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada brosch.tom@gmail.com",Neural Computation,20141224.0,2015,27,1.0,211,227,"<para>Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8 times on 2D images and up to 200 times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128 × 128 × 128 voxels practical, which opens new directions for using deep learning for medical image analysis.</para>",0899-7667;08997667,,10.1162/NECO_a_00682,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998135,,,,,,3.0,,,,,,Jan. 2015,,MIT Press,MIT Press Journals
263,Biopsy-guided learning with deep convolutional neural networks for Prostate Cancer detection on multiparametric MRI,Y. Tsehay; N. Lay; X. Wang; J. T. Kwak; B. Turkbey; P. Choyke; P. Pinto; B. Wood; R. M. Summers,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Department of Radiology and Imaging Science, National Institute of Health, Clinical Center, Bethesda, MD 20892, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,642,645,"Prostate Cancer (PCa) is highly prevalent and is the second most common cause of cancer-related deaths in men. Multiparametric MRI (mpMRI) is robust in detecting PCa. We developed a weakly supervised computer-aided detection (CAD) system that uses biopsy points to learn to identify PCa on mpMRI. Our CAD system, which is based on a deep convolutional neural network architecture, yielded an area under the curve (AUC) of 0.903±0.009 on a receiver operation characteristic (ROC) curve computed on 10 different models in a 10 fold cross-validation. 9 of the 10 ROCs were statistically significantly different from a competing support vector machine based CAD, which yielded a 0.86 AUC when tested on the same dataset (α = 0.05). Furthermore, our CAD system proved to be more robust in detecting high-grade transition zone lesions.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950602,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950602,Biopsy Database;Computer-Aided Detection;Holistically-nested Edge Detection;Prostate;Prostate-CAD;Radiology,Biopsy;Databases;Lesions;Principal component analysis;Solid modeling;Training,biomedical MRI;cancer;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis;support vector machines,biopsy-guided learning;computer-aided detection;deep convolutional neural network;multiparametric MRI;prostate cancer detection;receiver operation characteristic curve;support vector machine,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
264,Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse Patch Matching,Y. Guo; Y. Gao; D. Shen,"Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA",IEEE Transactions on Medical Imaging,20160331.0,2016,35,4.0,1077,1089,"Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods.",0278-0062;02780062,,10.1109/TMI.2015.2508280,10.13039/100000002 - National Institutes of Health; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353170,Deformable model;MR prostate segmentation;sparse patch matching;stacked sparse auto-encoder (SSAE),Biomedical imaging;Cancer;Deformable models;Feature extraction;Image segmentation;Machine learning;Shape,biological organs;biomedical MRI;cancer;feature extraction;image matching;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy,T2-wighted prostate MR images;accurate MR prostate localization;clinical applications;dataset;deep feature learning;deformable MR prostate segmentation;feature hierarchy;handcrafted features;inconsistent prostate boundary appearance;inhomogeneous prostate boundary appearance;latent feature representation;prostate cancer radiotherapy;prostate likelihood map;shape variation;sparse patch matching;stacked sparse autoencoder;state-of-the-art segmentation methods,,5.0,,56.0,,,20151211.0,April 2016,,IEEE,IEEE Journals & Magazines
265,Combining fully convolutional networks and graph-based approach for automated segmentation of cervical cell nuclei,L. Zhang; M. Sonka; L. Lu; R. M. Summers; J. Yao,"Radiology and Imaging Sciences Department, National Institutes of Health (NIH), Bethesda MD, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,406,409,"Cervical nuclei carry substantial diagnostic information for cervical cancer. Therefore, in automation-assisted reading of cervical cytology, automated and accurate segmentation of nuclei is essential. This paper proposes a novel approach for segmentation of cervical nuclei that combines fully convolutional networks (FCN) and graph-based approach (FCNG). FCN is trained to learn the nucleus high-level features to generate a nucleus label mask and a nucleus probabilistic map. The mask is used to construct a graph by image transforming. The map is formulated into the graph cost function in addition to the properties of the nucleus border and nucleus region. The prior constraints regarding the context of nucleus-cytoplasm position are also utilized to modify the local cost functions. The globally optimal path in the constructed graph is identified by dynamic programming. Validation of our method was performed on cell nuclei from Herlev Pap smear dataset. Our method shows a Zijdenbos similarity index (ZSI) of 0.92 ± 0.09, compared to the best state-of-the-art approach of 0.89 ± 0.15. The nucleus areas measured by our method correlated strongly with the independent standard (r<sup>2</sup> = 0.91).",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950548,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950548,Deep learning;FCN;Pap smear;graph-based segmentation,Computer architecture;Context;Cost function;Image segmentation;Imaging;Microprocessors;Shape,cancer;cellular biophysics;dynamic programming;image segmentation;medical image processing,Herlev Pap smear dataset;Zijdenbos similarity index;automated segmentation;automation-assisted reading;cervical cancer;cervical cell nuclei;cervical cytology;diagnostic information;dynamic programming;fully convolutional networks;graph cost function;graph-based approach;image transforming;nucleus high-level features;nucleus label mask;nucleus probabilistic map;nucleus-cytoplasm position,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
266,Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease,S. Liu; S. Liu; W. Cai; H. Che; S. Pujol; R. Kikinis; D. Feng; M. J. Fulham; ADNI,"Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney, Sydney, N.S.W., Australia",IEEE Transactions on Biomedical Engineering,20150318.0,2015,62,4.0,1132,1140,"The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.",0018-9294;00189294,,10.1109/TBME.2014.2372011,AADRF; ARC; NA-MIC; NAC; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963480,Alzheimer’s Disease;Alzheimer's disease (AD);Classification;Deep Learning;MRI;Neuroimaging;PET;classification;deep Learning;neuroimaging;positron emission tomography (PET),Biomarkers;Diseases;Feature extraction;Neuroimaging;Neurons;Positron emission tomography;Training,biomedical MRI;diseases;learning (artificial intelligence);neurophysiology;patient care;patient diagnosis;positron emission tomography;sensor fusion,Alzheimer disease binary classification;Alzheimer disease computer-aided diagnosis;Alzheimer disease multiclass classification;Alzheimer disease multiclass diagnosis;data fusion;machine learning method;multimodal neuroimaging feature learning;neuroimaging biomarker;patient care;zero-masking strategy,"Alzheimer Disease;Brain;Humans;Image Interpretation, Computer-Assisted;Multimodal Imaging;Neuroimaging;Support Vector Machine",27.0,,65.0,,,20141120.0,April 2015,,IEEE,IEEE Journals & Magazines
267,Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique,H. Greenspan; B. van Ginneken; R. M. Summers,"Biomedical Image Computing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel-Aviv University, Tel-Aviv, Israel",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1153,1159,"The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.",0278-0062;02780062,,10.1109/TMI.2016.2553401,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094,,Artificial neural networks;Biomedical image processing;Computer vision;Data analysis;Machine learning;Special issues and sections,,,,22.0,,38.0,,,,May 2016,,IEEE,IEEE Journals & Magazines
268,Exploring deep features from brain tumor magnetic resonance images via transfer learning,Renhao Liu; L. O. Hall; D. B. Goldgof; Mu Zhou; R. A. Gatenby; K. B. Ahmed,"Department of Computer Science and Engineering, University of South Florida, Tampa, USA",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,235,242,"Finding appropriate feature representations from radiological images is a vital task for prediction and diagnosis. Deep convolutional neural networks have recently achieved state-of-the-art performance in classification problems from several different domains. Research has also shown the feasibility of using a pre-trained deep neural network as a feature extractor when only a small dataset is available. This paper proposes a novel image feature extraction method for predicting survival time from brain tumor magnetic resonance images using pretrained deep neural networks. Since all tumors are different sizes, we also explore different image resizing methods in the paper. We demonstrate that deep features can result in better survival time prediction with the highest accuracy of 95.45% versus conventional feature extraction methods from magnetic resonance images of the brain.",,,10.1109/IJCNN.2016.7727204,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727204,,Histograms;Magnetic resonance imaging,biomedical MRI;brain;feature extraction;image representation;learning (artificial intelligence);medical image processing;neural nets;tumours,brain tumor magnetic resonance images;deep features;feature representations;image resizing methods;novel image feature extraction method;pretrained deep convolutional neural networks;survival time prediction;transfer learning,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
269,A comparison of deep learning and hand crafted features in medical image modality classification,S. Khan; S. P. Yong,"Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Malaysia",2016 3rd International Conference on Computer and Information Sciences (ICCOINS),20161215.0,2016,,,633,638,"Modality corresponding to medical images is a vital filter in medical image retrieval systems, as radiologists or physicians are interested in only one of radiology images e.g CT scan, MRI, X-ray. Various handcrafted feature schemes have been proposed for medical image modality classification. On the other hand not enough attempts have been made for deep learned feature extraction. A comparative evaluation of both handcrafted and deep learned features for medical image modality classification is presented in this paper. The experiments are performed on IMAGECLEF 2012 data. After carrying out the experiments it is shown that the handcrafted features outperforms the deep learned features and shows the potential of handcrafted feature extraction models in the medical image field.",,Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1,10.1109/ICCOINS.2016.7783289,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783289,Feature representations;deep learned features;handcrafted features;modality classification,Biomedical imaging;Computer architecture;Computers;Feature extraction;Machine learning;Visualization,feature extraction;image classification;image retrieval;learning (artificial intelligence);medical image processing;radiology,IMAGECLEF 2012 data;deep learned feature extraction;hand crafted features;handcrafted feature extraction models;medical image modality classification;medical image retrieval systems;radiology images,,,,,,,,15-17 Aug. 2016,,IEEE,IEEE Conference Publications
270,Cloud-based deep learning of big EEG data for epileptic seizure prediction,M. P. Hosseini; H. Soltanian-Zadeh; K. Elisevich; D. Pompili,"Dept. of Electrical and Computer Engineering, Rutgers University-New Brunswick, NJ, USA",2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP),20170424.0,2016,,,1151,1155,"Developing a Brain-Computer Interface (BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.",,Electronic:978-1-5090-4545-7; POD:978-1-5090-4546-4; USB:978-1-5090-4544-0,10.1109/GlobalSIP.2016.7906022,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906022,Big Data;Brain-Computer Interface;Cloud Computing;Deep Learning;EEG;Epilepsy;Seizure Prediction,Big Data;Cloud computing;Electrodes;Electroencephalography;Epilepsy;Feature extraction;Real-time systems,Big Data;biomedical electrodes;brain-computer interfaces;cloud computing;data analysis;data reduction;electroencephalography;feature extraction;medical signal processing;signal classification;unsupervised learning,Big EEG Data analysis;benchmark clinical dataset;brain data recording;brain-computer interface;classification accuracy;cloud-based BCI system;cloud-based deep learning approach;cloud-computing;communication bandwidth;dimensionality-reduction technique;epileptic patients;epileptic seizure prediction;implanted electrodes;manual feature extraction;nonstationary EEG signals;stacked autoencoder;unsupervised feature extraction,,,,,,,,7-9 Dec. 2016,,IEEE,IEEE Conference Publications
271,Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning,G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen,"Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA",IEEE Transactions on Biomedical Engineering,20160621.0,2016,63,7.0,1505,1516,"Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.",0018-9294;00189294,,10.1109/TBME.2015.2496253,10.13039/100000071 - National Institute of Child Health and Human Development; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314894,Deep learning;Deformable image registration;deep learning;deformable image registration;hierarchical feature representation,Biomedical imaging;Feature extraction;Image registration;Machine learning;Three-dimensional displays;Unsupervised learning,,,,3.0,,70.0,,,20151102.0,July 2016,,IEEE,IEEE Journals & Magazines
272,The importance of stain normalization in colorectal tissue classification with convolutional networks,F. Ciompi; O. Geessink; B. E. Bejnordi; G. S. de Souza; A. Baidoshvili; G. Litjens; B. van Ginneken; I. Nagtegaal; J. van der Laak,"Dept. of Pathology, Radboud University Medical Center, Nijmegen, Netherlands",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,160,163,"The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950492,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950492,Colorectal Cancer;Deep learning;Digital pathology,Algorithm design and analysis;Biomarkers;Blood;Cancer;Image color analysis;Training;Tumors,biological tissues;cancer;image classification;medical image processing;neural nets,ConvNets;colorectal tissue classification;convolutional networks;hematoxylin-eosin stained histopathology images;imaging biomarkers;stain normalization,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
273,Investigation of transfer learning on pulmonary nodule characteristics,A. Kaya; A. S. Keçeli; A. B. Can,"Hacettepe &#x00DC;niversitesi, Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Ankara, T&#x00FC;rkiye",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"Studies on the classification of small pulmonary nodules generally focus on the prediction of malignancy of the nodule. In the recent years, publicly available databases provided different types of data to researchers, such as nodule characteristics, apart from the lung image and malignancy degree. In this paper, a study on the classification of pulmonary nodule characteristics using conventional features and deep features obtained from transfer learning method has been proposed. The results were assessed by sensitivity, specificity, and classification accuracy. The results of the study can be used to form multi-level classifiers in predicting malignancy by combining different types of features.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960357,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960357,malignancy prediction;nodule characteristics;pulmonary nodules;tansfer learning,Biomedical imaging;Computed tomography;Image databases;Lungs;Machine learning;Radio frequency;Support vector machines,cancer;image classification;learning (artificial intelligence);lung;medical image processing;radiology,conventional features;deep features;lung image;malignancy prediction;pulmonary nodule characteristics classification;transfer learning,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
274,Segmentation label propagation using deep convolutional neural networks and dense conditional random field,M. Gao; Z. Xu; L. Lu; A. Wu; I. Nogues; R. M. Summers; D. J. Mollura,"Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1265,1268,"Availability and accessibility of large-scale annotated medical image datasets play an essential role in robust supervised learning of medical image analysis. Missed labeling of regions of interest is a common issue on existing medical image datasets due to the labor intensive nature of the annotation task which requires high levels of clinical proficiency. In this paper, we present a segmentation based label propagation method to a publicly available dataset on interstitial lung disease [3], to address the missing annotation challenge. Upon validation from an expert radiologist, the amount of available annotated training data is largely increased. Such a dataset expansion can can potentially increase the accuracy of Computer-aided Detection (CAD) systems. The proposed constrained segmentation propagation algorithm combines the cues from the initial annotations, deep convolutional neural networks and a dense fully-connected Conditional Random Field (CRF) that achieves high quantitative accuracy levels.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493497,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493497,Convolutional Neural Network;Dense Conditional Random Field;Interstitial Lung Disease;Multi-class Labeling;Segmentation Label Propagation,Biomedical imaging;Computed tomography;Image segmentation;Labeling;Lungs;Message passing;Neural networks,diseases;image segmentation;interstitials;learning (artificial intelligence);lung;medical image processing;neurophysiology,CAD systems;annotation task;computer-aided detection systems;dataset expansion;deep convolutional neural networks;dense conditional random field;dense fully-connected conditional Random field;high quantitative accuracy levels;interstitial lung disease;labor intensive nature;large-scale annotated medical image datasets;medical image analysis;medical image datasets;missing annotation challenge;regions of interest;robust supervised learning;segmentation based label propagation method;segmentation label propagation;segmentation propagation algorithm,,2.0,,11.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
275,Deep random forest-based learning transfer to SVM for brain tumor segmentation,S. Amiri; I. Rekik; M. A. Mahjoub,"SAGE laboratory, Higher Institute of Computer Science and Communication Techniques of Hammam sousse, Tunisia",2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),20160728.0,2016,,,297,302,"Using neuroimaging techniques to diagnose brain tumors and detect both visible and invisible cancer cells infiltration boundaries motivated the emergence of diverse tumor segmentation algorithms. Noting the large variability in both tumor appearance and shape, the task of automatic segmentation becomes more difficult. In this paper, we propose a random-forest (RF) based learning transfer to SVM classifier method for segmenting tumor lesions while capturing their complex characteristics. Our framework is composed of two cascaded stages. In the first stage, we train a random forest to learn the mapping from the image space to the tumor label space. In the testing stage, we use the predicted label output from the random forest and feed it along with the testing intensity image to an SVM classifier to get the refined segmentation. Then we make our RF-SVM cascaded classification steps deep through an iterative process. We tested our method on 20 patients with high-grade gliomas from the Brain Tumor Image Segmentation Challenge (BRATS) dataset. Our proposed framework significantly outperformed SVM-based segmentation and RF-based segmentation-when used solely.",,Electronic:978-1-4673-8526-8; POD:978-1-4673-8527-5,10.1109/ATSIP.2016.7523095,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523095,Brain tumor;MRI;Random Forest;SVM;Segmentation,Image segmentation;Lesions;Magnetic resonance imaging;Radio frequency;Support vector machines;Vegetation,brain;cancer;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neurophysiology;support vector machines;tumours,BRATS;RF-SVM cascaded classification;brain tumor diagnosis;brain tumor image segmentation;cancer cells infiltration boundaries;deep random forest-based learning transfer;iterative process;neuroimaging techniques,,,,,,,,21-23 March 2016,,IEEE,IEEE Conference Publications
276,Deep feature learning for pulmonary nodule classification in a lung CT,B. C. Kim; Y. S. Sung; H. I. Suk,"Department of Brain and Cognitive Engineering, Korea University, Republic of Korea",2016 4th International Winter Conference on Brain-Computer Interface (BCI),20160421.0,2016,,,1,3,"In this paper, we propose a novel method of identifying pulmonary nodules in a lung CT. Specifically, we devise a deep neural network by which we extract abstract information inherent in raw hand-crafted imaging features. We then combine the deep learned representations with the original raw imaging features into a long feature vector. By taking the combined feature vectors, we train a classifier, preceded by a feature selection via t-test. To validate the effectiveness of the proposed method, we performed experiments on our in-house dataset of 20 subjects; 3,598 pulmonary nodules (malignant: 178, benign: 3,420), which were manually segmented by a radiologist. In our experiments, we achieved the maximal accuracy of 95.5%, sensitivity of 94.4%, and AUC of 0.987, outperforming the competing method.",,Electronic:978-1-4673-7842-0; POD:978-1-4673-7843-7,10.1109/IWW-BCI.2016.7457462,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457462,Deep learning;Lung cancer;Pulmonary nodule classification;Stacked denoising autoencoder,Cancer;Computed tomography;Feature extraction;Lungs;Noise reduction;Training,computerised tomography;feature extraction;feature selection;image classification;learning (artificial intelligence);lung;medical image processing;neural nets,abstract information extraction;classifier training;deep feature learning;deep learned representations;deep neural network;feature selection;feature vector;hand-crafted imaging features;lung CT;pulmonary nodule classification;pulmonary nodule identification;raw imaging features;t-test,,,,13.0,,,,22-24 Feb. 2016,,IEEE,IEEE Conference Publications
277,Brain tumor grading based on Neural Networks and Convolutional Neural Networks,Y. Pan; W. Huang; Z. Lin; W. Zhu; J. Zhou; J. Wong; Z. Ding,"School of EEE, Nanyang Technological University, Singapore",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,699,702,"This paper studies brain tumor grading using multiphase MRI images and compares the results with various configurations of deep learning structure and baseline Neural Networks. The MRI images are used directly into the learning machine, with some combination operations between multiphase MRIs. Compared to other researches, which involve additional effort to design and choose feature sets, the approach used in this paper leverages the learning capability of deep learning machine. We present the grading performance on the testing data measured by the sensitivity and specificity. The results show a maximum improvement of 18% on grading performance of Convolutional Neural Networks based on sensitivity and specificity compared to Neural Networks. We also visualize the kernels trained in different layers and display some self-learned features obtained from Convolutional Neural Networks.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318458,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318458,,Artificial neural networks;Biological neural networks;Image segmentation;Kernel;Sensitivity and specificity;Training;Tumors,biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;tumours,baseline neural networks;brain tumor grading;convolutional neural networks;deep learning machine;deep learning structure;grading performance;multiphase MRI imaging;self-learned features;testing data,,,,8.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
278,A new NMF-autoencoder based CAD system for early diagnosis of prostate cancer,I. Reda; A. Shalaby; M. A. El-Ghar; F. Khalifa; M. Elmogy; A. Aboulfotouh; E. Hosseini-Asl; A. El-Baz; R. Keynton,"Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1237,1240,"In this paper, we propose a novel non-invasive framework for the early diagnosis of prostate cancer from diffusion-weighted magnetic reasoning imaging (DW-MRI). The proposed approach consists of three main steps. In the first step, the prostate is localized and segmented based on a new level-set model. This model is guided by a stochastic speed function that is derived using nonnegative matrix factorization (NMF). The NMF attributes are calculated using information from the MRI intensity, a probabilistic shape model, and the spatial interactions between prostate voxels. In the second step, the apparent diffusion coefficient (ADC) of the segmented prostate volume is mathematically calculated for different b-values. To preserve continuity, the calculated ADC values are normalized and refined using a Generalized Gauss-Markov Random Field (GGMRF) image model. The cumulative distribution function (CDF) of refined ADC for the prostate tissues at different b-values are then constructed. These CDFs are considered as global features which can be used to distinguish between benign and malignant tumors. Finally, a deep learning auto-encoder network, trained by a non-negativity constraint algorithm (NCAE), is used to classify the prostate tumor as benign or malignant based on the CDFs extracted from the previous step. Preliminary experiments on 42 clinical DW-MRI data sets resulted in 97.6% correct classification (sensitivity = 100% and specificity = 95.24%), indicating the high accuracy of the proposed framework.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493490,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493490,CAD;MGRF;NMF;Prostate cancer,Design automation;Image segmentation;Magnetic resonance imaging;Prostate cancer;Shape;Solid modeling,Markov processes;biodiffusion;biomedical MRI;cancer;feature extraction;image classification;image coding;image segmentation;learning (artificial intelligence);matrix decomposition;medical image processing;probability;random processes;set theory;tumours,DW-MRI;MRI intensity;NMF-autoencoder based CAD system;apparent diffusion coefficient;benign tumor;cumulative distribution function;deep learning autoencoder network;diffusion-weighted magnetic reasoning imaging;generalized Gauss-Markov random field image model;level-set model;malignant tumor;noninvasive framework;nonnegative matrix factorization;nonnegativity constraint algorithm;probabilistic shape model;prostate cancer early diagnosis;prostate tissues;prostate tumor classification;prostate voxels;segmented prostate volume;spatial interactions;stochastic speed function,,2.0,,21.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
279,DeepPap: Deep Convolutional Networks for Cervical Cell Classification,L. Zhang; L. Lu; I. Nogues; R. Summers; S. Liu; J. Yao,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA.(email:ling.zhang3@nih.gov)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into ”abnormal” and ”normal” categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells – without prior segmentation – based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.",2168-2194;21682194,,10.1109/JBHI.2017.2705583,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065,Cell classification;Cervical cytology;Deep learning;Neural networks;Pap smear,Feature extraction;Image segmentation;Imaging;Informatics;Neural networks;Testing;Training,,,,,,,,,20170519.0,,,IEEE,IEEE Early Access Articles
280,A Robust Deep Model for Improved Classification of AD/MCI Patients,F. Li; L. Tran; K. H. Thung; S. Ji; D. Shen; J. Li,"Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA",IEEE Journal of Biomedical and Health Informatics,20170520.0,2015,19,5.0,1610,1616,"Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.",2168-2194;21682194,,10.1109/JBHI.2015.2429556,10.13039/501100001677 - NIH grants; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101222,Alzheimer’s Disease;Alzheimer's disease (AD);Deep Learning;Early Diagnosis;MRI;PET;deep learning;early diagnosis;magnetic resonance imaging (MRI);positron emission tomography (PET),Computational modeling;Feature extraction;Magnetic resonance imaging;Positron emission tomography;Principal component analysis;Support vector machines;Training,biomedical MRI;cognition;diseases;learning (artificial intelligence);medical computing;neurophysiology;positron emission tomography,AD conversion diagnosis;AD diagnosis;AD patients;ADNI dataset;Alzheimer's disease;MCI conversion diagnosis;MCI patients;MRI;PET scans;adaptive learning factor;classical deep learning method;classification accuracy;deep learning framework;dropout technique;improved classification;memory impairment;mild cognitive impairment;multitask learning strategy;noninvasive imaging biomarkers;prodromal stage;progression stages;quality of life;robust deep learning system;stability selection;weight coadaptation,"0;Alzheimer Disease;Early Diagnosis;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Mild Cognitive Impairment;Models, Theoretical;Positron-Emission Tomography;Principal Component Analysis;Support Vector Machine",11.0,,29.0,,,20150504.0,Sept. 2015,,IEEE,IEEE Journals & Magazines
281,Residual and plain convolutional neural networks for 3D brain MRI classification,S. Korolev; A. Safiullin; M. Belyaev; Y. Dodonova,"Skolkovo Institute of Science and Technology, Russia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,835,838,"In the recent years there have been a number of studies that applied deep learning algorithms to neuroimaging data. Pipelines used in those studies mostly require multiple processing steps for feature extraction, although modern advancements in deep learning for image classification can provide a powerful framework for automatic feature generation and more straightforward analysis. In this paper, we show how similar performance can be achieved skipping these feature extraction steps with the residual and plain 3D convolutional neural network architectures. We demonstrate the performance of the proposed approach for classification of Alzheimer's disease versus mild cognitive impairment and normal controls on the Alzheimers Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950647,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950647,Alzheimer's Disease;Convolutional Neural Network;Deep Learning;MRI;Residual Neural Network,Alzheimer's disease;Biological neural networks;Feature extraction;Machine learning;Magnetic resonance imaging;Three-dimensional displays,biomedical MRI;brain;cognition;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural net architecture,3D brain MRI classification;3D plain convolutional neural network architecture;3D residual convolutional neural network architecture;3D structural MRI brain scans;ADNI dataset;Alzheimers disease national initiative dataset;cognitive impairment;deep learning algorithms;feature extraction;feature generation;image classification;neuroimaging data,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
282,Brain MRI super-resolution using deep 3D convolutional networks,C. H. Pham; A. Ducournau; R. Fablet; F. Rousseau,"Institut Mines T&#x00E9;l&#x00E9;com Atlantique, LaTIM U1101 INSERM, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,197,200,"Example-based single image super-resolution (SR) has recently shown outcomes with high reconstruction performance. Several methods based on neural networks have successfully introduced techniques into SR problem. In this paper, we propose a three-dimensional (3D) convolutional neural network to generate high-resolution (HR) brain image from its input low-resolution (LR) with the help of patches of other HR brain images. Our work demonstrates the need of fitting data and network parameters for 3D brain MRI.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950500,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950500,3D convolutional neural network;Super-resolution;brain MRI,Biological neural networks;Image resolution;Image restoration;Magnetic resonance imaging;Three-dimensional displays;Training;Two dimensional displays,biomedical MRI;brain;image reconstruction;image resolution;medical image processing;neural nets;neurophysiology,3D brain MRI;HR brain images;SR problem;brain MRI super-resolution;data fitting;deep 3D convolutional networks;example-based single image super-resolution;high reconstruction performance;high-resolution brain image;input low-resolution;network parameters;neural networks;three-dimensional convolutional neural network,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
283,On hierarchical brain tumor segmentation in MRI using fully convolutional neural networks: A preliminary study,S. Pereira; A. Oliveira; V. Alves; C. A. Silva,"CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal",2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG),20170330.0,2017,,,1,4,"Magnetic Resonance Imaging is the preferred imaging modality for assessing brain tumors, and segmentation is necessary for diagnosis and treatment planning. Thus, robust automatic segmentation methods are required. Machine learning proposals where the model is learned from data are quite successful. Hierarchical segmentation approaches firstly segment the whole tumor, followed by intra-tumor tissue identification. However, results comparing it with single stages approaches are needed, as state of the art results are also achieved by all-at-once strategies. Currently, fully convolutional networks approaches for segmentation are very efficient. In this paper, a hierarchical approach for brain tumor segmentation using a fully convolutional network is studied. The evaluation is performed on the Brain Tumor Segmentation Challenge 2013 dataset, and we report the metrics Dice Score Coefficient, Positive Predictive Value, and Sensitivity. Results show benefits from segmenting the complete tumor first, over all tissues in one stage. Moreover, the tumor core also benefits from such approach. This behavior may be justified by the high data imbalance observed between tumor and normal tissues, which is mitigated by considering the tumor as a whole.",,Electronic:978-1-5090-4801-4; POD:978-1-5090-4802-1,10.1109/ENBENG.2017.7889452,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889452,Brain tumor segmentation;Convolutional neural network;Magnetic Resonance Imaging,Image segmentation;Magnetic resonance imaging;Neural networks;Sensitivity;Training;Tumors,biomedical MRI;brain;image segmentation;medical image processing;neural nets;tumours,MRI;hierarchical brain tumor segmentation;machine learning;magnetic resonance imaging;neural networks,,,,,,,,16-18 Feb. 2017,,IEEE,IEEE Conference Publications
284,MRI image segmentation by fully convolutional networks,Y. Wang; Z. Sun; C. Liu; W. Peng; J. Zhang,"Key Laboratory of Convergence Medical Engineering System and Healthcare Technology, the Ministry of Industry and Information Technology, School of Life Science, Beijing Institute of Technology, Beijing, China",2016 IEEE International Conference on Mechatronics and Automation,20160905.0,2016,,,1697,1702,"With the development of various imaging technologies, medical imaging has been playing more important roles on providing scientific proof for doctors to make decisions on clinical diagnosis. At the same time, it is very important to excavate valuable information hidden in those images and take over some auxiliary medical works from doctors by the computer. Therefore, a large number of image segmentation methods, including some classic algorithms have been proposed and some of them perform well. In this paper, we built a deep convolutional neural network to segment the MRI brain images. Results show that the network has a good performance on segmentation of the gray and white matter of brains, it also had a good generalization ability.",,CD-ROM:978-1-5090-2394-3; Electronic:978-1-5090-2396-7; POD:978-1-5090-2397-4,10.1109/ICMA.2016.7558819,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558819,MRI brain imaging;deep convolutional network;fully convolutional network;image segmentation,Agriculture;Brain;Image segmentation;Magnetic resonance imaging;Medical diagnostic imaging;Medical services,biomedical MRI;image segmentation;medical image processing;neural nets,MRI brain images;MRI image segmentation;deep convolutional neural network;magnetic resonance imaging,,,,,,,,7-10 Aug. 2016,,IEEE,IEEE Conference Publications
285,Compressed sensing reconstruction of dynamic contrast enhanced MRI using GPU-accelerated convolutional sparse coding,T. M. Quan; W. K. Jeong,"School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,518,521,"In this paper, we propose a data-driven image reconstruction algorithm that specifically aims to reconstruct undersampled dynamic contrast enhanced (DCE) MRI data. The proposed method is based on the convolutional sparse coding algorithm, which leverages the Fourier convolution theorem to accelerate the process of learning a collections of filters and iteratively refines the reconstruction result using the sparse codes found during the reconstruction process. We introduce a novel energy formation based on the learning over time-varing DCE-MRI images, and propose an extension of Alternating Direction Method of Multiplier (ADMM) method to solve the constrained optimization problem efficiently using the GPU. We assess the performance of the proposed method by comparing with the state-of-the-art dictionary-based compressed sensing (CS) MRI method.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493321,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493321,Compressed Sensing;Convolutional Sparse Coding;GPU;MRI,Convolution;Convolutional codes;Dictionaries;Encoding;Fourier transforms;Image reconstruction;Magnetic resonance imaging,,,,,,7.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
286,Deep Convolutional Neural Networks for left ventricle segmentation,S. Molaei; M. Shiri; K. Horan; D. Kahrobaei; B. Nallamothu; K. Najarian,"Department of Emergency Medicine, University of Michigan, Ann Arbor, USA",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,668,671,"Left ventricle (LV) segmentation is crucial for quantitative cardiac function analysis. Manual segmentation of the endocardium and epicardium is highly cumbersome; physicians limit delineation to the end-diastolic and end-systolic phases. A fully automated system could provide an analysis of cardiac morphology for all phases in a much shorter time. Most of the current LV segmentation methods are semi-automated and require error prone manual initialization. A fully-automated LV segmentation method would expedite the functional analysis of the LV, reduce subjectivity and improve patient experience. We automatically segment the LV wall in cardiac MRI images with a Deep Convolutional Neural Network (DCNN). This algorithm first calculates the probability of a pixel belonging to the LV wall or background and then generates a label based on those probabilities without manual initialization. We then compare these results to the results obtained with another DCNN initialization method using Gabor filters. With Gabor DCNN we obtain an accuracy of 0.97, specificity of 0.984, sensitivity of 0.841 and mean accuracy of 0.902. This shows that Gabor filters perform better than random filters in the DCNN for LV segmentation.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036913,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036913,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
287,End-to-end learning of brain tissue segmentation from imperfect labeling,A. Fedorov; J. Johnson; E. Damaraju; A. Ozerin; V. Calhoun; S. Plis,"The Mind Research Network, Albuquerque, USA",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,3785,3792,"Segmenting a structural magnetic resonance imaging (MRI) scan is an important pre-processing step for analytic procedures and subsequent inferences about longitudinal tissue changes. Manual segmentation defines the current gold standard in quality but is prohibitively expensive. Automatic approaches are computationally intensive, incredibly slow at scale, and error prone due to usually involving many potentially faulty intermediate steps. In order to streamline the segmentation, we introduce a deep learning model that is based on volumetric dilated convolutions, subsequently reducing both processing time and errors. Compared to its competitors, the model has a reduced set of parameters and thus is easier to train and much faster to execute. The contrast in performance between the dilated network and its competitors becomes obvious when both are tested on a large dataset of unprocessed human brain volumes. The dilated network consistently outperforms not only another state-of-the-art deep learning approach, the up convolutional network, but also the ground truth on which it was trained. Not only can the incredible speed of our model make large scale analyses much easier but we also believe it has great potential in a clinical setting where, with little to no substantial delay, a patient and provider can go over test results.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7966333,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966333,,Brain modeling;Image segmentation;Kernel;Magnetic resonance imaging;Mathematical model;Training,biological tissues;biomedical MRI;brain;image segmentation;learning (artificial intelligence);medical image processing,MRI scan;brain tissue segmentation;deep learning model;dilated network;end-to-end learning;gold standard;imperfect labeling;longitudinal tissue changes;preprocessing step;structural magnetic resonance imaging scan;subsequent inferences;unprocessed human brain volumes;volumetric dilated convolutions,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
288,Hippocampus segmentation through multi-view ensemble ConvNets,Y. Chen; B. Shi; Z. Wang; P. Zhang; C. D. Smith; J. Liu,"School of EECS, Ohio University, Athens, 45701, United States of America",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,192,196,"Automated segmentation of brain structures from MR images is an important practice in many neuroimage studies. In this paper, we explore the utilization of a multi-view ensemble approach that relies on neural networks (NN) to combine multiple decision maps in achieving accurate hippocampus segmentation. Constructed under a general convolutional NN structure, our Ensemble-Net networks explore different convolution configurations to capture the complementary information residing in the multiple label probabilities produced by our U-Seg-Net (a modified U-Net) segmentation neural network. T1-weighted MRI scans and the associated Hippocampal masks of 110 healthy subjects from the ADNI project were used as the training and testing data. The combined U-Seg-Net + Ensemble-Net framework achieves over 89% Dice ratio on the test dataset.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950499,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950499,ADNI;Alzheimer's Disease;Convolutional Neural Networks;Hippocampal Segmentation,Convolution;Hippocampus;Image segmentation;Solid modeling;Three-dimensional displays;Training;Two dimensional displays,biomedical MRI;brain;diseases;image segmentation;neurophysiology;probability,ADNI project;Dice ratio;MR images;T1-weighted MRI scans;U-Seg-Net;associated hippocampal masks;automated segmentation;brain structures;complementary information;convolution configurations;ensemble-net framework;ensemble-net networks;general convolutional NN structure;hippocampus segmentation;modified U-Net;multiple decision maps;multiple label probabilities;multiview ensemble convnets;neural networks;neuroimage;test dataset;testing data;training data,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
289,A deep learning network for right ventricle segmentation in short-axis MRI,G. Luo; R. An; K. Wang; S. Dong; H. Zhang,"Harbin Institute of Technology, Harbin, China",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,485,488,"The segmentation of the right ventricle (RV) myocardium on MRI is a prerequisite step for the evaluation of RV structure and function, which is of great importance in the diagnose of most cardiac diseases, such as pulmonary hypertension, congenital heart disease, coronary heart disease, and dysplasia. However, RV segmentation is considered challenging, mainly because of the complex crescent shape of the RV across slices and phases. Hence this study aims to propose a new approach to segment RV endocardium and epicardium based on deep learning. The proposed method contains two subtasks: (1) localizing the region of interest (ROI), the biventricular region which contains more meaningful features and can facilitate the RV segmentation, and (2) segmenting the RV myocardium based on the localization. The two subtasks are integrated into a joint task learning framework, in which each task is solved via two multilayer convolutional neural networks. The experiments results show that the proposed method has big potential to be further researched and applied in clinical diagnosis.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868785,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868785,,Cardiac disease;Heart;Image segmentation;Machine learning;Magnetic resonance imaging;Neural networks;Training,biomedical MRI;cardiology;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets,RV endocardium;RV epicardium;RV function;RV structure;biventricular region;cardiac diseases;clinical diagnosis;congenital heart disease;convolutional neural networks;coronary heart disease;deep learning network;dysplasia;pulmonary hypertension;region-of-interest;right ventricle segmentation;short-axis MRI,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
290,Accelerating magnetic resonance imaging via deep learning,S. Wang; Z. Su; L. Ying; X. Peng; S. Zhu; F. Liang; D. Feng; D. Liang,"Paul C. Lauterbur Research Center for Biomedical Imaging, SIAT, CAS, Shenzhen, P.R. China",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,514,517,This paper proposes a deep learning approach for accelerating magnetic resonance imaging (MRI) using a large number of existing high quality MR images as the training datasets. An off-line convolutional neural network is designed and trained to identify the mapping relationship between the MR images obtained from zero-filled and fully-sampled k-space data. The network is not only capable of restoring fine structures and details but is also compatible with online constrained reconstruction methods. Experimental results on real MR data have shown encouraging performance of the proposed method for efficient and accurate imaging.,,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493320,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493320,Deep learning;convolutional neural network;magnetic resonance imaging;prior knowledge,Acceleration;Convolution;Image reconstruction;Magnetic resonance imaging;Neural networks;Training,,,,1.0,,9.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
291,Deep Learning with Edge Computing for Localization of Epileptogenicity Using Multimodal rs-fMRI and EEG Big Data,M. P. Hosseini; T. X. Tran; D. Pompili; K. Elisevich; H. Soltanian-Zadeh,"Dept. of Electr. & Comput. Eng., Rutgers Univ.-New Brunswick, New Brunswick, NJ, USA",2017 IEEE International Conference on Autonomic Computing (ICAC),20170810.0,2017,,,83,92,"Epilepsy is a chronic brain disorder characterized by the occurrence of spontaneous seizures of which about 30 percent of patients remain medically intractable and may undergo surgical intervention; despite the latter, some may still fail to attain a seizure-free outcome. Functional changes may precede structural ones in the epileptic brain and may be detectable using existing noninvasive modalities. Functional connectivity analysis through electroencephalography (EEG) and resting state-functional magnetic resonance imaging (rs-fMRI), complemented by diffusion tensor imaging (DTI), has provided such meaningful input in cases of temporal lobe epilepsy (TLE). Recently, the emergence of edge computing has provided competent solutions enabling context-aware and real-time response services for users. By leveraging the potential of autonomic edge computing in epilepsy, we develop and deploy both noninvasive and invasive methods for the monitoring, evaluation and regulation of the epileptic brain, with responsive neurostimulation (RNS; Neuropace). First, an autonomic edge computing framework is proposed for processing of big data as part of a decision support system for surgical candidacy. Second, an optimized model for estimation of the epileptogenic network using independently acquired EEG and rs-fMRI is presented. Third, an unsupervised feature extraction model is developed based on a convolutional deep learning structure for distinguishing interictal epileptic discharge (IED) periods from nonIED periods using electrographic signals from electrocorticography (ECoG). Experimental and simulation results from actual patient data validate the effectiveness of the proposed methods.",,Electronic:978-1-5386-1762-5; POD:978-1-5386-1763-2; USB:978-1-5386-1761-8,10.1109/ICAC.2017.41,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005336,Autonomic Computing;Deep Learning;EEG;Edge Computing;Epilepsy Seizure Localization;Health Monitoring and Treatment;Medical Big Data;rs-fMRI,Big Data;Cloud computing;Edge computing;Electroencephalography;Epilepsy;Feature extraction;Logic gates,Big Data;biomedical MRI;decision support systems;electroencephalography;feature extraction;learning (artificial intelligence);medical image processing;unsupervised learning,DTI;ECoG;EEG big data;IED periods;RNS;TLE;autonomic edge computing;chronic brain disorder;convolutional deep learning structure;decision support system;diffusion tensor imaging;electrocorticography;electroencephalography;electrographic signals;epileptogenicity;interictal epileptic discharge;multimodal rs-fMRI;responsive neurostimulation;state-functional magnetic resonance imaging;temporal lobe epilepsy;unsupervised feature extraction model,,,,,,,,17-21 July 2017,,IEEE,IEEE Conference Publications
292,Computer-aided classification of multi-types of dementia via convolutional neural networks,E. M. Alkabawi; A. R. Hilal; O. A. Basir,"Department of Electrical and Computer Engineering, University of Waterloo, Ontario, Canada",2017 IEEE International Symposium on Medical Measurements and Applications (MeMeA),20170720.0,2017,,,45,50,"With millions of people suffering from dementia worldwide, the global prevalence of dementia has a significant impact on the patients' lives, their caregivers' physical and emotional states, and the global economy. Early diagnosis of dementia helps in finding suitable therapies that reduce or even prevent further deterioration of patients' cognitive abilities. In recent years, state-of-the-art literature has proposed various computer-aided diagnosis systems based on 3-dimensional brain imagery analysis to identify early symptoms of dementia. These systems aim to assist radiologists in increasing the accuracy of diagnoses and reducing false positives. However, the early diagnosis of dementia is a challenging task due to the image quality, noise, and human brain irregularities. The state-of-the-art has focused on differentiating multi-stages of Alzheimer's disease, however, the diagnosis of various types of dementia is still a gap. This paper proposes a deep learning-based computer-aided diagnosis approach for the early detection of multi-type of dementia. To show the performance of the proposed CAD algorithm, three conventional CAD methods are implemented for comparison. The proposed algorithm yields a 74.93% accuracy in early diagnosis of multi-type of dementia and outperforms the state of the art CAD methods.",,Electronic:978-1-5090-2984-6; POD:978-1-5090-2985-3,10.1109/MeMeA.2017.7985847,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985847,Alzheimer's disease;Brain imaging;Computer-Aided Diagnosis;Convolutional Neural Networks;Dementia;Early diagnosis;Magnetic Resonance Imaging,Brain;Dementia;Feature extraction;Image edge detection;Image segmentation;Magnetic resonance imaging,biomedical MRI;cognition;convolution;diseases;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology,Alzheimer disease;caregiver physical states;caregiveremotional states;convolutional neural networks;deep learning-based computer-aided diagnosis approach;dementia;image quality;magnetic resonance imaging;patient cognitive abilities;three-dimensional brain imagery analysis,,,,,,,,7-10 May 2017,,IEEE,IEEE Conference Publications
293,Classification of brain tissues as lesion or healthy by 3D convolutional neural networks,C. Y. Aydoğdu; E. Albay; G. Ünal,"Bilgisayar ve Bili&#x015F;im Fak&#x00FC;ltesi, &#x0130;stanbul Teknik &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"In this paper, a three dimensional convolutional neural network based solution is proposed for classification of brain tissues as lesion or healthy in terms of ischemic stroke disease. Three dimensional data used in this work are obtained by magnetic resonance imaging technique. Proposed method is compared with traditional methods that are in the same category, via K-fold cross validation technique in terms of sensitivity, specificity and accuracy measures. In conclusion, it is obtained nearly 89% accuracy using our proposed method. Comparing this method with others, our proposed method is the best method.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960524,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960524,3D;classification;convolution;cross validation;ischemic stroke;magnetic resonance;neural networks,Biological neural networks;Biomedical imaging;Dogs;Image segmentation;Lesions;Magnetic resonance imaging;Three-dimensional displays,biological tissues;biomedical MRI;brain;convolution;diseases;image classification;medical image processing;neural nets,3D convolutional neural networks;brain tissues classification;healthy brain tissues;ischemic stroke disease;k-fold cross validation;lesion brain tissues;magnetic resonance imaging,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
294,SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D Convolutional Neural Networks,P. Ferdinand Christ; F. Ettlinger; G. Kaissis; S. Schlecht; F. Ahmaddy; F. Grün; A. Valentinitsch; S. A. Ahmadi; R. Braren; B. Menze,"Technische Universit&#x00E4;t M&#x00FC;nchen, Image-Based Biomedical Modeling Group, Arccisstrasse 21, 80333 Munich, Germany",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,839,843,Automatic non-invasive assessment of hepatocellular carcinoma (HCC) malignancy has the potential to substantially enhance tumor treatment strategies for HCC patients. In this work we present a novel framework to automatically characterize the malignancy of HCC lesions from DWI images. We predict HCC malignancy in two steps: As a first step we automatically segment HCC tumor lesions using cascaded fully convolutional neural networks (CFCN). A 3D neural network (SurvivalNet) then predicts the HCC lesions' malignancy from the HCC tumor segmentation. We formulate this task as a classification problem with classes being “low risk” and “high risk” represented by longer or shorter survival times than the median survival. We evaluated our method on DWI of 31 HCC patients. Our proposed framework achieves an end-to-end accuracy of 65% with a Dice score for the automatic lesion segmentation of 69% and an accuracy of 68% for tumor malignancy classification based on expert annotations. We compared the SurvivalNet to classical handcrafted features such as Histogram and Haralick and show experimentally that SurvivalNet outperforms the handcrafted features in HCC malignancy classification. End-to-end assessment of tumor malignancy based on our proposed fully automatic framework corresponds to assessment based on expert annotations with high significance (p > 0.95).,,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950648,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950648,3D Neural Network;Fully Convolutional Neural Networks;MRI;Survival Prediction,Cancer;Feature extraction;Image segmentation;Imaging;Lesions;Three-dimensional displays,biomedical MRI;cancer;image classification;image segmentation;medical image processing;neural nets;tumours,3D convolutional neural networks;DWI images;Dice score;HCC lesion malignancy classification;HCC tumor lesions;HCC tumor segmentation;Haralick;Histogram;SurvivalNet;automatic lesion segmentation;diffusion weighted magnetic resonance images;fully convolutional neural network;hepatocellular carcinoma;patient survival;tumor malignancy classification;tumor treatment strategies,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
295,Aneurysm detection in 3D cerebral angiograms based on intra-vascular distance mapping and convolutional neural networks,T. Jerman; F. Pernus; B. Likar; Ž. Špiclin,"University of Ljubljana, Faculty of Electrical Engineering, Tr&#x017E;a&#x0161;ka cesta 25, SI-1000, Slovenia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,612,615,"Early and more sensitive detection of small aneurysms in 3D cerebral angiograms is required to prevent potentially fatal rupture events. Herein, we propose a novel method that entails structure enhancement filtering to highlight potential aneurysm locations, intra-vascular distance mapping for regional vascular shape encoding and dimensionality reduction and a convolutional neural network to automatically determine optimal features and classification rules for aneurysm detection. Evaluation on 15 3D digital subtraction angiograms showed better performance of the proposed method compared to enhancement filtering and random forest based methods, as it achieved a 100% detection sensitivity at a low number of false positives (2.4 per dataset). The proposed method is also applicable to other angiographic modalities.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950595,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950595,aneurysms;convolutional neural networks;detection;intravascular ray-casting,Aneurysm;Image edge detection;Sensitivity;Three-dimensional displays;Training;Two dimensional displays;Visualization,biomedical MRI;brain;image recognition;medical disorders;medical image processing;neural nets,3D cerebral angiograms;3D digital subtraction angiograms;aneurysm detection;convolutional neural networks;dimensionality reduction;intra-vascular distance mapping;potentially fatal rupture events;random forest based methods;regional vascular shape encoding;structure enhancement filtering,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
296,Classification of MRI data using deep learning and Gaussian process-based model selection,H. Bertrand; M. Perrot; R. Ardon; I. Bloch,"LTCI, T&#x00E9;l&#x00E9;com ParisTech, Universit&#x00E9; Paris-Saclay, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,745,748,"The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20% for the most difficult classes).",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950626,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950626,Classification;Convolutional Neural Networks;Deep Learning;Gaussian Process;MRI;Model Selection,Abdomen;Biomedical imaging;Machine learning;Magnetic resonance imaging;Optimization;Pelvis;Training,Gaussian processes;biomedical MRI;computer vision;image classification;learning (artificial intelligence);medical image processing,Gaussian process-based model selection;MRI image classification;adaptive search;anatomical field of view;computer vision problems;deep learning;random search,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
297,Automatic detection of motion artifacts in MR images using CNNS,K. Meding; A. Loktyushin; M. Hirsch,"Max Planck Institute for Intelligent Systems, Department of Empirical Inference, Germany","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619.0,2017,,,811,815,"Considerable practical interest exists in being able to automatically determine whether a recorded magnetic resonance image is affected by motion artifacts caused by patient movements during scanning. Existing approaches usually rely on the use of navigators or external sensors to detect and track patient motion during image acquisition. In this work, we present an algorithm based on convolutional neural networks that enables fully automated detection of motion artifacts in MR scans without special hardware requirements. The approach is data driven and uses the magnitude of MR images in the spatial domain as input. We evaluate the performance of our algorithm on both synthetic and real data and observe adequate performance in terms of accuracy and generalization to different types of data. Our proposed approach could potentially be used in clinical practice to tag an MR image as motion-free or motion-corrupted immediately after a scan is finished. This process would facilitate the acquisition of high-quality MR images that are often indispensable for accurate medical diagnosis.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7952268,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952268,Convolutional Neural Networks;Deep Learning;MRI;Motion Artifacts;Quality Assessment,Brain;Kernel;Magnetic resonance imaging;Motion artifacts;Testing;Three-dimensional displays;Training,biomedical MRI;convolution;feature extraction;image motion analysis;neural nets;patient diagnosis,CNNS;MR images;MR scans;automatic motion artifacts detection;clinical practice;convolutional neural networks;data accuracy;data driven;data generalization;image acquisition;magnetic resonance image;medical diagnosis;motion-corrupted MR image;motion-free MR image;patient motion;patient movements,,,,,,,,5-9 March 2017,,IEEE,IEEE Conference Publications
298,Fast predictive multimodal image registration,X. Yang; R. Kwitt; M. Styner; M. Niethammer,"Department of Computer Science, UNC Chapel Hill, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,858,862,"We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950652,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950652,deep learning;deformation prediction;multimodal image similarity,Bayes methods;Convolutional codes;Deformable models;Image registration;Optimization;Three-dimensional displays;Training,Bayes methods;biomedical MRI;biomedical measurement;brain;image registration;learning (artificial intelligence);medical image processing;neural nets;optimisation,3D brain MRI dataset;Bayesian probabilistic version;LDDMM registration model;T1-weighted images;T2-weighted images;deep encoder-decoder architecture;image deformation prediction;image registration formulations;image-patch-based deep network;large deformation diffeomorphic metric mapping;multimodal image registration;multimodal image similarity measurement;multimodal images;optimization-based LDDMM,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
299,3-D functional brain network classification using Convolutional Neural Networks,D. Ren; Y. Zhao; H. Chen; Q. Dong; J. Lv; T. Liu,"College of Computer Science and Information Engineering, Tianjin University of Science and Technology, 300222, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1217,1221,"Several recent studies have shown that dictionary learning and sparse representation can effectively reconstruct hundreds of interacting functional brain networks simultaneously from whole-brain fMRI data. However, accurate classification and recognition of those hundreds of functional networks from an individual or a population of many subjects is still a challenging and open problem due to the intrinsic variability of functional networks and other noise sources. To tackle this problem, this paper presents an effective deep learning framework to train convolutional neural networks from a large dataset of hundreds of thousands of available brain network volume maps, which was then applied on testing samples for network classification and recognition. We effectively applied computer-labeled data as training set so the whole process can be automated. Experimental results showed that the proposed method is quite robust in handling noisy patterns in the dataset, which suggests that our work offers a new computational framework for modeling functional connectomes from fMRI big data in the future.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950736,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950736,3D convolutional neural networks;classification;deep learning;functional brain networks,Biological neural networks;Machine learning;Noise measurement;Testing;Three-dimensional displays;Training;Visualization,Big Data;biomedical MRI;brain;image classification;image denoising;learning (artificial intelligence);neural nets;neurophysiology,3-D functional brain network classification;brain network volume maps;computational framework;computer-labeled data;convolutional neural networks;deep learning framework;dictionary learning;fMRI big data;functional connectomes;functional networks;intrinsic variability;network recognition;noise sources;noisy patterns;sparse representation;training set;whole-brain fMRI data,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
300,Prostate segmentation in MR images using ensemble deep convolutional neural networks,H. Jia; Y. Xia; W. Cai; M. Fulham; D. D. Feng,"Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science, Northwestern Polytechnical University, Xi'an 710072, China",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,762,765,"The automated segmentation of the prostate gland from MR images is increasingly used for clinical diagnosis. Since deep learning demonstrates superior performance in computer vision applications, we propose a coarse-to-fine segmentation strategy using ensemble deep convolutional neural networks (DCNNs) to address prostate segmentation in MR images. First, we use registration-based coarse segmentation on pre-processed prostate MR images to define the potential boundary region. We then train four DCNNs as voxel-based classifiers and classify the voxel in the potential region is a prostate voxel when at least three DCNNs made that decision. Finally, we use boundary refinement to eliminate the outliers and smooth the boundary. We evaluated our approach on the MICCAI PROMIS12 challenge dataset and our experimental results verify the effectiveness of the proposed algorithms.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950630,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950630,MR prostate segmentation;deep convolutional neural network;voxel classification,Biomedical imaging;Convolution;Glands;Image segmentation;Magnetic resonance imaging;Probabilistic logic;Training,biomedical MRI;image registration;image segmentation;learning (artificial intelligence);medical image processing;neural nets;pattern classification,MICCAI PROMIS12 challenge dataset;MR images;automated prostate gland segmentation;computer vision applications;deep learning;ensemble deep convolutional neural networks;prostate MR image preprocessing;prostate voxel;registration-based coarse segmentation;voxel-based classifiers,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
301,Fast Fully Automatic Segmentation of the Severely Abnormal Human Right Ventricle from Cardiovascular Magnetic Resonance Images Using a Multi-Scale 3D Convolutional Neural Network,A. Giannakidis; K. Kamnitsas; V. Spadotto; J. Keegan; G. Smith; B. Glocker; D. Rueckert; S. Ernst; M. A. Gatzoulis; D. J. Pennell; S. Babu-Narayan; D. N. Firmin,"NIHR Cardiovascular Biomed. Res. Unit, R. Brompton Hosp., London, UK",2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),20170424.0,2016,,,42,46,"Cardiac magnetic resonance (CMR) is regarded as the reference examination for cardiac morphology in tetralogy of Fallot (ToF) patients allowing images of high spatial resolution and high contrast. The detailed knowledge of the right ventricular anatomy is critical in ToF management. The segmentation of the right ventricle (RV) in CMR images from ToF patients is a challenging task due to the high shape and image quality variability. In this paper we propose a fully automatic deep learning-based framework to segment the RV from CMR anatomical images of the whole heart. We adopt a 3D multi-scale deep convolutional neural network to identify pixels that belong to the RV. Our robust segmentation framework was tested on 26 ToF patients achieving a Dice similarity coefficient of 0.8281±0.1010 with reference to manual annotations performed by expert cardiologists. The proposed technique is also computationally efficient, which may further facilitate its adoption in the clinical routine.",,Electronic:978-1-5090-5698-9; POD:978-1-5090-5699-6,10.1109/SITIS.2016.16,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907443,3D convolutional neural network;cardiavascular magnetic resonance;deep learning;right ventricle;segmentation;tetralogy of Fallot,Heart;Image segmentation;Magnetic resonance;Manuals;Neurons;Three-dimensional displays;Training,biomedical MRI;image segmentation;learning (artificial intelligence);medical image processing;neural nets,CMR;Dice similarity coefficient;ToF patients;cardiovascular magnetic resonance images;deep learning-based framework;fast fully automatic segmentation;human right ventricle;image quality variability;multiscale 3D convolutional neural network;right ventricle;tetralogy of Fallot patients,,,,,,,,Nov. 28 2016-Dec. 1 2016,,IEEE,IEEE Conference Publications
302,Automatic segmentation of left ventricular myocardium by deep convolutional and de-convolutional neural networks,X. L. Yang; L. Gobeawan; S. Y. Yeo; W. T. Tang; Z. Z. Wu; Y. Su,"Institute of High Performance Computing, A&#x2217;STAR, Singapore",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,81,84,"Deep learning has been integrated into several existing left ventricle (LV) endocardium segmentation methods to yield impressive accuracy improvements. However, challenges remain for segmentation of LV epicardium due to its fuzzier appearance and complications from the right ventricular insertion points. Segmenting the myocardium collectively (i.e., endocardium and epicardium together) confers the potential for better segmentation results. In this work, we develop a computational platform based on deep learning to segment the whole LV myocardium simultaneously from a cardiac magnetic resonance (CMR) image. The deep convolutional network is constructed using Caffe platform, which consists of 6 convolutional layers, 2 pooling layers, and 1 de-convolutional layer. A preliminary result with Dice metric of 0.75±0.04 is reported on York MR dataset. While in its current form, our proposed one-step deep learning method cannot compete with state-of-art myocardium segmentation methods, it delivers promising first pass segmentation results.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868684,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868684,,Image segmentation;Machine learning;Magnetic resonance;Magnetic resonance imaging;Measurement;Myocardium;Neural networks,biomedical MRI;cardiology;image segmentation;medical image processing;neural nets,Caffe platform;automatic left ventricular myocardium segmentation;cardiac magnetic resonance image;deconvolutional neural network;deep convolutional neural network;deep learning;left ventricle endocardium segmentation method;right ventricular insertion,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
303,Moving from detection to pre-detection of Alzheimer's Disease from MRI data,K. A. N. N. P. Gunawardena; R. N. Rajapakse; N. D. Kodikara; I. U. K. Mudalige,"University of Colombo School of Computing, Sri Lanka",2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer),20170126.0,2016,,,324,324,"Alzheimer's Disease (AD) is the most common form of dementia, affecting approximately 10% of individuals under 65 years of age, with the prevalence doubling every 5 years up to age 80, above which the prevalence exceeds 40%. Currently diagnosis of AD is largely based on the examination of clinical history and tests such as MMSE (Mini-mental state examination) and PAL (Paired Associates Learning). However many present studies have highlighted the inaccuracies and limitations of such tests. Thus medical officers are now moving to the more accurate neuroimaging data (Magnetic Resonance Imaging- MRI) based diagnosis for these types of diseases where brain atrophy transpires. However it is a considerable challenge to analyse large numbers of images manually to get the most accurate diagnosis at present.",,CD:978-1-5090-6076-4; Electronic:978-1-5090-6078-8; POD:978-1-5090-6079-5; Paper:978-1-5090-6077-1,10.1109/ICTER.2016.7829940,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829940,Alzheimer's disease;Convolutional neural network;Image Processing;MRI,Alzheimer's disease;Atrophy;MATLAB;Magnetic resonance imaging;Neuroimaging;Support vector machines,biomedical MRI;brain;diseases;medical image processing,AD diagnosis;Alzheimer's disease detection;Alzheimer's disease predetection;MRI data;brain;dementia;magnetic resonance imaging;neuroimaging data,,,,,,,,1-3 Sept. 2016,,IEEE,IEEE Conference Publications
304,Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation,T. Brosch; L. Y. W. Tang; Y. Yoo; D. K. B. Li; A. Traboulsee; R. Tam,"Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1229,1239,"We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.",0278-0062;02780062,,10.1109/TMI.2016.2528821,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404285,Convolutional neural networks;deep learning;machine learning;magnetic resonance imaging (MRI);multiple sclerosis lesions;segmentation,Convolution;Feature extraction;Image segmentation;Imaging;Lesions;Neural networks;Training,biomedical MRI;feature extraction;image segmentation;medical image processing;neural nets,EMS;ISBI 2015 challenges;LST-LGA;LST-LPA;Lesion-TOADS;MICCAI 2008 challenges;MS clinical trial;MS lesion segmentation methods;SLS;automatic feature learning;convolutional pathway;deconvolutional pathway;deep 3D convolutional encoder networks;feature extraction;higher-level image features;interconnected pathways;low-level features;magnetic resonance images;multiple sclerosis lesion segmentation;multiscale feature integration;neural network;prediction pathways;publicly available data sets;segmentation task;shortcut connections;top-ranked state-of-the-art methods;voxel level,,14.0,,47.0,,,20160211.0,May 2016,,IEEE,IEEE Journals & Magazines
305,Automatic Segmentation of MR Brain Images With a Convolutional Neural Network,P. Moeskops; M. A. Viergever; A. M. Mendrik; L. S. de Vries; M. J. N. L. Benders; I. Išgum,"Image Sciences Institute, University Medical Center Utrecht, The Netherlands",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1252,1261,"Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T<sub>2</sub>-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T<sub>2</sub>-weighted images of preterm infants acquired at 40 weeks PMA, axial T<sub>1</sub>-weighted images of ageing adults acquired at an average age of 70 years, and T<sub>1</sub>-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.",0278-0062;02780062,,10.1109/TMI.2016.2548501,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444155,Adult brain;MRI;automatic image segmentation;convolutional neural networks;deep learning;preterm neonatal brain,Aging;Biomedical imaging;Brain;Convolution;Image segmentation;Kernel;Pediatrics,biological tissues;biomedical MRI;brain;image segmentation;medical image processing;neural nets;neurophysiology;paediatrics,acquisition protocol;ageing adults;automatic MR brain image segmentation;average Dice coefficients;axial T<sub>2</sub>-weighted images;convolutional neural network;coronal T<sub>2</sub>-weighted images;multiple convolution kernel sizes;multiple patch sizes;multiscale information;postmenstrual age;preterm infants;quantitative analysis;segmented tissue classes;spatial consistency;training data,,11.0,,46.0,,,20160330.0,May 2016,,IEEE,IEEE Journals & Magazines
306,Anatomical Landmark Detection in Medical Applications Driven by Synthetic Data,G. Riegler; M. Urschler; M. Rüther; H. Bischof; D. Stern,"Graz Univ. of Technol., Graz, Austria",2015 IEEE International Conference on Computer Vision Workshop (ICCVW),20160215.0,2015,,,85,89,"An important initial step in many medical image analysis applications is the accurate detection of anatomical landmarks. Most successful methods for this task rely on data-driven machine learning algorithms. However, modern machine learning techniques, e.g. convolutional neural networks, need a large corpus of training data, which is often an unrealistic setting for medical datasets. In this work, we investigate how to adapt synthetic image datasets from other computer vision tasks to overcome the under-representation of the anatomical pose and shape variations in medical image datasets. We transform both data domains to a common one in such a way that a convolutional neural network can be trained on the larger synthetic image dataset and fine-tuned on the smaller medical image dataset. Our evaluations on data of MR hand and whole body CT images demonstrate that this approach improves the detection results compared to training a convolutional neural network only on the medical data. The proposed approach may also be usable in other medical applications, where training data is scarce.",,Electronic:978-1-4673-9711-7; POD:978-1-4673-9712-4,10.1109/ICCVW.2015.21,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406370,,Biomedical imaging;Computed tomography;Shape;Three-dimensional displays;Training;Training data,biomedical MRI;computer vision;computerised tomography;learning (artificial intelligence);medical image processing;neural nets;object detection,MR hand;anatomical landmark detection;computer tomography;computer vision;convolutional neural network;magnetic resonance;medical image analysis;neural network training;whole body CT images,,,,20.0,,,,7-13 Dec. 2015,,IEEE,IEEE Conference Publications
307,Automatic cerebral microbleeds detection from MR images via Independent Subspace Analysis based hierarchical features,Q. Dou; H. Chen; L. Yu; L. Shi; D. Wang; V. C. Mok; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,7933,7936,"With the development of susceptibility weighted imaging (SWI) technology, cerebral microbleed (CMB) detection is increasingly essential in cerebrovascular diseases diagnosis and cognitive impairment assessment. Clinical CMB detection is based on manual rating which is subjective and time-consuming with limited reproducibility. In this paper, we propose a computer-aided system for automatic detection of CMBs from brain SWI images. Our approach detects the CMBs within three stages: (i) candidates screening based on intensity values (ii) compact 3D hierarchical features extraction via a stacked convolutional Independent Subspace Analysis (ISA) network (iii) false positive candidates removal with a support vector machine (SVM) classifier based on the learned representation features from ISA. Experimental results on 19 subjects (161 CMBs) achieve a high sensitivity of 89.44% with an average of 7.7 and 0.9 false positives per subject and per CMB, respectively, which validate the efficacy of our approach.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7320232,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320232,Cerebral microbleed;brain SWI;computer aided diagnosis;feature representation,Feature extraction;Imaging;Radio frequency;Sensitivity;Support vector machines;Three-dimensional displays;Training,biomedical MRI;brain;diseases;feature extraction;medical image processing;support vector machines,CMB automatic detection;MR images;automatic cerebral microbleeds detection;brain SWI images;cerebrovascular diseases diagnosis;clinical CMB detection;cognitive impairment assessment;compact 3D hierarchical feature extraction;computer-aided system;stacked convolutional independent subspace analysis network;subspace analysis-based hierarchical features;support vector machine classifier;susceptibility weighted imaging technology,,,,14.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
308,MR Image Reconstruction with Convolutional Characteristic Constraint (CoCCo),X. Peng; D. Liang,"Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, Shenzhen, China",IEEE Signal Processing Letters,20150120.0,2015,22,8.0,1184,1188,"The problem of recovering an image from limited or sparsely sampled Fourier measurements occurs in the application of magnetic resonance imaging. To address this problem, we propose a novel MR image reconstruction method with convolutional characteristic constraints. We first estimate the convolutional characteristics using standard compressed sensing method in a parallel fashion. Then we use the recovered image characteristics to constrain the target image function. The image characteristics should either be sparser or of higher SNR than the original image to enable superior performance. In this work, we studied using thirteen kernels and experiments based on a brain data set were conducted. It is demonstrated that the proposed method outperforms the existing methods in terms of high quality imaging due to multiple characteristic constraints and the robustness to measurement noise.",1070-9908;10709908,,10.1109/LSP.2014.2376699,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971076,Compressed sensing (CS);constrained imaging;convolutional characteristic;magnetic resonance imaging (MRI),Biomedical measurement;Convolution;Image reconstruction;Kernel;Magnetic resonance imaging;Noise;Standards,biomedical MRI;compressed sensing;convolution;image reconstruction;medical image processing,CoCCo;MR image reconstruction method;SNR;brain data set;convolutional characteristic constraint;image recovery;magnetic resonance imaging;measurement noise;sparsely sampled Fourier measurements;standard compressed sensing method;target image function,,4.0,,19.0,,,20141202.0,Aug. 2015,,IEEE,IEEE Journals & Magazines
309,Iterative deep convolutional encoder-decoder network for medical image segmentation,J. U. Kim; H. G. Kim; Y. M. Ro,"Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Republic of Korea",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,685,688,"In this paper, we propose a novel medical image segmentation using iterative deep learning framework. We have combined an iterative learning approach and an encoder-decoder network to improve segmentation results, which enables to precisely localize the regions of interest (ROIs) including complex shapes or detailed textures of medical images in an iterative manner. The proposed iterative deep convolutional encoder-decoder network consists of two main paths: convolutional encoder path and convolutional decoder path with iterative learning. Experimental results show that the proposed iterative deep learning framework is able to yield excellent medical image segmentation performances for various medical images. The effectiveness of the proposed method has been proved by comparing with other state-of-the-art medical image segmentation methods.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036917,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036917,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
310,A regularized deep learning approach for clinical risk prediction of acute coronary syndrome using electronic health records,Z. Huang; W. Dong; H. Duan; J. Liu,"College of Biomedical Engineering and Instrument Science of Zhejiang University, Hangzhou, Zhejiang China 310008 (e-mail: zhengxing.h@gmail.com)",IEEE Transactions on Biomedical Engineering,,2017,PP,99.0,1,1,"Objective: Acute coronary syndrome (ACS), as a common and severe cardiovascular disease, is a leading cause of death and the principal cause of serious long-term disability globally. Clinical risk prediction of ACS is important for early intervention and treatment. Existing ACS risk scoring models are based mainly on a small set of hand-picked risk factors and often dichotomize predictive variables to simplify the score calculation [1-3]. Methods: This study develops a regularized stacked denoising auto-encoder (SDAE) model to stratify clinical risks of ACS patients from a large volume of electronic health records (EHR). To capture characteristics of patients at similar risk levels, and preserve the discriminating information across different risk levels, two constraints are added on SDAE to make the reconstructed feature representations contain more risk information of patients, which contribute to a better clinical risk prediction result. Results: We validate our approach on a real clinical dataset consisting of 3,464 ACS patient samples. The performance of our approach for predicting ACS risk remains robust and reaches 0.868 and 0.73 in terms of both AUC and Accuracy, respectively. Conclusions: The obtained results show that the proposed approach achieves a competitive performance compared to state-of-the-art models in dealing with the clinical risk prediction problem. In addition, our approach can extract informative risk factors of ACS via a reconstructive learning strategy. Some of these extracted risk factors are not only consistent with existing medical domain knowledge, but also contain suggestive hypotheses that could be validated by further investigations in the medical domain.",0018-9294;00189294,,10.1109/TBME.2017.2731158,National Key Research and Development Program of China; National Nature Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990180,Acute coronary syndrome;Clinical risk prediction;Deep learning;Electronic Health Record;Stacked denoising auto-encoder,,,,,,,,,,20170724.0,,,IEEE,IEEE Early Access Articles
311,Deep learning of feature representation with multiple instance learning for medical image analysis,Y. Xu; T. Mo; Q. Feng; P. Zhong; M. Lai; E. I. C. Chang,"State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20140714.0,2014,,,1626,1630,"This paper studies the effectiveness of accomplishing high-level tasks with a minimum of manual annotation and good feature representations for medical images. In medical image analysis, objects like cells are characterized by significant clinical features. Previously developed features like SIFT and HARR are unable to comprehensively represent such objects. Therefore, feature representation is especially important. In this paper, we study automatic extraction of feature representation through deep learning (DNN). Furthermore, detailed annotation of objects is often an ambiguous and challenging task. We use multiple instance learning (MIL) framework in classification training with deep learning features. Several interesting conclusions can be drawn from our work: (1) automatic feature learning outperforms manual feature; (2) the unsupervised approach can achieve performance that's close to fully supervised approach (93.56%) vs. (94.52%); and (3) the MIL performance of coarse label (96.30%) outweighs the supervised performance of fine label (95.40%) in supervised deep learning features.",1520-6149;15206149,Electronic:978-1-4799-2893-4; POD:978-1-4799-2894-1; USB:978-1-4799-2892-7,10.1109/ICASSP.2014.6853873,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853873,deep learning;feature learning;multiple instance learning;supervised;un-supervised,Biomedical imaging;Cancer;Feature extraction;Manuals;Supervised learning;Training;Vectors,feature extraction;image representation;learning (artificial intelligence);medical image processing,DNN;HARR features;MIL framework;SIFT features;automatic feature representation extraction;classification training;clinical features;feature representation;manual annotation;medical image analysis;multiple instance learning;supervised deep learning features;unsupervised approach,,14.0,,26.0,,,,4-9 May 2014,,IEEE,IEEE Conference Publications
312,Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks With Jaccard Distance,Y. Yuan; M. Chao; Y. C. Lo,"Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA",IEEE Transactions on Medical Imaging,20170830.0,2017,36,9.0,1876,1886,"Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 <italic>skin lesion analysis towards melanoma detection</italic> challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.",0278-0062;02780062,,10.1109/TMI.2017.2695227,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7903636,Deep learning;dermoscopy;fully convolutional neural networks;image segmentation;jaccard distance;melanoma,Biomedical imaging;Databases;Image segmentation;Lesions;Malignant tumors;Skin,,,,,,,,,20170418.0,Sept. 2017,,IEEE,IEEE Journals & Magazines
313,Looking Under the Hood: Deep Neural Network Visualization to Interpret Whole-Slide Image Analysis Outcomes for Colorectal Polyps,B. Korbar; A. M. Olofson; A. P. Miraflor; C. M. Nicka; M. A. Suriawinata; L. Torresani; A. A. Suriawinata; S. Hassanpour,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824.0,2017,,,821,827,"Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. The process of characterization is time-intensive and requires years of specialized medical training. In this work, we propose a deep-learning-based image analysis approach that not only can accurately classify different types of polyps in whole-slide images, but also generates major regions and features on the slide through a model visualization approach. We argue that this visualization approach will make sense of the underlying reasons for the classification outcomes, significantly reduce the cognitive burden on clinicians, and improve the diagnostic accuracy for whole-slide image characterization tasks. Our results show the efficacy of this network visualization approach in recovering decisive regions and features for different types of polyps on whole-slide images according to the domain expert pathologists.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.114,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014848,,Agriculture;Backpropagation;Cancer;Computer architecture;Neural networks;Training;Visualization,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
314,Retrieval From and Understanding of Large-Scale Multi-modal Medical Datasets: A Review,H. Müller; D. Unay,"Information Systems Institute, HES-SO Valais, Sierre, Switzerland",IEEE Transactions on Multimedia,20170814.0,2017,19,9.0,2093,2104,"Content-based multimedia retrieval (CBMR) has been an active research domain since the mid 1990s. In medicine visual retrieval started later and has mostly remained a research instrument and less a clinical tool. The limited size of data sets due to privacy constraints is often mentioned as reason for these limitations. Nevertheless, much work has been done in CBMR, including the availability of increasingly large data sets and scientific challenges. Annotated data sets and clinical data for images have now become available and can be combined for multi-modal retrieval. Much has been learned on user behavior and application scenarios. This text is motivated by the advances in medical image analysis and the availability of public large data sets that often include clinical data. It is a systematic review of recent work (concentrating on the period 2011-2017) on multi-modal CBMR and image understanding in the medical domain, where image understanding includes techniques such as detection, localization, and classification for leveraging visual content. With the objective of summarizing the current state of research for multimedia researchers outside the medical field, the text provides ways to get data sets and identifies current limitations and promising research directions. The text highlights advances in the past six years and a trend to use larger scale training data and deep learning approaches that can replace/complement hand-crafted features. Using images alone will likely only work in limited domains but combining multiple sources of data for multi-modal retrieval has the biggest chances of success, particularly for clinical impact.",1520-9210;15209210,,10.1109/TMM.2017.2729400,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984864,Big data;content–based image retrieval;deep learning;large scale datasets;medical images;multi–modality,Machine learning;Medical diagnostic imaging;Multimedia communication;Tools;Visualization,Big Data;data privacy;image retrieval;information retrieval;medical administrative data processing;medical image processing,active research domain;annotated data sets;clinical data;clinical impact;clinical tool;content-based multimedia retrieval;deep learning;hand-crafted features;large-scale multimodal medical datasets;medical domain;medical field;medical image analysis;medicine visual retrieval;multimedia researchers;multimodal CBMR;multimodal retrieval;privacy constraints;public large data sets;research instrument;training data;user behavior,,,,,,,20170719.0,Sept. 2017,,IEEE,IEEE Journals & Magazines
315,Application of neural network based on SIFT local feature extraction in medical image classification,Shuqi Cui; Hong Jiang; Zheng Wang; Chaomin Shen,"East China Normal University, Department of Computer Center, Shanghai, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)",20170720.0,2017,,,92,97,"In the medical image analysis, ROI (Region of Interest) is one of the key features of clinical diagnostic analysis. The applying of local features of ROI to the deep learning of image classification has the advantage of noise eliminating and information reducing. Based on existing research results, using Scale Invariant Feature Transformation (SIFT) algorithm combined with SVM classifier and sliding window to extract the local features and describe ROI precisely in the image. Finally, the extracted feature is used as the input layer of BP neural network in mammary gland X - ray image classification. The experimental results show that the accuracy of neural network classifier based on SIFT is 96.57%, which is 3.44% higher than that of traditional SVM classification accuracy. It is verified that our classifier is important to support clinical diagnosis and diagnosis.",,DVD:978-1-5090-6236-2; Electronic:978-1-5090-6238-6; POD:978-1-5090-6239-3,10.1109/ICIVC.2017.7984525,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984525,BP neural network;ROI;SIFT;SVM;slide the window,Biological neural networks;Feature extraction;Image classification;Medical diagnostic imaging;Neurons,X-ray imaging;backpropagation;feature extraction;image classification;image denoising;medical image processing;neural nets;support vector machines;transforms,BP neural network;ROI;SIFT local feature extraction;SVM classifier;clinical diagnostic analysis;deep learning;information reduction;mammary gland X-ray image classification;medical image analysis;medical image classification;neural network classifier;noise elimination;region of interest;scale invariant feature transformation;sliding window,,,,,,,,2-4 June 2017,,IEEE,IEEE Conference Publications
316,Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection,D. Sarikaya; J. J. Corso; K. A. Guru,"Department of Computer Science and Engineering, SUNY Buffalo, NY, USA",IEEE Transactions on Medical Imaging,20170628.0,2017,36,7.0,1542,1549,"Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.",0278-0062;02780062,,10.1109/TMI.2017.2665671,10.13039/100006398 - Roswell Park Alliance Foundation Roswell Park Cancer Institute; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847313,Object detection;image classification;laparoscopes;multi-layer neural network;telerobotics,Neural networks;Object detection;Proposals;Robots;Surgery;Training;Videos,biomedical optical imaging;computer vision;gesture recognition;image fusion;image motion analysis;learning (artificial intelligence);manipulators;medical image processing;medical robotics;neural nets;object detection;surgery;telerobotics;video signal processing,ATLAS Dione;RAS videos;RPN;Roswell Park Cancer Institute;computer vision approach;daVinci Surgical System;deep learning;deep neural networks;effective skill acquisition;frame detection;gestures;human-robot collaborative surgeries;image fusion;localization open problem;mean computation time;medical imaging;multimodal convolutional neural networks;multimodal two stream convolutional network;object detection;objective skill assessment;objectness;real-time feedback;region detection;region proposal network;robot-assisted surgery videos;robotic tools;skill level;temporal motion cues;tool detection;tool localization,,,,,,,20170208.0,July 2017,,IEEE,IEEE Journals & Magazines
317,A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology,N. Kumar; R. Verma; S. Sharma; S. Bhargava; A. Vahadane; A. Sethi,"IIT Guwahati, Guwahati, India",IEEE Transactions on Medical Imaging,20170628.0,2017,36,7.0,1550,1560,"Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.",0278-0062;02780062,,10.1109/TMI.2017.2677499,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382,Annotation;boundaries;dataset;deep learning;nuclear segmentation;nuclei,Diseases;Image color analysis;Image segmentation;Machine learning;Measurement;Pathology;Training,biological organs;biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;object recognition;optical microscopy,H&E-stained images;Otsu thresholding;chromatin-sparse;computational pathology;conventional image processing techniques;crowded nuclei;deep learning;digital microscopic tissue images;disease states;generalized nuclear segmentation;hematoxylin and eosin-stained tissue images;high-quality feature extraction;image classification problems;machine learning algorithms;machine learning-based segmentation;nuclear appearances;nuclear boundaries;nuclear morphometrics;object recognition;object-level errors;organs;overlapping nuclei;pixel-level errors;right out-of-the-box;segmentation technique;watershed segmentation,,,,,,,20170306.0,July 2017,,IEEE,IEEE Journals & Magazines
318,Knowledge transfer for melanoma screening with deep learning,A. Menegola; M. Fornaciali; R. Pires; F. V. Bittencourt; S. Avila; E. Valle,"RECOD Lab, DCA, FEEC, University of Campinas (Unicamp), Brazil",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,297,300,"Knowledge transfer impacts the performance of deep learning - the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pretrained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950523,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950523,Melanoma screening;deep learning;dermoscopy;transfer learning,Biomedical imaging;Computer architecture;Lesions;Malignant tumors;Retinopathy;Training;Tuning,biomedical optical imaging;cancer;image classification;learning (artificial intelligence);medical image processing;neural nets;skin,AUC;ImageNet;are under curve;automated melanoma screening;deep learning;image classification;knowledge transfer;skin lesion datasets;transfer learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
319,A study on automated segmentation of blood regions in Wireless Capsule Endoscopy images using fully convolutional networks,X. Jia; M. Q. H. Meng,"Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,179,182,"Wireless Capsule Endoscopy (WCE) is a novel diagnostic modality of endoscopic imaging which facilitates direct visualization of the gastrointestinal (GI) tract. Many computational methods that can automatically detect and/or characterize the abnormalities from WCE sequences are developed to support medical decision-making. This paper presents a new approach for automated segmentation of blood regions in WCE images via a deep learning strategy. The proposed method first classify the bleeding samples into active and inactive subgroups based on the statistical features derived from the histogram probability of the color space. Then for each subgroup, we highlight the blood regions via fully convolutional networks (FCNs). Experimental results on the clinical WCE dataset demonstrate the efficacy of our approach, which achieves comparable or better performance than the state-of-the-art methods.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950496,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950496,Wireless capsule endoscopy;bleeding region segmentation;fully convolutional networks,Blood;Endoscopes;Hemorrhaging;Image color analysis;Image segmentation;Training;Wireless communication,biological organs;blood;decision making;endoscopes;image segmentation;image sequences;learning (artificial intelligence);medical image processing;statistical analysis,WCE sequences;automated segmentation;bleeding samples;blood regions;deep learning strategy;endoscopic imaging;fully convolutional networks;gastrointestinal tract;histogram probability;medical decision-making;statistical features;wireless capsule endoscopy images,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
320,Deep Learning for Health Informatics,D. Ravì; C. Wong; F. Deligianni; M. Berthelot; J. Andreu-Perez; B. Lo; G. Z. Yang,"Hamlyn Centre, Imperial College London, London, U.K.",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,4,21,"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.",2168-2194;21682194,,10.1109/JBHI.2016.2636665,EPSRC Smart Sensing for Surgery; EPSRC-NIHR HTC Partnership Award; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801947,Bioinformatics;deep learning;health informatics;machine learning;medical imaging;public health;wearable devices,Artificial neural networks;Biological neural networks;Biomedical imaging;Informatics;Machine learning;Neurons;Training,bioinformatics;learning (artificial intelligence);medical information systems;neural nets,artificial intelligence;deep learning;health informatics;machine learning;medical imaging;medical informatics;multimodality data;parallelization;pervasive sensing;public health;translational bioinformatics,,,,,,,20161229.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
321,Evaluation of feature descriptors for cancerous tissue recognition,P. Stanitsas; A. Cherian; Xinyan Li; A. Truskinovsky; V. Morellas; N. Papanikolopoulos,"Department of Computer Science and Engineering, University of Minnesota, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424.0,2016,,,1490,1495,"Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899848,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848,,Cancer;Covariance matrices;Feature extraction;Geometry;Histograms;Image color analysis;Symmetric matrices,cancer;computer vision;feature extraction;image representation;image texture;medical image processing;neural nets;object recognition,CAD;CKD;RCD;breast carcinomas;cancerous tissue recognition;computer vision;computer-aided diagnosis;convolutional neural networks;covariance based descriptors;covariance kernel descriptor;feature descriptors;feature representation;image descriptors;myometrial leiomyosarcomas;pathologists;prostate carcinomas;region covariance descriptors;texture recognition,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
322,Skin lesion classification from dermoscopic images using deep learning techniques,A. Romero Lopez; X. Giro-i-Nieto; J. Burdick; O. Marques,"Universitat Politecnica de Catalunya, Barcelona, Spain",2017 13th IASTED International Conference on Biomedical Engineering (BioMed),20170406.0,2017,,,49,54,"The recent emergence of deep learning methods for medical image analysis has enabled the development of intelligent medical imaging-based diagnosis systems that can assist the human expert in making better decisions about a patients health. In this paper we focus on the problem of skin lesion classification, particularly early melanoma detection, and present a deep-learning based approach to solve the problem of classifying a dermoscopic image containing a skin lesion as malignant or benign. The proposed solution is built around the VGGNet convolutional neural network architecture and uses the transfer learning paradigm. Experimental results are encouraging: on the ISIC Archive dataset, the proposed method achieves a sensitivity value of 78.66%, which is significantly higher than the current state of the art on that dataset.",,Electronic:978-0-88986-990-5; POD:978-1-5090-4908-0,10.2316/P.2017.852-053,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893267,Convolutional Neural Networks;Deep Learning;Machine Learning;Medical Decision Support Systems;Medical Image Analysis;Skin Lesions,Malignant tumors;Medical diagnostic imaging;Skin,,,,,,,,,,20-21 Feb. 2017,,IEEE,IEEE Conference Publications
323,Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks,L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Transactions on Medical Imaging,20170331.0,2017,36,4.0,994,1004,"Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.",0278-0062;02780062,,10.1109/TMI.2016.2642839,Research Grants Council of the Hong Kong Special Administrative Region; 10.13039/501100003453 - Guangdong Natural Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699,Automated melanoma recognition;fully convolutional neural networks;residual learning;skin lesion analysis;very deep convolutional neural networks,Biomedical imaging;Feature extraction;Image segmentation;Lesions;Malignant tumors;Skin;Training data,biomedical optical imaging;cancer;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;skin,automated melanoma recognition;classification network;deep convolutional neural networks;deep residual networks;dermoscopy images;fully convolutional residual network;low-level hand-crafted features;medical image analysis;skin lesion segmentation;training data,,,,,,,20161221.0,April 2017,,IEEE,IEEE Journals & Magazines
324,Predicting high-risk prognosis from diagnostic histories of adult disease patients via deep recurrent neural networks,Jung-Woo Ha; Adrian Kim; Dongwon Kim; J. Kim; Jeong-Whun Kim; Jin Joo Park; Borim Ryu,"NAVER Corp., Seongnam, Korea",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320.0,2017,,,394,399,"It is a critical issue to predict the prognosis of adult disease patients due to the possibility of spreading to high-risk symptoms in medical fields. Most studies for predicting prognosis have used complex data from patients such as biomedical images, biomarkers, and pathological measurements. We demonstrate a language model-like method for predicting high-risk prognosis from diagnosis histories of patients using deep recurrent neural networks (RNNs), i.e., prognosis prediction using RNN (PP-RNN). The proposed PP-RNN uses multiple RNNs for learning from diagnosis code sequences of patients in order to predict occurrences of high-risk diseases. The use of RNNs allows the model to learn the status changes of patients considering time, thus enhancing prediction accuracy. We evaluate our method on real-world diagnosis data of over 67,000 adult disease patients recorded for 14 years. Experimental results show the proposed PP-RNN outperforms other standard classification models. In particular, our method provides competitive performance with respect to recall and F1-score on high-risk diseases compared to other models. Furthermore, we investigate the effects of the parameters on the performances.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881742,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881742,adult disease;cardiovascular disease;cerebrovascular disease;deep learning;diagnosis sequence;prognosis prediction;recurrent neural network,Biomedical imaging;Diabetes;Diseases;History;Predictive models;Prognostics and health management;Recurrent neural networks,medical diagnostic computing;recurrent neural nets,PP-RNN;adult disease patients;classification models;deep recurrent neural networks;diagnostic histories;high-risk prognosis prediction;language model-like method,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
325,Advanced deep learning for blood vessel segmentation in retinal fundus images,Lua Ngo; Jae-Ho Han,"Dept. Brain and Cognitive Engineering, Korea University, Seoul, South Korea",2017 5th International Winter Conference on Brain-Computer Interface (BCI),20170220.0,2017,,,91,92,"Rising of deep learning methodologies draws huge attention to their application in image processing and classification. Catching up the trends, this study briefly presents state-of-the-art of deep learning applications in medical imaging interfered with achievements of blood vessel segmentation methods in neurosensory retinal fundus images. Successful segmentation based on deep learning offers advantage in diagnosing ophthalmological disease or pathology.",,Electronic:978-1-5090-5096-3; POD:978-1-5090-5097-0,10.1109/IWW-BCI.2017.7858169,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858169,Biomedical optical imaging;blood vessels;fundus images;image segmentation;medical image processing,,blood vessels;image segmentation;learning (artificial intelligence);medical image processing,advanced deep learning methodologies;blood vessel segmentation methods;image classification;image processing;ophthalmological disease;pathology;retinal fundus images,,,,,,,,9-11 Jan. 2017,,IEEE,IEEE Conference Publications
326,Medical Image Denoising Using Convolutional Denoising Autoencoders,L. Gondara,"Dept. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada",2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),20170202.0,2016,,,241,246,"Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.",,Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9,10.1109/ICDMW.2016.0041,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836672,Image denoising;convolutional autoencoder;denoising autoencoder,Biomedical imaging;Convolutional codes;Image denoising;Noise level;Noise measurement;Noise reduction;Training,convolutional codes;image denoising;image reconstruction;learning (artificial intelligence);medical image processing,convolutional denoising autoencoders;deep learning-based models;denoising performances;heterogeneous images;image reconstruction;medical image analysis;medical image denoising,,,,,,,,12-15 Dec. 2016,,IEEE,IEEE Conference Publications
327,A Perspective on Deep Imaging,G. Wang,"Department of Biomedical Engineering, Biomedical Imaging Center, Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, USA",IEEE Access,20170104.0,2016,4,,8914,8924,"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.",2169-3536;21693536,,10.1109/ACCESS.2016.2624938,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733110,Tomographic imaging;big data;data acquisition;deep learning;image analysis;image reconstruction;machine learning;medical imaging,Data acquisition;Deep learning;Image processing;Image reconstruction;Machine learning;Tomography,Big Data;image reconstruction;learning (artificial intelligence),Big data;deep imaging;deep learning;image analysis;image reconstruction techniques;image reconstruction theories;machine learning;medical imaging;tomographic imaging,,1.0,,,,,20161103.0,2016,,IEEE,IEEE Journals & Magazines
328,Super-resolution of medical image using representation learning,X. Yang; S. Zhant; C. Hu; Z. Liang; D. Xie,"School of Computer and Information, Hefei University of Technology, Hefei, China 230009",2016 8th International Conference on Wireless Communications & Signal Processing (WCSP),20161124.0,2016,,,1,6,"Super-resolution (SR) of single image is a meaningful challenge in medical images based diagnosis, while the image resolution is limited. Also, numerous deep neural networks based models were proposed and achieve excellent performance which is superior to the previous handcrafted methods. In this paper, we employ a deep convolutional neural networks for the super-resolution (SR) of single medical image, which learns the nonlinear mapping from the low-resolution space to high-resolution space directly. In addition, we use three sets imaging data (Mammary gland, Prostate tissue and Human brain) training deep network respectively. Firstly, we use Randomized Rectified Linear Unit (RReLU), which incorporates a nonzero slope for negative part to solve the problem of over compression. Secondly, for the purpose of enhancing the quality of reconstructed result and reducing the noise of over-fitting, Nesterov's Accelerated Gradient (NAG) method on the SRCNN is used to accelerate the convergence of loss function and avoid the large oscillations. A comparative performance evaluation is carried out over a set of experiments using real imaging data to verify the validity of proposed algorithm.",,Electronic:978-1-5090-2860-3; POD:978-1-5090-2861-0; USB:978-1-5090-2859-7,10.1109/WCSP.2016.7752617,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752617,Convolutional Neural Networks;Deep Learning;Feature Map;Image Super-Resolution;Representation Learning,Biological neural networks;Image reconstruction;Image resolution;Interpolation;Medical diagnostic imaging;Training,biological tissues;convergence;data handling;image denoising;image resolution;medical image processing;neural nets;patient diagnosis;set theory,NAG;Nesterov accelerated gradient method;RReLU;SR;SRCNN;deep convolutional neural networks;high-resolution space;human brain;imaging data;loss function convergence;low-resolution space;mammary gland;medical image based diagnosis;medical image super-resolution;nonlinear mapping;prostate tissue;randomized rectified linear unit,,,,,,,,13-15 Oct. 2016,,IEEE,IEEE Conference Publications
329,Mass detection using deep convolutional neural network for mammographic computer-aided diagnosis,S. Suzuki; X. Zhang; N. Homma; K. Ichiji; N. Sugita; Y. Kawasumi; T. Ishibashi; M. Yoshizawa,"Graduate School of Engineering, Tohoku University, Sendai, Japan",2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE),20161121.0,2016,,,1382,1386,"In recent years, a deep convolutional neural network (DCNN) has attracted great attention due to its outstanding performance in recognition of natural images. However, the DCNN performance for medical image recognition is still uncertain because collecting a large amount of training data is difficult. To solve the problem of the DCNN, we adopt a transfer learning strategy, and demonstrate feasibilities of the DCNN and of the transfer learning strategy for mass detection in mammographic images. We adopt a DCNN architecture that consists of 8 layers with weight, including 5 convolutional layers, and 3 fully-connected layers in this study. We first train the DCNN using about 1.2 million natural images for classification of 1,000 classes. Then, we modify the last fully-connected layer of the DCNN and subsequently train the DCNN using 1,656 regions of interest in mammographic image for two classes classification: mass and normal. The detection test is conducted on 198 mammographic images including 99 mass images and 99 normal images. The experimental results showed that the sensitivity of the mass detection was 89.9 % and the false positive was 19.2 %. These results demonstrated that the DCNN trained by transfer learning strategy has a potential to be a key system for mammographic mass detection computer-aided diagnosis (CAD). In addition, to the best of our knowledge, our study is the first demonstration of the DCNN for mammographic CAD application.",,Electronic:978-4-907764-50-0; POD:978-1-5090-0937-4,10.1109/SICE.2016.7749265,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749265,Computer-Aided Diagnosis/Detection;DCNN;Deep Learning;Medical and Welfare Systems;Neural Networks;Signal and/or Image Processing;Transfer Learning;mammogram,Breast cancer;Feature extraction;Mammography;Neurons;Training;Training data,image recognition;learning (artificial intelligence);mammography;medical image processing;neural nets,DCNN architecture;deep convolutional neural network;image classification;mammographic CAD application;mammographic computer-aided diagnosis;mass detection;medical image recognition;normal images;transfer learning strategy,,,,,,,,20-23 Sept. 2016,,IEEE,IEEE Conference Publications
330,Binary codes for tagging x-ray images via deep de-noising autoencoders,A. Sze-To; H. R. Tizhoosh; A. K. C. Wong,"Systems Design Engineering, University of Waterloo, Ontario, Canada N2L 3G1",2016 International Joint Conference on Neural Networks (IJCNN),20161103.0,2016,,,2864,2871,"A Content-Based Image Retrieval (CBIR) system which identifies similar medical images based on a query image can assist clinicians for more accurate diagnosis. The recent CBIR research trend favors the construction and use of binary codes to represent images. Deep architectures could learn the non-linear relationship among image pixels adaptively, allowing the automatic learning of high-level features from raw pixels. However, most of them require class labels, which are expensive to obtain, particularly for medical images. The methods which do not need class labels utilize a deep autoencoder for binary hashing, but the code construction involves a specific training algorithm and an ad-hoc regularization technique. In this study, we explored using a deep de-noising autoencoder (DDA), with a new unsupervised training scheme using only backpropagation and dropout, to hash images into binary codes. We conducted experiments on more than 14,000 x-ray images. By using class labels only for evaluating the retrieval results, we constructed a 16-bit DDA and a 512-bit DDA independently. Comparing to other unsupervised methods, we succeeded to obtain the lowest total error by using the 512-bit codes for retrieval via exhaustive search, and speed up 9.27 times with the use of the 16-bit codes while keeping a comparable total error. We found that our new training scheme could reduce the total retrieval error significantly by 21.9%. To further boost the image retrieval performance, we developed Radon Autoencoder Barcode (RABC) which are learned from the Radon projections of images using a de-noising autoencoder. Experimental results demonstrated its superior performance in retrieval when it was combined with DDA binary codes.",,,10.1109/IJCNN.2016.7727561,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727561,,Binary codes;Image retrieval;Medical diagnostic imaging;Noise reduction;Training;X-ray imaging,Radon transforms;X-ray imaging;backpropagation;binary codes;content-based retrieval;feature extraction;image coding;image denoising;image matching;image representation;image retrieval;medical image processing;unsupervised learning,CBIR system;DDA binary codes;RABC;Radon autoencoder barcode;Radon projections;X-ray image tagging;ad-hoc regularization;automatic learning;backpropagation;binary hashing;content-based image retrieval system;deep architectures;deep denoising autoencoders;dropout;high-level features;image pixels;medical images;query image;raw pixels;total retrieval error;unsupervised training scheme;word length 16 bit;word length 512 bit,,,,,,,,24-29 July 2016,,IEEE,IEEE Conference Publications
331,A deep bag-of-features model for the classification of melanomas in dermoscopy images,S. Sabbaghi; M. Aldeen; R. Garnavi,"Department of Electrical and Electronic Engineering, University of Melbourne, Australia",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,1369,1372,"Deep learning and unsupervised feature learning have received great attention in past years for their ability to transform input data into high level representations using machine learning techniques. Such interest has been growing steadily in the field of medical image diagnosis, particularly in melanoma classification. In this paper, a novel application of deep learning (stacked sparse auto-encoders) is presented for skin lesion classification task. The stacked sparse auto-encoder discovers latent information features in input images (pixel intensities). These high-level features are subsequently fed into a classifier for classifying dermoscopy images. In addition, we proposed a new deep neural network architecture based on bag-of-features (BoF) model, which learns high-level image representation and maps images into BoF space. Then, we examine how using this deep representation of BoF, compared with pixel intensities of images, can improve the classification accuracy. The proposed method is evaluated on a test set of 244 skin images. To test the performance of the proposed method, the area under the receiver operating characteristics curve (AUC) is utilized. The proposed method is found to achieve 95% accuracy.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590962,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590962,,Cancer;Feature extraction;Image color analysis;Lesions;Malignant tumors;Skin;Training,biomedical optical imaging;cancer;image classification;image representation;medical image processing;neural nets;sensitivity analysis;skin;unsupervised learning,AUC;BoF space;area under the receiver operating characteristics curve;classification accuracy;deep bag-of-features model;deep learning;deep neural network architecture;deep representation;dermoscopy image classification;high level representations;high-level image representation;image pixel intensity;input images;machine learning techniques;medical image diagnosis;melanoma classification;skin lesion classification task;stacked sparse auto-encoder;unsupervised feature learning,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
332,Membrane segmentation via active learning with deep networks,U. Gaur; M. Kourakis; E. Newman-Smith; W. Smith; B. S. Manjunath,"Department of Computer Science, University of California Santa Barbara",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,1943,1947,"Segmentation is a key component of several bio-medical image processing systems. Recently, segmentation methods based on supervised learning such as deep convolutional networks have enjoyed immense success for natural image datasets and biological datasets alike. These methods require large volumes of data to avoid overfitting which limits their applicability. In this work, we present a transfer learning mechanism based on active learning which allows us to utilize pre-trained deep networks for segmenting new domains with limited labelled data. We introduce a novel optimization criterion to allow feedback on the most uncertain, yet abundant image patterns thus provisioning for an expert in the loop albeit with minimum amount of guidance. Our experiments demonstrate the effectiveness of the proposed method in improving segmentation performance with very limited labelled data.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532697,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532697,Active Learning;Deep Networks;Image Segmentation;Transfer Learning,Computer architecture;Image segmentation;Microprocessors;Microscopy;Optimization;Training;Uncertainty,convolution;image segmentation;learning (artificial intelligence);optimisation,active learning;deep convolutional networks;deep networks;membrane segmentation;optimization criterion;supervised learning;transfer learning mechanism,,,,17.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
333,Retinal vessel segmentation via deep learning network and fully-connected conditional random fields,H. Fu; Y. Xu; D. W. K. Wong; J. Liu,"Ocular Imaging Department, Institute for Infocomm Research, Agency for Science, Technology and Research (A&#8727;STAR), Singapore",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,698,701,"Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493362,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493362,Conditional Random Fields;Convolutional Neural Networks;Vessel segmentation,Computer architecture;Image segmentation;Machine learning;Neural networks;Pathology;Retinal vessels,blood vessels;eye;image segmentation;medical image processing;neural nets,DRIVE dataset;STARE dataset;binary vessel segmentation;boundary detection;convolutional neural networks;deep learning network;fully-connected conditional random fields;fundus image;pathological region;retinal vessel segmentation;vessel probability map,,3.0,,18.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
334,Privacy-preserving deep learning,R. Shokri; V. Shmatikov,"UT Austin, United States","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)",20160407.0,2015,,,909,910,"Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extrajudicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we present a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our pr- vacy-preserving deep learning on benchmark datasets.",,Electronic:978-1-5090-1824-6; POD:978-1-5090-1825-3,10.1109/ALLERTON.2015.7447103,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447103,,Companies;Data models;Data privacy;Decision support systems;Machine learning;Privacy;Training,data privacy;gradient methods;learning (artificial intelligence);neural nets;optimisation,AI-based services;Internet;artificial intelligence;artificial neural networks;data classification;data collection;data modeling;data privacy;data recognition;optimization algorithms;privacy-preserving deep learning;stochastic gradient descent,,1.0,,1.0,,,,Sept. 29 2015-Oct. 2 2015,,IEEE,IEEE Conference Publications
335,The 3-dimensional medical image recognition of right and left kidneys by deep GMDH-type neural network,T. Kondo; S. Takao; J. Ueno,"Graduate School of Health Sciences, Tokushima University, 3-18-15 Kuramoto-cho Tokushima, 770-8509 Japan",2015 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),20160324.0,2015,,,313,320,"In this study, the deep multi-layered Group Method of Data Handling (GMDH)-type neural network algorithm using principal component-regression analysis is applied to recognition problems of the right and left kidney regions. The deep multi-layered GMDH-type neural network algorithm can automatically organize the deep neural network architectures which have many hidden layers and these deep neural networks can identify the characteristics of very complex nonlinear systems. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In this deep GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the deep GMDH-type neural network, and multi-colinearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the right and left kidney regions. The optimum neural network architectures, which fit the complexity of the right and left kidney regions, are automatically organized and the right and left kidney regions are automatically recognized and extracted by the organized deep GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this deep GMDH-type neural networks are useful for the medical image recognition problems of the right and left kidney regions.",,Electronic:978-1-4799-8562-3; POD:978-1-4799-8563-0; USB:978-1-4799-8561-6,10.1109/ICIIBMS.2015.7439548,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439548,Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network,Algorithm design and analysis;Biological neural networks;Biomedical imaging;Computer architecture;Input variables;Kidney;Neurons,data handling;feature extraction;kidney;learning (artificial intelligence);medical image processing;neural nets;principal component analysis;regression analysis,GMDH-type neural network;deep neural network;group method of data handling;kidney region extraction;medical image recognition;principal component analysis;regression analysis,,,,10.0,,,,28-30 Nov. 2015,,IEEE,IEEE Conference Publications
336,A Histopathological Image Feature Representation Method Based on Deep Learning,G. Zhang; L. Zhong; Y. Huang; Y. Zhang,"Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China",2015 7th International Conference on Information Technology in Medicine and Education (ITME),20160310.0,2015,,,13,17,"Automated annotation and grading for histopathological image plays an important role in CAD systems. It provides valuable information and support for medical diagnosis. Currently, computer-aid analysis of histopathological images mainly relies on some well-designed digital features, which requires abundant human efforts and experiences in problem domain. Learning a good feature representation from data can have positive effects on constructing the target model. We propose a novel method for histopathological image feature representation based on deep learning. The method extracts high level representation of raw pixels of a local region through a network model with several hidden layers, which can learn potential features automatically. The proposed method is evaluated on a real data set from a large local hospital with comparison to two current state-of-the-art methods. The result is promising indicating that it achieves significant improvement of the model performance. Moreover, our study suggests that features learned through deep models can achieve better performance than human designed features.",,CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5,10.1109/ITME.2015.34,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429087,deep learning;feature representation;histopathological image analysis;stacked autoencoder,Biomedical imaging;Data models;Feature extraction;Image color analysis;Medical services;Solid modeling;Training,biological tissues;image representation;learning (artificial intelligence);medical image processing,CAD systems;automated annotation;computer-aid analysis;deep learning;histopathological image feature representation method;large local hospital;medical diagnosis,,,,15.0,,,,13-15 Nov. 2015,,IEEE,IEEE Conference Publications
337,Transformation-Invariant Convolutional Jungles,D. Laptev; J. M. Buhmann,"ETH Zurich, Switzerland",2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20151015.0,2015,,,3043,3051,"Many Computer Vision problems arise from information processing of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or - in medical imaging - staining and local warping. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised feature learning approach, which efficiently extracts information from these constraints to produce interpretable, transformation-invariant features. The proposed method can incorporate a large class of transformations, e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel image classification and segmentation method, which we call Transformation-Invariant Convolutional Jungles (TICJ). We test the algorithm on two benchmarks in face recognition and medical imaging, where it achieves state of the art results, while being computationally significantly more efficient than Deep Neural Networks.",1063-6919;10636919,Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3,10.1109/CVPR.2015.7298923,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298923,,Computer vision;Feature extraction;Image segmentation;Kernel;Neural networks;Optimization;Training,computer vision;feature extraction;image classification;image segmentation;learning (artificial intelligence),TICJ;computer vision;face recognition;image classification;image segmentation;information extraction;interpretable feature;medical imaging;recognition algorithms;supervised feature learning approach;transformation-invariant convolutional jungles;transformation-invariant feature,,2.0,,29.0,,,,7-12 June 2015,,IEEE,IEEE Conference Publications
338,Medical Image Recognition of Abdominal Multi-organs by Hybrid Multi-layered GMDH-type Neural Network Using Principal Component-Regression Analysis,T. Kondo; J. Ueno; S. Takao,"Grad. Sch. of Health Sci., Univ. of Tokushima, Tokushima, Japan",2014 Second International Symposium on Computing and Networking,20150302.0,2014,,,157,163,"In this study, hybrid multi-layered Group Method of Data Handling (GMDH) type neural network algorithm using principal component-regression analysis is applied to recognition problems of the abdominal multi-organs such as the liver and spleen. In this GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the GMDH-type neural network which is a type of the deep neural network with many hidden layers. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In the GMDH-type neural network, the multi-co linearity occurs and the prediction values become unstable because the architecture of the neural network has many hidden layers whose characteristics are very complex. In the GMDH-type neural networks in this study, multi-co linearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the liver and spleen. The optimum neural network architectures, which fit the complexity of the liver and spleen images, are automatically organized from the multi-detector row CT (MDCT) image of the abdominal regions and the liver and spleen regions are automatically recognized and extracted by the organized GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this GMDH-type neural networks are useful for the medical image recognition problems of the abdominal multi-organs.",2379-1888;23791888,Electronic:978-1-4799-4152-0; POD:978-1-4799-4151-3,10.1109/CANDAR.2014.62,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052176,Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network,Biological neural networks;Computer architecture;Image recognition;Input variables;Liver;Neurons,backpropagation;computerised tomography;data handling;evolutionary computation;image recognition;liver;medical image processing;principal component analysis;regression analysis;self-organising feature maps,Akaike information criterion;abdominal multiorgans;back propagation method;deep neural network;evolutional computation;group method of data handling;heuristic self-organization method;hybrid multilayered GMDH-type neural network;learning algorithm;liver images;medical image recognition;multidetector row CT image;optimum neural network architectures;prediction error criterion;prediction sum of squares;principal component-regression analysis;sigmoid function neural network;spleen images,,0.0,,10.0,,,,10-12 Dec. 2014,,IEEE,IEEE Conference Publications
339,Hybrid feedback GMDH-type neural network using principal component-regression analysis and its application to medical image recognition of heart regions,T. Kondo; J. Ueno; S. Takao,"Grad. Sch. of Health Sci., Univ. of Tokushima, Tokushima, Japan",2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS),20150219.0,2014,,,1203,1208,"Hybrid feedback Group Method of Data Handling (GMDH)-type neural network using principal component-regression analysis is applied to the medical image recognition of the heart regions. In the GMDH-type neural network, the multi-layered deep neural networks are automatically organized so as to fit the complexity of the nonlinear system and, in general, the architectures of the GMDH-type neural network have many hidden layers and become complex for the nonlinear systems. In the multi-layered deep GMDH-type neural network with many hidden layers, the multi-colinearity occurs and the perdition accuracy become worse and the prediction values become unstable. In the GMDH-type neural network used in this study, the principal component-regression analysis is used as the learning algorithm of the neural network and the multi-colinearity do not occur and accurate and stable GMDH-type neural network architectures are automatically organized so as to fit the complexity of the nonlinear system. Furthermore, in this algorithm, three types of neural networks, such as sigmoid function neural network, radial basis function (RBF) neural network and polynomial neural network, can be generated using three types of neuron architectures, and the neural network architecture which fits the complexity of medical images, is selected from these three neural network architectures. This GMDH-type neural network is applied to the medical image recognition of the heart regions and it is shown that this GMDH-type neural network is useful for the medical image recognition of the heart regions.",,Electronic:978-1-4799-5955-6; POD:978-1-4799-5956-3; USB:978-1-4799-5954-9,10.1109/SCIS-ISIS.2014.7044800,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044800,Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network,Biological neural networks;Biomedical imaging;Computer architecture;Heart;Image recognition;Input variables;Neurons,cardiology;identification;image recognition;learning (artificial intelligence);medical image processing;neural net architecture;nonlinear systems;principal component analysis;radial basis function networks;regression analysis,GMDH-type neural network architectures;RBF neural network;group method of data handling;heart regions;hybrid feedback GMDH-type neural network;medical image recognition;multilayered deep neural networks;neural network learning algorithm;nonlinear systems;polynomial neural network;principal component-regression analysis;radial basis function neural network;sigmoid function neural network,,0.0,,9.0,,,,3-6 Dec. 2014,,IEEE,IEEE Conference Publications
340,Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,380,385,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.45,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178,,Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
341,Single Sensor Techniques for Sleep Apnea Diagnosis Using Deep Learning,R. K. Pathinarupothi; D. P. J; E. S. Rangan; G. E. A; V. R; K. P. Soman,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,524,529,"A large number of obstructive sleep apnea (OSA) cases are under-diagnosed due unavailability, inconvenience or expense of sleep labs. Hence, an automated detection by applying computational techniques to multivariate signals has already become a well-researched subject. However, the best-known techniques that use various features have not achieved the gold standard of polysomnography (PSG) tests. In this paper, we substantiate the medical conjecture that OSA directly impacts body parameters such as Instantaneous Heart Rate (IHR) and blood oxygen saturation (SpO2). We then use a deep learning technique called LSTM-RNN (long short-term memory recurrent neural networks) to experimentally prove that OSA severity detection can be solely based on either IHR or SpO2 signals, which can be easily, obtained using off-the-shelf non-intrusive wearable single sensors. The results obtained from LSTM-RNN model shows an area under curve (AUC) of 0.98 associated with very high accuracy on a dataset of more than 16,000 apnea non-apnea minutes. These results have encouraged our collaborating doctors to further come up with a diagnostic protocol that is based on LSTM-RNN, SpO2, and IHR, thereby increasing the chances of larger adoption among medical community.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.37,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031206,Instantaneous Heart Rate and SpO2;LSTM-RNN;Obstructive sleep apnea,Electrocardiography;Heart rate variability;Medical diagnostic imaging;Sleep apnea;Time series analysis,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
342,Deep tessellated retinal image detection using Convolutional Neural Networks,X. Lyu; H. Li; Y. Zhen; X. Ji; S. Zhang,"Computer Graphics and Imaging Lab, College of Computer Science and Technology, Zhejiang University, Hangzhou, China",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,676,680,"Tessellation in fundus is not only a visible feature for aged-related and myopic maculopathy but also confuse retinal vessel segmentation. The detection of tessellated images is an inevitable processing in retinal image analysis. In this work, we propose a model using convolutional neural network for detecting tessellated images. The input to the model is pre-processed fundus image, and the output indicate whether this photograph has tessellation or not. A database with 12,000 colour retinal images is collected to evaluate the classification performance. The best tessellation classifier achieves accuracy of 97.73% and AUC value of 0.9659 using pretrained GoogLeNet and transfer learning technique.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036915,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036915,Tessellated fundus;convolutional neural network;tessellation detection;transfer learning,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
343,Automated Analysis of Unregistered Multi-view Mammograms with Deep Learning,G. Carneiro; J. Nascimento; A. P. Bradley,"Australian Centre for Visual Technologies, University of Adelaide, Australia.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient’s risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calcifications). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calcification detection systems, and still producing accurate results. The semi-automated approach (using manually defined mass and micro-calcification segmentation maps) is tested on two publicly available datasets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class ”benign vs malignant” problem is over 0.9, and for the 2- class breast screening problem (malignancy vs normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class ”benign vs malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.",0278-0062;02780062,,10.1109/TMI.2017.2751523,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8032490,Deep learning;Mammogram;Multi-view classification;Transfer learning,Breast;Cancer;Lesions;Machine learning;Mammography;Training,,,,,,,,,20170912.0,,,IEEE,IEEE Early Access Articles
344,Structure Prediction for Gland Segmentation with Hand-Crafted and Deep Convolutional Features,S. Manivannan; W. Li; J. Zhang; E. Trucco; S. McKenna,"University of Jaffna, Sri Lanka and University of Dundee during the initial stages of the research reported here.",IEEE Transactions on Medical Imaging,,2017,PP,99.0,1,1,"We present a novel method to segment instances of glandular structures from colon histopathology images. We use a structure learning approach which represents local spatial configurations of class labels, capturing structural information normally ignored by sliding-window methods. This allows us to reveal different spatial structures of pixel labels (e.g., locations between adjacent glands, or far from glands), and to identify correctly neighbouring glandular structures as separate instances. Exemplars of label structures are obtained via clustering and used to train support vector machine classifiers. The label structures predicted are then combined and post-processed to obtain segmentation maps. We combine hand-crafted, multi-scale image features with features computed by a deep convolutional network trained to map images to segmentation maps. We evaluate the proposed method on the public domain GlaS dataset, which allows extensive comparisons with recent, alternative methods. Using the GlaS contest protocol, our method achieves the overall best performance.",0278-0062;02780062,,10.1109/TMI.2017.2750210,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030141,Gastrointestinal tract;Molecular and cellular imaging;Segmentation,Feature extraction;Glands;Image segmentation;Morphology;Support vector machines;Training,,,,,,,,,20170908.0,,,IEEE,IEEE Early Access Articles
345,HCNN: Heterogeneous Convolutional Neural Networks for Comorbid Risk Prediction with Electronic Health Records,J. Zhang; J. Gong; L. Barnes,"Dept. of Syst. & Inf. Eng., Univ. of Virginia, Charlottesville, VA, USA","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20170817.0,2017,,,214,221,"The increasing adoption of electronic health record (EHR) systems has brought tremendous opportunities in medicine enabling more personalized prognostic models. However, most work to date has investigated the binary classification problem for predicting the onset of one chronic disease, but little attention has been given to assessing risk of developing comorbidities that are major causes of morbidity and mortality. For example, type 2 diabetes and chronic kidney disease frequently accompany congestive heart failure. This paper is motivated by the problem of predicting comorbid diseases and aims to answer the following question: can we predict the comorbid risk using a patient's medical history? We propose a new predictive learning framework, Heterogeneous Convolutional Neural Network (HCNN), that represents EHRs as graphs with heterogeneous attributes (e.g. diagnoses, procedures, and medication), and then develop a novel deep learning methodology for risk prediction of multiple comorbid diseases. The main innovation of the framework is that it defines the distance between the heterogeneous attributes of the graph representation extracted from the EHR and develops an appropriate learning infrastructure that is a composition of sparse convolutional layers and local pooling steps that match with the local structure of the space of the heterogeneous attributes. As a result, the new method is capable of capturing features about the relationships between heterogeneous attributes of the graphs. Through a comparative study on patient EHR data, HCNN achieves better performance than traditional convolutional neural networks on the risk prediction of comorbid diseases.",,Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5,10.1109/CHASE.2017.80,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010635,Electronic Health Records;Heterogeneous Convolution;Neural Networks;Risk Prediction,Convolution;Correlation;Diseases;Feature extraction;Machine learning;Medical diagnostic imaging;Neural networks,diseases;electronic health records;feedforward neural nets;learning (artificial intelligence);pattern classification;risk management,EHR systems;HCNN;binary classification problem;chronic disease;comorbid diseases;comorbid risk prediction;deep learning methodology;electronic health records;graph representation;heterogeneous attributes;heterogeneous convolutional neural networks;learning infrastructure;local pooling steps;personalized prognostic models;predictive learning framework;sparse convolutional layers,,,,,,,,17-19 July 2017,,IEEE,IEEE Conference Publications
346,Leveraging deep preference learning for indexing and retrieval of biomedical images,S. Pang; M. A. Orgun; A. Du; Z. Yu,"College of Computer Science and Technology, Jilin University, Qianjin Street: 2699, China",2017 8th International IEEE/EMBS Conference on Neural Engineering (NER),20170814.0,2017,,,126,129,"This paper presents an original framework based on deep learning and preference learning to retrieve and characterize biomedical images for assisting physicians in diagnosing complex diseases with potentially only small differences between them. In particular, we use deep learning to extract the high-level and compact features for biomedical images. In contrast to the traditional biomedical algorithms or general image retrieval systems that only consider the use of pixel and/or hand-crafted features to represent images, we utilize deep neural networks for feature discovery of biomedical images. Moreover, in order to be able to index the similarly referenced images, we introduce preference learning in a novel way to learn what kinds of images we need so that we can obtain the similarity ranking list of biomedical images. We evaluate the performance of our system in detailed experiments over the well-known available OASIS-MRI database for whole brain neuroimaging as a benchmark and compare it with those of the traditional biomedical and general image retrieval approaches. Our proposed system exhibits an outstanding retrieval ability and efficiency for biomedical image applications.",,Electronic:978-1-5090-4603-4; POD:978-1-5090-4604-1,10.1109/NER.2017.8008308,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008308,,Biological system modeling;Biomedical imaging;Feature extraction;Image retrieval;Indexing;Visualization,biomedical MRI;brain;database indexing;feature extraction;image retrieval;medical image processing;neurophysiology,OASIS-MRI database;biomedical image diagnosing;biomedical image indexing;biomedical image retrieval;brain neuroimaging;deep learning;deep neural networks;deep preference learning;diseases;feature discovery;high-level features;similarity ranking list,,,,,,,,25-28 May 2017,,IEEE,IEEE Conference Publications
347,Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition From Human Brain EEG Sensors,R. Majid Mehmood; R. Du; H. J. Lee,"Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea",IEEE Access,20170809.0,2017,5,,14797,14806,"Recent advancements in human–computer interaction research have led to the possibility of emotional communication via brain–computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12–14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.",,,10.1109/ACCESS.2017.2724555,"Brain Korea 21 PLUS Project; Ministry of Science, ICT and Future Planning, Korea, under the Information Technology Research Center, supervised by the Institute for Information and Communications Technology Promotion; NUPTSF; 10.13039/100010002 - Basic Science Research Program through the NRF of South Korea, through the Ministry of Education; 10.13039/501100001809 - National Natural Science Foundation for Young Scholars of China; 10.13039/501100003725 - National Research Foundation (NRF) of Korea; 10.13039/501100004608 - Natural Science Foundation for Young Scholars of Jiangsu Province; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997991,EEG emotion recognition;EEG feature extraction;EEG pattern recognition;Hjorth parameter,Electrodes;Electroencephalography;Emotion recognition;Feature extraction;Medical services;Sensors;Support vector machines,,,,,,,,,20170731.0,2017,,IEEE,IEEE Journals & Magazines
348,Adaptive smoothing in fMRI data processing neural networks,A. Vilamala; K. H. Madsen; L. K. Hansen,Technical University of Denmark,2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI),20170720.0,2017,,,1,4,"Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data processing pipelines to accurately determine brain activity; among them, the crucial step of spatial smoothing. These pipelines are commonly suboptimal, given the local optimisation strategy they use, treating each step in isolation. With the advent of new tools for deep learning, recent work has proposed to turn these pipelines into end-to-end learning networks. This change of paradigm offers new avenues to improvement as it allows for a global optimisation. The current work aims at benefitting from this paradigm shift by defining a smoothing step as a layer in these networks able to adaptively modulate the degree of smoothing required by each brain volume to better accomplish a given data analysis task. The viability is evaluated on real fMRI data where subjects did alternate between left and right finger tapping tasks.",,Electronic:978-1-5386-3159-1; POD:978-1-5386-3160-7,10.1109/PRNI.2017.7981499,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981499,,Biological neural networks;Correlation;Data processing;Pipelines;Smoothing methods;Standards;Training,biomedical MRI;brain;data analysis;learning (artificial intelligence);medical image processing;neural nets;optimisation,adaptive smoothing;brain volume;data analysis;deep learning;end-to-end learning networks;fMRI data;fMRI data processing;functional magnetic resonance imaging;left finger tapping tasks;local optimisation strategy;multistep data processing pipelines;neural networks;paradigm shift;right finger tapping tasks,,,,,,,,21-23 June 2017,,IEEE,IEEE Conference Publications
349,Deep learning approach for EEG compression in mHealth system,A. Ben Said; A. Mohamed; T. Elfouly,"Computer Science and Engineering Department, Qatar Univrsity 2713, Doha, Qatar",2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC),20170720.0,2017,,,1508,1512,"The emergence of mobile health (mHealth) systems has risen the challenges and concerns due to the sensitivity of the data involved in such systems. It is essential to ensure that these data are well delivered to the health monitoring center for accurate and perfect diagnosis and follow-up. Due to the wireless network constraints, these requirements become more challenging. In this paper, we propose a deep learning approach for EEG data compression in mHealth system. We show that the stacked autoencoder neural network architecture is efficient for EEG data compression. We conduct a comprehensive comparative study that demonstrates the effectiveness of our system for EEG compression in addition to preserving the total energy consumption.",,Electronic:978-1-5090-4372-9; POD:978-1-5090-4373-6,10.1109/IWCMC.2017.7986507,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986507,EEG;compression;mHealth;stacked autoencoder,Data compression;Discrete wavelet transforms;Distortion;Electroencephalography;Image coding;Machine learning;Neural networks,electroencephalography;learning (artificial intelligence);medical signal processing;mobile computing;neural nets;patient monitoring,EEG data compression;deep learning approach;health monitoring center;mHealth system;mobile health systems;patient diagnosis;stacked autoencoder neural network architecture;wireless network constraints,,,,,,,,26-30 June 2017,,IEEE,IEEE Conference Publications
350,Automating Papanicolaou Test Using Deep Convolutional Activation Feature,J. Hyeon; H. J. Choi; K. N. Lee; B. D. Lee,"Sch. of Comput., KAIST, Daejeon, South Korea",2017 18th IEEE International Conference on Mobile Data Management (MDM),20170703.0,2017,,,382,385,"Cervical cancer is the women's fourth most common cancer worldwide, with 266,000 deaths in a year. Cervical cancer can be diagnosed by the Papanicolaou test. In this test, a cytopathologist observes a microscopic image of the cervix cells and decides whether the patient is abnormal or not. According to research, the accuracy of the cervical cytology is reported as 89.7%. Because it is associated with the patient's life, it is important to improve the accuracy of this test. Many systems have been proposed to help judge experts to improve the accuracy of tests in the medical field, but development has been limited to areas where there are cleanly quantified test data. In this paper, we design and train a model to automatically classify the normal/abnormal state of cervical cells from microscopic images by using a convolutional neural network and several machine learning classifiers. As a result, the support vector machine achieves the highest performance with 78% F1 score.",,Electronic:978-1-5386-3932-0; POD:978-1-5386-3933-7,10.1109/MDM.2017.66,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962484,Cervical Cancer;Cervical Cancer Screening Test;Convolutional Neural Network;Deep Convolutional Activation Feature;Papanicolaou Test,Biological neural networks;Cervical cancer;Feature extraction;Microscopy;Support vector machines,biomedical optical imaging;cancer;cellular biophysics;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;microscopes;support vector machines,F1 score;Papanicolaou test automation;automatic abnormal state classification;automatic normal state classification;cervical cancer;cervical cytology;cervix cells;convolutional neural network;deep convolutional activation feature;machine learning classifiers;medical field;microscopic image;support vector machine,,,,,,,,May 29 2017-June 1 2017,,IEEE,IEEE Conference Publications
351,Deep learning of texture and structural features for multiclass Alzheimer's disease classification,C. V. Dolph; M. Alam; Z. Shboul; M. D. Samad; K. M. Iftekharuddin,"Vision Lab at Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA 23529",2017 International Joint Conference on Neural Networks (IJCNN),20170703.0,2017,,,2259,2266,"This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6% and 58.0% tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4% and 56.8%. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2% in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7966129,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966129,ADNI;Alzheimer's disease;biomarkers;deep learning;dropout learning;hippocampus;neuroimaging classification,Brain modeling;Computational modeling;Diseases;Feature extraction;Machine learning;Magnetic resonance imaging;Sensitivity,biomedical MRI;feature extraction;feature selection;image classification;image texture;matrix algebra;medical image processing;neural nets,CADDementia competition;SAE deep neural network;cognitive normal;cortical surface area;cortical thickness;feature selection;fractal Brownian motion co-occurrence matrices;gray matter volumes;magnetic resonance imaging;mild cognitive impairment;multiclass Alzheimer's disease classification;multiclass deep learning classification;stacked auto-encoder;structural MRI;structural feature;subcortical area specific feature extraction;texture feature;true positive fraction;white matter volumes,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
352,"Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation",J. Egede; M. Valstar; B. Martinez,"Sch. of Comput. Sci., Univ. of Nottingham, Ningbo, China",2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),20170629.0,2017,,,689,696,"Automatic continuous time, continuous value assessment of a patient's pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth.",,Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7,10.1109/FG.2017.87,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961808,,Estimation;Face;Feature extraction;Machine learning;Pain;Physiology;Shape,learning (artificial intelligence);mean square error methods;medical image processing;pose estimation;time series;video coding,RMSE error;automatic pain estimation;deep learning;deep-learned features;face video;hand-crafted features;medical diagnosis;shape encoding;time series,,,,,,,,May 30 2017-June 3 2017,,IEEE,IEEE Conference Publications
353,Cells classification with deep learning,A. Sezer; U. Çekmez,"Bilgisayar M&#x00FC;hendisligi B&#x00F6;l&#x00FC;m&#x00FC;, Y&#x0131;ld&#x0131;z Teknik &#x00DC;niversitesi, 34220 Istanbul, T&#x00FC;rkiye",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"Proteomic analysis is a rapidly developing research field that has recently been used in the diagnosis and treatment of various diseases by analyzing the structure and functions of protein patterns in the cell. Numerous computer based decision support mechanisms implemented in this context have mostly used special image processing techniques until now. Recently, high performance self-learning deep learning methods have taken place in the classification studies over the conventional methods examining the structural features of the patterns, shapes and the texture properties in the images. In this study, different intracellular patterns of HeLa cells taken by the microscope used in the testing of pattern analysis and the output is compared by classifying these patterns by using both deep learning methods and bag-of-features method. As a result of the experiments, it is seen that the success of the proposed deep learning model has a very high performance in classifying compared to the existing models and bag-of-features technique.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960647,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960647,classification;convolutional deep neural networks;deep learning;genome;protein;proteomic analysis,Bioinformatics;Dogs;Genomics;Machine learning;Neural networks;Pattern recognition;Proteins,cellular biophysics;diseases;image classification;image texture;learning (artificial intelligence);medical image processing;patient diagnosis;patient treatment;proteins,HeLa cell intracellular patterns;cell classification;computer-based decision support mechanisms;high-performance self-learning deep learning;image processing;pattern structural features;proteomic analysis;texture properties,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
354,Deep neural network based diagnosis system for melanoma skin cancer,A. Baştürk; M. E. Yüksei; H. Badem; A. Çalışkan,"Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Erciyes &#x00DC;niversitesi, Kayseri",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629.0,2017,,,1,4,"Melanoma is a serious cancer that causes many people to lose their lives. This disease can be diagnosed by a dermatologist as a result of interpretation of the dermoscopy images by the ABCD rule. In this study, a deep neural network (DNN) is used as a new method for diagnosis of melanoma skin cancer. This method is compared with the-state-art-methods in literature. According to the obtained results, DNN was more successful than the comparative methods.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960563,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960563,ABCD rule;deep learning;deep neural network;melanoma,Cancer;Dogs;Lesions;Malignant tumors;Neural networks;Niobium;Support vector machines,cancer;medical diagnostic computing;neural nets;skin,ABCD rule;DNN;deep neural network based diagnosis system;dermoscopy images;melanoma skin cancer,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
355,Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods,G. Carneiro; T. Peng; C. Bayer; N. Navab,"Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia",IEEE Transactions on Medical Imaging,20170628.0,2017,36,7.0,1405,1417,"In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitativ- results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.",0278-0062;02780062,,10.1109/TMI.2017.2677479,10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Experienced Researchers; 10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Postdoctoral Researchers; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869416,Microscopy;deep learning;high-order loss functions;structured output learning;weakly-supervised training,Biomedical imaging;Cancer;Computational modeling;Manuals;Medical treatment;Training;Tumors,biomedical optical imaging;cancer;fluorescence;image classification;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;tumours,HE images;IF images;acutely hypoxic regions;automatic quantification;baseline method;cancer patients;chronically hypoxic regions;clinical annotations;estimated annotations;fixed grid;fully-convolutional neural network;hematoxylin and eosin stained images;high resolution immunofluorescence images;high-order loss function;histological specimen;hypoxia-modified therapies;input images;latent structured output learning formulation;machine learning-based methodologies;majority classifier;manual annotations;multimodal microscopy images;naive method;normoxic regions;pixel-wise labeling;standard cancer treatments;standardized measurement;tumor hypoxia;tumour hypoxia;weakly-supervised learning methods;weakly-supervised structured output classification problem,,,,,,,20170302.0,July 2017,,IEEE,IEEE Journals & Magazines
356,Nuclei segmentation in histopathology images using deep neural networks,P. Naylor; M. Laé; F. Reyal; T. Walter,"MINES ParisTech, PSL Research University, CBIO - Centre de Bioinformatique, 77300 Fontainebleau, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,933,936,"Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950669,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950669,Breast Cancer;Cellular Phenotyping;Convolutional Neural Networks;Deep Learning;Digital Pathology;Histopathology;Nuclei Segmentation,Cancer;Computer architecture;Image segmentation;Machine learning;Microprocessors;Neural networks;Semantics,biomedical optical imaging;cancer;cellular biophysics;image segmentation;medical image processing;neural nets;probability;tumours,automatic segmentation;cancer diagnosis;cancer prognosis;cell nuclei;cellular phenotypes;complex image data;deep neural networks;digital pathology;fully automated workflow;histopathology image data;image data set;manually annotated images;nuclei segmentation;pathologists;posterior probability maps;scientific community;split jointly segmented nuclei;stained tumor sections;tissular phenotypes,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
357,Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images,B. Ehteshami Bejnordi; J. Lin; B. Glass; M. Mullooly; G. L. Gierach; M. E. Sherman; N. Karssemeijer; J. van der Laak; A. H. Beck,"Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, Netherlands",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,929,932,"Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950668,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950668,Breast Cancer;Convolutional Neural Networks;Digital pathology;Tumor Associated Stroma,Breast cancer;Feature extraction;Training;Tumors,biomedical optical imaging;cancer;cellular biophysics;learning (artificial intelligence);mammography;medical image processing;patient diagnosis;tumours,breast cancer diagnosis;breast carcinomas diagnosis;breast tissue biopsies;convolutional neural networks;deep learning-based assessment;epithelial cells;epithelial regions;epithelial tissue architecture;hematoxyli-and-eosin stained breast specimens;histopathology images;tumor-associated stroma,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
358,Multi-stage segmentation of the fovea in retinal fundus images using fully Convolutional Neural Networks,S. Sedai; R. Tennakoon; P. Roy; K. Cao; R. Garnavi,"IBM Research - Australia, Melbourne, VIC, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1083,1086,"The fovea is one of the most important anatomical landmarks in the eye and its localization is required in automated analysis of retinal diseases due to its role in sharp central vision. In this paper, we propose a two-stage deep learning framework for accurate segmentation of the fovea in retinal colour fundus images. In the first stage, coarse segmentation is performed to localize the fovea in the fundus image. The location information from the first stage is then used to perform fine-grained segmentation of the fovea region in the second stage. The proposed method performs end-to-end pixelwise segmentation by creating a deep learning model based on fully convolutional neural networks, which does not require the prior knowledge of the location of other retinal structures such as optic disc (OD) and vasculature geometry. We demonstrate the effectiveness of our method on a dataset with 400 retinal images with average localization error of 14 ± 7 pixels.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950704,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950704,Convolution neural network;Fovea Segmentation;Retinal Imaging,Convolution;Image segmentation;Machine learning;Neural networks;Optical imaging;Retina;Training,biomedical optical imaging;eye;image segmentation;medical image processing;neural nets,end-to-end pixelwise segmentation;fovea;fully convolutional neural networks;multistage segmentation;optic disc;retinal colour fundus images;retinal structures;two-stage deep learning framework;vasculature geometry,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
359,Wide residual networks for mitosis detection,E. Zerhouni; D. Lányi; M. Viana; M. Gabrani,"IBM Research Zurich, S&#x00E4;umerstrasse 4, 8803 R&#x00FC;schlikon, Switzerland",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,924,928,"One of the most important prognostic markers to assess proliferation activity of breast tumors is estimating the number of mitotic figures in H&E stained tissue. We propose the use of a recently published convolutional neural network architecture, Wide Residual Networks, for mitosis detection in breast histology images. The model is trained to classify each pixel of on an image using as context a patch centered on the pixel. We apply post-processing on the network output in order to filter out noise and select true mitosis. Finally, we combine the output of several networks using majority vote. Our approach ranked 2nd in the MICCAI TUPAC 2016 competition for mitosis detection, outperforming most other contestants by a significant margin.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950667,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950667,Mitotic activity;convolutional network;deep learning;tumor proliferation;wide-residual network,Image processing;Machine learning;Neural networks;Pathology;Shape;Testing;Training,cellular biophysics;diagnostic radiography;image classification;image filtering;mammography;medical image processing;neural nets;tumours,breast histology images;breast tumors;convolutional neural network architecture;hematoxylin-and-eosin-stained-tissue;image classification;image filtering;mitosis detection;mitotic figures;proliferation activity;wide residual networks,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
360,Epithelium-stroma classification in histopathological images via convolutional neural networks and self-taught learning,Y. Huang; H. Zheng; C. Liu; G. Rohde; D. Zeng; J. Wang; X. Ding,"School of Information Science and Engineering, Xiamen University, 361005, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619.0,2017,,,1073,1077,"Epithelium-stroma classification is always considered as an important preprocessing step for morphological quantitative analysis in image-based histological researches of oncologic diseases. However, large-scale accurate ground-truth labeling is expensive in histopathological image analysis, thus the classification performances will still be limited with the insufficient labeled training samples. Considering that acquisition of public unlabeled histopathological images is much cheaper, an epithelium-stroma classification framework is developed, based on the deep convolutional neural network framework and the strategies of self-taught learning. The method has the ability of taking advantage of large-scale unlabeled public histopathological data as auxiliary data, and then transferring the knowledge to enhance the performances in epithelium-stroma classification with limited labeled training data. The experiments demonstrate that the proposed method outperforms traditional CNNs when the labeled training data size is decreasing dramatically.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7952321,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952321,convolutional neural networks;epithelium-stroma classification;histopathological image analysis;self-taught learning;transfer learning,Dictionaries;Feature extraction;Kernel;Neural networks;Testing;Training;Training data,diseases;image classification;medical image processing;neural nets;tumours,auxiliary data;convolutional neural networks;deep convolutional neural network framework;epithelium-stroma classification framework;histopathological image analysis;image-based histological researches;labeled training data size;large-scale accurate ground-truth labeling;large-scale unlabeled public histopathological data;morphological quantitative analysis;oncologic diseases;public unlabeled histopathological images;self-taught learning,,,,,,,,5-9 March 2017,,IEEE,IEEE Conference Publications
361,Disease grading of heterogeneous tissue using convolutional autoencoder,E. Zerhouni; B. Prisacari; Q. Zhong; P. Wild; M. Gabrani,"IBM Research-Z&#x00FC;rich, Saeumerstrasse 4, 8803 Rueschlikon, Switzerland",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,596,599,"One of the main challenges of histological image analysis is the high dimensionality of the images. This can be addressed via summarizing techniques or feature engineering. However, such approaches can limit the performance of subsequent machine learning models, particularly when dealing with highly heterogeneous tissue samples. One possible alternative is to employ unsupervised learning to determine the most relevant features automatically. In this paper, we propose a method of generating representative image signatures that are robust to tissue heterogeneity. At the core of our approach lies a novel deep-learning based mechanism to simultaneously produce representative image features as well as perform dictionary learning to further reduce dimensionality. By integrating this mechanism in a broader framework for disease grading, we show significant improvement in terms of grading accuracy compared to alternative local feature extraction methods.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950591,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950591,Convolutional Autoencoder;Dimensionality Reduction;Image Description;Tissue Heterogeneity,Dictionaries;Diseases;Feature extraction;Image color analysis;Image reconstruction;Morphology;Training,biological tissues;convolutional codes;diseases;feature extraction;image coding;learning (artificial intelligence);medical image processing,convolutional autoencoder;deep-learning;dictionary learning;disease grading;feature engineering;feature extraction;heterogeneous tissue;histological image analysis;image features;machine learning;tissue heterogeneity;unsupervised learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
362,Deep learning based multi-label classification for surgical tool presence detection in laparoscopic videos,S. Wang; A. Raju; J. Huang,"The University of Texas at Arlington, Dept. of Computer Science and Engineering, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,620,623,"Automatic recognition of surgical workflow is an unresolved problem among the community of computer-assisted interventions. Among all the features used for surgical workflow recognition, one important feature is the presence of the surgical tools. Extracting this feature leads to the surgical tool presence detection problem to detect what tools are used at each time in surgery. This paper proposes a deep learning based multi-label classification method for surgical tool presence detection in laparoscopic videos. The proposed method combines two state-of-the-art deep neural networks and uses ensemble learning to solve the tool presence detection problem as a multi-label classification problem. The performance of the proposed method has been evaluated in the surgical tool presence detection challenge held by Modeling and Monitoring of Computer Assisted Interventions workshop. The proposed method shows superior performance compared to other methods and has won the first place of the challenge.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950597,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950597,Deep learning;Ensemble;Multi-label classification;Surgical tool detection,Machine learning;Neural networks;Surgery;Testing;Tools;Training;Videos,image classification;learning (artificial intelligence);medical image processing;neural nets;surgery,automatic surgical workflow recognition;computer-assisted intervention;deep learning-based multilabel classification;deep neural networks;ensemble learning;laparoscopic video;surgical tool presence detection,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
363,A fully automatic deep learning method for atrial scarring segmentation from late gadolinium-enhanced MRI images,G. Yang; X. Zhuang; H. Khan; S. Haldar; E. Nyktari; X. Ye; G. Slabaugh; T. Wong; R. Mohiaddin; J. Keegan; D. Firmin,"Cardiovascular Biomedical Research Unit, Royal Brompton Hospital, SW3 6NP, London, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,844,848,"Precise and objective segmentation of atrial scarring (SAS) is a prerequisite for quantitative assessment of atrial fibrillation using non-invasive late gadolinium-enhanced (LGE) MRI. This also requires accurate delineation of the left atrium (LA) and pulmonary veins (PVs) geometry. Most previous studies have relied on manual segmentation of LA wall and PVs, which is a tedious and error-prone procedure with limited reproducibility. There are many attempts on automatic SAS using simple thresholding, histogram analysis, clustering and graph-cut based approaches; however, in general, these methods are considered as unsupervised learning thus subject to limited segmentation accuracy. In this study, we present a fully-automated multi-atlas based whole heart segmentation method to derive the LA and PVs geometry objectively that is followed by a fully automatic deep learning method for SAS. Our deep learning method consists of a feature extraction step via super-pixel over-segmentation and a supervised classification step via stacked sparse auto-encoders. We demonstrate the efficacy of our method on 20 clinical LGE MRI scans acquired from a longstanding persistent atrial fibrillation cohort. Both quantitative and qualitative results show that our fully automatic method obtained accurate segmentation results compared to the manual segmentation based ground truths.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950649,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950649,,Feature extraction;Geometry;Image segmentation;Magnetic resonance imaging;Manuals;Myocardium;Synthetic aperture sonar,biomedical MRI;blood vessels;cardiology;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing,atrial fibrillation;atrial scarring segmentation;clustering approach;feature extraction step;fully automatic deep learning method;fully-automated multiatlas based whole heart segmentation method;graph-cut based approach;histogram analysis;late gadolinium-enhanced MRI images;left atrium;pulmonary veins;simple thresholding;stacked sparse autoencoders;super-pixel over-segmentation;supervised classification step;unsupervised learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
364,Parcellation of visual cortex on high-resolution histological brain sections using convolutional neural networks,H. Spitzer; K. Amunts; S. Harmeling; T. Dickscheid,"Institute of Neuroscience and Medicine (INM-1), Forschungszentrum J&#x00FC;lich, Germany",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,920,923,"Microscopic analysis of histological sections is considered the “gold standard” to verify structural parcellations in the human brain. Its high resolution allows the study of laminar and columnar patterns of cell distributions, which build an important basis for the simulation of cortical areas and networks. However, such cytoarchitectonic mapping is a semiautomatic, time consuming process that does not scale with high throughput imaging. We present an automatic approach for parcellating histological sections at 2μm resolution. It is based on a convolutional neural network that combines topological information from probabilistic atlases with the texture features learned from high-resolution cell-body stained images. The model is applied to visual areas and trained on a sparse set of partial annotations. We show how predictions are transferable to new brains and spatially consistent across sections.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950666,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950666,Brain Parcellation;Convolutional Networks;Deep Learning;Human Brain;Mapping,Brain modeling;Data models;Image resolution;Image segmentation;Probabilistic logic;Training;Visualization,biomedical optical imaging;brain;cellular biophysics;image texture;medical image processing;neural nets;probability,cell distributions;columnar patterns;convolutional neural networks;cortical areas;cortical networks;cytoarchitectonic mapping;gold standard;high throughput imaging;high-resolution cell-body stained images;high-resolution histological brain sections;histological sections;human brain;laminar patterns;microscopic analysis;partial annotations;probabilistic atlases;structural parcellations;texture features;topological information;visual areas;visual cortex parcellation,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
365,A generalized MRI-based CAD system for functional assessment of renal transplant,F. Khalifa; M. Shehata; A. Soliman; M. Abou El-Ghar; T. El-Diasty; A. C. Dwyer; M. El-Melegy; G. Gimel'farb; R. Keynton; A. El-Baz,"Bioengineering Department, University of Louisville, KY, USA",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,758,761,"In recent years, magnetic resonance imaging (MRI) has been explored for non-invasive assessment of renal transplant function. This paper proposes a computer-aided diagnostic (CAD) system for the assessment of renal transplant status, which integrates both clinical and MRI-derived biomarkers. The latter are derived from either 3D (2D + time) dynamic contrast-enhanced MRI or 4D (3D + b-value) diffusion-weighted (DW) MRI. In order to extract the MRI-based biomarkers, our framework performs multiple image processing steps, including MRI data alignment to handle the motion effects, kidney segmentation using a geometric deformable model, local motion correction, and estimation of image-based biomarkers. These biomarkers are fused with clinical biomarkers (creatinine clearance and serum plasma creatinine) for the classification of transplant status using a machine learning classifier. Our CAD system has been tested on a cohort of 100 subjects (50 DCE-MRI and 50 DW-MRI) using a “leave-one-subject-out” approach and distinguished rejection from non-rejection transplants with an overall accuracy of 98% for both DCE-MRI and DW-MRI data sets. These preliminary results demonstrate the promise of the proposed CAD system as a reliable non-invasive diagnostic tool for renal transplant assessment.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950629,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950629,CAD;Deep Learning;MRI;Renal rejection,Biomarkers;Image segmentation;Kidney;Machine learning;Magnetic resonance imaging;Motion segmentation;Three-dimensional displays,biomedical MRI;image classification;learning (artificial intelligence);medical image processing,MRI-based CAD system;MRI-based biomarkers;computer-aided diagnostic system;machine learning classifier;magnetic resonance imaging;renal transplant assessment;renal transplant function,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
366,Automated 5-year mortality prediction using deep learning and radiomics features from chest computed tomography,G. Carneiro; L. Oakden-Rayner; A. P. Bradley; J. Nascimento; L. Palmer,"Australian Centre for Visual Technologies, The University of Adelaide, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,130,134,"In this paper, we propose new prognostic methods that predict 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on two state-of-the-art deep learning models extended to 3-D inputs, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection and extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning models produces a mean 5-year mortality prediction AUC in [68.8%,69.8%] and accuracy in [64.5%,66.5%], while radiomics produces a mean AUC of 64.6% and accuracy of 64.6%. The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950485,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950485,computed tomography;deep learning;feature learning;five-year mortality;hand-designed features;radiomics,Biomedical imaging;Computed tomography;Fats;Feature extraction;Image segmentation;Machine learning;Training,computerised tomography;feature extraction;geriatrics;health care;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation,3-D inputs;CT image;annotated chest CT;automated 5-year mortality prediction;chest computed tomography;classifier learning process;deep learning models;elderly individuals;feature extraction;hand-crafted radiomics features;mean 5-year mortality prediction AUC;multiple anatomic structures;multistage framework;personalised healthcare;preventive healthcare;prognostic methods;segmentation maps;single optimisation process;unified framework,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
367,Skin melanoma segmentation using recurrent and convolutional neural networks,M. Attia; M. Hossny; S. Nahavandi; A. Yazdabadi,"Institute for Intelligent Systems Research and Innovation, Deakin University, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,292,296,"Skin melanoma is one of the highly addressed health problems in many countries. Dermatologists diagnose melanoma by visual inspections of mole using clinical assessment tools such as ABCD. However, computer vision tools have been introduced to assist in quantitative analysis of skin lesions. Deep learning is one of the trending machine learning techniques that have been successfully utilized to solve many difficult computer vision tasks. We proposed using a hybrid method that utilizes two popular deep learning methods: convolutional and recurrent neural networks. The proposed method was trained using 900 images and tested on 375 images. Images were obtained from “Skin Lesion Analysis Toward Melanoma Detection” challenge which was hosted by ISBI 2016 conference. We achieved segmentation average accuracy of 0.98 and Jaccard index of 0.93. Results were compared with other state-of-the-art methods, including winner of ISBI 2016 challenge for skin melanoma segmentation, along with the same evaluation criteria.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950522,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950522,convolutional neural networks;deep learning;dermoscopy;melanoma;recurrent neural networks;segmentation;skin lesion,Feature extraction;Image segmentation;Lesions;Malignant tumors;Recurrent neural networks;Skin,cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;skin,convolutional neural networks;deep learning;recurrent neural networks;skin melanoma segmentation,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
368,Age estimation from brain MRI images using deep learning,T. W. Huang; H. T. Chen; R. Fujimoto; K. Ito; K. Wu; K. Sato; Y. Taki; H. Fukuda; T. Aoki,"Department of Computer Science, National Tsing-Hua University, Taiwan",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,849,852,"Estimating human age from brain MR images is useful for early detection of Alzheimer's disease. In this paper we propose a fast and accurate method based on deep learning to predict subject's age. Compared with previous methods, our algorithm achieves comparable accuracy using fewer input images. With our GPU version program, the time needed to make a prediction is 20 ms. We evaluate our methods using mean absolute error (MAE) and our method is able to predict subject's age with MAE of 4.0 years.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950650,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950650,MRI;T1-weighted image;age estimation;brain-aging;deep learning,Biological neural networks;Graphics processing units;Kernel;Magnetic resonance imaging;Testing;Training,biomedical MRI;diseases;image recognition;learning (artificial intelligence),Alzheimer's disease early detection;GPU version program;age estimation;brain MRI images;deep learning;mean absolute error,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
369,MIMO-Net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images,S. E. A. Raza; L. Cheung; D. Epstein; S. Pelengaris; M. Khan; N. M. Rajpoot,"Department of Computer Science, University of Warwick, Coventry, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,337,340,"We propose a novel multiple-input multiple-output convolution neural network (MIMO-Net) for cell segmentation in fluorescence microscopy images. The proposed network trains the network parameters using multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The MIMO-Net allows us to deal with variable intensity cell boundaries and highly variable cell size in the mouse pancreatic tissue by adding extra convolutional layers which bypass the max-pooling operation. The results show that our method outperforms state-of-the-art deep learning based approaches for segmentation.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950532,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950532,Cell Segmentation;Deep Learning;Fluorescence Microscopy,Biomembranes;Computer architecture;Convolution;Image segmentation;Machine learning;Microprocessors;Microscopy,biomedical optical imaging;fluorescence;image resolution;image segmentation;medical image processing;neural nets;optical microscopy,cell segmentation;deep learning based approaches;fluorescence microscopy images;mouse pancreatic tissue;multipleinput multipleoutput convolution neural network;multiresolution deconvolution filters;variable intensity cell boundaries,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
370,Deep residual Hough voting for mitotic cell detection in histopathology images,T. Wollmann; K. Rohr,"University of Heidelberg, BIOQUANT, IPMB, and DKFZ Heidelberg, Dept. Bioinformatics and Functional Genomics, Biomedical Computer Vision Group, Im Neuenheimer Feld 267, 69120, Germany",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,341,344,"Cell detection in microscopy images is a common and challenging task. We propose a new approach for mitotic cell detection in histopathology images, which is based on a Deep Residual Network architecture combined with Hough voting. We propose a voting layer for neural networks and introduce a novel loss function. Our approach is learned from scratch using cell centroids and the original images. We benchmarked our approach on the challenging AMIDA13 dataset containing histology images of invasive breast carcinoma. It turned out that our approach achieved competitive results.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950533,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950533,Deep Learning;Dilated Convolution;Hough transform;Microscopy;Residual Network,Biomedical imaging;Computer architecture;Convolution;Microprocessors;Neural networks;Training;Transforms,Hough transforms;biological organs;cellular biophysics;learning (artificial intelligence);medical image processing;neural net architecture,AMIDA13 dataset;cell centroids;deep residual Hough voting;deep residual network architecture;histopathology images;invasive breast carcinoma;microscopy images;mitotic cell detection,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
371,Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images,T. H. Song; V. Sanchez; H. EIDaly; N. M. Rajpoot,"Department of Computer Science, University of Warwick, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,1040,1043,"Automated cell detection is a critical step for a number of computer-assisted pathology related image analysis algorithm. However, automated cell detection is complicated due to the variable cytomorphological and histological factors associated with each cell. In order to efficiently resolve the challenge of automated cell detection, deep learning strategies are widely applied and have recently been shown to be successful in histopathological images. In this paper, we concentrate on bone marrow trephine biopsy images and propose a hybrid deep autoencoder (HDA) network with Curvature Gaussian model for efficient and precise bone marrow hematopoietic stem cell detection via related high-level feature correspondence. The accuracy of our proposed method is up to 94%, outperforming other supervised and unsupervised detection approaches.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950694,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950694,Autoencoder;Bone marrow;Deep learning;Nuclei Detection,Biological system modeling;Biopsy;Bones;Decoding;Feature extraction;Shape;Training,Gaussian processes;bone;cellular biophysics;learning (artificial intelligence);medical image processing,automated cell detection;bone marrow hematopoietic stem cell detection;bone marrow trephine biopsy images;curvature Gaussian model;deep learning;hybrid deep autoencoder,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
372,Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View,A. H. Abdi; C. Luong; T. Tsang; G. Allan; S. Nouranian; J. Jue; D. Hawley; S. Fleming; K. Gin; J. Swift; R. Rohling; P. Abolmaesumi,"Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada",IEEE Transactions on Medical Imaging,20170602.0,2017,36,6.0,1221,1230,"Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 ± 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.",0278-0062;02780062,,10.1109/TMI.2017.2690836,10.13039/501100000024 - Canadian Institutes of Health Research; 10.13039/501100000038 - Natural Sciences and Engineering Research Council; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892028,Convolutional neural network;apical four-chamber;deep learning;echocardiography;quality assessment;swarm optimization,Convolutional neural networks;Echocardiography;Machine learning;Particle swarm optimization;Quality assessment,data acquisition;echocardiography;medical image processing;neural nets;particle swarm optimisation,A4C echo;apical four-chamber view;automatic quality assessment;data acquisition;deep convolutional neural networks;echo quality;echocardiograms;graphics processing unit;intra-rater reliability;operator feedback;particle swarm optimization;stochastic approach;training-validation data,,,,,,,20170404.0,June 2017,,IEEE,IEEE Journals & Magazines
373,Integrating Online and Offline Three-Dimensional Deep Learning for Automated Polyp Detection in Colonoscopy Videos,L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,65,75,"Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.",2168-2194;21682194,,10.1109/JBHI.2016.2637004,Hong Kong Special Administrative Region; Shenzhen Science and Technology Program; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776845,Automated polyp detection;colonoscopy video;computer-aided diagnosis;convolutional neural networks (CNNs);deep learning,Cancer;Colonoscopy;Feature extraction;MIMICs;Shape;Three-dimensional displays;Videos,cancer;endoscopes;learning (artificial intelligence);medical image processing;neural nets;video signal processing,3D fully convolutional network;automated polyp detection;colonoscopy videos;colorectal cancer diagnosis;colorectal cancer prevention;offline three-dimensional deep learning integration framework;online three-dimensional deep learning integration framework;spatiotemporal features,,,,,,,20161207.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
374,$mathtt {Deepr}$: A Convolutional Net for Medical Records,P. Nguyen; T. Tran; N. Wickramasinghe; S. Venkatesh,"Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Geelong, Vic, Australia",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,22,30,"Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space.",2168-2194;21682194,,10.1109/JBHI.2016.2633963,Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762861,Convolutional neural networks;deep learning;medical records,Diseases;Electronic medical records;Feature extraction;Hospitals;Machine learning;Medical diagnostic imaging,diseases;electronic health records;feature extraction;hospitals;learning (artificial intelligence);neural nets,Deepr;coded time gaps;convolutional neural networks;disease;electronic medical records;end-to-end deep learning system;feature extraction;hospital data;hospital transfers,,,,,,,20161201.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
375,Diabetic retinopathy detection using deep convolutional neural networks,D. Doshi; A. Shenoy; D. Sidhpura; P. Gharpure,"Sardar Patel Institute of Technology, India","2016 International Conference on Computing, Analytics and Security Trends (CAST)",20170501.0,2016,,,261,266,"Diabetic retinopathy is when damage occurs to the retina due to diabetes, which affects up to 80 percent of all patients who have had diabetes for 10 years or more. The expertise and equipment required are often lacking in areas where diabetic retinopathy detection is most needed. Most of the work in the field of diabetic retinopathy has been based on disease detection or manual extraction of features, but this paper aims at automatic diagnosis of the disease into its different stages using deep learning. This paper presents the design and implementation of GPU accelerated deep convolutional neural networks to automatically diagnose and thereby classify high-resolution retinal images into 5 stages of the disease based on severity. The single model accuracy of the convolutional neural networks presented in this paper is 0.386 on a quadratic weighted kappa metric and ensembling of three such similar models resulted in a score of 0.3996.",,Electronic:978-1-5090-1338-8; POD:978-1-5090-1339-5,10.1109/CAST.2016.7914977,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7914977,Computer vision;Convolutional Neural Networks;Deep learning;Diabetic Retinopathy;Quadratic weighted kappa metric,Convolution;Diabetes;Neural networks;Retina;Retinopathy;Training,diseases;eye;feedforward neural nets;graphics processing units;image classification;image resolution;learning (artificial intelligence);medical image processing,GPU accelerated deep convolutional neural networks;automatic disease diagnosis;deep learning;diabetic retinopathy detection;disease detection;ensembling;high-resolution retinal image classification;quadratic weighted kappa metric,,,,,,,,19-21 Dec. 2016,,IEEE,IEEE Conference Publications
376,Detecting Cardiovascular Disease from Mammograms With Deep Learning,J. Wang; H. Ding; F. A. Bidgoli; B. Zhou; C. Iribarren; S. Molloi; P. Baldi,"Department of Computer Science, Institute for Genomics and Bioinformatics, University of California at Irvine, Irvine, CA, USA",IEEE Transactions on Medical Imaging,20170501.0,2017,36,5.0,1172,1181,"Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected in mammograms, can be useful risk markers associated with the disease. We investigate the feasibility of automated and accurate detection of BACs in mammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-response receiver operating characteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection in mammograms to help identify and assess patients with cardiovascular risks.",0278-0062;02780062,,10.1109/TMI.2017.2655486,"10.13039/100000001 - National Science Foundation; 10.13039/100000050 - National Heart, Lung, and Blood Institute (Bethesda, MD) to CI and SM; 10.13039/100006785 - Google Faculty Research Award under Grant; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827150,Breast arterial calcification (BAC);coronary artery disease;deep learning;mammography,Arteries;Breast;Calcium;Diseases;Machine learning;Mammography;Neural networks,cardiovascular system;diseases;learning (artificial intelligence);mammography;medical image processing;neural nets,12-layer convolutional neural network;BAC detection;FROC analysis;breast arterial calcifications;calcium mass quantification analysis;cardiovascular disease detection;coronary artery disease;death;deep learning;digital mammograms;patch based procedure,,,,,,,20170119.0,May 2017,,IEEE,IEEE Journals & Magazines
377,Predicting heart rejection using histopathological whole-slide imaging and deep neural network with dropout,L. Tong; R. Hoffman; S. R. Deshpande; M. D. Wang,"Dept. of Biomedical Engineering, Georgia Institute of Technology and Emory University, Atlanta, GA 30332, USA",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413.0,2017,,,1,4,"Cardiac allograft rejection is one major limitation for long-term survival for patients with heart transplants. The endomyocardial biopsy is one gold standard to screen heart rejection for patients that have heart transplantation. However, manual identification of heart rejection is expensive and time-consuming. With the development of imaging processing techniques and machine learning tools, automatic prediction of heart rejection using whole-slide images is one promising approach to improve the care of patients with heart transplants. In this paper, we first develop a histopathological whole-slide image processing pipeline to extract features automatically. Then, we construct deep neural networks with and without regularization and dropout to classify the patients into non-rejection and rejection respectively. Our results show that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracies.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897190,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897190,,Biopsy;Cost function;Feature extraction;Heart;Neural networks;Testing;Training,biomedical optical imaging;cardiology;diseases;feature extraction;learning (artificial intelligence);medical image processing;pipeline processing;prosthetics,cardiac allograft rejection;classification algorithms;deep neural network;endomyocardial biopsy;feature extraction;heart rejection;heart transplants;histopathological whole-slide imaging;machine learning,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
378,Automatic polyp detection in endoscopy videos: A survey,B. Taha; N. Werghi; J. Dias,"Department of Electrical and Computer Engineering, Khalifa University, UAE",2017 13th IASTED International Conference on Biomedical Engineering (BioMed),20170406.0,2017,,,233,240,"Early detection of polyps play an essential role for the prevention of colorectal cancer. Manual clinical inspection have many limitations and could result to either false or missed polyps. Computer aided diagnosis system has been used to help the medical expert and to provide more accurate diagnosis. Since their introduction, many types of algorithms have been proposed in the literature using different types of features and classifiers. This paper provides a state-of-the-art for the automatic detection of polyps using endoscopic videos. Given the increasing evolution of medical imaging technologies and algorithms, it is important to have a recent review in order to know the current state of the art, and the opportunities for improving existing algorithms, or developing innovative ones. The paper divides the work done on this research area according to the type of features and classification methods implemented. The features have been divided into shape, texture or fusion features. Future directions and challenges for more accurate polyp detection in endoscopy videos are also discussed.",,Electronic:978-0-88986-990-5; POD:978-1-5090-4908-0,10.2316/P.2017.852-031,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893296,Deep learning;Endoscopy videos;Feature fusion;Polyp detection;Shape features;Texture features,Biomedical imaging;Computers;Endoscopes;Histograms;Image segmentation;Support vector machines;Videos,,,,,,,,,,20-21 Feb. 2017,,IEEE,IEEE Conference Publications
379,Automated blood vessel segmentation based on de-noising auto-encoder and neural network,Z. Fan; J. J. Mo,"Guangdong Key Laboratory of Digital Signal and Image Processing, Shantou University, Shantou 515063, China",2016 International Conference on Machine Learning and Cybernetics (ICMLC),20170309.0,2016,2,,849,856,"Retinal vessel segmentation has been widely used for screening, diagnosis and treatment of cardiovascular and ophthalmologic diseases. In this paper, we propose an automated approach for vessel segmentation in digital retinal images based on de-noising auto-encoders layer-wise initialized neural networks. The proposed method utilized a deep neural network, which is layer-wise initialized by de-noising auto-encoders and fine-tuned by BP algorithm, to segment vessel structures in retinal images. The proposed method is very competitive with the state-of-the-art methods. It achieves an average accuracy of 0.9612, 0.9614, 0.6761, sensitivity of 0.7814, 0.7234, 0.9702, and specificity of 0.9788, 0.9799, 0.9702 on 3 public databases DRIVE, STARE, and CHASE_DB1 respectively. The proposed method is promising for automated blood vessel segmentation.",,CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4,10.1109/ICMLC.2016.7872998,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872998,Denoising auto-encoders;Neural networks;Retinal images;Vessel segmentation,Biological neural networks;Databases;Feature extraction;Image segmentation;Neurons;Retina;Training,backpropagation;cardiovascular system;diseases;image denoising;medical image processing;neural nets,BP algorithm;STARE;autoencoder denoising;automated blood vessel segmentation;cardiovascular;deep neural networks;digital retinal images;ophthalmologic diseases;public databases DRIVE;retinal vessel segmentation,,,,,,,,10-13 July 2016,,IEEE,IEEE Conference Publications
380,A combined multi-scale deep learning and random forests approach for direct left ventricular volumes estimation in 3D echocardiography,S. Dong; G. Luo; G. Sun; K. Wang; H. Zhang,"Harbin Institute of Technology, Harbin, China",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,889,892,"Estimation of left ventricular (LV) volumes from 3D echocardiography (3DE) is a popular clinical approach in accurate assessment of left ventricular function for the diagnosis of cardiac disease. The segmentation of 3DE volumes is a crucial step in traditional methods. Nevertheless, segmentation itself is an extremely challenging problem due to the presence of speckle noise and discontinuous edges. Therefore, direct left ventricular volumes estimation methods without the segmentation become attractive in cardiac function analysis. The aim of this paper is to present a fully learning framework to estimate the left ventricular volume in 3DE. The proposed method combined unsupervised multi-scale convolutional deep network and random forests. The multi-scale convolution deep network adopted multi-scale convolutional filters to represent features of unlabeled end-diastolic and end-systolic 3DE volumes (EDV and ESV). And then we formulated left ventricular volume estimation as a regression problem and used random forests for efficient volume estimation. The experiments results suggested that our proposed method is feasible and can achieve higher accuracy, even in case of echocardiography images with irregular geometry.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868886,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868886,,Active contours;Echocardiography;Image segmentation;Training;Vegetation;Volume measurement,diseases;echocardiography;image denoising;image filtering;image segmentation;medical image processing;speckle,3D echocardiography;3DE volume segmentation;cardiac disease diagnosis;cardiac function analysis;clinical approach;combined multiscale deep learning;direct left ventricular volumes estimation;discontinuous edges;echocardiography images;end-systolic 3DE volumes;fully learning framework;multiscale convolution deep network adopted multiscale convolutional filters;random forests approach;speckle noise;unlabeled end-diastolic 3DE volumes,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
381,A left ventricular segmentation method on 3D echocardiography using deep learning and snake,S. Dong; G. Luo; G. Sun; K. Wang; H. Zhang,"Harbin Institute of Technology, Harbin, China",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,473,476,"Segmentation of left ventricular (LV) endocardium from 3D echocardiography is important for clinical diagnosis because it not only can provide some clinical indices (e.g. ventricular volume and ejection fraction) but also can be used for the analysis of anatomic structure of ventricle. In this work, we proposed a new full-automatic method, combining the deep learning and deformable model, for the segmentation of LV endocardium. We trained convolutional neural networks to generate a binary cuboid to locate the region of interest (ROI). And then, using ROI as the input, we trained stacked autoencoder to infer the LV initial shape. At last, we adopted snake model initiated by inferred shape to segment the LV endocardium. In the experiments, we used 3DE data, from CETUS challenge 2014 for training and testing by segmentation accuracy and clinical indices. The results demonstrated the proposed method is accuracy and efficiency respect to expert's measurements.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868782,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868782,,Deformable models;Echocardiography;Image segmentation;Machine learning;Shape;Three-dimensional displays;Ultrasonic imaging,echocardiography;image segmentation;learning (artificial intelligence);medical image processing;neural nets,LV endocardium;anatomic structure;autoencoder;binary cuboid;clinical diagnosis;convolutional neural networks;deep learning;deformable model;left ventricular segmentation method;region-of-interest;snake model;three dimensional echocardiography,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
382,A novel left ventricular volumes prediction method based on deep learning network in cardiac MRI,G. Luo; G. Sun; K. Wang; S. Dong; H. Zhang,"Harbin Institute of Technology, Harbin, China",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,89,92,"Accurate estimation of left ventricle (LV) volumes plays an essential role in clinical diagnosis of cardiac diseases using MRI. Conventional methods of estimating ventricular volumes depend on the results of manual or automatic segmentation of MRI. However, manual segmentation of MRI sequences is extremely time-consuming and subjective, and automatic segmentation is still a challenging task. Therefore, this study aims to develop a new LV volumes prediction method without segmentation, motivated by deep learning technology and the large scale cardiac MRI (CMR) datasets from the second Annual Data Science Bowl (ADSB) in 2016. The experiments results shows that the predicted LV volumes have high correlation with the ground truth. These results prove that the proposed method has big potential to be researched and applied in clinical diagnosis and screening of cardiac diseases.",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868686,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868686,,Cardiac disease;Image segmentation;Machine learning;Magnetic resonance imaging;Predictive models;Training,biomedical MRI;cardiology;diseases;learning (artificial intelligence);medical image processing;patient diagnosis,ADSB;Annual Data Science Bowl;MRI sequences;automatic MRI image segmentation;cardiac MRI dataset;cardiac disease diagnosis;deep learning network;deep learning technology;left ventricle volume estimation;left ventricular volume prediction method,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
383,Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification,P. Rodriguez; G. Cucurull; J. Gonzàlez; J. M. Gonfaus; K. Nasrollahi; T. B. Moeslund; F. X. Roca,"Computer Vision Center, Universitat Aut&#x00F2;noma de Barcelona, 08193 Barcelona, Spain.",IEEE Transactions on Cybernetics,,2017,PP,99.0,1,11,"Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively, there is the need for automatic systems to measure it. In this paper, contrary to current state-of-the-art techniques in pain assessment, which are based on facial features only, we suggest that the performance can be enhanced by feeding the raw frames to deep learning models, outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline, our approach first uses convolutional neural networks (CNNs) to learn facial features from VGG_Faces, which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result, we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition, to evaluate the generalization properties of our proposed methodology on facial motion recognition, we also report competitive results in the Cohn Kanade+ facial expression database.",2168-2267;21682267,,10.1109/TCYB.2017.2662199,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849133,Affective computing;computer applications;cybercare industry applications;human factors engineering in medicine and biology;medical services;monitoring;patient monitoring computers and information processing;pattern recognition,Databases;Estimation;Face;Face recognition;Feature extraction;Hidden Markov models;Pain,,,,,,,,,20170209.0,,,IEEE,IEEE Early Access Articles
384,Feature Fusion for Denoising and Sparse Autoencoders: Application to Neuroimaging Data,A. Moussavi-Khalkhali; M. Jamshidi; S. Wijemanne,"Dept. of Electr. & Comput. Eng., Univ. of Texas at San Antonio, San Antonio, TX, USA",2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),20170202.0,2016,,,605,610,"Although there is no cure to date, Alzheimer's disease detection in early stages has a significant impact on the patient's life in terms of cost, the progress, and helping to plan in advance for an appropriate healthcare in the life ahead as well as providing clinical etiologies for further research. This paper discusses implementing a feature fusion method utilizing sparse and denoising autoencoders to reveal the stage of Alzheimer's disease. Four cohorts consisted of individuals with Alzheimer's disease, late mild cognitive impairment, early mild cognitive impairment, and normal control groups are classified using multinomial logistic regression fueled by the fusion of high-level and low-level features. The high-level features are extracted from the stacked autoencoders. The results show that feature fusion enhance the performance of typical autoencoders. However, the performance of feature fusion using denoising autoencoders is superior to that of the sparse training of autoencoders in terms of overall accuracy, precision, and recall.",,Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6,10.1109/ICMLA.2016.0106,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838210,Alzheimer's disease stage detection;deep learning;feature fusion;sacked sparse autoencoders;stacked denoising autoencoders,Alzheimer's disease;Classification algorithms;Feature extraction;Magnetic resonance imaging;Noise reduction;Training,diseases;feature extraction;image coding;image denoising;image fusion;medical image processing;neurophysiology;regression analysis,Alzheimer disease detection;clinical etiologies;denoising autoencoders;feature fusion method;high-level features;low-level features;mild cognitive impairment;multinomial logistic regression;neuroimaging data;normal control groups;sparse autoencoder training;stacked autoencoders,,,,,,,,18-20 Dec. 2016,,IEEE,IEEE Conference Publications
385,Automated atrial fibrillation detection based on deep learning network,C. Yuan; Y. Yan; L. Zhou; J. Bai; L. Wang,"Information and communication engineering, Wuhan university of technology, Wuhan, Hubei Province China",2016 IEEE International Conference on Information and Automation (ICIA),20170202.0,2016,,,1159,1164,"Aiming at the shorting of the existing atrial fibrillation (AF) detection algorithms and improve the ability of intelligent recognition and extraction of AF signals. Recently, deep learning theory with massive data has been used on image, voice and other filed widely. In this paper, a method based on the stack sparse autoencoder neural network, a instance of deep learning strategy, was proposed for AF detection. Greedy layer-wise training algorithms and massive unlabeled hotter data from a hospital were used to train the deep learning system, and Back Propagation algorithm and half of the MIT-BIH standard databases were applied to optimized the whole system. Another half of the standard data were used to evaluated the performance of this method. The autoencoder learns the high level features which can describe the necessary information better from the raw data The experimental results show that the accuracy of the algorithm based on stack sparse autoencoder is 98.309%, so this approach is of great significance on the real-time monitoring of atrial fibrillation signal in electrocardiogram.",,Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5,10.1109/ICInfA.2016.7831994,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831994,AF detection;autoencoder;deep learning;intelligent recognition,Atrial fibrillation;Biological neural networks;Databases;Electrocardiography;Feature extraction;Machine learning;Training,backpropagation;diseases;electrocardiography;feature extraction;graph theory;medical signal detection;network theory (graphs);neural nets;patient diagnosis,AF detection;AF signal extraction;AF signal recognition;atrial fibrillation detection;back propagation algorithm;deep learning network;electrocardiogram;greedy layer-wise training algorithm;stack sparse autoencoder neural network,,,,,,,,1-3 Aug. 2016,,IEEE,IEEE Conference Publications
386,Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding,Z. Zhu; C. Yin; B. Qian; Y. Cheng; J. Wei; F. Wang,"Xi'an Jiaotong Univ., Xi'an, China",2016 IEEE 16th International Conference on Data Mining (ICDM),20170202.0,2016,,,749,758,"Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. Aproper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is the Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector based representation for patients, which typically requires aggregation of medical events over a certain time period. As aconsequence, the temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture, and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines.",,Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9,10.1109/ICDM.2016.0086,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837899,Deep Matching;Medical Concept Embedding;Patient Similarity,Context;Diseases;Medical diagnostic imaging;Natural language processing;Neural networks,electronic health records;health care;patient care;pattern matching;unsupervised learning,EHR;clinical similarities;convolutional neural network architecture;deep architecture;electronic health records;healthcare informatics;medical concept embedding;medical interpretations;patient similarity evaluation;patient similarity measurement;patient similarity research;real-world clinical data;temporal longitudinal patient EHR matching;vector based representation,,,,,,,,12-15 Dec. 2016,,IEEE,IEEE Conference Publications
387,Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation,A. Fakhry; T. Zeng; S. Ji,"Department of Computer Science, Old Dominion University, Norfolk, VA, USA",IEEE Transactions on Medical Imaging,20170201.0,2017,36,2.0,447,456,"Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.",0278-0062;02780062,,10.1109/TMI.2016.2613019,"10.13039/100000153 - National Science Foundation, Old Dominion University, and Washington State University; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638,Brain circuit reconstruction;deconvolutional networks;deep learning;electron microscopy;image segmentation;residual learning,Convolution;Deconvolution;Feature extraction;Image reconstruction;Image segmentation;Predictive models;Three-dimensional displays,brain;deconvolution;electron microscopy;image segmentation;medical image processing;neural nets;neurophysiology,3D neurite segmentation;EM image segmentation;anatomical connection reconstruction;brain electron microscopy;circuit mapping;contextual information;dense output prediction;electron microscopy images;full-resolution features;full-resolution predictions;hand-crafted features;human-level performance;information pathways;neurite segmentation challenge dataset;neurons;residual deconvolutional networks,,,,,,,20160923.0,Feb. 2017,,IEEE,IEEE Journals & Magazines
388,Extraction of GGO candidate regions from the LIDC database using deep learning,K. Hirayama; J. K. Tan; H. Kim,"Kyushu Institute of Technology, 1-1, Sensui, Tobata, Kitakyushu 804-8550, Japan","2016 16th International Conference on Control, Automation and Systems (ICCAS)",20170126.0,2016,,,724,727,"In recent years, development of the computer-aided diagnosis (CAD) systems for the purpose of reducing the false positive on visual screening and improving accuracy of lesion detection has been advanced. Lung cancer is the leading cause of cancer death in the world. Among them, GGO (Ground Glass Opacity) that exhibited early in the before cancer lesion and carcinoma in situ shows a pale concentration, have been concerned about the possibility of undetected on the screening. In this paper, we propose an automatic extraction method of GGO candidate regions from the chest CT image. Our proposed image processing algorithms is consist of four main steps; (1) segmentation of volume of interest from the chest CT image and removing the blood vessel regions, bronchus regions based on 3D line filter, (2) first detection of GGO regions based on density and gradient which is selected the initial GGO candidate regions, (3) identification of the final GGO candidate regions based on DCNN (Deep Convolutional Neural Network) algorithms. Finally, we calculates the statistical features for reducing the false-positive (FP) shadow by the rule-based method, performs identification of the final GGO candidate regions by SVM (Support Vector Machine). Our proposed method performed on to the 31 cases of the LIDC (Lung Image Database Consortium) database, and final identification performance of TP: 93.02[%], FP: 128.52[/case] are obtained respectively.",,Electronic:978-89-93215-11-3; POD:978-1-4673-9058-3; USB:978-89-93215-12-0,10.1109/ICCAS.2016.7832398,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832398,Computer Aided Diagnosis;Deep Convolutional Neural Network;Ground Glass Opacity;Lung Image Database Consortium;Support Vector Machine,Biomedical imaging;Blood vessels;Cancer;Lungs;Machine learning;Support vector machines;Three-dimensional displays,blood vessels;cancer;computerised tomography;feature extraction;feedforward neural nets;image filtering;image segmentation;learning (artificial intelligence);lung;medical image processing;opacity;statistical analysis;support vector machines;visual databases,3D line filter;DCNN algorithms;FP shadow reduction;GGO region detection;GGO region extraction;LIDC database;Lung Image Database Consortium database;SVM;automatic extraction method;blood vessel region removal;bronchus regions;cancer lesion;carcinoma;chest CT image;computer-aided diagnosis systems;deep learning;deep-convolutional neural network algorithms;false-positive shadow reduction;ground glass opacity;image processing algorithm;lung cancer;pale concentration;rule-based method;statistical features;support vector machine;volume-of-interest segmentation,,,,,,,,16-19 Oct. 2016,,IEEE,IEEE Conference Publications
389,Size-Invariant Fully Convolutional Neural Network for vessel segmentation of digital retinal images,Y. Luo; H. Cheng; L. Yang,"Machine Intelligence Institute, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu",2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),20170119.0,2016,,,1,7,"Vessel segmentation of digital retinal images plays an important role in diagnosis of diseases such as diabetics, hypertension and retinopathy of prematurity due to these diseases impact the retina. In this paper, a novel Size-Invariant Fully Convolutional Neural Network (SIFCN) is proposed to address the automatic retinal vessel segmentation problems. The input data of the network is the patches of images and the corresponding pixel-wise labels. A consecutive convolution layers and pooling layers follow the input data, so that the network can learn the abstract features to segment retinal vessel. Our network is designed to hold the height and width of data of each layer with padding and assign pooling stride so that the spatial information maintain and up-sample is not required. Compared with the pixel-wise retinal vessel segmentation approaches, our patch-wise segmentation is much more efficient since in each cycle it can predict all the pixels of the patch. Our overlapped SIFCN approach achieves accuracy of 0.9471, with the AUC of 0.9682. And our non-overlap SIFCN is the most efficient approach among the deep learning approaches, costing only 3.68 seconds per image, and the overlapped SIFCN costs 31.17 seconds per image.",,Electronic:978-9-8814-7682-1; POD:978-1-5090-2401-8,10.1109/APSIPA.2016.7820677,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820677,,Convolution;Diabetes;Diseases;Image segmentation;Kernel;Retinal vessels,eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets,diabetics;digital retinal images;disease diagnosis;hypertension;patch-wise segmentation;pixel-wise retinal vessel segmentation;retinopathy;size-invariant fully convolutional neural network,,,,,,,,13-16 Dec. 2016,,IEEE,IEEE Conference Publications
390,Deep convolutional neural network for survival analysis with pathological images,X. Zhu; J. Yao; J. Huang,"Department of Computer Science and Engineering, The University of Texas at Arlington, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,544,547,"Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822579,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822579,Deep learning;Lung cancer;Pathological images;Survival analysis,Analytical models;Data models;Feature extraction;Hazards;Lungs;Pathology;Predictive models,cancer;feature extraction;lung;medical image processing;neural nets,DeepConvSurv;NLST lung cancer data;cox proportional hazard model;deep convolutional neural network;hand-crafted feature;medical imaging technology;national lung screening trial;pathological image;survival analysis;survival prediction;unstructured medical image,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
391,Combining deep learning and hand-crafted features for skin lesion classification,T. Majtner; S. Yildirim-Yayilgan; J. Y. Hardeberg,"Faculty of Computer Science and Media Technology, NTNU Norwegian University of Science and Technology, Gj&#x2298;vik, Norway","2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)",20170119.0,2016,,,1,6,"Melanoma is one of the most lethal forms of skin cancer. It occurs on the skin surface and develops from cells known as melanocytes. The same cells are also responsible for benign lesions commonly known as moles, which are visually similar to melanoma in its early stage. If melanoma is treated correctly, it is very often curable. Currently, much research is concentrated on the automated recognition of melanomas. In this paper, we propose an automated melanoma recognition system, which is based on deep learning method combined with so called hand-crafted RSurf features and Local Binary Patterns. The experimental evaluation on a large publicly available dataset demonstrates high classification accuracy, sensitivity, and specificity of our proposed approach when it is compared with other classifiers on the same dataset.",,Electronic:978-1-4673-8910-5; POD:978-1-4673-8911-2,10.1109/IPTA.2016.7821017,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821017,Convolutional Neural Network;Local Binary Patterns;RSurf Features;SVM;Skin Lesion Classification,Feature extraction;Histograms;Image color analysis;Lesions;Malignant tumors;Skin;Support vector machines,biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;skin,Local Binary Patterns;automated melanoma recognition system;benign lesions;cells;classification accuracy;deep learning method;hand-crafted RSurf features;melanocytes;moles;skin cancer;skin lesion classification,,,,,,,,12-15 Dec. 2016,,IEEE,IEEE Conference Publications
392,New Deep Neural Nets for Fine-Grained Diabetic Retinopathy Recognition on Hybrid Color Space,H. H. Vo; A. Verma,"Dept. of Comput. Sci., California State Univ., Fullerton, CA, USA",2016 IEEE International Symposium on Multimedia (ISM),20170119.0,2016,,,209,215,"Automatic diabetes retinopathy (DR) recognition can help DR carriers to receive treatment in early stages and avoid the risk of vision loss. In this paper, we emphasize the role of multiple filter sizes in learning fine-grained discriminant features and propose: (i) two deep convolutional neural networks - Combined Kernels with Multiple Losses Network (CKML Net) and VGGNet with Extra Kernel (VNXK), which are an improvement upon GoogLeNet and VGGNet in context of DR tasks. Learning from existing research, (ii) we propose a hybrid color space, LGI, for DR recognition via proposed nets. (iii) Transfer learning is applied to solve the challenge of imbalanced dataset. The effectiveness of proposed new nets and color space is evaluated using two grand challenge retina datasets: EyePACS and Messidor. Our experimental results show: (iv) CKML Net improves upon GoogLeNet and VNXK improves upon VGGNet on both datasets using the LGI color space. Additionally, proposed methodology improves upon other state of the art results on Messidor dataset for referable/non-referable screening.",,Electronic:978-1-5090-4571-6; POD:978-1-5090-4572-3,10.1109/ISM.2016.0049,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823616,CKML Net;LGI;VNXK;convolutional neural networks;diabetic retinopathy recognition;hybrid color space;transfer learning,Diabetes;Feature extraction;Image color analysis;Neural networks;Retina;Retinopathy;Training,computer vision;diseases;filtering theory;image colour analysis;learning (artificial intelligence);medical image processing;neural nets,CKML Net;DR carriers;DR recognition;LGI color space;Messidor dataset;VGGNet with Extra Kernel;VNXK;automatic diabetes retinopathy;combined kernels with multiple losses network;fine grained diabetic retinopathy recognition;hybrid color space;imbalanced dataset;learning fine-grained discriminant features;multiple filter sizes;new deep neural nets;vision loss,,,,,,,,11-13 Dec. 2016,,IEEE,IEEE Conference Publications
393,Facial expression recognition based on LLENet,Dan Meng; Guitao Cao; Zhihai He; Wenming Cao,"School of Computer Scinence and Software Engineering, East China Normal University, Shanghai, China 200062",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,1915,1917,"Facial expression recognition plays an important role in lie detection, and computer-aided diagnosis. Many deep learning facial expression feature extraction methods have a great improvement in recognition accuracy and robutness than traditional feature extraction methods. However, most of current deep learning methods need special parameter tuning and ad hoc fine-tuning tricks. This paper proposes a novel feature extraction model called Locally Linear Embedding Network (LLENet) for facial expression recognition. The proposed LLENet first reconstructs image sets for the cropped images. Unlike previous deep convolutional neural networks that initialized convolutional kernels randomly, we learn multi-stage kernels from reconstructed image sets directly in a supervised way. Also, we create an improved LLE to select kernels, from which we can obtain the most representative feature maps. Furthermore, to better measure the contribution of these kernels, a new distance based on kernel Euclidean is proposed. After the procedure of multi-scale feature analysis, feature representations are finally sent into a linear classifier. Experimental results on facial expression datasets (CK+) show that the proposed model can capture most representative features and thus improves previous results.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822814,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822814,Face expression recognition;deep learning;kernel distance;locally linear embedding (LLE),Computational modeling;Euclidean distance,face recognition;feature extraction;image reconstruction;learning (artificial intelligence);medical image processing;operating system kernels,LLENet method;ad hoc fine-tuning tricks;computer-aided diagnosis;convolutional kernels;deep convolutional neural networks;deep learning facial expression feature extraction methods;facial expression recognition;image reconstruction;kernel Euclidean;lie detection;locally linear embedding network;parameter tuning,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
394,Sparse Autoencoder Based Deep Neural Network for Voxelwise Detection of Cerebral Microbleed,Y. D. Zhang; X. X. Hou; Y. D. Lv; H. Chen; Y. Zhang; S. H. Wang,"Sch. of Comput. Sci. & Technol., Nanjing Normal Univ., Nanjing, China",2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS),20170119.0,2016,,,1229,1232,"In order to detect cerebral microbleed more efficiently, we developed a novel computer-aided detection method based on susceptibility-weighted imaging. We enrolled five CADASIL patients and five healthy controls. We used a 20x20 neighboring window to generate samples on each slice of the volumetric brain images. The sparse autoencoder (SAE) was used to unsupervised feature learning. Then, a deep neural network was established using the learned features. The results over 10x10-fold cross validation showed our method yielded a sensitivity of 93.20±1.37%, a specificity of 93.25±1.38%, and an accuracy of 93.22±1.37%. Our result is better than Roy's method, which was proposed in 2015.",1521-9097;15219097,Electronic:978-1-5090-4457-3; POD:978-1-5090-5382-7,10.1109/ICPADS.2016.0166,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823881,cerebral microbleed;cross validation;deep neural network;sparse autoencoder;susceptibility weighted imaging,Biological neural networks;Biomedical imaging;Blood vessels;Brain models;Image processing;Neural networks;Neurology;Patient monitoring;Sensitivity;Standards,biomedical MRI;learning (artificial intelligence);medical image processing;neural nets,CADASIL patients;MRI;SWI;cerebral microbleed detection;computer-aided detection method;deep neural network;sparse autoencoder;susceptibility-weighted imaging;unsupervised feature learning;voxelwise detection,,,,,,,,13-16 Dec. 2016,,IEEE,IEEE Conference Publications
395,Early diagnosis of Alzheimer's disease: A multi-class deep learning framework with modified k-sparse autoencoder classification,P. Bhatkoti; M. Paul,"School of Computing and Mathematics, Charles Sturt University, Australia",2016 International Conference on Image and Vision Computing New Zealand (IVCNZ),20170105.0,2016,,,1,5,"Successful, timely diagnosis of neuropsychiatry diseases is key to management. Research efforts in the area of diagnosis of Alzheimer's disease have used various aspects of computer-aided multi-class diagnosis approaches with varied degrees of success. However, there is still need for more efficient and reliable approaches to successful diagnosis of the disease. This research used deep learning framework with modified k-sparse autoencoder (oKSA) classification to locate neutrally degenerated areas of the brain magnetic resonance imaging (MRI), low amyloid beta 1-42 imaging in cerebrospinal fluid (CSF) and positron emission tomography (PET) imaging of amyloid; each with a sample of 150 images. Results show a correlation between computational demarcation of infected regions and the images. Degeneration in the studied areas was evidenced by high phosphorylated t-/p-tau levels in CSF, regional fluorodeoxyglucose PET, and the presence of atrophy patterns. The use of σKSA algorithm in boosting classification helped to improve the classifier performance. The KSA method with deep learning framework is used for the first time to produce accurate results in diagnosis of Alzheimer's disease.",,Electronic:978-1-5090-2748-4; POD:978-1-5090-2749-1; USB:978-1-5090-2747-7,10.1109/IVCNZ.2016.7804459,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804459,Alzheimer's;aKSA;deep learning;diagnosis;k-sparse;neuroimaging,Alzheimer's disease;Classification algorithms;Encoding;Feature extraction;Machine learning;Magnetic resonance imaging,biomedical MRI;diseases;image classification;image coding;learning (artificial intelligence);medical image processing;positron emission tomography,σKSA algorithm;CSF;MRI;PET imaging;atrophy patterns;brain magnetic resonance imaging;cerebrospinal fluid;classifier performance;computational demarcation;computer-aided multiclass diagnosis;early Alzheimer's disease diagnosis;infected images;infected regions;low amyloid beta 1-42 imaging;modified k-sparse autoencoder classification;multiclass deep learning;neuropsychiatric disease diagnosis;neutrally degenerated areas;phosphorylated t-/p-tau levels;positron emission tomography;regional hypometabolism fluorodeoxyglucose PET,,,,,,,,21-22 Nov. 2016,,IEEE,IEEE Conference Publications
396,A deep learning-based segmentation method for brain tumor in MR images,Zhe Xiao; Ruohan Huang; Yi Ding; Tian Lan; RongFeng Dong; Zhiguang Qin; Xinjie Zhang; Wei Wang,"School of Information and Software Engineering, University of Electronic Science and Technology of China, No.4, Section 2, North Jianshe Road, Chengdu, Sichuan, 610054, China",2016 IEEE 6th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS),20170102.0,2016,,,1,6,"Accurate tumor segmentation is an essential and crucial step for computer-aided brain tumor diagnosis and surgical planning. Subjective segmentations are widely adopted in clinical diagnosis and treating, but they are neither accurate nor reliable. An automatical and objective system for brain tumor segmentation is strongly expected. But they are still facing some challenges such as lower segmentation accuracy, demanding a priori knowledge or requiring the human intervention. In this paper, a novel and new coarse-to-fine method is proposed to segment the brain tumor. This hierarchical framework consists of preprocessing, deep learning network based classification and post-processing. The preprocessing is used to extract image patches for each MR image and obtains the gray level sequences of image patches as the input of the deep learning network. The deep learning network based classification is implemented by a stacked auto-encoder network to extract the high level abstract feature from the input, and utilizes the extracted feature to classify image patches. After mapping the classification result to a binary image, the post-processing is implemented by a morphological filter to get the final segmentation result. In order to evaluate the proposed method, the experiment was applied to segment the brain tumor for the real patient dataset. The final performance shows that the proposed brain tumor segmentation method is more accurate and efficient.",,Electronic:978-1-5090-4199-2; POD:978-1-5090-4200-5,10.1109/ICCABS.2016.7802771,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802771,Brain tumor detection;Brain tumor segmentation;Computer Aided Diagnosis (CAD);Deep Learning;Stacked Auto-Encoder (SAE);Stacked Denoising Auto-Encoder (SDAE),Image reconstruction;Image segmentation;Surgery,biomedical MRI;brain;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours,MR images;brain tumor segmentation;clinical diagnosis;computer-aided brain tumor diagnosis;deep learning network based classification;deep learning-based segmentation method;gray level sequences;image patches;morphological filter;stacked autoencoder network;surgical planning,,,,,,,,13-15 Oct. 2016,,IEEE,IEEE Conference Publications
397,Deep learning application trial to lung cancer diagnosis for medical sensor systems,R. Shimizu; S. Yanagawa; Y. Monde; H. Yamagishi; M. Hamada; T. Shimizu; T. Kuroda,"Faculty of Science and Technology, Keio University, Yokohama, Japan",2016 International SoC Design Conference (ISOCC),20161229.0,2016,,,191,192,"Personal and easy-to-use health checking system is an attractive application of sensor systems. Sensing data analysis for diagnosis is important as well as preparing small and mobile sensor nodes because sensing data include variations and noises reflecting individual difference of people and sensing conditions. Deep Neural Network, or Deep Learning, is a well-known method of machine learning and it is effective for feature extraction from pictures. Then, we thought Deep Learning also can extract features from sensing data. In this paper, we tried to build a diagnosis system of lung cancer based on Deep Learning. Input data of the system was generated from human urine by Gas Chromatography Mass Spectrometer (GC-MS) and our system achieved 90% accuracy in judging whether the patient had lung cancer or not. This system will be useful for pre- and personal diagnosis because collecting urine is very easy and not harmful to human body. We are targeting installation of this system not only to gas chromatography systems but also to some combination of multiple sensors for detecting gases of low concentration.",,Electronic:978-1-5090-3219-8; POD:978-1-5090-3220-4; USB:978-1-5090-3218-1,10.1109/ISOCC.2016.7799852,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7799852,Deep Learning;Deep Neural Network;Gas Chromatography Mass Spectrometer(GC-MS);Stacked Autoencoder,Cancer;Feature extraction;Lungs;Machine learning;Medical diagnostic imaging;Neural networks;Sensors,biosensors;cancer;chromatography;learning (artificial intelligence);lung;mass spectrometers;neural nets;patient diagnosis,deep learning application;deep neural network;easy-to-use health checking system;feature extraction;gas chromatography mass spectrometer;gas detection;human urine;lung cancer diagnosis;machine learning;medical sensor systems;mobile sensor nodes;personal diagnosis;personal health checking system;pre-diagnosis;sensing data analysis,,,,,,,,23-26 Oct. 2016,,IEEE,IEEE Conference Publications
398,Segmentation of the Left Ventricle in Echocardiography Using Contextual Shape Model,G. Belous; A. Busch; D. Rowlands; Y. Gao,"Sch. of Eng., Griffith Univ., Brisbane, QLD, Australia",2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA),20161226.0,2016,,,1,7,"Accurate localization of the left ventricle (LV) boundary from echocardiogram images is of vital importance for the diagnosis and treatment of heart disease. Statistical shape models such as active shape models (ASM) have been commonly used to perform automatic detection of this boundary. Such models perform well when there is low variability in the underlying shape subspace and an accurate initialization can be provided, however in the absence of these conditions results are often much poorer. In the case of LV echocardiogram images, such variability is often encountered in patients with abnormal LV function. In this paper we propose a fully automatic segmentation technique using deep learning in a Bayesian nonparametric framework. Our model uses a dynamic statistical shape model comprised of training shapes from select weighted subsets of the feature subspace. Subsets are chosen during the iterative segmentation process according to a latent temporal component allocation variable, determined from joint deep features and LV landmark information using a Dirichlet process mixture model with Chinese restaurant process prior. Testing is performed on a data set comprising images of the LV acquired from patients exhibiting both normal and abnormal LV function, and the results using our technique compared to both the ASM and other state of the art techniques. Results from this testing show an improvement in the LV localization accuracy, particularly when LV function is abnormal.",,Electronic:978-1-5090-2896-2; POD:978-1-5090-2897-9,10.1109/DICTA.2016.7797080,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797080,,Active shape model;Bayes methods;Image segmentation;Machine learning;Mixture models;Shape;Training,Bayes methods;echocardiography;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;mixture models;patient treatment;shape recognition,ASM;Bayesian nonparametric framework;Chinese restaurant process prior;Dirichlet process mixture model;LV echocardiogram images;active shape models;automatic detection;contextual shape model;deep learning;dynamic statistical shape model;echocardiography;feature subspace;heart disease diagnosis;heart disease treatment;iterative segmentation process;latent temporal component allocation variable;left ventricle localization;left ventricle segmentation;shape subspace;statistical shape models,,,,,,,,Nov. 30 2016-Dec. 2 2016,,IEEE,IEEE Conference Publications
399,Classification of Exacerbation Frequency in the COPDGene Cohort Using Deep Learning with Deep Belief Networks,J. Ying; J. Dutta; N. Guo; C. Hu; D. Zhou; A. Sitek; Q. Li,,IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"This study aims to develop an automatic classifier based on deep learning for exacerbation frequency in patients with chronic obstructive pulmonary disease (COPD). A threelayer deep belief network (DBN) with two hidden layers and one visible layer was employed to develop classification models and the models’ robustness to exacerbation was analyzed. Subjects from the COPDGene cohort were labeled with exacerbation frequency, defined as the number of exacerbation events per year. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 91.99%, using a 10-fold cross validation experiment. The analysis of DBN weights showed that there was a good visual spatial relationship between the underlying critical features of different layers. Our findings show that the most sensitive features obtained from the DBN weights are consistent with the consensus showed by clinical rules and standards for COPD diagnostics. We thus demonstrate that DBN is a competitive tool for exacerbation risk assessment for patients suffering from COPD.",2168-2194;21682194,,10.1109/JBHI.2016.2642944,10.13039/100000049 - National Institute on Aging; 10.13039/100000050 - National Heart Lung and Blood Institute; 10.13039/100000070 - National Institute of Biomedical Imaging and Bioengineering; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792616,Chronic obstructive pulmonary disease (COPD);Fisher score;deep belief network (DBN);deep learning;exacerbation,Diseases;Electronic mail;Feature extraction;Gold;Hospitals;Lungs;Machine learning,,,,,,,,,20161221.0,,,IEEE,IEEE Early Access Articles
400,DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation,H. Chen; X. Qi; L. Yu; P. A. Heng,"Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212.0,2016,,,2487,2496,"The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.273,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780642,,Biomedical imaging;Cancer;Feature extraction;Glands;Image segmentation;Neural networks;Training,biomedical optical imaging;cancer;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;probability;statistical analysis;tumours,2015 MICCAI Gland Segmentation Challenge;DCAN;deep contour-aware networks;gland morphology;gland segmentation;hierarchical architecture;histology images;large-scale histopathological data;malignancy degree;morphological statistics;multitask regularization;probability maps;unified multitask learning,,3.0,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
401,Convolutional neural networks for deep feature learning in retinal vessel segmentation,A. F. Khalaf; I. A. Yassine; A. S. Fahmy,"Systems and Biomedical Engineering Department, Faculty of Engineering, Cairo University",2016 IEEE International Conference on Image Processing (ICIP),20161208.0,2016,,,385,388,"Analysis of retinal vessels in fundus images provides a valuable tool for characterizing many retinal and systemic diseases. Accurate automatic segmentation of these vessels is usually required as an essential analysis step. In this work, we propose a new formulation of deep Convolutional Neural Networks that allows simple and accurate segmentation of the retinal vessels. A major modification in this work is to reduce the intra-class variance by formulating the problem as a Three-class problem that differentiates: large vessels, small vessels, and background areas. In addition, different sizes of the convolutional kernels have been studied and it was found that a combination of kernels with different sizes achieve the best sensitivity and specificity. The proposed method was tested using DRIVE dataset and it showed superior performance compared to several other state of the art methods. The segmentation sensitivity, specificity and accuracy were found to be 83.97%, 95.62% and 94.56% respectively.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532384,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532384,Convolutional Neural Networks;Deep Learning;Pattern Classification;Retinal Blood Vessel Segmentation,,diseases;image segmentation;medical image processing;neural nets,DRIVE dataset;convolutional neural networks;deep feature learning;fundus images;retinal blood vessel segmentation;retinal diseases;retinal vessel analysis;segmentation sensitivity;systemic diseases,,,,,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
402,Deepmole: Deep neural networks for skin mole lesion classification,V. Pomponiu; H. Nejati; N. M. Cheung,"Information System, Technology and Desing (SUTD) Singapore University of Technology and Design (ISTD) Somapah Road 8, Singapore, 487372",2016 IEEE International Conference on Image Processing (ICIP),20161208.0,2016,,,2623,2627,"Nowadays, the occurrence of skin cancer cases has grown worldwide due to the extended exposure to the harmful radiation from the Sun. Most common approach to detect the malignancy of skin moles is by visual inspection performed by an expert dermatologist, using a set of specific clinical rules. Computer-aided diagnosis, based on skin mole imaging, is another concurrent method which has experienced major advancements due to improvement of imaging sensors and processing power. However, these schemes use hand-crafted features which are difficult to tune and perform poorly on new cases due to lack of generalization power. In this study we present a method that use a pretrained deep neural network (DNN) to automatically extract a set of representative features that can be later used to diagnose a sample of skin lesion for malignancy. The experimental tests carried out on a clinical dataset show that the classification performance using DNN-based features performs better than the state-of-the-art techniques.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532834,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532834,Skin mole classification;deep neural networks;feature extraction;malignant melanoma;transfer learning,Feature extraction;Image color analysis;Lesions;Malignant tumors;Neural networks;Skin;Skin cancer,cancer;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;skin,DNN-based features;automatic representative feature extraction;clinical dataset;computer-aided diagnosis;deepmole;malignancy;pretrained deep neural network;skin cancer;skin mole imaging;skin mole lesion classification,,1.0,,,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
403,Segmenting Retinal Blood Vessels With Deep Neural Networks,P. Liskowski; K. Krawiec,"Institute of Computing Science, Poznan University of Technology, Poland",IEEE Transactions on Medical Imaging,20161103.0,2016,35,11.0,2369,2380,"The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.",0278-0062;02780062,,10.1109/TMI.2016.2546227,10.13039/501100005632 - Narodowe Centrum Bada? i Rozwoju; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440871,Classification;deep learning;feature learning;fundus;neural networks;retina;retinopathy;structured prediction;vessel segmentation,Biomedical imaging;Blood vessels;Convolution;Databases;Image segmentation;Neural networks;Pathology,blood vessels;eye;image classification;image segmentation;medical image processing;neural nets;sensitivity analysis;unsupervised learning,CHASE databases;DRIVE databases;ROC curve;STARE databases;central vessel reflex;deep neural networks;diagnostic factor;fundus imaging;gamma corrections;geometric transformations;global contrast normalization;hemorrhages;human eye;image classification;microaneurysms;nontrivial task;ophthalmology;retinal blood vessel segmentation;structured prediction;supervised segmentation;vascular network;zero-phase whitening,,7.0,,,,,20160324.0,Nov. 2016,,IEEE,IEEE Journals & Magazines
404,Automatic tissue characterization of air trapping in chest radiographs using deep neural networks,A. Mansoor; G. Perez; G. Nino; M. G. Linguraru,"Sheikh Zayed Institute for Pediatric Surgical Innovation, Children's National Health System, Washington DC",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,97,100,"Significant progress has been made in recent years for computer-aided diagnosis of abnormal pulmonary textures from computed tomography (CT) images. Similar initiatives in chest radiographs (CXR), the common modality for pulmonary diagnosis, are much less developed. CXR are fast, cost effective and low-radiation solution to diagnosis over CT. However, the subtlety of textures in CXR makes them hard to discern even by trained eye. We explore the performance of deep learning abnormal tissue characterization from CXR. Prior studies have used CT imaging to characterize air trapping in subjects with pulmonary disease; however, the use of CT in children is not recommended mainly due to concerns pertaining to radiation dosage. In this work, we present a stacked autoencoder (SAE) deep learning architecture for automated tissue characterization of air-trapping from CXR. To our best knowledge this is the first study applying deep learning framework for the specific problem on 51 CXRs, an F-score of ≈ 76.5% and a strong correlation with the expert visual scoring (R=0.93, p =<; 0.01) demonstrate the potential of the proposed method to characterization of air trapping.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590649,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590649,,Charge carrier processes;Computed tomography;Lungs;Machine learning;Shape;Training;Visualization,biological tissues;computerised tomography;diseases;learning (artificial intelligence);medical image processing;neural nets;pneumodynamics,CXR textures;abnormal pulmonary textures;air trapping;automatic tissue characterization;chest radiographs;computed tomography images;computer-aided diagnosis;deep neural networks;pulmonary diagnosis;pulmonary disease;radiation dosage;stacked autoencoder deep learning architecture,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
405,Recent machine learning advancements in sensor-based mobility analysis: Deep learning for Parkinson's disease assessment,B. M. Eskofier; S. I. Lee; J. F. Daneault; F. N. Golabchi; G. Ferreira-Carvalho; G. Vergara-Diaz; S. Sapienza; G. Costante; J. Klucken; T. Kautz; P. Bonato,"Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg (FAU), Erlangen, Germany",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,655,658,"The development of wearable sensors has opened the door for long-term assessment of movement disorders. However, there is still a need for developing methods suitable to monitor motor symptoms in and outside the clinic. The purpose of this paper was to investigate deep learning as a method for this monitoring. Deep learning recently broke records in speech and image classification, but it has not been fully investigated as a potential approach to analyze wearable sensor data. We collected data from ten patients with idiopathic Parkinson's disease using inertial measurement units. Several motor tasks were expert-labeled and used for classification. We specifically focused on the detection of bradykinesia. For this, we compared standard machine learning pipelines with deep learning based on convolutional neural networks. Our results showed that deep learning outperformed other state-of-the-art machine learning algorithms by at least 4.6 % in terms of classification rate. We contribute a discussion of the advantages and disadvantages of deep learning for sensor-based movement assessment and conclude that deep learning is a promising method for this field.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590787,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590787,,Biomedical monitoring;Convolution;Feature extraction;Machine learning;Pipelines;Standards;Training,biomechanics;biomedical measurement;body sensor networks;diseases;inertial systems;learning (artificial intelligence),Parkinson's disease assessment;bradykinesia;deep learning;inertial measurement units;machine learning;sensor-based mobility analysis,,1.0,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
406,Automatic prostate segmentation on MR images with deep network and graph model,K. Yan; C. Li; X. Wang; A. Li; Y. Yuan; D. Feng; M. Khadra; J. Kim,"School of Information Technologies, University of Sydney, Australia",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,635,638,"Automated prostate diagnoses and treatments have gained much attention due to the high mortality rate of prostate cancer. In particular, unsupervised (automatic) prostate segmentation is an active and challenging research. Most conventional works usually utilize handcrafted (low-level) features for prostate segmentation; however they often fail to extract the intrinsic structure of the prostate, especially on images with blurred boundaries. In this paper, we propose a novel automated prostate segmentation model with learned features from deep network. Specifically, we first generate a set of prostate proposals in transverse plane via recognizing the position and coarse estimate of the shape of the prostate on the global prostate image and using the deep network to extract highly effective features for the boundary refinement in a finer scale. With consideration of the correlations among different sequential images, we then construct a graph to select the best prostate proposals from proposal set for its use in 3D prostate segmentation. Experimental evaluation demonstrates that our proposed deep network and graph based method is superior to state-of-the-art couterparts, in terms of both dice similarity coefficient and Hausdorff distance, on public dataset.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590782,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590782,,Algorithm design and analysis;Context;Feature extraction;Image segmentation;Proposals;Shape;Three-dimensional displays,biomedical MRI;cancer;graph theory;image segmentation;image sequences;medical image processing;unsupervised learning,3D prostate segmentation;Hausdorff distance;MR images;automated prostate diagnosis;automated prostate segmentation model;automated prostate treatments;boundary refinement;deep network;dice similarity coefficient;global prostate image;graph model;handcrafted features;prostate cancer;prostate intrinsic structure;prostate shape;public dataset;sequential images;transverse plane;unsupervised prostate segmentation,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
407,Pain detection from facial images using unsupervised feature learning approach,R. Kharghanian; A. Peiravi; F. Moradi,"Department of Electrical Engineering, Ferdowsi University of Mashhad, Mashhad, Iran",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,419,422,"In this paper a new method for continuous pain detection is proposed. One approach to detect the presence of pain is by processing images taken from the face. It has been reported that expression of pain from the face can be detected utilizing Action Units (AUs). In this manner, each action units must be detected separately and then combined together through a linear expression. Also, pain detection can be directly done from a painful face. There are different methods to extract features of both shape and appearance. Shape and appearance features must be extracted separately, and then used to train a classifier. Here, a hierarchical unsupervised feature learning approach is proposed in order to extract the features needed for pain detection from facial images. In this work, features are extracted using convolutional deep belief network (CDBN). The extracted features include different properties of painful images such as head movements, shape and appearance information. The proposed model was tested on the publicly available UNBC MacMaster Shoulder Pain Archive Database and we achieved near 95% for the area under ROC curve metric that is prominent with respect to the other reported results.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590729,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590729,,Active appearance model;Face;Feature extraction;Pain;Probabilistic logic;Shape;Support vector machines,belief networks;face recognition;feature extraction;image classification;medical image processing;sensitivity analysis;unsupervised learning,ROC curve metrics;UNBC MacMaster Shoulder Pain Archive Database;action units;convolutional deep belief network;facial images;feature extraction;head movements;hierarchical unsupervised feature learning approach;image processing;pain detection,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
408,Integrating holistic and local deep features for glaucoma classification,A. Li; J. Cheng; D. W. K. Wong; J. Liu,"Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,1328,1331,"Automated glaucoma detection is an important application of retinal image analysis. Compared with segmentation based approaches, image classification based approaches have a potential of better performance. However, it still remains a challenging problem for two reasons. Firstly, due to insufficient sample size, learning effective features is difficult. Secondly, the shape variations of optic disc introduce misalignment. To address these problem, a new classification based approach for glaucoma detection is proposed, in which deep convolutional networks derived from large-scale generic dataset is used to representing the visual appearance and holistic and local features are combined to mitigate the influence of misalignment. The proposed method achieves an area under the receiver operating characteristic curve of 0.8384 on the Origa dataset, which clearly demonstrates its effectiveness.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590952,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590952,,Feature extraction;Optical imaging;Optical receivers;Optical sensors;Retina;Shape;Visualization,diseases;image classification;image segmentation;medical image processing,Origa dataset;automated glaucoma detection;glaucoma classification;image classification;image segmentation;large-scale generic dataset;optic disc;receiver operating characteristic curve;retinal image analysis;sample size;visual appearance,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
409,Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography,A. Lahiri; A. G. Roy; D. Sheet; P. K. Biswas,"Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology Kharagpur, India",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,1340,1343,"Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590955,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590955,,Biomedical imaging;Feature extraction;Image segmentation;Kernel;Retinal vessels;Standards;Training,biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;feature extraction;image classification;image coding;image denoising;image segmentation;medical image processing;unsupervised learning,2-level fusion;DRIVE dataset;Kappa agreement coefficient;SoftMax classifier;architectural revision;autoencoders fosters diversity;automated segmentation;bootstrap samples;cardiovascular diseases;classification task;computed aided diagnosis;deep neural ensemble;diabetic retinopathy;first level training;hypertensive disorders;label-free angiography;label-free fundus images;learning dictionary;maximum average accuracy;medical image analysis;network architectures;noisy background;ophthalmic pathologies;physical appearance;retinal blood vessels;retinal vessel segmentation;sparsely trained denoised stacked autoencoder;standard deviation;unsupervised hierarchical feature learning;visual kernels,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
410,Text-mining the neurosynth corpus using deep boltzmann machines,R. Monti; R. Lorenz; R. Leech; C. Anagnostopoulos; G. Montana,"Department of Mathematics, Imperial College London",2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI),20160901.0,2016,,,1,4,"Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.",,Electronic:978-1-4673-6530-7; POD:978-1-4673-6531-4,10.1109/PRNI.2016.7552329,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552329,Deep Boltzmann machines;meta-analysis;text-mining;topic models,Brain modeling;Context;Monte Carlo methods;Neuroimaging;Proposals;Semantics;Vocabulary,Boltzmann machines;biomedical MRI;data mining;learning (artificial intelligence);medical image processing;text analysis,DBM;NeuroSynth database;brain activation coordinate;deep Boltzmann machine;fMRI;functional magnetic resonance imaging;human brain function;machine learning;neuroimaging data metaanalysis;text mining,,2.0,,,,,,22-24 June 2016,,IEEE,IEEE Conference Publications
411,Learning a multiscale patch-based representation for image denoising in X-RAY fluoroscopy,Y. Matviychuk; B. Mailhé; X. Chen; Q. Wang; A. Kiraly; N. Strobel; M. Nadar,"Siemens Healthcare, Medical Imaging Technologies, Princeton, NJ, USA",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,2330,2334,"Denoising is an indispensable step in processing low-dose X-ray fluoroscopic images that requires development of specialized high-quality algorithms able to operate in near real-time. We address this problem with an efficient deep learning approach based on the process-centric view of traditional iterative thresholding methods. We develop a novel trainable patch-based multiscale framework for sparse image representation. In a computationally efficient way, it allows us to accurately reconstruct important image features on multiple levels of decomposition with patch dictionaries of reduced size and complexity. The flexibility of the chosen machine learning approach allows us to tailor the learned basis for preserving important structural information in the image and noticeably minimize the amount of artifacts. Our denoising results obtained with real clinical data demonstrate significant quality improvement and are computed much faster in comparison with the BM3D algorithm.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532775,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532775,Radiography;iterative algorithms;neural networks,Approximation algorithms;Dictionaries;Image reconstruction;Image representation;Iterative methods;Neural networks;Noise reduction,diagnostic radiography;feature extraction;image denoising;image reconstruction;image representation;image segmentation;iterative methods;learning (artificial intelligence);medical image processing,BM3D algorithm;X-ray fluoroscopy;artifact minimization;clinical data;complexity;deep learning approach;image decomposition;image denoising;image feature reconstruction;iterative thresholding method;low-dose X-ray fluoroscopic image;machine learning;multiscale patch-based representation learning;patch dictionary;sparse image representation;structural information preservation;trainable patch-based multiscale framework,,1.0,,38.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
412,A Deep Learning Method for Microaneurysm Detection in Fundus Images,J. Shan; L. Li,"Dept. of Comput. Sci., Pace Univ. New York City, New York, NY, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20160818.0,2016,,,357,358,"Diabetic Retinopathy (DR) is the leading cause of blindness in the working-age population. Microaneurysms (MAs), due to leakage from retina blood vessels, are the early signs of DR. However, automated MA detection is complicated because of the small size of MA lesions and the low contrast between the lesion and its retinal background. Recently deep learning (DL) strategies have been used for automatic feature extraction and classification problems, especially for image analysis. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a DL strategy, is presented for MA detection in fundus images. Small image patches are generated from the original fundus images. The SSAE learns high-level features from pixel intensities alone in order to identify distinguishing features of MA. The high-level features learned by SSAE are fed into a classifier to categorize each image patch as MA or non-MA. The public benchmark DIARETDB is utilized to provide the training/testing data and ground truth. Among the 89 images, totally 2182 image patches with MA lesions, serve as positive data, and another 6230 image patches without MA lesions are generated by a randomly sliding window operation, to serve as negative data. Without any blood vessel removal or complicated preprocessing operations, SSAE learned directly from the raw image patches, and automatically extracted the distinguishing features to classify the patches using Softmax Classifier. By employing the fine-tuning operation, an improved F-measure 91.3% and an average area under the ROC curve (AUC) 96.2% were achieved using 10-fold cross-validation.",,Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2,10.1109/CHASE.2016.12,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545864,automated microaneurysm detection;deep learning;diabetic retinopathy;feature representation;stacked sparse autoencoder,Biomedical image processing;Conferences;Feature extraction;Lesions;Machine learning,blood vessels;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing,10-fold cross-validation;DIARETDB public benchmark;DR;F-measure;SSAE;automated MA detection;automatic feature extraction;blindness;classification problems;deep learning method;diabetic retinopathy;fine-tuning operation;fundus images;image analysis;microaneurysm detection;microaneurysms;randomly sliding window operation;retina blood vessels;softmax classifier;stacked sparse autoencoder;working-age population,,1.0,,,,,,27-29 June 2016,,IEEE,IEEE Conference Publications
413,Automatic burn area identification in color images,M. S. Badea; C. Vertan; C. Florea; L. Florea; S. Bădoiu,"The Image Processing and Analysis Lab (LAPI), Politehnica University of Bucharest, Romania",2016 International Conference on Communications (COMM),20160804.0,2016,,,65,68,"This papers presents the use of color imaging as a starting point of burn wound evaluation, by the discrimination between healthy skin and burn wound. The skin/burn area identification is performed pixel-wise, according to the properties of an entire encompassing patch. The classification is learned under a supervised scenario, according to a ground truth defined by specialist surgeons from a large pediatric case database, by a deep learned convolutional neural network. The database is extensive and was recorded over several months in real, hospital conditions The proposed approach achieves an overall performance comparable to the literature-reported average performance of a specialist surgeon.",,DVD:978-1-4673-8196-3; Electronic:978-1-4673-8197-0; POD:978-1-4673-8198-7,10.1109/ICComm.2016.7528325,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528325,burn identification;color imaging;color segmentation;deep learning convolutional networks;diagnosis support,Color;Databases;Image color analysis;Neural networks;Skin;Surgery;Wounds,biomedical optical imaging;convolution;image colour analysis;learning (artificial intelligence);medical image processing;neural nets;paediatrics;skin;wounds,automatic burn area identification;burn wound evaluation;color imaging;deep learned convolutional neural network;ground truth;healthy skin;large pediatric case database;skin-burn area identification,,,,,,,,9-10 June 2016,,IEEE,IEEE Conference Publications
414,Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation,A. Mansoor; J. J. Cerrolaza; R. Idrees; E. Biggs; M. A. Alsharid; R. A. Avery; M. G. Linguraru,"Children's National Health System, Washington",IEEE Transactions on Medical Imaging,20160729.0,2016,35,8.0,1856,1865,"Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.",0278-0062;02780062,,10.1109/TMI.2016.2535222,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420737,Anterior visual pathway;MRI;intensity normalization;partitioned statistical model;shape model;sparse learning,Biomedical optical imaging;Machine learning;Magnetic resonance imaging;Optical imaging;Pathology;Robustness;Shape,biomedical MRI;curvelet transforms;eye;image segmentation;image sequences;medical image processing;paediatrics;statistical analysis,Dice similarity coefficient;MRI sequences;anatomic structures;anterior visual pathway segmentation;conditional space deep learning;cranial nerve systems;deep learning feature representation;deep learning guided partitioned shape model;joint partitioned statistical shape model;low-grade gliomas;optic nerve;pathologic AVP segmentation;pediatric subjects;robust statistical model;shape localization method;sparse appearance-based learning approach;volumetric multiscale curvelet transform-based intensity normalization method,,3.0,,,,,20160226.0,Aug. 2016,,IEEE,IEEE Journals & Magazines
415,Clinical decision support for Alzheimer's disease based on deep learning and brain network,C. Hu; R. Ju; Y. Shen; P. Zhou; Q. Li,"School of Engineering and Applied Sciences, Harvard University",2016 IEEE International Conference on Communications (ICC),20160714.0,2016,,,1,6,"Modern e-health systems have undergone rapid development thanks to the advances in communications, computing and machine learning technology. Especially, deep learning has great superiority in image analysis and disease prediction. In this paper, we use Alzheimer's Disease (AD) as an example to show advantages of deep learning in diagnosing brain diseases and providing clinical decision support. Firstly, we convert raw functional magnetic resonance imaging (fMRI) to a matrix to represent activity of 90 brain regions. Secondly, to represent the functional connectivity between different brain regions, a correlation matrix is obtained by calculating the correlation between each pair of brain regions. In the next, a targeted autoencoder network is built to classify the correlation matrix, which is sensitive to AD. Finally, the experiment results show that our proposed method for AD prediction achieves much better effects than traditional means. It finds the correlations between different brain regions efficiently, provides strong reference for AD prediction. Compared to Support Vector Machine (SVM), about 25% improvement is gained in prediction accuracy. The e-health field becomes more complete and effective owing to that. Our work helps predict AD at an early stage and take measures to slow down or even prevent the onset of it.",,Electronic:978-1-4799-6664-6; POD:978-1-4799-6665-3,10.1109/ICC.2016.7510831,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7510831,,Correlation;Cost function;Diseases;Machine learning;Positron emission tomography;Support vector machines;Time series analysis,biomedical MRI;brain;decision support systems;diseases;learning (artificial intelligence);matrix algebra;medical image processing,AD prediction;Alzheimer's Disease;brain diseases diagnosing;brain network;clinical decision support;correlation matrix;deep learning;disease prediction;e-health field;fMRI;functional connectivity;image analysis;modern e-health systems;raw functional magnetic resonance imaging;targeted autoencoder network,,,,,,,,22-27 May 2016,,IEEE,IEEE Conference Publications
416,HEp-2 cell classification using a deep neural network trained for natural image classification,B. Benligiray; H. Ç. Akakın,"Elektrik-Elektronik M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, Anadolu &#220;niversitesi, Eski&#351;ehir, T&#252;rkiye",2016 24th Signal Processing and Communication Application Conference (SIU),20160623.0,2016,,,1361,1364,"Deep convolutional neural networks is a recently developed method that yields very successful results in image classification. Deep neural networks, which have a high number of parameters, require a large amount of data to avoid overfitting during training. For applications in which the available data is not adequate to train a deep neural network from the scratch, deep neural networks trained for similar objectives can be used as a starting point. In this study, cell images are classified using a deep neural network trained to classify objects in natural images. Even though classification of natural images and cell images are very different objectives, cell images are able to be classified with 74.1% mean class accuracy. The results show that features used for visual classification by deep convolutional neural networks may be more universal than assumed.",,Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5,10.1109/SIU.2016.7496001,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496001,HEp-2 cells;deep convolutional neural networks;image classification;indirect immunofluorescence,Computer vision;Conferences;Image recognition;Neural networks;Pattern recognition;Training;Visualization,image classification;learning (artificial intelligence);medical image processing;neural nets,HEp-2 cell classification;deep convolutional neural network training;natural image classification;visual classification,,,,,,,,16-19 May 2016,,IEEE,IEEE Conference Publications
417,Detection of articulated instruments in retinal microsurgery,M. Alsheakhali; A. Eslami; N. Navab,"Technical University of Munich (TUM), Germany",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,107,110,"Instrument detection in retinal microsurgery is still one of the most challenging operations due to illumination changes, fast motion, cluttered background and deformable shape of the instrument. In this work, a new technique is proposed to detect an articulated forceps instrument by modeling it using Conditional Random Field (CRF). The unary potentials of the CRF, which represent the instrument parts, are detected using the deep convolutional neural network, where two probability distribution maps for both the forceps center and its shaft are estimated. The pairwise potentials are modeled using a regression random forest to learn the relation between the instrument parts based on their joint structural features. Sampled combinations from both unary distributions are selected, and each is tested using the regression forest to compute its similarity to the medical instrument structure. The best combination candidate chosen by the CRF predicts the forceps center point (instrument joint point) and the orientation of its shaft. The approach shows high detection accuracy on public datasets and real videos for retinal microsurgery operations.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493222,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493222,Conditional Random Field;Deep Learning;Instrument Detection;Retinal Microsurgery,Feature extraction;Instruments;Microsurgery;Retina;Shafts;Testing,,,,,,11.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
418,Comprehensive autoencoder for prostate recognition on MR images,K. Yan; C. Li; X. Wang; Y. Yuan; A. Li; J. Kim; B. Li; D. Feng,"School of Information Technologies, University of Sydney",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1190,1194,"Automatic recognition of anatomical structures is an essential prerequisite in computer aided diagnoses (CAD) such as tissue segmentation, physiological signal measurement and disease classification. However, insufficient color and speckle information in medical images pose challenges to the recognition of anatomical structures. Such challenges are evident with prostate recognition on magnetic resonance (MR) images and thus remain an open problem, although prostate cancer is an important problem that are attracting increasing interests in medical imaging. In this study, we propose an automatic approach for prostate recognition on MR images. Firstly, compared to existing works which integrate autoencoder with a specific type of classifier, we let autoencoder itself serve as a classifier and therefore lessening the impact from irregular and complex background found in prostate recognition. Secondly, an image energy minimization scheme with consideration of the coherence information from neighboring pixels is proposed to improve the recognition results with clear boundary appearance. We evaluate our method in comparison with three widely applied classifiers and the phase of atlas-based seeds-selection in prostate segmentation on a public prostate database. Our experiment results demonstrate significant superiority of our method in terms of both precision and F-measure.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493479,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493479,autoencoder;classification;deep learning;prostate recognition,Anatomical structure;Feature extraction;Image recognition;Image reconstruction;Image segmentation;Training,,,,,,29.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
419,Automated mitosis detection with deep regression networks,H. Chen; X. Wang; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1204,1207,"Mitosis counting is one of the strongest prognostic markers for invasive breast cancer diagnosis. Clinical visual examination on histology slides by pathologists is tedious, error-prone and time-consuming. Furthermore, with the advent of whole slide imaging for high-throughput digitization, a large quantity of histology images need to be analyzed. Therefore, automated mitosis detection methods are highly demanded in clinical practice. In this paper, we proposed a deep regression network (DRN) to meet these challenges. It consisted of a downsampling path for extracting the high level information and an upsampling path for outputting the score map with original input size, thus it can be trained in an end-to-end way. In addition, we transferred knowledge learned from cross domains to mitigate the issue of insufficient medical training data. Experimental results on the benchmark dataset 2012ICPR Mitosis Detection Challenge demonstrated the efficacy of our approach, which achieved comparable or better performance than the state-of-the-art methods.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493482,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493482,Mitosis detection;convolutional neural network;deep learning;regression,Benchmark testing;Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Pathology;Training,biological tissues;cancer;learning (artificial intelligence);medical image processing;regression analysis,automated mitosis detection;benchmark dataset 2012 ICPR Mitosis Detection Challenge;clinical visual examination;deep regression network;deep regression networks;downsampling path;high level information;high-throughput digitization;histology imaging;histology slides;invasive breast cancer diagnosis;medical training data;prognostic markers;upsampling path;whole slide imaging,,,,20.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
420,Multi-loss convolutional networks for gland analysis in microscopy,A. BenTaieb; J. Kawahara; G. Hamarneh,"Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,642,645,"Manual tissue diagnosis is the most prevalent approach to cancer diagnosis. However, it mainly relies on a subjective visual quantification of specific morphometric features, which often leads to a relatively limited reproducibility among experts. In most computational techniques proposed to automate the diagnostic procedure, accurate segmentation is paramount as a precursor to the extraction of relevant morphometric features. Since the ultimate goal of segmentation is generally classification, yet a given class imparts an expected tissue appearance beneficial to segmentation, we pose the problem of automatic tissue analysis as the joint task of segmentation and classification. We propose a novel multi-objective learning method that optimizes a single unified deep fully convolutional neural network with two distinct loss functions. We illustrate our reasoning on the task of colon adenocarcinomas diagnosis and show how glands' classification can facilitate their segmentation by adding class-specific spatial priors. The final classification also benefits from this joint learning framework yielding an improvement of 6% over classification-only models.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493349,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493349,Classification;Deep Learning;Histopathology;Segmentation,Cancer;Colon;Feature extraction;Glands;Image segmentation;Training;Tumors,cancer;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation,automatic tissue analysis;cancer diagnosis;colon adenocarcinoma diagnosis;gland classification;image segmentation;microscopy;morphometric feature extraction;multiloss convolutional networks;multiobjective learning method;optimization,,,,6.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
421,Detection of age-related macular degeneration via deep learning,P. Burlina; D. E. Freund; N. Joshi; Y. Wolfson; N. M. Bressler,"Applied Physics Laboratory, Johns Hopkins University",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,184,188,"Age-related macular generation (AMD) - when left untreated - is the main cause of blindness for individuals over the age of 50. With the US population now counting over 100 million individuals over 50, it is imperative to develop methods that can effectively determine which individuals with an earlier, often asymptomatic stage, are at risk of developing the advanced stage that can cause severe vision loss. This paper studies the appropriateness of the transfer of image features computed from pre-trained deep neural networks to the problem in AMD detection. Tests using over 5600 images from the NIH AREDS dataset (the largest dataset used thus far for AMD image analysis studies) show good preliminary results (between nearly 92% and 95% accuracy).",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493240,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493240,Age-related macular degeneration;deep learning;pre-trained networks,Blindness;Classification algorithms;Feature extraction;Machine learning;Retina;Sensitivity;Support vector machines,biomedical optical imaging;feature extraction;learning (artificial intelligence);medical image processing;neural nets;vision defects,AMD detection;AMD image analysis;NIH AREDS dataset;age-related macular degeneration;asymptomatic stage;blindness;deep learning;image features;pretrained deep neural networks;severe vision loss,,,,11.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
422,Towards grading gleason score using generically trained deep convolutional neural networks,H. Källén; J. Molin; A. Heyden; C. Lundström; K. Åström,"Centre for Mathematical Sciences, Lund University",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616.0,2016,,,1163,1167,"We developed an automatic algorithm with the purpose to assist pathologists to report Gleason score on malignant prostatic adenocarcinoma specimen. In order to detect and classify the cancerous tissue, a deep convolutional neural network that had been pre-trained on a large set of photographic images was used. A specific aim was to support intuitive interaction with the result, to let pathologists adjust and correct the output. Therefore, we have designed an algorithm that makes a spatial classification of the whole slide into the same growth patterns as pathologists do. The 22-layer network was cut at an earlier layer and the output from that layer was used to train both a random forest classifier and a support vector machines classifier. At a specific layer a small patch of the image was used to calculate a feature vector and an image is represented by a number of those vectors. We have classified both the individual patches and the entire images. The classification results were compared for different scales of the images and feature vectors from two different layers from the network. Testing was made on a dataset consisting of 213 images, all containing a single class, benign tissue or Gleason score 35. Using 10-fold cross validation the accuracy per patch was 81 %. For whole images, the accuracy was increased to 89 %.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493473,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493473,Convolutional Neural Networks;Deep Learning;Gleason Score;Prostate cancer,Cancer;Feature extraction;Kernel;Neural networks;Radio frequency;Support vector machines;Training,,,,,,15.0,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
423,TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning,A. Murali; A. Garg; S. Krishnan; F. T. Pokorny; P. Abbeel; T. Darrell; K. Goldberg,"EECS & IEOR, University of California, Berkeley USA",2016 IEEE International Conference on Robotics and Automation (ICRA),20160609.0,2016,,,4150,4157,"The growth of robot-assisted minimally invasive surgery has led to sizable datasets of fixed-camera video and kinematic recordings of surgical subtasks. Segmentation of these trajectories into locally-similar contiguous sections can facilitate learning from demonstrations, skill assessment, and salvaging good segments from otherwise inconsistent demonstrations. Manual, or supervised, segmentation can be prone to error and impractical for large datasets. We present Transition State Clustering with Deep Learning (TSC-DL), a new unsupervised algorithm that leverages video and kinematic data for task-level segmentation, and finds regions of the visual feature space that correlate with transition events using features constructed from layers of pre-trained image classification Deep Convolutional Neural Networks (CNNs). We report results on three datasets comparing Deep Learning architectures (AlexNet and VGG), choice of convolutional layer, dimensionality reduction techniques, visual encoding, and the use of Scale Invariant Feature Transforms (SIFT). We find that the deep architectures extract features that result in up-to a 30.4% improvement in Silhouette Score (a measure of cluster tightness) over the traditional “shallow” features from SIFT. We also present cases where TSC-DL discovers human annotator omissions. Supplementary material, data and code is available at: http://berkeleyautomation.github.io/tsc-dl/.",,Electronic:978-1-4673-8026-3; POD:978-1-4673-8027-0,10.1109/ICRA.2016.7487607,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487607,,Feature extraction;Hidden Markov models;Kinematics;Machine learning;Motion segmentation;Visualization,control engineering computing;convolution;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;medical robotics;neural nets;pattern clustering;robot vision;surgery;transforms;video signal processing,TSC-DL unsupervised algorithm;dimensionality reduction;feature extraction;fixed-camera video;image classification deep convolutional neural networks;learning from demonstrations;multimodal surgical demonstrations;robot-assisted minimally invasive surgery;scale invariant feature transforms;surgical subtask kinematic recordings;task-level segmentation;transition state clustering with deep learning;unsupervised trajectory segmentation;visual encoding;visual feature space,,1.0,,29.0,,,,16-21 May 2016,,IEEE,IEEE Conference Publications
424,A deep convolutional neural network trained on representative samples for circulating tumor cell detection,Y. Mao; Z. Yin; J. Schober,Missouri University of Science and Technology,2016 IEEE Winter Conference on Applications of Computer Vision (WACV),20160526.0,2016,,,1,6,"The number of Circulating Tumor Cells (CTCs) in blood indicates the tumor response to chemotherapeutic agents and disease progression. In early cancer diagnosis and treatment monitoring routine, detection and enumeration of CTCs in clinical blood samples have significant applications. In this paper, we design a Deep Convolutional Neural Network (DCNN) with automatically learned features for image-based CTC detection. We also present an effective training methodology which finds the most representative training samples to define the classification boundary between positive and negative samples. In the experiment, we compare the performance of auto-learned feature from DCNN and hand-crafted features, in which the DCNN outperforms hand-crafted feature. We also prove that the proposed training methodology is effective in improving the performance of DCNN classifiers.",,Electronic:978-1-5090-0641-0; POD:978-1-5090-0642-7,10.1109/WACV.2016.7477603,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477603,,Blood;Cancer;Cells (biology);Detectors;Feature extraction;Training;Tumors,diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;tumours,CTC detection;DCNN training;chemotherapeutic agent;circulating tumor cell detection;classification boundary;deep convolutional neural network;disease progression;feature autolearning;tumor response,,,,16.0,,,,7-10 March 2016,,IEEE,IEEE Conference Publications
425,Deep multi-view representation learning for multi-modal features of the schizophrenia and schizo-affective disorder,J. Qi; J. Tejedor,"Electrical Engineering, University of Washington, Seattle, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519.0,2016,,,952,956,"This work is originated from the MLSP 2014 Classification Challenge which tries to automatically detect subjects with schizophrenia and schizo-affective disorder by analyzing multi-modal features derived from magnetic resonance imaging (MRI) data. We employ Deep Neural Network (DNN)-based multi-view representation learning for combining multimodal features. The DNN-based multi-view models include deep canonical correlation analysis (DCCA) and deep canonically correlated auto-encoders (DCCAE). In addition, support vector machine with Gaussian kernel is used to conduct classification with the compact bottleneck features learned by the deep multi-view models. Our experiments on the dataset provided by the MLSP Classification Challenge show that bottleneck features learned via deep multi-view models obtain better results than the trimming features used in the baseline system in terms of the receiver operating characteristic (ROC) area under the curve (AUC).",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7471816,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471816,Deep Canonical Correlation Analysis;Deep Canonically Correlated Auto-encoders;MRI;ROC/AUC;Schizophrenia;Support Vector Machine,Correlation;Covariance matrices;Feature extraction;Kernel;Magnetic resonance imaging;Support vector machines;Training,Gaussian processes;biomedical MRI;diseases;feature extraction;image representation;learning (artificial intelligence);medical image processing;support vector machines,AUC;DCCA;Gaussian kernel;MRI data;ROC;area under the curve;deep canonical correlation analysis;deep multiview representation learning;deep neural network;magnetic resonance imaging data;multimodal features;receiver operating characteristic;schizo-affective disorder;schizophrenia disorder;support vector machine,,,,16.0,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
426,Latent feature representation with 3-D multi-view deep convolutional neural network for bilateral analysis in digital breast tomosynthesis,D. H. Kim; S. T. Kim; Y. M. Ro,"School of Electrical Engineering, KAIST, Daejeon, 305-701, Republic of Korea","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519.0,2016,,,927,931,"In clinical studies of breast cancer, masses appear as asymmetric densities between the left and the right breasts, which show different breast tissue structures. For classifying breast masses, most researchers have developed hand-crafted bilateral features by extracting the asymmetric information in 2-D mammograms. In digital breast tomosynthesis (DBT), which has 3D volume data, effective bilateral features are needed to detect masses. In this paper, we propose latent bilateral feature representation with 3-D multi-view deep convolutional neural network (DCNN) in the DBT reconstructed volume. The proposed DCNN is designed to discover hidden or latent bilateral feature representation of masses in self-taught learning. Experimental results show that the proposed latent bilateral feature representation outperforms conventional hand-crafted features by achieving a high area under the receiver operating characteristic curve.",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7471811,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471811,Digital breast tomosynthesis;bilateral analysis;deep learning;false positive reduction;latent features,Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Three-dimensional displays;Training,cancer;learning (artificial intelligence);mammography;medical signal processing,2D mammograms;3D multiview deep convolutional neural network;DBT;DCNN;asymmetric densities;breast cancer;breast tissue structures;digital breast tomosynthesis;hand-crafted bilateral features;latent bilateral feature representation;left breasts;receiver operating characteristic curve;right breasts;self-taught learning,,,,21.0,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
427,Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring,M. Kallenberg; K. Petersen; M. Nielsen; A. Y. Ng; P. Diao; C. Igel; C. M. Vachon; K. Holland; R. R. Winkel; N. Karssemeijer; M. Lillholm,"University of Copenhagen, Copenhagen, Denmark",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1322,1331,"Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.",0278-0062;02780062,,10.1109/TMI.2016.2532122,Danish Advanced Technology Foundation; European Seventh Framework Programme FP7; Innovation Fund Denmark; 10.13039/501100000780 - European Commission; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412749,Breast cancer;deep learning;mammograms;prognosis;risk factor;segmentation;unsupervised feature learning,Breast cancer;Computer architecture;Image segmentation;Machine learning;Mammography;Manuals,cancer;image classification;image segmentation;image texture;mammography;medical image processing;risk analysis;tumours;unsupervised learning,breast cancer risk;breast density segmentation;clinical datasets;handcrafted feature extraction;learned features;learned texture scores;mammographic risk scoring;mammographic texture;population sparsity;simple classifier;sparsity regularizer;unlabeled data;unsupervised deep learning,,7.0,,67.0,,,20160218.0,May 2016,,IEEE,IEEE Journals & Magazines
428,q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI Scans,V. Golkov; A. Dosovitskiy; J. I. Sperl; M. I. Menzel; M. Czisch; P. Sämann; T. Brox; D. Cremers,"The Department of Computer Science, Technical University of Munich, Garching, Germany",IEEE Transactions on Medical Imaging,20160429.0,2016,35,5.0,1344,1351,"Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.",0278-0062;02780062,,10.1109/TMI.2016.2551324,Deutsche Telekom Foundation; GE Global Research; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448418,Artificial neural networks;diffusion kurtosis imaging (DKI);diffusion magnetic resonance imaging (diffusion MRI);neurite orientation dispersion and density imaging (NODDI),Data processing;Diffusion tensor imaging;Fitting;Machine learning;Pipelines;Training,biodiffusion;biomedical MRI;data acquisition;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optimisation,accurate microstructural characterization;acquisition times;adults;artificial neural networks;children;classical data processing;clinical routine;data points;diffusion MRI data processing;diffusion kurtosis;diffusion magnetic resonance imaging;model-free diffusion MRI scans;neurite orientation dispersion;neuroimaging;noninvasive microstructure assessment method;q-space deep learning;scalar measures;single optimized step;suboptimal data processing pipelines;twelve-fold reduced scan time;twelve-fold shorter diffusion MRI scans,,4.0,,52.0,,,20160406.0,May 2016,,IEEE,IEEE Journals & Magazines
429,Multimodal fusion of brain structural and functional imaging with a deep neural machine translation approach,M. F. Amin; S. M. Plis; E. Damaraju; D. Hjelm; K. Cho; V. D. Calhoun,"The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA",2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI),20160428.0,2016,,,1,4,"In this work, we study a novel approach of deep neural machine translation to find linkage between multimodal brain imaging data, such as structural MRI (sMRI) and functional MRI (fMRI). The idea is to consider two different imaging views of the same brain like two different languages conveying some common concepts or facts. An important aspect of the translation model is an attention network module that learns alignment between features from fMRI and sMRI. We use independent component analysis (ICA) based features for the translation model. Our study shows significant group differences between healthy controls and patients with schizophrenia in the learned alignments. Furthermore, this novel approach reveals a group differential relation between a cognitive score (attention and vigilance) and alignments that could not be found when individual modality of data were considered.",,Electronic:978-1-4673-9919-7; POD:978-1-4673-9920-3; USB:978-1-4673-9918-0,10.1109/SSIAI.2016.7459160,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459160,,Brain modeling;Correlation;Feedforward neural networks;Loading;Magnetic resonance imaging,biomedical MRI;image fusion;independent component analysis;language translation;medical image processing;neural nets,ICA based features;attention network module;brain functional imaging;brain structural imaging;cognitive score;data modality;deep neural machine translation approach;fMRI;healthy patients;independent component analysis;magnetic resonance imaging;multimodal fusion;patients with schizophrenia;sMRI,,,,10.0,,,,6-8 March 2016,,IEEE,IEEE Conference Publications
430,Multi-modal learning-based pre-operative targeting in deep brain stimulation procedures,Y. Liu; B. M. Dawant,"Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN 37205 USA",2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI),20160421.0,2016,,,17,20,"Deep brain stimulation, as a primary surgical treatment for various neurological disorders, involves implanting electrodes to stimulate target nuclei within millimeter accuracy. Accurate pre-operative target selection is challenging due to the poor contrast in its surrounding region in MR images. In this paper, we present a learning-based method to automatically and rapidly localize the target using multi-modal images. A learning-based technique is applied first to spatially normalize the images in a common coordinate space. Given a point in this space, we extract a heterogeneous set of features that capture spatial and intensity contextual patterns at different scales in each image modality. Regression forests are used to learn a displacement vector of this point to the target. The target is predicted as a weighted aggregation of votes from various test samples, leading to a robust and accurate solution. We conduct five-fold cross validation using 100 subjects and compare our method to three indirect targeting methods, a state-of-the-art statistical atlas-based approach, and two variations of our method that use only a single modality image. With an overall error of 2.63±1.37mm, our method improves upon the single modality-based variations and statistically significantly outperforms the indirect targeting ones. Our technique matches state-of-the-art registration methods but operates on completely different principles. Both techniques can be used in tandem in processing pipelines operating on large databases or in the clinical flow for automated error detection.",,Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8,10.1109/BHI.2016.7455824,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455824,,Feature extraction;Imaging;Regression tree analysis;Robustness;Surgery;Training;Vegetation,brain;feature extraction;learning (artificial intelligence);medical image processing;patient treatment;regression analysis,deep brain stimulation procedures;feature extraction;multimodal learning-based preoperative targeting;regression forests;single modality image,,,,20.0,,,,24-27 Feb. 2016,,IEEE,IEEE Conference Publications
431,Combining deep convolutional networks and SVMs for mass detection on digital mammograms,I. Wichakam; P. Vateekul,"Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand",2016 8th International Conference on Knowledge and Smart Technology (KST),20160324.0,2016,,,239,244,"It is important to detect breast cancers as early as possible, which are commonly diagnosed as a mass region on mammograms. Deep Convolutional networks (ConvNets) have been specially designed for various computer vision tasks. In image classification, it contains many layers to automatically extract image features and employs the softmax function at the last layer to predict a probability. Although it excels in feature extraction, the classification is still limited. In this paper, we propose to apply SVMs into ConvNets to detect a mass on mammograms. To overcome the scarcity of training images, a data augmentation technique is employed to increase the sample data. To further enhance the accuracy, two recent techniques in ConvNets are applied including (i) rectified linear units and (ii) dropout. The experiment was conducted on the INbreast data set. The result showed that the proposed method achieved an accuracy at 98.44%, which is superior to the baseline (ConvNets) for 8%.",,Electronic:978-1-4673-8139-0; POD:978-1-4673-8140-6,10.1109/KST.2016.7440527,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440527,Breast Cancer Detection;Convolutional Networks (ConvNets);Deep Learning;Digital Mammograms;Image Classification;Support Vector Machines (SVMs),Breast cancer;Decision support systems;Feature extraction;Image classification;Kernel;Mammography;Support vector machines,feature extraction;feedforward neural nets;gynaecology;image classification;mammography;medical image processing;support vector machines,ConvNets;INbreast data set;SVM;breast cancers;computer vision tasks;deep convolutional networks;digital mammograms;image classification;image feature extraction;mass detection;rectified linear units;softmax function,,,,25.0,,,,3-6 Feb. 2016,,IEEE,IEEE Conference Publications
432,Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy,B. Jiang; X. Wang; J. Luo; X. Zhang; Y. Xiong; H. Pang,"Guangzhou Inst. of Biomed. & Health, Guangzhou, China","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)",20160215.0,2015,,,122,126,"The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.",,Electronic:978-1-4673-7723-2; POD:978-1-4673-7724-9,10.1109/IMCCC.2015.33,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405812,bright-field microscopy;convolutional neural networks;deep learning;machine learning;non-invasive;non-labelled;trans-differentiated neural progenitor cells,Biological neural networks;Electronic mail;Feature extraction;Image recognition;Machine learning;Microscopy;Morphology,cellular biophysics;image classification;medical image processing;neural nets;optical microscopy,NPC classification;automatic recognition;bright-field microscopic imaging analysis;convolutional neural networks;image pre-processing;nonNPC classification;trans-differentiated neural progenitor cells,,,,17.0,,,,18-20 Sept. 2015,,IEEE,IEEE Conference Publications
433,Integrated Optic Disc and Cup Segmentation with Deep Learning,G. Lim; Y. Cheng; W. Hsu; M. L. Lee,"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore",2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),20160107.0,2015,,,162,169,"Glaucoma is a widespread ocular disorder leading to irreversible loss of vision. Therefore, there is a pressing need for cost-effective screening, such that preventive measures can be taken. This can be achieved with an accurate segmentation of the optic disc and cup from retinal images to obtain the cup-to-disc ratio. We describe a comprehensive solution based on applying convolutional neural networks to feature exaggerated inputs emphasizing disc pallor without blood vessel obstruction, as well as the degree of vessel kinking. The produced raw probability maps then undergo a robust refinement procedure that takes into account prior knowledge about retinal structures. Analysis of these probability maps further allows us to obtain a confidence estimate on the correctness of the segmentation, which can be used to direct the most challenging cases for manual inspection. Tests on two large real-world databases, including the publicly-available MESSIDOR collection, demonstrate the effectiveness of our proposed system.",1082-3409;10823409,Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0,10.1109/ICTAI.2015.36,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372132,Glaucoma screening;optic cup segmentation;optic disc segmentation,Adaptive optics;Image segmentation;Neural networks;Optical computing;Optical imaging;Retina;Training,biomedical optical imaging;eye;feature extraction;image segmentation;learning (artificial intelligence);medical disorders;medical image processing;probability,convolutional neural networks;cost-effective screening;cup segmentation;cup-to-disc ratio;deep learning;disc pallor;feature-exaggerated inputs;glaucoma;integrated optic disc;ocular disorder;publicly-available MESSIDOR collection;raw probability maps;real-world databases;retinal images;retinal structures;robust refinement procedure;vision loss,,,,33.0,,,,9-11 Nov. 2015,,IEEE,IEEE Conference Publications
434,Automated Mass Detection in Mammograms Using Cascaded Deep Learning and Random Forests,N. Dhungel; G. Carneiro; A. P. Bradley,,2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA),20160107.0,2015,,,1,8,"Mass detection from mammograms plays a crucial role as a pre- processing stage for mass segmentation and classification. The detection of masses from mammograms is considered to be a challenging problem due to their large variation in shape, size, boundary and texture and also because of their low signal to noise ratio compared to the surrounding breast tissue. In this paper, we present a novel approach for detecting masses in mammograms using a cascade of deep learning and random forest classifiers. The first stage classifier consists of a multi-scale deep belief network that selects suspicious regions to be further processed by a two-level cascade of deep convolutional neural networks. The regions that survive this deep learning analysis are then processed by a two-level cascade of random forest classifiers that use morphological and texture features extracted from regions selected along the cascade. Finally, regions that survive the cascade of random forest classifiers are combined using connected component analysis to produce state-of-the-art results. We also show that the proposed cascade of deep learning and random forest classifiers are effective in the reduction of false positive regions, while maintaining a high true positive detection rate. We tested our mass detection system on two publicly available datasets: DDSM-BCRP and INbreast. The final mass detection produced by our approach achieves the best results on these publicly available datasets with a true positive rate of 0.96 ± 0.03 at 1.2 false positive per image on INbreast and true positive rate of 0.75 at 4.8 false positive per image on DDSM-BCRP.",,Electronic:978-1-4673-6795-0; POD:978-1-4673-6796-7,10.1109/DICTA.2015.7371234,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371234,,Breast cancer;Feature extraction;Image resolution;Machine learning;Mammography;Support vector machines,belief networks;biological tissues;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);mammography;medical image processing;neural nets;object detection;principal component analysis;trees (mathematics),DDSM-BCRP;INbreast;automated mass detection;boundary variation;breast tissue;cascaded deep learning;connected component analysis;deep convolutional neural network;false positive region reduction;mammogram;mass classification;mass segmentation;morphological feature extraction;multiscale deep belief network;random forest classifier;shape variation;signal to noise ratio;size variation;texture feature extraction;texture variation,,3.0,,37.0,,,,23-25 Nov. 2015,,IEEE,IEEE Conference Publications
435,Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images,J. Xu; L. Xiang; Q. Liu; H. Gilmore; J. Wu; J. Tang; A. Madabhushi,"Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China",IEEE Transactions on Medical Imaging,20160104.0,2016,35,1.0,119,130,"Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.",0278-0062;02780062,,10.1109/TMI.2015.2458702,; 10.13039/100000062 - National Institute of Diabetes and Digestive and Kidney Diseases; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353,Automated nuclei detection;breast cancer histopathology;deep learning;digital pathology;feature representation learning;stacked sparse autoencoder,Breast cancer;Decoding;Feature extraction;Image color analysis;Pathology;Training,biological tissues;cancer;image classification;image coding;image representation;image resolution;learning (artificial intelligence);medical image processing,Nottingham histologic score system;automated grading;automated nuclear detection;average area under Precision-Recall curve;breast cancer tissue specimens;computer assisted pathology related image analysis algorithms;deep learning strategy;high resolution digitized pathology images;high-level features;high-resolution breast cancer histopathological images;image classifier;image patch representation;nuclei detection;pixel intensity;sliding window operation;stacked sparse autoencoder,,18.0,,43.0,,,20150720.0,Jan. 2016,,IEEE,IEEE Journals & Magazines
436,Improving EEG feature learning via synchronized facial video,X. Li; X. Jia; G. Xun; A. Zhang,"Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.",2015 IEEE International Conference on Big Data (Big Data),20151228.0,2015,,,843,848,"Morden physiological analysis begins to involve more and more types of information. Electroencephalogram (EEG) signals as a typical example is starting to be analyzed with facial expressions videos to detect emotions. Emotions play an important role in the daily life of human beings, the need and importance of automatic emotion recognition has grown with increasing role of human computer interface applications. In this paper, we concentrate on recognition of the emotions jointly from ""inner"" and ""outer"" reactions, which are electroencephalogram (EEG) signals and facial expression video. Due to the streaming nature of this problem, the data volume and velocity is very challenging. We address these challenges from the theoretic perspective and propose a real time algorithm based on EEG signals and synchronized facial video to learn feature vector jointly. Our algorithm consists of an unsupervisedly EEG dictionary component based on deep learning theorem, and a probability pooling component transforms a continuous sequential signal into an EEG ""sentence"" which consists of a sequence of EEG words. The EEG sentence is then jointly learned with video features into a new fixed length feature representation for emotion classification. We overcome several computational challenges on the data based on the idea of convolution and pooling, and we conduct extensive evaluation for each component of our model. We also demonstrate the state-of-the-art classification result on real-world dataset. The superior performances on the emotion recognition task indicates that 1) the natural language scenario can be applied in EEG sequences and 2) borrowing video modality can increase the overall performance.",,Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9,10.1109/BigData.2015.7363831,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363831,Classification;Data stream;EEG;Feature Learning,Dictionaries;Electroencephalography;Emotion recognition;Physiology;Streaming media;Synchronization;Transforms,electroencephalography;emotion recognition;face recognition;medical image processing;probability;video signal processing,EEG dictionary component;EEG feature learning;automatic emotion recognition;deep learning theorem;electroencephalogram signal;emotion classification;emotion detection;facial expression video;feature vector;human computer interface;natural language;physiological analysis;probability pooling component;synchronized facial video;video modality,,,,10.0,,,,Oct. 29 2015-Nov. 1 2015,,IEEE,IEEE Conference Publications
437,Deep learning of tissue fate features in acute ischemic stroke,N. Stier; N. Vincent; D. Liebeskind; F. Scalzo,"Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,1316,1321,"In acute ischemic stroke treatment, prediction of tissue survival outcome plays a fundamental role in the clinical decision-making process, as it can be used to assess the balance of risk vs. possible benefit when considering endovascular clot-retrieval intervention. For the first time, we construct a deep learning model of tissue fate based on randomly sampled local patches from the hypoperfusion (Tmax) feature observed in MRI immediately after symptom onset. We evaluate the model with respect to the ground truth established by an expert neurologist four days after intervention. Experiments on 19 acute stroke patients evaluated the accuracy of the model in predicting tissue fate. Results show the superiority of the proposed regional learning framework versus a single-voxel-based regression model.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359869,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359869,,Biomedical imaging;Convolution;Predictive models,biological tissues;biomedical MRI;decision making;image fusion;learning (artificial intelligence);medical image processing;prediction theory,MRI;acute ischemic stroke treatment;clinical decision-making process;deep learning;endovascular clot-retrieval intervention;hypoperfusion feature;regional learning framework;tissue fate features;tissue fate prediction;tissue survival outcome prediction,,3.0,,32.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
438,Temporal Pattern and Association Discovery of Diagnosis Codes Using Deep Learning,S. Mehrabi; S. Sohn; D. Li; J. J. Pankratz; T. Therneau; J. L. S. Sauver; H. Liu; M. Palakal,"Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA",2015 International Conference on Healthcare Informatics,20151210.0,2015,,,408,416,"Longitudinal health records contain data on patients' visits, condition, treatment, and test results representing progression of their health status over time. In poorly understood patient populations, such data are particularly helpful in characterizing disease progression and early detection. In this work we developed a deep learning algorithm for temporal pattern discovery over Rochester Epidemiology Project data. We modeled each patient's records as a matrix of temporal clinical events with ICD9 and HCUP CSS diagnosis codes as rows and years of diagnosis as columns. Patients aged 18 or younger at the time of diagnosis were selected. A deep Boltzmann machine network with three hidden layers was constructed with each patient's diagnosis matrix values as visible nodes. The final weights of the network model were analyzed as the common features among patients' records.",,Electronic:978-1-4673-9548-9; POD:978-1-4673-9549-6,10.1109/ICHI.2015.58,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349719,Deep Learning;Rochester Epidemiology Project;Temporal Pattern Discovery,Cascading style sheets;Diseases;Machine learning;Medical diagnostic imaging;Sociology;Statistics,Boltzmann machines;data mining;electronic health records;learning (artificial intelligence);matrix algebra;patient diagnosis,HCUP CSS diagnosis;ICD9 diagnosis code;Rochester Epidemiology Project data;deep Boltzmann machine network;deep learning;deep learning algorithm;disease progression;longitudinal health record;patient diagnosis matrix value;temporal association discovery;temporal pattern discovery,,4.0,,45.0,,,,21-23 Oct. 2015,,IEEE,IEEE Conference Publications
439,Clinical deep brain stimulation region prediction using regression forests from high-field MRI,J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel,"Department of Electrical and Computer Engineering, Duke University, Durham, USA",2015 IEEE International Conference on Image Processing (ICIP),20151210.0,2015,,,2480,2484,"This paper presents a prediction framework of brain subcortical structures which are invisible on clinical low-field MRI, learning detailed information from ultrahigh-field MR training data. Volumetric segmentation of Deep Brain Stimulation (DBS) structures within the Basal ganglia is a prerequisite process for reliable DBS surgery. While ultrahigh-field MR imaging (7 Tesla) allows direct visualization of DBS targeting structures, such ultrahigh-fields are not always clinically available, and therefore the relevant structures need to be predicted from the clinical data. We address the shape prediction problem with a regression forest, non-linearly mapping predictors to target structures with high confidence, exploiting ultrahigh-field MR training data. We consider an application for the subthalamic nucleus (STN) prediction as a crucial DBS target. Experimental results on Parkinson's patients validate that the proposed approach enables reliable estimation of the STN from clinical 1.5T MRI.",,Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7,10.1109/ICIP.2015.7351248,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351248,Deep brain stimulation;regression forests;statistical shape models;ultrahigh-field MRI,Magnetic resonance imaging;Predictive models;Satellite broadcasting;Shape;Training;Training data;Vegetation,biomedical MRI;brain;image segmentation;medical image processing;neuromuscular stimulation;regression analysis;surgery,Basal ganglia;DBS surgery;Parkinson patients;STN prediction;brain subcortical structure prediction framework;clinical deep brain stimulation region prediction;clinical low-field MRI;deep brain stimulation structures;direct DBS targeting structure visualization;high-field MRI;nonlinearly mapping predictors;regression forests;shape prediction problem;subthalamic nucleus prediction;ultrahigh-field MR training data;ultrahighfield MR imaging;volumetric segmentation,,,1.0,21.0,,,,27-30 Sept. 2015,,IEEE,IEEE Conference Publications
440,Lung segmentation in chest radiographs using distance regularized level set and deep-structured learning and inference,T. A. Ngo; G. Carneiro,"Australia Centre for Visual Technologies, The University of Adelaide, Australia",2015 IEEE International Conference on Image Processing (ICIP),20151210.0,2015,,,2140,2143,"Computer-aided diagnosis of digital chest X-ray (CXR) images critically depends on the automated segmentation of the lungs, which is a challenging problem due to the presence of strong edges at the rib cage and clavicle, the lack of a consistent lung shape among different individuals, and the appearance of the lung apex. From recently published results in this area, hybrid methodologies based on a combination of different techniques (e.g., pixel classification and deformable models) are producing the most accurate lung segmentation results. In this paper, we propose a new methodology for lung segmentation in CXR using a hybrid method based on a combination of distance regularized level set and deep structured inference. This combination brings together the advantages of deep learning methods (robust training with few annotated samples and top-down segmentation with structured inference and learning) and level set methods (use of shape and appearance priors and efficient optimization techniques). Using the publicly available Japanese Society of Radiological Technology (JSRT) dataset, we show that our approach produces the most accurate lung segmentation results in the field. In particular, depending on the initialization used, our methodology produces an average accuracy on JSTR that varies from 94.8% to 98.5%.",,Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7,10.1109/ICIP.2015.7351179,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351179,Deep learning;Level set methods;Lung segmentation,Databases;Image segmentation;Level set;Lungs;Optimization;Shape;Training,diagnostic radiography;image segmentation;inference mechanisms;learning (artificial intelligence);lung;medical image processing;set theory,CXR images;JSRT dataset;Japanese Society of Radiological Technology dataset;automated lung segmentation;chest radiographs;clavicle;computer-aided diagnosis;deep learning methods;deep-structured inference;deep-structured learning;deformable models;digital chest X-ray images;distance regularized level set method;lung apex appearance;lung shape;optimization techniques;pixel classification;rib cage;top-down segmentation,,3.0,,14.0,,,,27-30 Sept. 2015,,IEEE,IEEE Conference Publications
441,Deep structured learning for mass segmentation from mammograms,N. Dhungel; G. Carneiro; A. P. Bradley,"School of Information Technology and Electrical Engineering, The University of Queensland",2015 IEEE International Conference on Image Processing (ICIP),20151210.0,2015,,,2950,2954,"In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to assess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computation of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.",,Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7,10.1109/ICIP.2015.7351343,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351343,Mammograms;mass segmentation;structured inference;structured learning,Breast cancer;Image segmentation;Indexes;Mammography;Training,image segmentation;learning (artificial intelligence);mammography;medical image processing;support vector machines,DDSM-BCRP;Dice index;INbreast;SSVM;breast mass segmentation;deep structured learning;image regions;mammograms;structured support vector machine,,1.0,,19.0,,,,27-30 Sept. 2015,,IEEE,IEEE Conference Publications
442,FingerNet: Deep learning-based robust finger joint detection from radiographs,S. Lee; M. Choi; H. s. Choi; M. S. Park; S. Yoon,"EECS, Seoul National University, Seoul, 151-744, Korea",2015 IEEE Biomedical Circuits and Systems Conference (BioCAS),20151207.0,2015,,,1,4,"Radiographic image assessment is the most common method used to measure physical maturity and diagnose growth disorders, hereditary diseases and rheumatoid arthritis, with hand radiography being one of the most frequently used techniques due to its simplicity and minimal exposure to radiation. Finger joints are considered as especially important factors in hand skeleton examination. Although several automation methods for finger joint detection have been proposed, low accuracy and reliability are hindering full-scale adoption into clinical fields. In this paper, we propose FingerNet, a novel approach for the detection of all finger joints from hand radiograph images based on convolutional neural networks, which requires little user intervention. The system achieved 98.02% average detection accuracy for 130 test data sets containing over 1,950 joints. Further analysis was performed to verify the system robustness against factors such as epiphysis and metaphysis in different age groups.",,Electronic:978-1-4799-7234-0; POD:978-1-4799-7235-7; USB:978-1-4799-7233-3,10.1109/BioCAS.2015.7348440,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348440,,Bones;Convolution;Joints;Merging;Radiography;Thumb,diagnostic radiography;learning (artificial intelligence);medical image processing;neural nets,FingerNet;convolutional neural networks;deep learning-based robust finger joint detection;epiphysis;metaphysis;radiograph images,,,,18.0,,,,22-24 Oct. 2015,,IEEE,IEEE Conference Publications
443,A deep architecture for visually analyze Pap cells,O. Chang; P. Constante; A. Gordon; M. Singania; F. Acuna,"Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150",2015 IEEE 2nd Colombian Conference on Automatic Control (CCAC),20151203.0,2015,,,1,6,"This work proposes a deep ANN architecture which accomplishes the reliable visual classification of abnormal Pap smear cell. The system is driven by independent agents where the first agent consists of a three layer ANN pretrained to closely track a reticle pattern. This net participates in a local close loop that oscillates and produces unique time-space versions of the visual data. This information is stabilized and sparsed in order to obtain compact data representations, with implicit space time content. The obtained representations are delivered to second level agents, formed by independent three layers ANNs dedicated to learning and recognition activities. To train the system a noise-balanced algorithm is employed, where the training set is composed by pap cells and white noise. This combination operating on finite databases and in a self controlled learning loop, auto develops enough cell recognition knowledge as to classify whole classes of Pap smear cells. The system has been tested in real time utilizing documented data bases.",,Electronic:978-1-4673-9305-8; POD:978-1-4673-9306-5; USB:978-1-4673-9304-1,10.1109/CCAC.2015.7345210,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345210,Pap cell recognition;deep architectures;deep learning;noise balanced learning,Artificial neural networks;Backpropagation;Computer architecture;Databases;Neurons;Tracking;Training,image classification;medical image processing;neural net architecture,abnormal Pap smear cell;deep ANN architecture;finite databases;noise-balanced algorithm;reticle pattern;visual classification,,,,16.0,,,,14-16 Oct. 2015,,IEEE,IEEE Conference Publications
444,Deep independence network analysis of structural brain imaging: A simulation study,E. Castro; D. Hjelm; S. Plis; L. Dinh; J. Turner; V. Calhoun,"The Mind Research Network, NM, USA",2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),20151112.0,2015,,,1,6,"The objective of this paper is to further validate theoretically and empirically a nonlinear independent component analysis (ICA) algorithm implemented with a deep learning architecture. We first revisited its formulation to verify its consistency with the criterion of minimization of mutual information. Then, we applied the nonlinear independent component estimation algorithm (NICE) to synthetic 2D images that resemble structural magnetic resonance imaging (sMRI) data. This data was generated by mixing spatial components that represent axial slices of sMRI tissue concentration images. Next, we generated the images under linear and mildly nonlinear mixtures, being able to show that NICE matches ICA when the data is generated by using the conventional linear mixture and outperforms ICA for the nonlinear mixture of components. The obtained results are promising and suggest that NICE has potential to find richer brain networks if applied to real sMRI data, provided that small conditioning adjustments are performed along with this approach.",1551-2541;15512541,Electronic:978-1-4673-7454-5; POD:978-1-4673-7455-2,10.1109/MLSP.2015.7324318,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324318,NICE;Nonlinear ICA;deep learning;simulation;structural MRI,Brain;Couplings;Imaging;Independent component analysis;Jacobian matrices;Machine learning;Mutual information,biological tissues;biomedical MRI;brain;independent component analysis;learning (artificial intelligence);medical image processing;minimisation;network analysis;neurophysiology,ICA;MRI data;MRI tissue concentration imaging;NICE matches;axial slices;brain networks;conventional linear mixture;deep independence network analysis;deep learning architecture;mildly nonlinear mixtures;minimization criterion;nonlinear independent component analysis;spatial components;structural brain imaging;structural magnetic resonance imaging data;synthetic 2D imaging,,1.0,,18.0,,,,17-20 Sept. 2015,,IEEE,IEEE Conference Publications
445,Synthetic structural magnetic resonance image generator improves deep learning prediction of schizophrenia,A. Ulloa; S. Plis; E. Erhardt; V. Calhoun,"Dept. of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA",2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),20151112.0,2015,,,1,6,"Despite the rapidly growing interest, progress in the study of relations between physiological abnormalities and mental disorders is hampered by complexity of the human brain and high costs of data collection. The complexity can be captured by deep learning approaches, but they still may require significant amounts of data. In this paper, we seek to mitigate the latter challenge by developing a generator for synthetic realistic training data. Our method greatly improves generalization in classification of schizophrenia patients and healthy controls from their structural magnetic resonance images. A feed forward neural network trained exclusively on continuously generated synthetic data produces the best area under the curve compared to classifiers trained on real data alone.",1551-2541;15512541,Electronic:978-1-4673-7454-5; POD:978-1-4673-7455-2,10.1109/MLSP.2015.7324379,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324379,,Biological neural networks;Generators;Machine learning;Magnetic resonance imaging;Neuroimaging;Probability density function;Training,biomedical MRI;brain;medical disorders,data collection;deep learning approaches;human brain;mental disorders;physiological abnormalities;schizophrenia patients;synthetic realistic training data;synthetic structural magnetic resonance image generator,,,,19.0,,,,17-20 Sept. 2015,,IEEE,IEEE Conference Publications
446,Investigating deep learning for fNIRS based BCI,J. Hennrich; C. Herff; D. Heger; T. Schultz,"Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,2844,2847,"Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318984,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318984,,Accuracy;Biological neural networks;Feature extraction;Standards;Training;Yttrium,biomedical optical imaging;brain-computer interfaces;electroencephalography;feature extraction;infrared spectroscopy;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;signal classification,EEG-based BCIs;brain activation pattern classification;brain activity measurement;brain-computer interfaces;conventional machine learning methods;deep learning;deep neural networks;fNIRS based BCI;functional near infrared spectroscopy;single trial analysis classifiers;single trial analysis features;young modality,,3.0,,23.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
447,Glaucoma detection based on deep convolutional neural network,X. Chen; Y. Xu; D. W. Kee Wong; T. Y. Wong; J. Liu,"Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,715,718,"Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318462,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318462,,Biomedical optical imaging;Diseases;Machine learning;Neural networks;Optical imaging;Prediction algorithms;Training,diseases;image representation;learning (artificial intelligence);medical image processing;neural nets;object detection,AUC;CNNs;DL architecture;ORIGA datasets;SCES datasets;area under curve;automated glaucoma diagnosis;data augmentation strategy;deep convolutional neural network;deep learning architecture;dropout augmentation strategy;glaucoma detection;glaucoma patterns;hierarchical image representation;irreversible eye disease;nonglaucoma patterns;receiver operating characteristic curve,,,,18.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
448,A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks,C. Wang; X. Yan; M. Smith; K. Kochhar; M. Rubin; S. M. Warren; J. Wrobel; H. Lee,"Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,2415,2418,"Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7318881,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318881,,Feature extraction;Image segmentation;Neural networks;Support vector machines;Training;Visualization;Wounds,biological tissues;feature extraction;image segmentation;medical image processing;neural nets;unsupervised learning,deep convolutional neural networks;deep learning method;handcrafted features;healing progress prediction;infection detection;large-scale wound database;long-term predictions;prognostic information;task-relevant visual features;tissue quality;tissue quantity;unified automatic wound segmentation framework;unsupervised approach;wound healing process;wound images;wound surface area measurements;wound treatment,,,,24.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
449,Deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images,D. Maji; A. Santara; S. Ghosh; D. Sheet; P. Mitra,"Indian Institute of Technology Kharagpur, WB 721302, India",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105.0,2015,,,3029,3032,"Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7319030,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319030,Computational imaging;deep learning;denoising auto-encoder;random forests;vessel detection,Biomedical imaging;Image analysis;Radio frequency;Retinal vessels;Vegetation,biomedical optical imaging;blood vessels;diseases;eye;image denoising;learning (artificial intelligence);medical image processing;vision defects,DNN response;DRIVE database;ROC curve;blood vessels;computational imaging framework;deep network;deep neural network;disease diagnosis;fundus color imaging;large-scale screening;learning based hybrid architecture;pathological damage;periodic screening;random forest hybrid architecture;retinal vessel detection;sparse trained denoising autoencoders;unsupervised learning;vision impairment,,1.0,,16.0,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
450,Detection of exudates in fundus photographs using convolutional neural networks,P. Prentašić; S. Lončarić,"University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Image Processing Group 10000, Croatia",2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA),20151026.0,2015,,,188,192,"Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.",1845-5921;18455921,Electronic:978-1-4673-8032-4; POD:978-1-4673-8033-1,10.1109/ISPA.2015.7306056,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306056,,Convolution;Diabetes;Neural networks;Optical imaging;Optical sensors;Retina;Retinopathy,biomedical optical imaging;colour photography;diseases;eye;image colour analysis;medical image processing;neural nets;object detection,color fundus photographs;convolutional neural networks;deep neural networks;diabetic retinopathy;early diagnosis;exudate detection;machine learning technique;preventable blindness,,,,31.0,,,,7-9 Sept. 2015,,IEEE,IEEE Conference Publications
451,Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning,X. Gao; S. Lin; T. Y. Wong,"Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore",IEEE Transactions on Biomedical Engineering,20151026.0,2015,62,11.0,2693,2701,"Goal: Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. Methods: Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade. Results: The proposed system is validated on a large population-based dataset of 5378 images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error (ε) of 0.304, a 70.7% exact integral agreement ratio (R<sub>0</sub>), an 88.4% decimal grading error ≤0.5 (R<sub>e0.5</sub>), and a 99.0% decimal grading error ≤1.0 (R<sub>e1.0</sub>). Significance: The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.",0018-9294;00189294,,10.1109/TBME.2015.2444389,"10.13039/501100001348 - Agency for Science, Technology and Research, Singapore; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122265,Automatic Feature Learning;Automatic feature learning;Cataract Grading;Deep Learning;cataract grading;deep learning,Feature extraction;Image color analysis;Lenses;Neural networks;Standards;Training;Visualization,biomedical optical imaging;diseases;eye;image denoising;image representation;learning (artificial intelligence);medical image processing;regression analysis;support vector machines;vision defects,automatic feature learning;clinical grading;clinical management;clouding;decimal grading error;deep learning;diagnosis monitoring;eye diseases;image patch clustering;integral agreement ratio;large-population screening;lens;local filters;mean absolute error;noisy representation;nuclear cataracts;population-based dataset;progression monitoring;slit-lamp imaging,"Adult;Algorithms;Cataract;Diagnostic Techniques, Ophthalmological;Humans;Lens, Crystalline;Neural Networks (Computer)",3.0,,38.0,,,20150611.0,Nov. 2015,,IEEE,IEEE Journals & Magazines
452,A Configurable Deep Network for high-dimensional clinical trial data,J. O' Donoghue; M. Roantree; M. Van Boxtel,"Insight Centre for Data Analytics, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland",2015 International Joint Conference on Neural Networks (IJCNN),20151001.0,2015,,,1,8,"Clinical studies provide interesting case studies for data mining researchers, given the often high degree of dimensionality and long term nature of these studies. In areas such as dementia, accurate predictions from data scientists provide vital input into the understanding of how certain features (representing lifestyle) can predict outcomes such as dementia. Most research involved has used traditional or shallow data mining approaches which have been shown to offer varying degrees of accuracy in datasets with high dimensionality. In this research, we explore the use of deep learning architectures, as they have been shown to have high predictive capabilities in image and audio datasets. The purpose of our research is to build a framework which allows easy reconfiguration for the performance of experiments across a number of deep learning approaches. In this paper, we present our framework for a configurable deep learning machine and our evaluation and analysis of two shallow approaches: regression and multi-layer perceptron, as a platform to a deep belief network, and using a dataset created over the course of 12 years by researchers in the area of dementia.",2161-4393;21614393,Electronic:978-1-4799-1960-4; POD:978-1-4799-1961-1,10.1109/IJCNN.2015.7280841,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280841,,Dementia,belief networks;data mining;learning (artificial intelligence);medical disorders;medical information systems;multilayer perceptrons,audio dataset;clinical study;configurable deep learning machine;configurable deep network;data mining researcher;data scientist;deep belief network;deep learning architecture;dementia;easy reconfiguration;high-dimensional clinical trial data;image dataset;multilayer perceptron;predictive capability;regression perception,,1.0,,27.0,,,,12-17 July 2015,,IEEE,IEEE Conference Publications
453,Multi-Layer and Recursive Neural Networks for Metagenomic Classification,G. Ditzler; R. Polikar; G. Rosen,"Department of Electrical & Computer Engineering, Drexel University, Philadelphia",IEEE Transactions on NanoBioscience,20150828.0,2015,14,6.0,608,616,"Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.",1536-1241;15361241,,10.1109/TNB.2015.2461219,Department of Energy; 10.13039/100000001 - National Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432,Comparative metagenomics;metagenomics;microbiome;neural networks,Feature extraction;Machine learning;Nanobioscience;Neural networks;Organisms;Training;Vegetation,DNA;genomics;image classification;image representation;learning (artificial intelligence);medical image processing;molecular biophysics;multilayer perceptrons;natural language processing;neurophysiology,classification accuracy;data structure;deep belief network;deep learning methods;generalization performance;hierarchical representations;image classification;language modeling;machine learning;machine learning community;metagenomic classification;metagenomic data analysis;metagenomic literature;multilayer perceptron;multilayer-recursive neural networks;natural language processing;neural networks;nonlinear feature representations;prediction algorithm;predictive metagenomic analysis;unsupervised fashion,Algorithms;Metagenomics;Microbiota;Neural Networks (Computer),4.0,,44.0,,,20150824.0,Sept. 2015,,IEEE,IEEE Journals & Magazines
454,Deep learninig of EEG signals for emotion recognition,Y. Gao; H. J. Lee; R. M. Mehmood,"Division of Computer Science and Engineering, Chonbuk National University, Jeonju 561-756, Korea",2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),20150730.0,2015,,,1,5,"Emotion recognition is an important task for computer to understand the human status in brain computer interface (BCI) systems. It is difficult to perceive the emotion of some disabled people through their facial expression, such as functional autism patient. EEG signal provides us a non-invasive way to recognize the emotion of these disable people through EEG headset electrodes placed on their scalp. In this paper, we propose a deep learning algorithm to simultaneously learn the features and classify the emotions of EEG signals. It differs from the conventional methods as we apply deep learning on the raw signal without explicit hand-crafted feature extraction. Because the EEG signal has subject dependency, it is better to train the emotion model subject-wise, while there is not much epochs available for each subject. Deep learning algorithm provides a solution with a pre-training way using three layers of restricted Boltzmann machines (RBMs). Thus, we can use epochs of all subjects to pre-training the deep network, and use back-propagation to fine tuning the network subject by subject. Experiment results show that our proposed framework achieves better recognition accuracy than conventional algorithms.",,Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3,10.1109/ICMEW.2015.7169796,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169796,Deep learning;EEG;Emotion Recognition;RBM,Artificial neural networks;Biomedical imaging;Brain modeling;Electroencephalography;Feature extraction;Image recognition;Support vector machines,Boltzmann machines;behavioural sciences computing;electrodes;electroencephalography;emotion recognition;handicapped aids;learning (artificial intelligence);medical signal processing,BCI;EEG headset electrodes;EEG signals;RBM;backpropagation;brain computer interface systems;deep learning algorithm;deep network;disabled people;emotion model;emotion recognition;facial expression;functional autism patient;human status;restricted Boltzmann machines;subject dependency,,1.0,,21.0,,,,June 29 2015-July 3 2015,,IEEE,IEEE Conference Publications
455,Clinical subthalamic nucleus prediction from high-field brain MRI,J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel,"Department of Electrical and Computer Engineering, Duke University, Durham, USA",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,1264,1267,"The subthalamic nucleus (STN) within the sub-cortical region of the Basal ganglia is a crucial targeting structure for Parkinson's Deep brain stimulation (DBS) surgery. Volumetric segmentation of such small and complex structure, which is elusive in clinical MRI protocols, is thereby a pre-requisite process for reliable DBS direct targeting. While direct visualization of the STN is facilitated with advanced ultrahigh-field MR imaging (7 Tesla), such high fields are not always clinically available. In this paper, we aim at the automatic prediction of the STN region on clinical low-field MRI, exploiting dependencies between the STN and its adjacent structures, learned from ultrahigh-field MRI. We present a framework based on a statistical shape model to learn such shape relationship on high quality MR data sets. This allows for an accurate prediction and visualization of the STN structure, given detectable predictors on the low-field MRI. Experimental results on Parkinson's patients demonstrate that the proposed approach enables accurate estimation of the STN on clinical 1.5T MRI.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7164104,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164104,Deep brain stimulation;high-field MRI;statistical shape models;subthalamic nucleus,Image segmentation;Magnetic resonance imaging;Predictive models;Satellite broadcasting;Shape;Training,biomedical MRI;brain;diseases;image segmentation;medical image processing;physiological models;statistical analysis;surgery,DBS direct targeting;Parkinson's Deep brain stimulation surgery;STN structure prediction;STN structure visualization;adjacent structures;basal ganglia;clinical MRI protocols;clinical low-field MRI;clinical subthalamic nucleus prediction;direct visualization;high quality MR data;high-field brain MRI;magnetic flux density 1.5 T;magnetic flux density 7 T;shape relationship;statistical shape model;subcortical region;ultrahigh-field MR imaging;ultrahigh-field MRI;volumetric segmentation,,0.0,,20.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
456,Tree RE-weighted belief propagation using deep learning potentials for mass segmentation from mammograms,N. Dhungel; G. Carneiro; A. P. Bradley,"ACVT, School of Computer Science, The University of Adelaide",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,760,763,"In this paper, we propose a new method for the segmentation of breast masses from mammograms using a conditional random field (CRF) model that combines several types of potential functions, including one that classifies image regions using deep learning. The inference method used in this model is the tree re-weighted (TRW) belief propagation, which allows a learning mechanism that directly minimizes the mass segmentation error and an inference approach that produces an optimal result under the approximations of the TRW formulation. We show that the use of these inference and learning mechanisms and the deep learning potential functions provides gains in terms of accuracy and efficiency in comparison with the current state of the art using the publicly available datasets INbreast and DDSM-BCRP.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163983,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163983,Deep learning;Gaussian Mixture model;Mammograms;mass segmentation;tree re-weighted belief propagation,Belief propagation;Image segmentation;Machine learning;Mammography;Shape;Solid modeling;Training,belief networks;cancer;image segmentation;inference mechanisms;learning (artificial intelligence);mammography;medical image processing;minimisation;tumours,CRF model;DDSM-BCRP dataset;INbreast dataset;TRW formulation;breast masses;conditional random field;deep learning potentials;error minimization;inference approach;mammograms;mass segmentation;tree re-weighted belief propagation,,1.0,,20.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
457,Generation of synthetic structural magnetic resonance images for deep learning pre-training,E. Castro; A. Ulloa; S. M. Plis; J. A. Turner; V. D. Calhoun,"The Mind Research Network, Albuquerque, NM",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,1057,1060,"Deep learning methods have significantly improved classification accuracy in different areas such as speech, object and text recognition. However, this field has only began to be explored in the brain imaging field, which differs from other fields in terms of the amount of data available, its data dimensionality and other factors. This paper proposes a methodology to generate an extensive synthetic structural magnetic resonance imaging (sMRI) dataset to be used at the pre-training stage of a shallow network model to address the issue of having a limited amount of data available. Our results show that by extending our dataset using 5,000 synthetic sMRI volumes for pretraining, which accounts to approximately 10 times the size of the original dataset, we can obtain a 5% average improvement on classification results compared to the regular approach on a schizophrenia dataset. While the use of synthetic sMRI data for pre-training has only been tested on a shallow network, this can be readily applied to deeper networks.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7164053,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164053,deep learning;pretraining;schizophrenia;simulation;structural MRI,Data models;Machine learning;Magnetic resonance imaging;Neuroimaging;Probability density function;Support vector machines;Training,biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing,brain imaging field;classification accuracy;deep learning pretraining method;deeper networks;magnetic resonance imaging;object recognition;schizophrenia dataset;shallow network model;speech recognition;structural MRI;synthetic sMR image;synthetic sMRI volume;text recognition,,0.0,,15.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
458,Automatic detection of cerebral microbleeds via deep learning based 3D feature representation,H. Chen; L. Yu; Q. Dou; L. Shi; V. C. T. Mok; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,764,767,"Clinical identification and rating of the cerebral microbleeds (CMBs) are important in vascular diseases and dementia diagnosis. However, manual labeling is time-consuming with low reproducibility. In this paper, we present an automatic method via deep learning based 3D feature representation, which solves this detection problem with three steps: candidates localization with high sensitivity, feature representation, and precise classification for reducing false positives. Different from previous methods by exploiting low-level features, e.g., shape features and intensity values, we utilize the deep learning based high-level feature representation. Experimental results validate the efficacy of our approach, which outperforms other methods by a large margin with a high sensitivity while significantly reducing false positives per subject.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163984,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163984,cerebral microbleeds;deep learning;feature representation;object detection,Biomedical imaging;Feature extraction;Machine learning;Radio frequency;Sensitivity;Three-dimensional displays;Training,biomedical MRI;blood vessels;brain;diseases;image representation;learning (artificial intelligence);medical image processing;object detection,CMB;automatic detection;candidate localization;cerebral microbleed rating;clinical identification;deep learning based 3D feature representation;deep learning based high-level feature representation;dementia diagnosis;detection problem;false positive reduction;intensity values;low-level features;manual labeling;precise classification;shape features;vascular diseases,,2.0,,13.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
459,Using deep learning for robustness to parapapillary atrophy in optic disc segmentation,R. Srivastava; J. Cheng; D. W. K. Wong; J. Liu,"Institute for Infocomm Research, Singapore 138632",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,768,771,"Optic Disc (OD) segmentation from retinal fundus images is important for many applications such as detecting other optic structures and early detection of glaucoma. One of the major problems in segmenting OD is the presence of Para-papillary Atrophy (PPA) which in many cases looks similar to the OD. Researchers have used many different features to distinguish between PPA and OD, however each of these features has some limitation or the other. In this paper, we propose to use a deep neural network for OD segmentation which can learn features to distinguish PPA from OD. Using simple image intensity based features, the proposed method has the least mean overlapping error (9.7%) among the state-of-the-art works for OD segmentation in fundus images with PPA.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163985,Optic Disc segmentation;deep learning;parapapillary atrophy;retinal fundus images,Adaptive optics;Atrophy;Feature extraction;Image segmentation;Optical imaging;Retina;Training,eye;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology,deep learning;deep neural network;glaucoma detection;image intensity based features;least mean overlapping error;optic disc segmentation;optic structures;parapapillary atrophy;retinal fundus images,,1.0,,10.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
460,Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology,D. Sheet; S. P. K. Karri; A. Katouzian; N. Navab; A. K. Ray; J. Chatterjee,"Department of Electrical Engineering, Indian Institute of Technology Kharagpur, India",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723.0,2015,,,777,780,"Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163987,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163987,Representation learning;cutaneous wounds;denoising autoencoders;in situ histology;optical coherence tomography;tissue characterization,Adaptive optics;Biomedical optical imaging;Machine learning;Optical imaging;Skin;Speckle;Wounds,biomedical optical imaging;image coding;image denoising;medical image processing;neural nets;optical tomography;skin;speckle;tissue engineering;unsupervised learning;wounds,OCT;choroid;clinical reporting variability;coherent sensing;conventional histology;coronary vessels;critical organs;deep learning;deep neural network-based architecture;denoising autoencoders;healing wounds;histologically characterizing healthy skin;in situ histology;interobserver reporting variability challenges;intraobserver reporting variability challenges;laser source incident;leveraged rapid clinical translation;live tissues;noninvasive high-resolution in situ imaging;optical coherence tomography;photon diffraction;retina;speckle image formation;speckle representations;stochastic nature;swept-source OCT;tissue specific speckle appearance;tissue specific speckle representations;unsupervised learning;wounded mice skin,,0.0,,11.0,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
461,Disease Inference from Health-Related Questions via Sparse Deep Learning,L. Nie; M. Wang; L. Zhang; S. Yan; B. Zhang; T. S. Chua,"School of Computing, National University of Singapore, Singapore",IEEE Transactions on Knowledge and Data Engineering,20150706.0,2015,27,8.0,2107,2119,"Automatic disease inference is of importance to bridge the gap between what online health seekers with unusual symptoms need and what busy human doctors with biased expertise can offer. However, accurately and efficiently inferring diseases is non-trivial, especially for community-based health services due to the vocabulary gap, incomplete information, correlated medical concepts, and limited high quality training samples. In this paper, we first report a user study on the information needs of health seekers in terms of questions and then select those that ask for possible diseases of their manifested symptoms for further analytic. We next propose a novel deep learning scheme to infer the possible diseases given the questions of health seekers. The proposed scheme is comprised of two key components. The first globally mines the discriminant medical signatures from raw features. The second deems the raw features and their signatures as input nodes in one layer and hidden nodes in the subsequent layer, respectively. Meanwhile, it learns the inter-relations between these two layers via pre-training with pseudo-labeled data. Following that, the hidden nodes serve as raw features for the more abstract signature mining. With incremental and alternative repeating of these two components, our scheme builds a sparsely connected deep architecture with three hidden layers. Overall, it well fits specific tasks with fine-tuning. Extensive experiments on a real-world dataset labeled by online doctors show the significant performance gains of our scheme.",1041-4347;10414347,,10.1109/TKDE.2015.2399298,NUS-Tsinghua Extreme Search Project; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029673,Community-based Health Services;Community-based health services;Deep Learning;Disease Inference;Question Answering;deep learning;disease inference;question answering,Cancer;Diseases;Educational institutions;Medical diagnostic imaging;Training,data mining;diseases;health care;inference mechanisms;information needs;learning (artificial intelligence);medical information systems,abstract signature mining;automatic disease inference;community-based health services;discriminant medical signatures;health-related questions;information needs;online health seekers;pseudolabeled data;sparse deep learning;sparsely connected deep architecture,,23.0,,51.0,,,20150203.0,Aug. 1 2015,,IEEE,IEEE Journals & Magazines
462,Self-supervised learning model for skin cancer diagnosis,A. Masood; A. Al- Jumaily; K. Anam,"University of Technology Sydney, P.O. Box 123 Broadway, NSW 2007 Australia",2015 7th International IEEE/EMBS Conference on Neural Engineering (NER),20150702.0,2015,,,1012,1015,"Automated diagnosis of skin cancer is an active area of research with different classification methods proposed so far. However, classification models based on insufficient labeled training data can badly influence the diagnosis process if there is no self-advising and semi supervising capability in the model. This paper presents a semi supervised, self-advised learning model for automated recognition of melanoma using dermoscopic images. Deep belief architecture is constructed using labeled data together with unlabeled data, and fine tuning done by an exponential loss function in order to maximize separation of labeled data. In parallel a self-advised SVM algorithm is used to enhance classification results by counteracting the effect of misclassified data. To increase generalization capability and redundancy of the model, polynomial and radial basis function based SA-SVMs and Deep network are trained using training samples randomly chosen via a bootstrap technique. Then the results are aggregated using least square estimation weighting. The proposed model is tested on a collection of 100 dermoscopic images. The variation in classification error is analyzed with respect to the ratio of labeled and unlabeled data used in the training phase. The classification performance is compared with some popular classification methods and the proposed model using the deep neural processing outperforms most of the popular techniques including KNN, ANN, SVM and semi supervised algorithms like Expectation maximization and transductive SVM.",1948-3546;19483546,Electronic:978-1-4673-6389-1; POD:978-1-4673-6387-7,10.1109/NER.2015.7146798,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146798,,Lesions;Malignant tumors;Skin cancer;Support vector machines;Training;Training data,cancer;image classification;image segmentation;learning (artificial intelligence);least squares approximations;medical image processing;skin;support vector machines,ANN;KNN;SA-SVM;automated recognition;bootstrap technique;classification error;classification models;classification performance;deep belief architecture;deep neural processing outperforms;dermoscopic imaging;expectation maximization;exponential loss function;least square estimation weighting;melanoma;misclassified data effect;polynomial basis function;radial basis function;self-advised SVM algorithm;self-supervised learning model;semisupervised algorithms;skin cancer diagnosis;training data;transductive SVM,,1.0,,29.0,,,,22-24 April 2015,,IEEE,IEEE Conference Publications
463,Classification on ADHD with Deep Learning,D. Kuang; L. He,"Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China",2014 International Conference on Cloud Computing and Big Data,20150319.0,2014,,,27,32,"Effective discrimination of attention deficit hyperactivity disorder (ADHD) using imaging and functional biomarkers would have fundamental influence on public health. In usual, the discrimination is based on the standards of American Psychiatric Association. In this paper, we modified one of the deep learning method on structure and parameters according to the properties of ADHD data, to discriminate ADHD on the unique public dataset of ADHD-200. We predicted the subjects as control, combined, inattentive or hyperactive through their frequency features. The results achieved improvement greatly compared to the performance released by the competition. Besides, the imbalance in datasets of deep learning model influenced the results of classification. As far as we know, it is the first time that the deep learning method has been used for the discrimination of ADHD with fMRI data.",,Electronic:978-1-4799-6621-9; POD:978-1-4799-6622-6,10.1109/CCBD.2014.42,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062868,ADHD;Deep Belief Network;Deep Learning;fMRI,Accuracy;Brain modeling;Data models;Feature extraction;Learning systems;Magnetic resonance;Training,learning (artificial intelligence);medical computing;medical disorders;pattern classification,ADHD discrimination;ADHD-200;American Psychiatric Association;attention deficit hyperactivity disorder;classification;datasets imbalance;deep learning method;fMRI data;frequency features;functional biomarkers;imaging;public health,,1.0,,19.0,,,,12-14 Nov. 2014,,IEEE,IEEE Conference Publications
464,Deep learning for brain decoding,O. Firat; L. Oztekin; F. T. Y. Vural,"Department of Computer Engineering, Middle East Technical University, Ankara, Turkey",2014 IEEE International Conference on Image Processing (ICIP),20150129.0,2014,,,2784,2788,"Learning low dimensional embedding spaces (manifolds) for efficient feature representation is crucial for complex and high dimensional input spaces. Functional magnetic resonance imaging (fMRI) produces high dimensional input data and with a less then ideal number of labeled samples for a classification task. In this study, we explore deep learning methods for fMRI classification tasks in order to reduce dimensions of feature space, along with improving classification performance for brain decoding. We employ sparse autoencoders for unsupervised feature learning, leveraging unlabeled fMRI data to learn efficient, non-linear representations as the building blocks of a deep learning architecture by stacking them. Proposed method is tested on a memory encoding/retrieval experiment with ten classes. The results support the efficiency compared to the baseline multi-voxel pattern analysis techniques.",1522-4880;15224880,Electronic:978-1-4799-5751-4; POD:978-1-4799-5752-1,10.1109/ICIP.2014.7025563,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025563,Deep Learning;MVPA;Stacked Autoencoders;brain state decoding;fMRI,Computer architecture;Decoding;Encoding;Feature extraction;Magnetic resonance imaging;Manifolds;Pattern analysis,biomedical MRI;brain;decoding;feature extraction;image classification;medical image processing;neurophysiology;unsupervised learning,baseline multi-voxel pattern analysis techniques;brain decoding;complex input spaces;deep learning architecture;deep learning methods;efficient feature representation;fMRI classification tasks;feature space dimension;functional magnetic resonance imaging;high dimensional input data;high dimensional input spaces;low dimensional embedding spaces;manifolds;memory encoding;memory retrieval;nonlinear representations;sample classification;sparse autoencoders;unlabeled fMRI data;unsupervised feature learning,,0.0,,26.0,,,,27-30 Oct. 2014,,IEEE,IEEE Conference Publications
465,Experimental Study of Unsupervised Feature Learning for HEp-2 Cell Images Clustering,Y. Zhao; Z. Gao; L. Wang; L. Zhou,"Univ. of Wollongong, Wollongong, NSW, Australia",2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA),20150115.0,2014,,,1,8,"Automatic identification of HEp-2 cell images has received an increasing research attention. Feature representations play a critical role in achieving good identification performance. Much recent work has focused on supervised feature learning. Typical methods consist of BoW model (based on hand-crafted features) and deep learning model (learning hierarchical features). However, these labels used in supervised feature learning are very labour-intensive and time-consuming. They are commonly manually annotated by specialists and very expensive to obtain. In this paper, we follow this fact and focus on unsupervised feature learning. We have verified and compared the features of these two typical models by clustering. Experimental results show the BoW model generally perform better than deep learning models. Also, we illustrate BoW model and deep learning models have complementarity properties.",,Electronic:978-1-4799-5409-4; POD:978-1-4799-5410-0,10.1109/DICTA.2014.7008108,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008108,,Decoding;Feature extraction;Image coding;Neural networks;Noise reduction;Training;Vectors,feature extraction;image representation;medical image processing;unsupervised learning,BoW model;HEp-2 cell image clustering;deep learning model;feature representation;unsupervised feature learning,,0.0,,18.0,,,,25-27 Nov. 2014,,IEEE,IEEE Conference Publications
466,Deep learning for healthcare decision making with EMRs,Z. Liang; G. Zhang; J. X. Huang; Q. V. Hu,"School of Information Technology, York University, Toronto, ON, M3J1P3, Canada",2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20150115.0,2014,,,556,559,"Computer aid technology is widely applied in decision-making and outcome assessment of healthcare delivery, in which modeling knowledge and expert experience is technically important. However, the conventional rule-based models are incapable of capturing the underlying knowledge because they are incapable of simulating the complexity of human brains and highly rely on feature representation of problem domains. Thus we attempt to apply a deep model to overcome this weakness. The deep model can simulate the thinking procedure of human and combine feature representation and learning in a unified model. A modified version of convolutional deep belief networks is used as an effective training method for large-scale data sets. Then it is tested by two instances: a dataset on hypertension retrieved from a HIS system, and a dataset on Chinese medical diagnosis and treatment prescription from a manual converted electronic medical record (EMR) database. The experimental results indicate that the proposed deep model is able to reveal previously unknown concepts and performs much better than the conventional shallow models.",,Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8,10.1109/BIBM.2014.6999219,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999219,deep belief network;deep learning;restricted Boltzmann machine;syndrome classification;unsupervised feature learning,Brain modeling;Data models;Hypertension;Medical diagnostic imaging;Support vector machines;Training,belief networks;brain;brain-computer interfaces;decision making;electronic health records;health care;patient diagnosis;patient treatment;unsupervised learning,Chinese medical diagnosis;Chinese medical treatment prescription;EMR database;HIS system;belief networks;computer aid technology;decision making;deep learning;electronic medical record database;feature representation;healthcare;human brains;shallow models;training method,,3.0,,18.0,,,,2-5 Nov. 2014,,IEEE,IEEE Conference Publications
467,EEG-based emotion classification using deep belief networks,W. L. Zheng; J. Y. Zhu; Y. Peng; B. L. Lu,"Department of Computer Science and Engineering, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng., Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai, China",2014 IEEE International Conference on Multimedia and Expo (ICME),20140908.0,2014,,,1,6,"In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.",1945-7871;19457871,Electronic:978-1-4799-4761-4; POD:978-1-4799-4760-7,10.1109/ICME.2014.6890166,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890166,Affective Computing;Deep Belief Network;EEG;Emotion Classification,Accuracy;Brain models;Electroencephalography;Feature extraction;Hidden Markov models;Support vector machines,electroencephalography;emotion recognition;entropy;feature extraction;hidden Markov models;learning (artificial intelligence);medical signal processing;support vector machines,DBN-HMM model;EEG data;EEG-based emotion classification;GELM;Graph regularized Extreme Learning Machine;KNN;SVM;advanced deep learning model;deep architectures;deep belief networks;deep model;differential entropy feature extraction;emotional categories;emotional stage switching;hidden Markov model;image;multichannel EEG;speech;unsupervised feature learning,,12.0,,9.0,,,,14-18 July 2014,,IEEE,IEEE Conference Publications
468,Convolutional deep belief networks for feature extraction of EEG signal,Y. Ren; Y. Wu,"Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China",2014 International Joint Conference on Neural Networks (IJCNN),20140904.0,2014,,,2850,2853,"In recent years, deep learning approaches have been successfully used to learn hierarchical representations of image data, audio data etc. However, to our knowledge, these deep learning approaches have not been extensively studied for electroencephalographic (EEG) data. Considering the properties of EEG data, high-dimensional and multichannel, we applied convolutional deep belief networks to the feature learning of EEG data and evaluated it on the datasets from previous BCI competitions. Compared with other state-of-the-art feature extraction methods, the learned features using convolutional deep belief network have better performance.",2161-4393;21614393,CD-ROM:978-1-4799-6627-1; Electronic:978-1-4799-1484-5; POD:978-1-4799-1482-1,10.1109/IJCNN.2014.6889383,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6889383,EEG;convolutional deep belief networks;deep learning;feature learning,Accuracy;Convolution;Convolutional codes;Electroencephalography;Feature extraction;Probabilistic logic;Training,belief networks;convolution;data analysis;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing,BCI competitions;EEG signal;audio data;convolutional deep belief networks;datasets;deep learning approaches;electroencephalographic data;feature extraction methods;hierarchical representations;high-dimensional data;image data;multichannel data,,7.0,,14.0,,,,6-11 July 2014,,IEEE,IEEE Conference Publications
469,Early diagnosis of Alzheimer's disease with deep learning,S. Liu; S. Liu; W. Cai; S. Pujol; R. Kikinis; D. Feng,"BMIT Research Group, School of IT, University of Sydney, Australia",2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI),20140731.0,2014,,,1015,1018,"The accurate diagnosis of Alzheimer's disease (AD) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped. Although many studies have applied machine learning methods for computer-aided-diagnosis (CAD) of AD recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models. In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of AD and its prodromal stage, Mild Cognitive Impairment (MCI). Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge. A significant performance gain on classification of all diagnosis groups was achieved in our experiments.",1945-7928;19457928,Electronic:978-1-4673-1961-4; POD:978-1-4673-1960-7; USB:978-1-4673-1959-1,10.1109/ISBI.2014.6868045,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868045,Alzheimer's disease;classification;neuroimaging,Alzheimer's disease;Feature extraction;Magnetic resonance imaging;Neurons;Support vector machines;Training,biomedical MRI;brain;cognition;diseases;health care;image classification;image coding;learning (artificial intelligence);medical disorders;medical image processing,Alzheimer disease diagnosis;computer-aided-diagnosis;irreversible brain damages;machine learning methods;magnetic resonance imaging;mild cognitive impairment;minimal domain prior knowledge;patient care;stacked auto-encoders,,13.0,,30.0,,,,April 29 2014-May 2 2014,,IEEE,IEEE Conference Publications
470,Stacked Sparse Autoencoder (SSAE) based framework for nuclei patch classification on breast cancer histopathology,J. Xu; L. Xiang; R. Hang; J. Wu,"Nanjing University of Information Science and Technology, Nanjing 210044, China",2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI),20140731.0,2014,,,999,1002,"In this paper, a Stacked Sparse Autoencoder (SSAE) based framework is presented for nuclei classification on breast cancer histopathology. SSAE works very well in learning useful high-level feature for better representation of input raw data. To show the effectiveness of proposed framework, SSAE+Softmax is compared with conventional Softmax classifier, PCA+Softmax, and single layer Sparse Autoencoder (SAE)+Softmax in classifying the nuclei and non-nuclei patches extracted from breast cancer histopathology. The SSAE+Softmax for nuclei patch classification yields an accuracy of 83.7%, F1 score of 82%, and AUC of 0.8992, which outperform Softmax classifier, PCA+Softmax, and SAE+Softmax.",1945-7928;19457928,Electronic:978-1-4673-1961-4; POD:978-1-4673-1960-7; USB:978-1-4673-1959-1,10.1109/ISBI.2014.6868041,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868041,Breast Cancer Histopathology;Deep learning;Sparse Autoencoder,Breast cancer;Decoding;Neural networks;Principal component analysis;Testing;Training,biological organs;cancer;cellular biophysics;feature extraction;image classification;image coding;medical image processing,SSAE based framework;breast cancer histopathology;high-level feature extraction;nuclei patch classification;stacked sparse autoencoder,,4.0,,6.0,,,,April 29 2014-May 2 2014,,IEEE,IEEE Conference Publications
471,Generative Adversarial Learning for Reducing Manual Annotation in Semantic Segmentation on Large Scale Miscroscopy Images: Automated Vessel Segmentation in Retinal Fundus Image as Test Case,A. Lahiri; K. Ayush; P. K. Biswas; P. Mitra,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824.0,2017,,,794,800,"Convolutional Neural Network(CNN) based semantic segmentation require extensive pixel level manual annotation which is daunting for large microscopic images. The paper is aimed towards mitigating this labeling effort by leveraging the recent concept of generative adversarial network(GAN) wherein a generator maps latent noise space to realistic images while a discriminator differentiates between samples drawn from database and generator. We extend this concept to a multi task learning wherein a discriminator-classifier network differentiates between fake/real examples and also assigns correct class labels. Though our concept is generic, we applied it for the challenging task of vessel segmentation in fundus images. We show that proposed method is more data efficient than a CNN. Specifically, with 150K, 30K and 15K training examples, proposed method achieves mean AUC of 0.962, 0.945 and 0.931 respectively, whereas the simple CNN achieves AUC of 0.960, 0.921 and 0.916 respectively.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.110,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014844,,Biomedical imaging;Gallium nitride;Generators;Image segmentation;Manuals;Semantics;Training,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
472,Recurrent neural networks with missing information imputation for medical examination data prediction,Han-Gyu Kim; Gil-Jin Jang; Ho-Jin Choi; Minho Kim; Young-Won Kim; Jaehun Choi,"School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea 34141",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320.0,2017,,,317,323,"In this work, we use recurrent neural network (RNN) to predict the medical examination data with missing parts. There often exist missing parts in medical examination data due to various human factors, for instance, because human subjects occasionally miss their annual examinations. Such missing parts make it hard to predict the future examination data by machines. Thus, imputation of the missing information is needed for accurate prediction of medical examination data. Among various types of RNNs, we choose simple recurrent network (SRN) and long short-term memory (LSTM) to predict the missing information as well as the future medical examination data, as they show good performance in many relevant applications. In our proposed method, the temporal trajectories of the medical examination measurements are modeled by RNNs with the missed measurements compensated, which is then used to predict the future measurements to be used as diagnosing the diseases of the subjects in advance. We have carried out experiments using a medical examination database of Korean people for 12 consecutive years with 13 medical fields. In this database, 11500 people took the medical check-up every year, and 7400 people missed their examination occasionally. We use complete data to train RNNs, and the data with missing parts are used to evaluate the imputation and future measurement prediction performance. In terms of root mean squared error (RMSE) and source to noise ratio (SNR) between the prediction and the actual measurements, the experimental results show that the proposed RNNs predicts medical examination data much better than the conventional linear regression in most of the examination items.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881685,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881685,long short-term memory;medical examination data prediction;recurrent neural network,Diseases;Linear regression;Logic gates;Medical diagnostic imaging;Predictive models;Recurrent neural networks,learning (artificial intelligence);medical information systems;recurrent neural nets,Korea;LSTM;RMSE;RNN training;SNR;SRN;disease diagnosis;human factors;long short-term memory;measurement prediction performance evaluation;medical examination data prediction;medical examination database;medical examination measurements;missing information imputation;missing parts;recurrent neural networks;root mean squared error;simple-recurrent network;source-to-noise ratio;temporal trajectories,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
473,Recurrent Neural Network Based Classification of ECG Signal Features for Obstruction of Sleep Apnea Detection,M. Cheng; W. J. Sori; F. Jiang; A. Khan; S. Liu,"Coll. of Command Inf. Syst., PLA Univ. of Sci. & Technol., Nanjing, China",2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC),20170810.0,2017,2,,199,202,"This paper introduces an OSA detection method based on Recurrent Neural network. At the first step, RR interval (time interval from one R wave to the next R wave) is employed to extract the signals from Apnea- Electrocardiogram (ECG) where all extracted features are then used as an input for the designed deep model. Then an architecture having four recurrent layers and batch normalization layers are designed and trained with the extracted features for OSA detection. Apnea-ECG datasets from physionet.org are used for training and testing our model. Experimental results reveal that our automatic OSA detection model provides better classification accuracy.",,Electronic:978-1-5386-3221-5; POD:978-1-5386-3222-2; Paper:978-1-5386-3220-8,10.1109/CSE-EUC.2017.220,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005997,Apnea Detection;ECG;Feature extraction;RNN;Sleep Apnea,Electrocardiography;Feature extraction;Mathematical model;Recurrent neural networks;Sleep apnea;Testing;Training,electrocardiography;feature extraction;medical signal detection;recurrent neural nets;signal classification,Apnea- electrocardiogram signals;OSA detection method;RR interval;batch normalization layers;electrocardiogram;feature extraction;obstructive sleep apnea detection;recurrent neural network based classification,,,,,,,,21-24 July 2017,,IEEE,IEEE Conference Publications
474,A review on emotion recognition using speech,S. Basu; J. Chakraborty; A. Bag; M. Aftabuddin,"School of Medical Science and Technology, Indian Institute of Technology Kharagpur and Department of Computer Science and Engineering, Maulana Abul Kalam Azad University of Technology, West Bengal",2017 International Conference on Inventive Communication and Computational Technologies (ICICCT),20170717.0,2017,,,109,114,"Emotion recognition or affect detection from speech is an old and challenging problem in the field of artificial intelligence. Many significant research works have been done on emotion recognition. In this paper, the recent works on affect detection using speech and different issues related to affect detection has been presented. The primary challenges of emotion recognition are choosing the emotion recognition corpora (speech database), identification of different features related to speech and an appropriate choice of a classification model. Different types of methods to collect emotional speech data and issues related to them are covered by this presentation along with the previous works review. Literature survey on different features used for recognizing emotion from human speech has been discussed. The significance of various classification models has been presented along with some recent research works review. A detailed description of a prime feature extraction technique named Mel Frequency Cepstral Coefficient (MFCC) and brief description of the working principle of some classification models are also discussed here. In this paper terms like affect detection and emotion recognition are used interchangeably.",,Electronic:978-1-5090-5297-4; POD:978-1-5090-5298-1,10.1109/ICICCT.2017.7975169,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975169,Affect Detection;Back Propagation;Classifier;Corpora;Features;GMM (Gaussian Mixture Model);HMM (Hidden Markov Model);KNN (K-Nearest Neighbors);LPC (Linear Prediction Coefficients);LPCC (Linear Prediction Cepstral Coefficients);MFCC (Mel Frequency Cepstral Coefficient);MLP (Multi Layer Perceptron);Neural Network;RNN (Recurrent Neural Network),Databases;Emotion recognition;Feature extraction;Filter banks;Mel frequency cepstral coefficient;Speech;Speech recognition,emotion recognition;feature extraction;pattern classification;speech recognition,Mel frequency cepstral coefficient;affect detection;artificial intelligence;classification model;emotion recognition corpora;emotional speech data collect;feature identification;human speech;prime feature extraction;speech database,,,,,,,,10-11 March 2017,,IEEE,IEEE Conference Publications
475,Recurrent neural network based retinal nerve fiber layer defect detection in early glaucoma,R. Panda; N. B. Puhan; A. Rao; D. Padhy; G. Panda,"School of Electrical Sciences, IIT Bhubaneswar, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619.0,2017,,,692,695,"Retinal nerve fiber layer defect (RNFLD) is the earliest objective evidence of glaucoma in fundus images. Glaucoma is an optic neuropathy which causes irreversible vision impairment. Early glaucoma detection and its prevention are the only way to prevent further damage to human vision. In this paper, we propose a new automated method for RNFLD detection in fundus images through patch features driven recurrent neural network (RNN). A new dataset of fundus images is created for evaluation purpose which contains several challenging RNFLD boundaries. The true boundary pixels are classified using the RNN trained by novel cumulative zero count local binary pattern (CZC-LBP), directional differential energy (DDE) patch features. The experimental results demonstrate high RNFLD detection rate along with accurate boundary localization.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950614,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950614,Glaucoma;RNFLD;fundus image;patch feature;recurrent neural network,Feature extraction;Nerve fibers;Optical fibers;Optical imaging;Recurrent neural networks;Retina,biomedical optical imaging;diseases;eye;medical image processing;neurophysiology;optical tomography;recurrent neural nets,cumulative zero count local binary pattern;directional differential energy patch features;early glaucoma;fundus images;irreversible vision impairment;optic neuropathy;recurrent neural network;retinal nerve fiber layer defect detection,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
476,ECG-based biometrics using recurrent neural networks,R. Salloum; C. C. J. Kuo,"Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, United States of America","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20170619.0,2017,,,2062,2066,"In this paper, we propose the use of recurrent neural networks (RNNs) to develop an effective solution to two problems in electrocardiogram (ECG)-based biometrics: identification/classification and authentication. Different RNN architectures with various parameter settings were evaluated, including traditional, long short-term memory (LSTM), gated recurrent unit (GRU), unidirectional, and bidirectional networks. Unlike many existing methods, the RNN-based method does not require any feature extraction. The method was evaluated using two publicly available datasets: ECG-ID and MIT-BIH Arrhythmia (MITDB). For the identification problem, nearly 100% classification accuracy on the ECG-ID dataset was achieved, and similar results were observed for the MITDB dataset. For the authentication problem, an RNN was trained and the hidden state at the final time step was extracted to make a decision. We evaluated the effect of the training size on the equal error rate (EER), and showed that the EER drops from approximately 3.5% to nearly 0% as we increased the percentage of subjects used for training from approximately 15% to 80%.",,Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9,10.1109/ICASSP.2017.7952519,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952519,Authentication;Biometrics;ECG;Identification;RNN,Authentication;Biometrics (access control);Electrocardiography;Heart beat;Recurrent neural networks;Testing;Training,biometrics (access control);electrocardiography;feature extraction;medical image processing;recurrent neural nets,ECG-ID dataset;ECG-based biometrics;EER;GRU;LSTM;MIT-BIH Arrhythmia;MITDB dataset;RNN architectures;bidirectional networks;electrocardiogram;equal error rate;feature extraction;gated recurrent unit;identification/classification;long short-term memory;publicly available datasets;recurrent neural networks,,,,,,,,5-9 March 2017,,IEEE,IEEE Conference Publications
477,Instantaneous heart rate as a robust feature for sleep apnea severity detection using deep learning,R. K. Pathinarupothi; R. Vinaykumar; E. Rangan; E. Gopalakrishnan; K. P. Soman,Amrita Center for Wireless Networks,2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413.0,2017,,,293,296,"Automated sleep apnea detection and severity identification has largely focused on multivariate sensor data in the past two decades. Clinically too, sleep apnea is identified using a combination of markers including blood oxygen saturation, respiration rate etc. More recently, scientists have begun to investigate the use of instantaneous heart rates for detection and severity measurement of sleep apnea. However, the best-known techniques that use heart rate and its derivatives have been able to achieve less than 85% accuracy in classifying minute-to-minute apnea data. In our research reported in this paper, we apply a deep learning technique called LSTM-RNN (long short-term memory recurrent neural network) for identification of sleep apnea and its severity based only on instantaneous heart rates. We have tested this model on multiple sleep apnea datasets and obtained perfect accuracy. Furthermore, we have also tested its robustness on an arrhythmia dataset (that is highly probable in mimicking sleep apnea heart rate variability) and found that the model is highly accurate in distinguishing between the two.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897263,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897263,,Computer architecture;Electrocardiography;Heart rate;Machine learning;Sleep apnea;Testing;Training,electrocardiography;learning (artificial intelligence);medical disorders;medical signal processing;neural nets;sleep,arrhythmia dataset;automated sleep apnea detection;blood oxygen saturation;deep learning technique;instantaneous heart rate;minute-to-minute apnea data classification;multivariate sensor data;respiration rate;short-term memory recurrent neural network;sleep apnea dataset;sleep apnea heart rate variability;sleep apnea severity detection;sleep apnea severity measurement,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
478,Using deep gated RNN with a convolutional front end for end-to-end classification of heart sound,C. Thomae; A. Dominik,"THM University of Applied Sciences, KITE Kompetenzzentrum f&#x00FC;r Informationstechnologie, Giessen, Germany",2016 Computing in Cardiology Conference (CinC),20170302.0,2016,,,625,628,"Classification of heart sounds of a diverse set of phono-cardiograms (PCGs) from different recording settings is the challenging objective of the 2016 PhysioNet Challenge. We suggest an end-to-end deep neural network, which is fed with raw PCGs and which learns to autonomously extract features and to classify the recordings. Our architecture combines convolutional and recurrent layers, followed by an attention mechanism, which weights time steps by importance and a dense multilayer perceptron as classifier. Whereas currently trending deep neural networks in speech recognition or computer vision use up to a million of training samples, a restricted set of only 3,153 heart sound recordings is available as training data. We workaround this limitation by artificially increasing the training set by means of augmentation of the raw PCGs using various audio effects. Using this moderately sized neural network, we attain high validation scores of 0.89 on validation data; however the resulting scores on the hidden test data ofthe challenge diverge in range (0.82).",,Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4,10.23919/CIC.2016.7868820,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868820,,Computer architecture;Convolution;Feature extraction;Feeds;Heart;Neural networks;Training,convolution;feature extraction;medical signal processing;multilayer perceptrons;phonocardiography;signal classification,2016 PhysioNet Challenge;attention mechanism;classifier;computer vision;convolutional front end;convolutional layers;deep gated RNN;dense multilayer perceptron;end-to-end classification;end-to-end deep neural network;feature extraction;heart sound classification;heart sound recording;phonocardiograms;raw PCG;recurrent layers;speech recognition;time steps,,,,,,,,11-14 Sept. 2016,,IEEE,IEEE Conference Publications
479,Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks,C. Esteban; O. Staeck; S. Baier; Y. Yang; V. Tresp,"Siemens AG & Ludwig Maximilian, Univ. of Munich, Munich, Germany",2016 IEEE International Conference on Healthcare Informatics (ICHI),20161208.0,2016,,,93,101,"In clinical data sets we often find static information (e.g. patient gender, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs, specifically designed for the clinical domain, that combines static and dynamic information in order to predict future events. We work with a database collected in the Charité Hospital in Berlin that contains complete information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, based on information recorded in the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, with a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also used the same models for a second task, i.e., next event prediction, and found that here the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task.",,Electronic:978-1-5090-6117-4; POD:978-1-5090-6118-1,10.1109/ICHI.2016.16,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776332,,Diseases;Kidney;Medical diagnostic imaging;Predictive models;Recurrent neural networks,electronic health records;feedforward neural nets;hospitals;recurrent neural nets;regression analysis,Berlin;Charite Hospital;RNN;clinical data sets;clinical event prediction;data sequences;dynamic information;electronic health record;feedforward neural network;gated recurrent units;logistic regression model;machine learning;patient information;recurrent neural networks;static information,,,,,,,,4-7 Oct. 2016,,IEEE,IEEE Conference Publications
480,Combined long short-term memory based network employing wavelet coefficients for MI-EEG recognition,M. Li; M. Zhang; X. Luo; J. Yang,"College of Electronic Information & Control Engineering, Beijing University of Technology, Beijing 100124, China",2016 IEEE International Conference on Mechatronics and Automation,20160905.0,2016,,,1971,1976,"Motor Imagery Electroencephalography (MI-EEG) plays an important role in brain computer interface (BCI) based rehabilitation robot, and its recognition is the key problem. The Discrete Wavelet Transform (DWT) has been applied to extract the time-frequency features of MI-EEG. However, the existing EEG classifiers, such as support vector machine (SVM), linear discriminant analysis (LDA) and BP network, did not make full use of the time sequence information in time-frequency features, the resulting recognition performance were not very ideal. In this paper, a Long Short-Term Memory (LSTM) based recurrent Neural Network (RNN) is integrated with Discrete Wavelet Transform (DWT) to yield a novel recognition method, denoted as DWT-LSTM. DWT is applied to analyze the each channel of MI-EEG and extract its effective wavelet coefficients, representing the time-frequency features. Then a LSTM based RNN is used as a classifier for the patten recognition of observed MI-EEG data. Experiments are conducted on a publicly available dataset, and the 5-fold cross validation experimental results show that DWT-LSTM yields relatively higher classification accuracies compared to the existing approaches. This is helpful for the further research and application of RNN in processing of MI-EEG.",,CD-ROM:978-1-5090-2394-3; Electronic:978-1-5090-2396-7; POD:978-1-5090-2397-4,10.1109/ICMA.2016.7558868,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558868,Brain computer interface;Discrete Wavelet Transform;Long Short-Term Memory;Recurrent Neural Network;motor imagery EEG,Discrete wavelet transforms;Electroencephalography;Feature extraction;Logic gates;Recurrent neural networks;Time-frequency analysis,backpropagation;brain-computer interfaces;discrete wavelet transforms;electroencephalography;feature extraction;medical signal processing;neural nets;patient rehabilitation;signal classification;support vector machines;time-frequency analysis,BCI;BP network;DWT-LSTM;EEG classifiers;LDA;LSTM based RNN;MI-EEG data;MI-EEG recognition;Motor Imagery Electroencephalography;SVM;brain computer interface based rehabilitation robot;discrete wavelet transform;effective wavelet coefficients;linear discriminant analysis;long short-term memory based network;patten recognition;recognition performance;recurrent neural network;support vector machine;time sequence information;time-frequency feature extraction,,,,,,,,7-10 Aug. 2016,,IEEE,IEEE Conference Publications
481,Modelling the indentation force response of non-uniform soft tissue using a recurrent neural network,R. Nowell; B. Shirinzadeh; J. Smith; Y. Zhong,"Department of Mechanical and Aerospace Engineering, Monash University, Vic, Australia",2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob),20160728.0,2016,,,377,382,"A scaled recurrent neural network (RNN) model is developed which accurately predicts the force response from the indentation of a non-uniform soft tissue sample. The model consists of two components. The RNN is used to predict the force response of indentation using data from a reference tissue sample. A two-parameter component then scales the neural networks predictions relative to previously determined properties of the test sample. This component is based on a strain inverse model of force, which is used to account for the non-uniformity of the tissue between the test and reference data. Experimental force measurements were performed on a highly non-uniform soft tissue analogue to develop and validate the model. Using the visco-elastic Hunt-Crossley model as a benchmark, the developed model provides significantly better prediction. Future research will investigate applying this model to surgical simulations and verifying its application to different biological tissues.",,Electronic:978-1-5090-3287-7; POD:978-1-5090-3288-4,10.1109/BIOROB.2016.7523655,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523655,,Biological tissues;Computational modeling;Data models;Force;Neural networks;Strain;Training,force control;force measurement;medical robotics;neurocontrollers;recurrent neural nets;robot dynamics;viscoelasticity,RNN;biological tissues;force response;indentation force response;nonuniform soft tissue;recurrent neural network;reference tissue sample;strain inverse force model;surgical simulations;two-parameter component;visco-elastic Hunt-Crossley model,,,,,,,,26-29 June 2016,,IEEE,IEEE Conference Publications
482,EEG signal analysis based on fixed-value shift compression algorithm,Xueyan Guo; Huanyu Zhao; Xiaoyun Li; Tongliang Li; Mingfang Dai,"SJZ JKSS Technology Co. Ltd, Shijiazhuang, China",2015 11th International Conference on Natural Computation (ICNC),20160111.0,2015,,,959,963,"The analysis of Electroencephalogram (EEG) signals plays a very important role in the biomedical domain and has many applications. It is extensively used in the Brain-Computer Interface (BCI) system and can be used for disease diagnosis, disease treatment, etc. The two main technologies of EEG signal analysis is feature extraction and pattern recognition. The key features of EEG signals can be obtained through time-domain and frequency-domain analysis. The wavelet analysis is one kind of time-frequency analysis and has been considered very promising for data compression. The conventional method find wavelet synopsis to minimize the total mean squared error (L2). It cannot control the approximation error of each single data element in the data vector. Usually, the nonlinear classification algorithms perform better than the linears, also more time-consuming in the meantime. In this paper, one method is provided to realize the feature extraction and pattern recognition of EEG signals. The data compression algorithm Fixed-value Shift (F-Shift) proposed by Pang et al. takes a novel method to construct unrestricted Haar wavelet synopsis under uniform norm (L∞) error bound. In their algorithm, the maximum approximation error of each individual element can be bounded by an given error bound. We apply this method to EEG signal compression, thus the key features are obtained. Then a fast nonlinear classification algorithm, one Randomize Neural Network (RNN), is provided to identify different patterns of EEG signals. The experiments indicate that (1) the F-Shift algorithm can compress EEG signals effectively and obtain the key features at the same time and (2) the RNN can discriminate different patterns of EEG signals based on the extracted features.",,CD-ROM:978-1-4673-7678-5; Electronic:978-1-4673-7679-2; POD:978-1-4673-7680-8,10.1109/ICNC.2015.7378121,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378121,Electroencephalogram (EEG) signal analysis;Fixed-value Shift (F-Shift) algorithm;Randomize Neural Network (RNN);Signal compression,Approximation algorithms;Biological neural networks;Data compression;Diseases;Electroencephalography;Signal processing algorithms;Wavelet transforms,Haar transforms;brain-computer interfaces;data compression;diseases;electroencephalography;feature extraction;frequency-domain analysis;mean square error methods;medical signal processing;neural nets;time-domain analysis;wavelet transforms,BCI;Brain-Computer Interface system;EEG signal analysis;EEG signal compression;F-Shift algorithm;Haar wavelet synopsis;RNN;Randomize Neural Network;biomedical domain;data compression algorithm;data vector;disease diagnosis;disease treatment;electroencephalogram signals;fast nonlinear classification algorithm;feature extraction;fixed-value shift compression algorithm;frequency-domain analysis;maximum approximation error;nonlinear classification algorithms;pattern recognition;single data element;time-domain analysis;time-frequency analysis;total mean squared error;uniform norm error bound;wavelet analysis,,,,16.0,,,,15-17 Aug. 2015,,IEEE,IEEE Conference Publications
483,Biomedical named entity recognition based on extended Recurrent Neural Networks,Lishuang Li; Liuke Jin; Zhenchao Jiang; Dingxin Song; Degen Huang,"School of Computer Science and Technology, Dalian University of Technology, China",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,649,652,"Biomedical named entity recognition (bio-NER), which extracts important entities such as genes and proteins, has become one of the most fundamental tasks in biomedical knowledge acquisition. However, the performance of traditional NER systems is always limited to the construction of complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area. In this paper we mainly focus on building a simple and efficient system for bio-NER with the extended Recurrent Neural Network (RNN) which considers the predicted information from the prior node and external context information (topical information & clustering information). Extracting complex hand-designed features is skipped and replaced with word embeddings. The experiments conducted on the BioCreative II GM data set demonstrate RNN models outperform CRF model and deep neural networks (DNN); furthermore, the extended RNN model performs better than the original RNN model.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359761,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359761,bio-NER;context information;hand-designed features;recurrent neural network;word embedding,Artificial neural networks;Biological system modeling,bioinformatics;genetics;knowledge acquisition;medical computing;pattern clustering;proteins;recurrent neural nets;text analysis,BioCreative II GM data set;bio-NER;biomedical knowledge acquisition;biomedical named entity recognition;clustering information;context information;entity extraction;extended RNN;extended recurrent neural networks;genes;linguistic analysis;proteins;topical information;word embeddings,,1.0,,12.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
484,Multi task sequence learning for depression scale prediction from video,L. Chao; J. Tao; M. Yang; Y. Li; J. Tao,"National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China",2015 International Conference on Affective Computing and Intelligent Interaction (ACII),20151207.0,2015,,,526,531,"Depression is a typical mood disorder, which affects people in mental and even physical problems. People who suffer depression always behave abnormal in visual behavior and the voice. In this paper, an audio visual based multimodal depression scale prediction system is proposed. Firstly, features are extracted from video and audio are fused in feature level to represent the audio visual behavior. Secondly, long short memory recurrent neural network (LSTM-RNN) is utilized to encode the dynamic temporal information of the abnormal audio visual behavior. Thirdly, emotion information is utilized by multi-task learning to boost the performance further. The proposed approach is evaluated on the Audio-Visual Emotion Challenge (AVEC2014) dataset. Experiments results show the dimensional emotion recognition helps to depression scale prediction.",,Electronic:978-1-4799-9953-8; POD:978-1-4799-9954-5; USB:978-1-4799-9952-1,10.1109/ACII.2015.7344620,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344620,affective computing;deep learning;depression recognition;multi-task learning,Context;Emotion recognition;Face;Feature extraction;Shape;Training;Visualization,audio signal processing;audio-visual systems;feature extraction;learning (artificial intelligence);medical disorders;psychology;recurrent neural nets;sensor fusion;video signal processing,AVEC2014 dataset;Audio-Visual Emotion Challenge dataset;LSTM-RNN;abnormal audio visual behavior;depression scale prediction;dimensional emotion recognition;dynamic temporal information encoding;emotion information;feature extraction;feature level fusion;long short memory recurrent neural network;mood disorder;multimodal depression scale prediction system;multitask sequence learning;video features,,1.0,,35.0,,,,21-24 Sept. 2015,,IEEE,IEEE Conference Publications
485,Inferring the dynamics of gene regulatory networks via optimized recurrent neural network and dynamic Bayesian network,A. Akutekwe; H. Seker,"Bio-Health Informatics Research Group, Department of Computer Science and Digital Technologies, University of Northumbria at Newcastle, Newcastle upon, Tyne, NE1 8ST, United Kingdom",2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB),20151019.0,2015,,,1,8,"Inferring gene regulatory networks (GRNs) from time-course expression data is a major challenge in systems biology and comprehensive understanding of its dynamics is difficult. Most temporal inference methods for the dynamics of GRNs assume linear dependencies among genes but this strong assumption of linearity among genes does not truly represent the dynamics of the GRNs which are inherently nonlinear. Other parametric and non-parametric methods for modeling nonlinear dynamical systems such as the S-systems and causal identification structure (CSI) have been proposed for modeling time-course nonlinearities in GRNs; however, these methods are statistically inefficient and analytically intractable especially in high dimensions. To overcome these problems, we propose an algorithm based on optimized recurrent neural network (RNN) and dynamic Bayesian (DBN) network called RNN-DBN. The inference algorithm for our DBN is based on nonlinear state space Elman recurrent neural network. Results on Drosophila Melanogaster nonlinear time-course benchmark dataset shows our method outperforms the G1DBN inference method based on linear model assumptions. The algorithm is further applied to time-course ovarian cancer dataset. The results show that the expression levels of three of five significant hub genes (flap structure-specific endonuclease 1, kinesin family member 11 and CDC6 cell division cycle 6 homolog (S. cerevisiae)) were decreased by oxaliplatin, but remained constant with cisplatin platinum drugs. These may therefore be potential drug candidates for ovarian cancer.",,Electronic:978-1-4799-6926-5; POD:978-1-4799-6927-2,10.1109/CIBCB.2015.7300322,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300322,Dynamic Bayesian Network;Gene Regulatory Networks;Ovarian Carcinoma;Recurrent Neural Network;Time Series,Bayes methods;Biological system modeling;Gene expression;Heuristic algorithms;Inference algorithms;Mathematical model;Recurrent neural networks,Bayes methods;biological organs;cancer;cellular biophysics;drugs;enzymes;gene therapy;medical computing;molecular biophysics;optimisation;platinum compounds;recurrent neural nets;tumours,CDC6 cell division cycle 6 homolog;Drosophila Melanogaster nonlinear time-course benchmark dataset;G1DBN inference method;S-systems;S. cerevisiae;causal identification structure;cisplatin platinum drugs;dynamic Bayesian network;flap structure-specific endonuclease 1;gene regulatory network dynamics;kinesin family member 11;nonlinear dynamical systems;nonlinear state space Elman recurrent neural network;nonparametric methods;optimized recurrent neural network;oxaliplatin;potential drug candidates;systems biology;time-course expression data;time-course nonlinearities;time-course ovarian cancer dataset,,,,30.0,,,,12-15 Aug. 2015,,IEEE,IEEE Conference Publications
486,Multi-model data fusion to improve an early warning system for hypo-/hyperglycemic events,R. H. Botwey; E. Daskalaki; P. Diem; S. G. Mougiakakou,"Diabetes Technology Research Group, ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106.0,2014,,,4843,4846,"Correct predictions of future blood glucose levels in individuals with Type 1 Diabetes (T1D) can be used to provide early warning of upcoming hypo-/hyperglycemic events and thus to improve the patient's safety. To increase prediction accuracy and efficiency, various approaches have been proposed which combine multiple predictors to produce superior results compared to single predictors. Three methods for model fusion are presented and comparatively assessed. Data from 23 T1D subjects under sensor-augmented pump (SAP) therapy were used in two adaptive data-driven models (an autoregressive model with output correction - cARX, and a recurrent neural network - RNN). Data fusion techniques based on i) Dempster-Shafer Evidential Theory (DST), ii) Genetic Algorithms (GA), and iii) Genetic Programming (GP) were used to merge the complimentary performances of the prediction models. The fused output is used in a warning algorithm to issue alarms of upcoming hypo-/hyperglycemic events. The fusion schemes showed improved performance with lower root mean square errors, lower time lags, and higher correlation. In the warning algorithm, median daily false alarms (DFA) of 0.25%, and 100% correct alarms (CA) were obtained for both event types. The detection times (DT) before occurrence of events were 13.0 and 12.1 min respectively for hypo-/hyperglycemic events. Compared to the cARX and RNN models, and a linear fusion of the two, the proposed fusion schemes represents a significant improvement.",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6944708,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944708,,Adaptation models;Data integration;Data models;Diabetes;Genetic algorithms;Predictive models;Sugar,biochemistry;diseases;genetic algorithms;medical signal processing;neural nets;patient treatment;regression analysis;sensor fusion,DST;Dempster-Shafer evidential theory;RNN;SAP therapy;Type 1 diabetes;adaptive data driven models;autoregressive model;blood glucose levels;cARX;data fusion techniques;early warning system;fusion scheme;genetic algorithms;genetic programming;hyperglycemic events;hypoglycemic events;model fusion methods;multimodel data fusion;output correction;recurrent neural network;sensor augmented pump therapy,,0.0,,16.0,,,,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
487,Non-invasive detection of hypoglycemic episodes in Type 1 diabetes using intelligent hybrid rough neural system,S. H. Ling; P. P. San; H. K. Lam; H. T. Nguyen,"Centre for Health Technologies, Faculty of Engineering and IT, University of Technology Sydney, Ultimo, NSW, Australia",2014 IEEE Congress on Evolutionary Computation (CEC),20140922.0,2014,,,1238,1242,"Insulin-dependent diabetes mellitus is classified as Type 1 diabetes and it can be further classified as immune-mediated or idiopathic. Through the analysis of electrocar-diographic (ECG) signals of 15 children with T1DM, an effective hypoglycemia detection system, hybrid rough set based neural network (RNN) is developed by the use of physiological parameters of ECG signal. In order to detect the status of hypoglycemia, the feature of ECG of type 1 diabetics are extracted and classified according to corresponding glucose levels. In this technique, the applied physiological inputs are partitioned into predicted (certain) or random (uncertain) parts using defined lower and boundary of rough regions. In this way, the neural network is designed to deal only with the boundary region which mainly consists of a random part of applied input signal causing inaccurate modeling of the data set. A global training algorithm, hybrid particle swarm optimization with wavelet mutation (HPSOWM) is introduced for parameter optimization of proposed RNN. The experiment is carried out using real data collected at Department of Health, Government of Western Australia. It indicated that the proposed hybrid architecture is efficient for hypoglycemia detection by achieving better sensitivity and specificity with less number of design parameters.",1089-778X;1089778X,CD-ROM:978-1-4799-6626-4; Electronic:978-1-4799-1488-3; POD:978-1-4799-1486-9,10.1109/CEC.2014.6900229,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900229,,Approximation methods;Diabetes;Heart rate;Neural networks;Pediatrics;Sensitivity;Training,diseases;electrocardiography;feature extraction;medical signal processing;neural nets;particle swarm optimisation;rough set theory;signal classification;wavelet transforms,Department of Health;ECG signals;Government of Western Australia;HPSOWM;RNN;boundary region;electrocardiographic signals;feature classification;feature extraction;glucose levels;hybrid particle swarm optimization with wavelet mutation;hypoglycemia detection system;hypoglycemic episodes detection;idiopathic diabetes;immune-mediated diabetes;insulin-dependent diabetes mellitus;intelligent hybrid rough neural system;type 1 diabetes,,0.0,,20.0,,,,6-11 July 2014,,IEEE,IEEE Conference Publications
488,Capturing Human Body Dynamics Using RNN Based on Persistent Excitation Data Generator,A. Abdulrahman; K. Iqbal,"Electr. & Comput. Eng., Univ. of Arkansas at Little Rock, Little Rock, AR, USA",2014 IEEE 27th International Symposium on Computer-Based Medical Systems,20140825.0,2014,,,221,226,"Human body walking movement involves both single and double support phases and is considered difficult to model. The aim of this study was to develop a method to capture human body dynamics during walking using Recurrent Neural Networks (RNN). In addition, a novel method using persistent excitation data generator is proposed to generate kinematic data to train the RNN in the absence of laboratory measurements. Kinematic data were applied to human body mathematical model to obtain required joint torques during bipedal walking. The RNN was used to approximate human body kinematics resulting from the joints torques for the walking movement. In order to test validity of the RNN model, model output was compared with human walking data captured in the laboratory. Simulation results show the model was able to approximate the joint angles during human walk with a low (10<sup>-4</sup> m) mean squared error for one stride.",1063-7125;10637125,Electronic:978-1-4799-4435-4; POD:978-1-4799-4434-7,10.1109/CBMS.2014.145,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881880,Biomechanics;Recurrent Neural Network;Walking,Biological system modeling;Data models;Equations;Joints;Kinematics;Legged locomotion;Mathematical model,data handling;gait analysis;learning (artificial intelligence);medical computing;recurrent neural nets,RNN model validity;RNN training;bipedal walking;human body dynamics;human body kinematics;human body mathematical model;human body walking movement;kinematic data generation;laboratory measurements;persistent excitation data generator;recurrent neural networks,,0.0,,19.0,,,,27-29 May 2014,,IEEE,IEEE Conference Publications
489,Muscle Fatigue Tracking with Evoked EMG via Recurrent Neural Network: Toward Personalized Neuroprosthetics,Z. Li; M. Hayashibe; C. Fattal; D. Guiraud,"LIRMM, Univ. of Montpellier II, Montpellier, France",IEEE Computational Intelligence Magazine,20140410.0,2014,9,2.0,38,46,"One of the challenging issues in computational rehabilitation is that there is a large variety of patient situations depending on the type of neurological disorder. Human characteristics are basically subject specific and time variant; for instance, neuromuscular dynamics may vary due to muscle fatigue. To tackle such patient specificity and time-varying characteristics, a robust bio-signal processing and a precise model-based control which can manage the nonlinearity and time variance of the system, would bring break-through and new modality toward computational intelligence (CI) based rehabilitation technology and personalized neuroprosthetics. Functional electrical stimulation (FES) is a useful technique to assist restoring motor capability of spinal cord injured (SCI) patients by delivering electrical pulses to paralyzed muscles. However, muscle fatigue constraints the application of FES as it results in the time-variant muscle response. To perform adaptive closedloop FES control with actual muscle response feedback taken into account, muscular torque is essential to be estimated accurately. However, inadequacy of the implantable torque sensor limits the direct measurement of the time-variant torque at the joint. This motivates the development of methods to estimate muscle torque from bio-signals that can be measured. Evoked electromyogram (eEMG) has been found to be highly correlated with FES-induced torque under various muscle conditions, indicating that it can be used for torque/force prediction. A nonlinear ARX (NARX) type model is preferred to track the relationship between eEMG and stimulated muscular torque. This paper presents a NARX recurrent neural network (NARX-RNN) model for identification/prediction of FES-induced muscular dynamics with eEMG. The NARX-RNN model may possess novelty of robust prediction performance. Due to the difficulty of choosing a proper forgetting factor of Kalman filter for predicting time-variant torque with eEMG, the presente- NARX-RNN could be considered as an alternative muscular torque predictor. Data collected from five SCI patients is used to evaluate the proposed NARX-RNN model, and the results show promising estimation performances. In addition, the general importance regarding CI-based motor function modeling is introduced along with its potential impact in the rehabilitation domain. The issue toward personalized neuroprosthetics is discussed in detail with the potential role of CI-based identification and the benefit for motor-impaired patient community.",1556-603X;1556603X,,10.1109/MCI.2014.2307224,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786477,,Communities;Computational intelligence;Computational modeling;Electromyography;Fatigue;Motion control;Muscles;Patient rehabilitation;Predictive models;Prosthetics;Recurrent neural networks;Torque measurement,Kalman filters;adaptive control;closed loop systems;control nonlinearities;electromyography;medical signal processing;neurophysiology;patient rehabilitation;prosthetics;recurrent neural nets,CI-based motor function modeling;FES-induced torque;Kalman filter;NARX recurrent neural network model;NARX-RNN model;adaptive closed-loop FES control;computational intelligence based rehabilitation technology;electrical pulses;evoked EMG;evoked electromyogram;model-based control;motor-impaired patient community;muscle fatigue tracking;muscle response feedback;neurological disorder;neuromuscular dynamics;nonlinear ARX type model;paralyzed muscles;personalized neuroprosthetics;robust biosignal processing;spinal cord injured patients;stimulated muscular torque;system nonlinearity;system time variance;time-variant torque,,19.0,,35.0,,,,May 2014,,IEEE,IEEE Journals & Magazines
490,Automatic Detection and Classification of Colorectal Polyps by Transferring Low-Level CNN Features From Nonmedical Domain,R. Zhang; Y. Zheng; T. W. C. Mak; R. Yu; S. H. Wong; J. Y. W. Lau; C. C. Y. Poon,"Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR",IEEE Journal of Biomedical and Health Informatics,20170520.0,2017,21,1.0,41,47,"Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.",2168-2194;21682194,,10.1109/JBHI.2016.2635662,"Chow Yuk Ho Technology Centre for Innovative Medicine; Hong Kong Innovation and Technology Fund, Shaw Endoscopy Center; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769237,Colorectal cancer;deep learning;health informatics;polyp diagnosis,Biomedical imaging;Cancer;Colonoscopy;Feature extraction;Machine learning;Neurons;Training,biological tissues;biomedical optical imaging;cancer;endoscopes,NBI endoscopic polyp images;adenomatous colorectal polyps;cancer deaths;colonoscopy;colorectal cancer;colorectal polyps automatic detection;colorectal polyps classification;deep convolutional neural network;endoscopic images;endoscopic nonpolyp images;fully automatic algorithm;human visual observation;hyperplasia;hyperplastic colorectal polyps;learning application;low-level CNN features;narrowband imaging endoscopy;nonmedical datasets;optical magnification;polyp histology;polypectomy;tissue;white-light endoscopy,,,,,,,20161205.0,Jan. 2017,,IEEE,IEEE Journals & Magazines
491,Comparison of hand-craft feature based SVM and CNN based deep learning framework for automatic polyp classification,Y. Shin; I. Balasingham,"Department Electronics and Telecommunications at Norwegian University of Science and Technology (NTNU), Trondheim, Norway",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,3277,3280,"Colonoscopy is a standard method for screening polyps by highly trained physicians. Miss-detected polyps in colonoscopy are potential risk factor for colorectal cancer. In this study, we investigate an automatic polyp classification framework. We aim to compare two different approaches named hand-craft feature method and convolutional neural network (CNN) based deep learning method. Combined shape and color features are used for hand craft feature extraction and support vector machine (SVM) method is adopted for classification. For CNN approach, three convolution and pooling based deep learning framework is used for classification purpose. The proposed framework is evaluated using three public polyp databases. From the experimental results, we have shown that the CNN based deep learning framework shows better classification performance than the hand-craft feature based methods. It achieves over 90% of classification accuracy, sensitivity, specificity and precision.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037556,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037556,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
492,Combining Convolutional and Recurrent Neural Networks for Human Skin Detection,H. Zuo; H. Fan; E. Blasch; H. Ling,"Department of Chemical Equipment and Control Engineering, China University of Petroleum, Qingdao, China",IEEE Signal Processing Letters,20170209.0,2017,24,3.0,289,293,"Skin detection from images, typically used as a preprocessing step, has a wide range of applications such as dermatology diagnostics, human computer interaction designs, and etc. It is a challenging problem due to many factors such as variation in pigment melanin, uneven illumination, and differences in ethnicity geographics. Besides, age and gender introduce additional difficulties to the detection process. It is hard to determine whether a single pixel is skin or nonskin without considering the context. An efficient traditional hand-engineered skin color detection algorithm requires extensive work by domain experts. Recently, deep learning algorithms, especially convolutional neural networks (CNNs), have achieved great success in pixel-wise labeling tasks. However, CNN-based architectures are not sufficient for modeling the relationship between pixels and their neighbors. In this letter, we integrate recurrent neural networks (RNNs) layers into the fully convolutional neural networks (FCNs), and develop an end-to-end network for human skin detection. In particular, FCN layers capture generic local features, while RNN layers model the semantic contextual dependencies in images. Experimental results on the COMPAQ and ECU skin datasets validate the effectiveness of the proposed approach, where RNN layers enhance the discriminative power of skin detection in complex background situations.",1070-9908;10709908,,10.1109/LSP.2017.2654803,U.S. National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004543 - China Scholarship Council; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820144,Convolutional neural networks (CNNs);recurrent neural networks (RNNs);skin classification;skin detection;skin segmentation,Convolution;Detection algorithms;Image color analysis;Lighting;Recurrent neural networks;Skin,image colour analysis;image recognition;recurrent neural nets;skin,COMPAQ skin datasets;ECU skin datasets;FCN layers;complex background situations;convolutional neural networks;deep learning algorithms;dermatology diagnostics;discriminative power;domain experts;end-to-end network;ethnicity geographics;generic local features;hand-engineered skin color detection;human computer interaction;human skin detection;pigment melanin;pixel-wise labeling tasks;preprocessing step;recurrent neural networks;semantic contextual dependency;uneven illumination,,,,,,,20170117.0,March 2017,,IEEE,IEEE Journals & Magazines
493,Human induced pluripotent stem cell region recognition in microscopy images using Convolutional Neural Networks,Y. H. Chang; K. Abe; H. Yokota; K. Sudo; Y. Nakamura; C. Y. Lin; M. D. Tsai,"Department of Information and Computer Engineering, Chung-Yuan Christian University, Chung-Li, 32023, Taiwan",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,4058,4061,"We present a deep learning architecture Convolutional Neural Networks (CNNs) for automatic classification and recognition of reprogramming and reprogrammed human Induced Pluripotent Stem (iPS) cell regions in microscopy images. The differentiated cells that possibly undergo reprogramming to iPS cells can be detected by this method for screening reagents or culture conditions in iPS induction. The learning results demonstrate that our CNNs can achieve the Top-1 and Top-2 error rates of 9.2% and 0.84%, respectively, to produce probability maps for the automatic analysis. The implementation results show that this automatic method can successfully detect and localize the human iPS cell formation, thereby yield a potential tool for helping iPS cell culture.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037747,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037747,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
494,Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images,Y. T. Su; Y. Lu; M. Chen; A. A. Liu,"School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China.",IEEE Access,,2017,PP,99.0,1,1,"We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a Convolutional Neural Network (CNN) and a Long Short Term Memory network (LSTM) to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data we validate the proposed method on include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 dataset, and the F score of 96.5% on the C3H10 dataset. The results on both datasets outperform traditional graph model based approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of CNN-LSTM network.",,,10.1109/ACCESS.2017.2745544,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019789,biomedical imaging;computer vision;machine learning;mitosis detection;stem cell,Computer architecture;Feature extraction;Hidden Markov models;Machine learning;Microscopy;Spatiotemporal phenomena;Visualization,,,,,,,,,20170829.0,,,IEEE,IEEE Early Access Articles
495,An efficient method for neuronal tracking in electron microscopy images,L. Yin; C. Xiao; Q. Xie; X. Chen; L. Shen; H. Han,"Computational Mathematics, Hubei University, Wuhan, 430062, China",2017 IEEE International Conference on Mechatronics and Automation (ICMA),20170824.0,2017,,,1865,1870,"With the introduction of deep learning, a wave of artificial intelligence research has been set off again. Scientists focus on brain-inspired intelligence, namely, try to get inspiration from the brain nervous system and cognitive behavior mechanism, to develop intelligent computing models as well as algorithms with stronger information representation, processing and learning ability. So, the study of neurons and the connections between neurons of brain are needed. One major obstacle of reconstruction lies in segmenting and tracking neuronal processes. Electron microscopy is producing neurons images rapidly. In response, we propose an efficient method for neuronal tracking in electron microscopy images to help scientists reconstruct complex neurons. First, we track neurons by kernelized correlation filter to get candidate neuron; then we calculate overlap area and distance of the contours between two consecutive images to get final neuron. We evaluate the performance of our method on a public electron microscopy dataset. The method is superior in accuracy and efficiency.",,CD:978-1-5090-6757-2; Electronic:978-1-5090-6759-6; POD:978-1-5090-6760-2; Paper:978-1-5090-6758-9,10.1109/ICMA.2017.8016102,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016102,3D reconstruction;correlation filters;electron microscopy;neuron;tracking,Correlation;Electron microscopy;Image segmentation;Neurons;Target tracking,,,,,,,,,,6-9 Aug. 2017,,IEEE,IEEE Conference Publications
496,Deep convolutional neural networks for detecting secondary structures in protein density maps from cryo-electron microscopy,R. Li; D. Si; T. Zeng; S. Ji; J. He,"Department of Computer Science, Old Dominion University, Norfolk, Virginia 23529, United States of America",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,41,46,"The detection of secondary structure of proteins using three dimensional (3D) cryo-electron microscopy (cryo-EM) images is still a challenging task when the spatial resolution of cryo-EM images is at medium level (5-10Å). Prior researches focused on the usage of local features that may not capture the global information of image objects. In this study, we propose to use deep learning methods to extract high representative global features and then automatically detect secondary structures of proteins. In particular, we build a convolutional neural network (CNN) classifier that predicts the probability of label for every individual voxel in 3D cryo-EM image with respect to the secondary structure elements of proteins such as α-helix, β-sheet and background. To effectively incorporate the 3D spatial information in protein structures, we propose to perform 3D convolutions in the convolutional layers of CNNs. We show that the proposed CNN classifier can outperform existing SVM method on identifying the secondary structure elements of proteins from 3D cryo-EM medium resolution images.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822490,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822490,,Computational modeling;Convolution;Deconvolution;Image resolution;Protein engineering;Proteins;Three-dimensional displays,biology computing;electron microscopy;feature extraction;image resolution;molecular biophysics;molecular configurations;neural nets;probability;proteins;support vector machines,α-helix;β-sheet;3D convolutions;3D cryo-EM image;3D spatial information;CNN classifier;SVM;convolutional layers;convolutional neural network classifier;deep convolutional neural networks;deep learning methods;high-representative global feature extraction;image objects;local features;probability;protein density maps;secondary structure detection;spatial resolution;three-dimensional cryo-electron microscopy images,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
497,Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks,J. T. Kwak; S. M. Hewitt,"Department of Computer Science and Engineering, Sejong University, Seoul, Korea 05006.",IEEE Access,,2017,PP,99.0,1,1,"In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei were utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training dataset including 73 benign and 89 cancer samples and validated on the testing dataset comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison to the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value<5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.",,,10.1109/ACCESS.2017.2747838,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023758,artificial neural network;cancer detection;computer-aided diagnosis;microscopy;pattern recognition,Biological tissues;Kernel;Machine learning;Neurons;Pathology;Prostate cancer,,,,,,,,,20170831.0,,,IEEE,IEEE Early Access Articles
498,Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning,M. I. Razzak; S. Naz,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824.0,2017,,,801,807,"Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.111,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014845,Blood Sample Analysis;ELM;KWFLICM;RBC;cell morphology;image analysis,Blood;Diseases;Feature extraction;Image color analysis;Image segmentation;Microscopy;Shape,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
499,Crowdsourcing for Chromosome Segmentation and Deep Classification,M. Sharma; O. Saha; A. Sriraman; R. Hebbalaguppe; L. Vig; S. Karande,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824.0,2017,,,786,793,"Metaphase chromosome analysis is one of the primary techniques utilized in cytogenetics. Observations of chromosomal segments or translocations during metaphase can indicate structural changes in the cell genome, and is often used for diagnostic purposes. Karyotyping of the chromosomes micro-photographed under metaphase is done by characterizing the individual chromosomes in cell spread images. Currently, considerable effort and time is spent to manually segment out chromosomes from cell images, and classifying the segmented chromosomes into one of the 24 types, or for diseased cells to one of the known translocated types. Segmenting out the chromosomes in such images can be especially laborious and is often done manually, if there are overlapping chromosomes in the image which are not easily separable by image processing techniques. Many techniques have been proposed to automate the segmentation and classification of chromosomes from spread images with reasonable accuracy, but given the criticality of the domain, a human in the loop is often still required. In this paper, we present a method to segment out and classify chromosomes for healthy patients using a combination of crowdsourcing, preprocessing and deep learning, wherein the non-expert crowd from CrowdFlower is utilized to segment out the chromosomes from the cell image, which are then straightened and fed into a (hierarchical) deep neural network for classification. Experiments are performed on 400 real healthy patient images obtained from a hospital. Results are encouraging and promise to significantly reduce the cognitive burden of segmenting and karyotyping chromosomes.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.109,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014843,,Biological cells;Conferences;Crowdsourcing;Image segmentation;Machine learning;Microscopy;Pipelines,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
500,Mitosis Detection in Phase Contrast Microscopy Image Sequences of Stem Cell Populations: A Critical Review,A. A. Liu; Y. Lu; M. Chen; Y. Su,"Electronics Information Engineering, Tianjin University, Tianjin, Tianjin China 300072 (e-mail: anan0422@gmail.com)",IEEE Transactions on Big Data,,2017,PP,99.0,1,1,"Detecting mitosis from cell population is a fundamental problem in many biological researches and biomedical applications. In modern researches, advanced imaging technologies have been applied to generate large amount of microscope images of cells. However, detecting all mitotic cells from these images with human eye is tedious and time-consuming. In recent years, several approaches have been proposed to help humans finish this job automatically with high efficiency and accuracy. In this review paper, we first described some commonly used datasets for mitosis detection, and then discussed different kinds of methods for mitosis detection, like tracking based methods, tracking free methods, hybrid methods, and the most recently proposed works based on deep learning architecture. We compared these methods on same datasets, and found that deep learning based approaches have achieved a great improvement in performance. At last, we discussed the future possible approaches on mitosis detection, to combine the success of previous works and the advantage of big data in modern researches. Considering expertise is highly required in biomedical area, we will further discuss the possibility to learn information from biomedical big data with less expert annotation.",,,10.1109/TBDATA.2017.2721438,China Scholarship Council; Elite Scholar Program of Tianjin University; National Natural Science Foundation of China; Tianjin Research Program of Application Foundation and Advanced Technology; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962189,big data;biomedical image;computer vision;microscopy image;mitosis detection;stem cell,Big Data;Computer architecture;Image sequences;Microscopy;Sociology;Statistics;Stem cells,,,,,,,,,20170629.0,,,IEEE,IEEE Early Access Articles
501,Malaria Parasite Detection From Peripheral Blood Smear Images Using Deep Belief Networks,D. Bibin; M. S. Nair; P. Punitha,"Department of Research and Development Centre, Bharathiar University, Coimbatore, India",IEEE Access,20170619.0,2017,5,,9099,9108,"In this paper, we propose a novel method to identify the presence of malaria parasites in human peripheral blood smear images using a deep belief network (DBN). This paper introduces a trained model based on a DBN to classify 4100 peripheral blood smear images into the parasite or non-parasite class. The proposed DBN is pre-trained by stacking restricted Boltzmann machines using the contrastive divergence method for pre-training. To train the DBN, we extract features from the images and initialize the visible variables of the DBN. A concatenated feature of color and texture is used as a feature vector in this paper. Finally, the DBN is discriminatively fine-tuned using a backpropagation algorithm that computes the probability of class labels. The optimum size of the DBN architecture used in this paper is 484-600-600-600-600-2, in which the visible layer has 484 nodes and the output layer has two nodes with four hidden layers containing 600 hidden nodes in every layer. The proposed method has performed significantly better than the other state-of-the-art methods with an F-score of 89.66%, a sensitivity of 97.60%, and specificity of 95.92%. This paper is the first application of a DBN for malaria parasite detection in human peripheral blood smear images.",2169-3536;21693536,,10.1109/ACCESS.2017.2705642,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931565,Deep learning;contrastive divergence;deep belief network;discriminative training;malaria parasite detection;restricted Boltzmann machine,Blood;Computer architecture;Diseases;Feature extraction;Image color analysis;Microscopy;Training,Boltzmann machines;backpropagation;belief networks;biology computing;blood;feature extraction;image classification;learning (artificial intelligence),DBN;backpropagation algorithm;contrastive divergence method;deep belief networks;feature extraction;feature vector;human peripheral blood smear images;malaria parasite detection;restricted Boltzmann machines,,,,,,,20170518.0,2017,,IEEE,IEEE Journals & Magazines
502,Cell proposal network for microscopy image analysis,S. U. Akram; J. Kannala; L. Eklund; J. Heikkilä,"Center for Machine Vision Research, University of Oulu, Finland",2016 IEEE International Conference on Image Processing (ICIP),20160819.0,2016,,,3199,3203,"Robust cell detection plays a key role in the development of reliable methods for automated analysis of microscopy images. It is a challenging problem due to low contrast, variable fluorescence, weak boundaries, conjoined and overlapping cells, causing most cell detection methods to fail in difficult situations. One approach for overcoming these challenges is to use cell proposals, which enable the use of more advanced features from ambiguous regions and/or information from adjacent frames to make better decisions. However, most current methods rely on simple proposal generation and scoring methods, which limits the performance they can reach. In this paper, we propose a convolutional neural network based method which generates cell proposals to facilitate cell detection, segmentation and tracking. We compare our method against commonly used proposal generation and scoring methods and show that our method generates significantly better proposals, and achieves higher final recall and average precision.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532950,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532950,cell detection;cell proposals;cell tracking;deep learning;fully convolutional network,Feature extraction;Image analysis;Image segmentation;Microscopy;Proposals;Shape;Training,image segmentation;medical image processing;neural nets,cell proposal network;cell segmentation;cell tracking;convolutional neural network based method;microscopy image analysis;robust cell detection,,1.0,,16.0,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
503,Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions,G. Carneiro; T. Peng; C. Bayer; N. Navab,"Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia",2015 IEEE International Conference on Computer Vision (ICCV),20160218.0,2015,,,648,656,"We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.",,Electronic:978-1-4673-8391-2; POD:978-1-4673-8392-9,10.1109/ICCV.2015.81,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410438,,Cancer;Computer vision;Manuals;Microscopy;Support vector machines;Training;Tumors,cancer;graph theory;medical image processing;neural nets;support vector machines;tumours,DCNN model;FLSSVM;cancer treatment;deep convolutional neural network;flexible graph;flexible latent structure support vector machine;high-order loss function;latent graph;malignant tumour;microcirculatory supply units;microscopy imaging;weakly-supervised structured output learning,,,,37.0,,,,7-13 Dec. 2015,,IEEE,IEEE Conference Publications
504,Staged Inference using Conditional Deep Learning for energy efficient real-time smart diagnosis,M. Parsa; P. Panda; S. Sen; K. Roy,"School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, USA",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,78,81,"Recent progress in biosensor technology and wearable devices has created a formidable opportunity for remote healthcare monitoring systems as well as real-time diagnosis and disease prevention. The use of data mining techniques is indispensable for analysis of the large pool of data generated by the wearable devices. Deep learning is among the promising methods for analyzing such data for healthcare applications and disease diagnosis. However, the conventional deep neural networks are computationally intensive and it is impractical to use them in real-time diagnosis with low-powered on-body devices. We propose Staged Inference using Conditional Deep Learning (SICDL), as an energy efficient approach for creating healthcare monitoring systems. For smart diagnostics, we observe that all diagnoses are not equally challenging. The proposed approach thus decomposes the diagnoses into preliminary analysis (such as healthy vs unhealthy) and detailed analysis (such as identifying the specific type of cardio disease). The preliminary diagnosis is conducted real-time with a low complexity neural network realized on the resource-constrained on-body device. The detailed diagnosis requires a larger network that is implemented remotely in cloud and is conditionally activated only for detailed diagnosis (unhealthy individuals). We evaluated the proposed approach using available physiological sensor data from Physionet databases, and achieved 38% energy reduction in comparison to the conventional deep learning approach.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036767,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036767,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
505,Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database,C. Y. Hung; W. C. Chen; P. T. Lai; C. H. Lin; C. C. Lee,"Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,3110,3113,"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037515,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037515,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
506,Oro Vision: Deep Learning for Classifying Orofacial Diseases,R. Anantharaman; V. Anantharaman; Y. Lee,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,39,45,"This experiment is an attempt to apply deep learning techniques to orofacial image analysis. Health promotion is recognized as a viable approach to preventing diseases and disorders and promoting changes in health behaviors or practices. Each year, oral cancer kills more people in the US than does cervical cancer, malignant melanoma, or Hodgkin's disease. A first line of defense against oral diseases is an orofacial selfexamination. The goal of this experiment titled ""Oro Vision"" is to provide an assessment tool for field workers to perform initial examinations of orofacial diseases, using a camera enabled mobile phone. For this experiment, we chose to implement Oro Vision to detect mouth sores. The goal is to extend this model to identify several other Oral diseases such as Thrush, Leukoplakia, Lichenplanus, etc. One variety of mouth sore, referred to as the ""cold sore"" is highly contagious and an infected person can easily pass on the infection to another person just through skin to skin contact. ""Oro Vision"" is implemented as an HTML5 mobile responsive web app that can be accessed through any mobile or standard browser. Oro Vision uses deep learning to train a model and subsequently uses this trained model to distinguish a cold sore from a canker sore. In addition, an accurate diagnosis by a trained healthcare professional is required before any kind of treatment is discussed since several other conditions of the mouth including oral cancer may mimic canker sores.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.69,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031130,clarifai;deep learning;mouth sore;retrained inception,Cancer;Machine learning;Mobile communication;Mouth;Tools,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
507,Continuous Assessment of Children’s Emotional States Using Acoustic Analysis,Y. Gong; C. Poellabauer,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,171,178,"Emotional and behavioral disorders (EBD) are a widespread healthcare concern in children and adolescents. Prevention and early intervention are the most powerful tools in ameliorating the problem, and therefore, timely and accurate detection of abnormal emotional patterns is of vital importance. In this paper, we propose a system that detects second-level emotional states of children using hour-level audio recordings. The proposed system consists of an audio segmentation and speaker tracking front-end along with an emotion recognition back-end. Supervised support vector machine is used in the front-end to improve its robustness to short and inconsistent child speech pattern and end-to-end deep learning is used in the emotion recognition back-end to improve its robustness to noise and segmentation error. We further demonstrate the potential of the proposed system as an automated emotion analysis tool.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.53,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031145,children emotion recognition;continuous emotion assessment;deep neural networks;emotional and behavioral disorders,Acoustics;Emotion recognition;Medical treatment;Pediatrics;Speaker recognition;Speech;Speech recognition,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
508,A Deep-Learning-Based Method of Estimating Water Intake,Y. Yamada; T. Saito; S. Kawasaki; D. Iketa; M. Katagiri; M. Nishimura; H. Mineno,,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),20170911.0,2017,2,,96,101,"In Japan, which has become a very aged society, the increasing burden of nursing care is an issue. Services and systems related to automatic recording of healthcare management of elderly people have been proposed in order to reduce the burden of nursing care. Water intake is one of the items necessary for healthcare management of elderly people. However, it is not currently automated, which is a burden on caregivers. In the case of the conventional method, the swallowing sound is used for estimating the water intake. However, the estimation error for each subject is large. Accuracy of estimated water intake is improved by using deep learning. Specifically, three features, namely, mel frequency cepstral coefficient (MFCC), duration of water intake, and a RASTA filter auditory spectrum, are extracted from a subject's swallowing sound (which is thought to be highly correlated with water intake). A method of estimating water intake, which considers abstract features that are difficult for people to find, is proposed and verified. It is experimentally shown that RMSE of water intake estimated by the proposed method using deep learning is reduced compared with that estimated by conventional methods.",0730-3157;07303157,Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0,10.1109/COMPSAC.2017.14,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029900,deep learning;estimation;healthcare;water intake,Feature extraction;Hidden Markov models;Lungs;Machine learning;Medical services;Microphones;Senior citizens,,,,,,,,,,4-8 July 2017,,IEEE,IEEE Conference Publications
509,Classification of various daily behaviors using deep learning and smart watch,M. C. Kwon; M. Ju; S. Choi,"Dept. of Secured Smart Electric Vehicle, Kookmin University, Seoul, Korea 02707",2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN),20170727.0,2017,,,735,740,"In traditional healthcare and therapy, human behavior has been classified into only two categories: specific behavior and active behavior. As internet of things and wearable devices become popular, however, it is necessary to classify human behavior into more various categories for providing useful services. In this paper, we propose a novel classification scheme that classifies human behavior into 11 different categories including active and inactive activities in daily life. We collect data with smart watch and use deep learning model with a neural network for the classification. Extensive evaluation shows that various daily human behavior can be classified with 99.24% accuracy, and that the classification of human behavior can be used for various services.",,Electronic:978-1-5090-4749-9; POD:978-1-5090-4750-5; USB:978-1-5090-4748-2,10.1109/ICUFN.2017.7993888,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7993888,classification;deep learning;human behavior;internet of things;smart watch,Acceleration;Feature extraction;Games;Intelligent sensors;Machine learning;Medical services,Internet of Things;behavioural sciences computing;medical computing;neural nets;patient monitoring;patient treatment;pattern classification;wearable computers,Internet of things;active behavior;classification scheme;daily behaviors classification;daily life;deep learning;healthcare;human behavior;inactive activities;neural network;smart watch;specific behavior;therapy;wearable devices,,,,,,,,4-7 July 2017,,IEEE,IEEE Conference Publications
510,Deep learning Parkinson's from smartphone data,C. Stamate; G. D. Magoulas; S. Kueppers; E. Nomikou; I. Daskalopoulos; M. U. Luchini; T. Moussouri; G. Roussos,"Birkbeck College, University of London, UK",2017 IEEE International Conference on Pervasive Computing and Communications (PerCom),20170504.0,2017,,,31,40,"The cloudUPDRS app is a Class I medical device, namely an active transient non-invasive instrument, certified by the Medicines and Healthcare products Regulatory Agency in the UK for the clinical assessment of the motor symptoms of Parkinson's Disease. The app follows closely the Unified Parkinson's Disease Rating Scale which is the most commonly used protocol in the clinical study of PD; can be used by patients and their carers at home or in the community; and, requires the user to perform a sequence of iterated movements which are recorded by the phone sensors. This paper discusses how the cloudUPDRS system addresses two key challenges towards meeting essential consistency and efficiency requirements, namely: (i) How to ensure high-quality data collection especially considering the unsupervised nature of the test, in particular, how to achieve firm user adherence to the prescribed movements; and (ii) How to reduce test duration from approximately 25 minutes typically required by an experienced patient, to below 4 minutes, a threshold identified as critical to obtain significant improvements in clinical compliance. To address the former, we combine a bespoke design of the user experience tailored so as to constrain context, with a deep learning approach used to identify failures to follow the movement protocol while at the same time limiting false positives to avoid unnecessary repetition. We address the latter by developing a machine learning approach to personalise assessments by selecting those elements of the UPDRS protocol that most closely match individual symptom profiles and thus offer the highest inferential power hence closely estimating the patent's overall UPRDS score.",,Electronic:978-1-5090-4327-9; POD:978-1-5090-4328-6,10.1109/PERCOM.2017.7917848,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917848,,Conferences;Diseases;Machine learning;Market research;Pervasive computing;Protocols;Sensors,cloud computing;learning (artificial intelligence);medical computing;medical disorders;smart phones,Medicines-and-Healthcare products Regulatory Agency;UK;UPDRS protocol;active transient noninvasive instrument;class-I medical device;clinical assessment;cloudUPDRS application;data collection;deep learning approach;false positives;machine learning approach;motor symptoms;smart phone data;test duration reduction;unified Parkinson's disease rating scale;user experience,,,,,,,,13-17 March 2017,,IEEE,IEEE Conference Publications
511,Semi-automated annotation of signal events in clinical EEG data,S. Yang; S. López; M. Golmohammadi; I. Obeid; J. Picone,"Neural Engineering Data Consortium, Temple University, Philadelphia, Pennsylvania, USA",2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),20170209.0,2016,,,1,5,"To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation.",,Electronic:978-1-5090-6713-8; POD:978-1-5090-6714-5,10.1109/SPMB.2016.7846855,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846855,,Brain models;Data models;Electroencephalography;Hidden Markov models;Sensitivity;Training,electroencephalography;health care;iterative methods;learning (artificial intelligence);medical signal processing,EEG corpus;clinical EEG data;confidence scores;deep learning;healthcare;high-performance automated decision support systems;initial training iteration;machine learning;signal events semi-automated annotation,,,,,,,,3-3 Dec. 2016,,IEEE,IEEE Conference Publications
512,The effects of deep network topology on mortality prediction,H. Du; M. M. Ghassemi; M. Feng,"School of Electrical and Electronic Engineering, Nanyang Technology University, Singapore",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,2602,2605,"Deep learning has achieved remarkable results in the areas of computer vision, speech recognition, natural language processing and most recently, even playing Go. The application of deep-learning to problems in healthcare, however, has gained attention only in recent years, and it's ultimate place at the bedside remains a topic of skeptical discussion. While there is a growing academic interest in the application of Machine Learning (ML) techniques to clinical problems, many in the clinical community see little incentive to upgrade from simpler methods, such as logistic regression, to deep learning. Logistic regression, after all, provides odds ratios, p-values and confidence intervals that allow for ease of interpretation, while deep nets are often seen as `black-boxes' which are difficult to understand and, as of yet, have not demonstrated performance levels far exceeding their simpler counterparts. If deep learning is to ever take a place at the bedside, it will require studies which (1) showcase the performance of deep-learning methods relative to other approaches and (2) interpret the relationships between network structure, model performance, features and outcomes. We have chosen these two requirements as the goal of this study. In our investigation, we utilized a publicly available EMR dataset of over 32,000 intensive care unit patients and trained a Deep Belief Network (DBN) to predict patient mortality at discharge. Utilizing an evolutionary algorithm, we demonstrate automated topology selection for DBNs. We demonstrate that with the correct topology selection, DBNs can achieve better prediction performance compared to several bench-marking methods.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7591263,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591263,,Bioinformatics;Genomics;Machine learning;Network topology;Neural networks;Topology;Training,belief networks;electronic health records;evolutionary computation;health care;learning (artificial intelligence);medical computing,EMR dataset;deep belief network;deep network topology;deep-learning methods;electronic medical record;evolutionary algorithm;healthcare;intensive care unit patients;network structure;patient mortality prediction,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
513,An adaptive deep learning approach for PPG-based identification,V. Jindal; J. Birjandtalab; M. B. Pouyan; M. Nourani,"Quality of Life Technology Laboratory, The University of Texas at Dallas, Richardson, TX 75080",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018.0,2016,,,6401,6404,"Wearable biosensors have become increasingly popular in healthcare due to their capabilities for low cost and long term biosignal monitoring. This paper presents a novel two-stage technique to offer biometric identification using these biosensors through Deep Belief Networks and Restricted Boltzman Machines. Our identification approach improves robustness in current monitoring procedures within clinical, e-health and fitness environments using Photoplethysmography (PPG) signals through deep learning classification models. The approach is tested on TROIKA dataset using 10-fold cross validation and achieved an accuracy of 96.1%.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7592193,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592193,,Biological system modeling;Biomedical monitoring;Brain modeling;Feature extraction;Machine learning;Neural networks;Training,Boltzmann machines;belief networks;biometrics (access control);biosensors;body sensor networks;health care;learning (artificial intelligence);medical signal processing;patient monitoring;photoplethysmography,10-fold cross validation;Deep Belief Networks;PPG-based identification;Restricted Boltzman Machines;TROIKA dataset;adaptive deep learning approach;biometric identification;biosignal monitoring;clinical environments;deep learning classification models;e-health environments;fitness environments;healthcare;photoplethysmography signals;two-stage technique;wearable biosensors,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
514,Deep learning for human activity recognition: A resource efficient implementation on low-power devices,D. Ravi; C. Wong; B. Lo; G. Z. Yang,"The Hamlyn Centre, Imperial College London, London",2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN),20160721.0,2016,,,71,76,"Human Activity Recognition provides valuable contextual information for wellbeing, healthcare, and sport applications. Over the past decades, many machine learning approaches have been proposed to identify activities from inertial sensor data for specific applications. Most methods, however, are designed for offline processing rather than processing on the sensor node. In this paper, a human activity recognition technique based on a deep learning methodology is designed to enable accurate and real-time classification for low-power wearable devices. To obtain invariance against changes in sensor orientation, sensor placement, and in sensor acquisition rates, we design a feature generation process that is applied to the spectral domain of the inertial data. Specifically, the proposed method uses sums of temporal convolutions of the transformed input. Accuracy of the proposed approach is evaluated against the current state-of-the-art methods using both laboratory and real world activity datasets. A systematic analysis of the feature generation parameters and a comparison of activity recognition computation times on mobile devices and sensor nodes are also presented.",,Electronic:978-1-5090-3087-3; POD:978-1-5090-3088-0,10.1109/BSN.2016.7516235,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516235,ActiveMiles;Deep Learning;HAR;Low-Power Devices,Convolution;Data mining;Feature extraction;Machine learning;Spectrogram;Time-frequency analysis,convolution;feature extraction;image recognition;learning (artificial intelligence);sensor placement,contextual information;deep learning;feature generation;human activity recognition;inertial sensor data;low-power wearable devices;machine learning;mobile devices;offline processing;resource efficient implementation;sensor acquisition rates;sensor nodes;sensor placement;spectral domain;temporal convolutions,,,,,,,,14-17 June 2016,,IEEE,IEEE Conference Publications
515,A restricted Boltzmann machine based two-lead electrocardiography classification,Y. Yan; X. Qin; Y. Wu; N. Zhang; J. Fan; L. Wang,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN),20151019.0,2015,,,1,9,"An restricted Boltzmann machine learning algorithm were proposed in the two-lead heart beat classification problem. ECG classification is a complex pattern recognition problem. The unsupervised learning algorithm of restricted Boltzmann machine is ideal in mining the massive unlabelled ECG wave beats collected in the heart healthcare monitoring applications. A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. In this paper a deep belief network was constructed and the RBM based algorithm was used in the classification problem. Under the recommended twelve classes by the ANSI/AAMI EC57: 1998/(R)2008 standard as the waveform labels, the algorithm was evaluated on the two-lead ECG dataset of MIT-BIH and gets the performance with accuracy of 98.829%. The proposed algorithm performed well in the two-lead ECG classification problem, which could be generalized to multi-lead unsupervised ECG classification or detection problems.",2376-8886;23768886,Electronic:978-1-4673-7201-5; POD:978-1-4673-7202-2,10.1109/BSN.2015.7299399,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299399,big data;deep belief network;electrocardiography classification;restricted Boltzmann machine,Accuracy;Data models;Electrocardiography;Feature extraction;Heart beat;Signal processing algorithms;Training,Boltzmann machines;diseases;electrocardiography;learning (artificial intelligence);medical signal processing;signal classification;stochastic processes,MIT-BIH arrhythmia database;RBM learning algorithm;complex pattern recognition;heart healthcare monitoring;massive unlabelled ECG wave beat;multilead unsupervised ECG classification;probability distribution;restricted Boltzmann machine;stochastic artificial neural network;two-lead ECG classification;two-lead ECG dataset;two-lead heart beat classification,,2.0,,31.0,,,,9-12 June 2015,,IEEE,IEEE Conference Publications
516,Inexpensive user tracking using Boltzmann Machines,E. Mocanu; D. C. Mocanu; H. B. Ammar; Z. Zivkovic; A. Liotta; E. Smirnov,"Department of Electrical Engineering, Eindhoven University of Technology, Netherlands","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",20141204.0,2014,,,1,6,"Inexpensive user tracking is an important problem in various application domains such as healthcare, human-computer interaction, energy savings, safety, robotics, security and so on. Yet, it cannot be easily solved due to its probabilistic nature, high level of abstraction and uncertainties, on the one side, and to the limitations of our current technologies and learning algorithms, on the other side. In this paper, we tackle this problem by using the Multi-integrated Sensor Technology, which comes at a low price. At the same time, we are aiming to address the lightweight learning requirements by investigating Factored Conditional Restricted Boltzmann Machines (FCRBMs), a form of Deep Learning, that has proven to be an efficient and effective machine learning framework. However, due to their construction properties, the conventional FCRBMs are only capable of performing predictions but are not capable of making classification. Herein, we are proposing extended FCRBMs (eFCRBMs), which incorporate a novel classification scheme, to solve this problem. Experiments performed on both artificially generated as well as real-world data demonstrate the effectiveness and efficiency of the proposed technique. We show that eFCRBMs outperform popular approaches including Support Vector Machines, Naive Bayes, AdaBoost, and Gaussian Mixture Models.",1062-922X;1062922X,Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1,10.1109/SMC.2014.6973875,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973875,,Computers;History;Neurons;Probabilistic logic;Robot sensing systems;TV;Time series analysis,Boltzmann machines;Gaussian processes;image classification;image fusion;learning (artificial intelligence);object detection;object tracking;recurrent neural nets;support vector machines,AdaBoost;Gaussian mixture models;classification scheme;deep learning;energy savings;extended FCRBM;factored conditional restricted Boltzmann machines;healthcare;human-computer interaction;inexpensive user tracking;learning requirements;machine learning framework;multiintegrated sensor technology;naive Bayes;people detection;people tracking;robotics;support vector machines,,4.0,,17.0,,,,5-8 Oct. 2014,,IEEE,IEEE Conference Publications
517,Predictive Modeling of Therapy Decisions in Metastatic Breast Cancer with Recurrent Neural Network Encoder and Multinomial Hierarchical Regression Decoder,Y. Yang; P. A. Fasching; V. Tresp,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,46,55,"The increasing availability of novel health-related data sources —e.g., from molecular analysis, health Apps and electronic health records— might eventually overwhelm the physician, and the community is investigating analytics approaches that might be useful to support clinical decisions. In particular, the success of the latest developments in Deep Learning has demonstrated that machine learning models are capable of handling —and actually profiting from— high dimensional and possibly sequential data. In this work, we propose an encoder-decoder network approach to model the physician's therapy decisions. Our approach also provides physicians with a list of similar historical patient cases to support the recommended decisions. By using a combination of a Recurrent Neural Network Encoder and a Multinomial Hierarchical Regression Decoder, we specifically tackle two common challenges in modeling clinical data:First, the issue of handling episodic data of variable lengths and, second, the need to represent hierarchical decision procedures. We conduct experiments on a large real-world dataset collected from thousands of metastatic breast cancer patients and show that our model outperforms more traditional approaches.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.51,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031131,Decision Support;Encoder-Decoder Framework;Hierarchical Regression;Recurrent Neural Networks,Breast cancer;Decoding;Metastasis;Predictive models;Surgery,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
518,Language-Based Process Phase Detection in the Trauma Resuscitation,Y. Gu; X. Li; S. Chen; H. Li; R. A. Farneth; I. Marsic; R. S. Burd,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,239,247,"Process phase detection has been widely used in surgical process modeling (SPM) to track process progression. These studies mostly used video and embedded sensor data, but spoken language also provides rich semantic information directly related to process progression. We present a long-short term memory (LSTM) deep learning model to predict trauma resuscitation phases using verbal communication logs. We first use an LSTM to extract the sentence meaning representations, and then sequentially feed them into another LSTM to extract the mean-ing of a sentence group within a time window. This information is ultimately used for phase prediction. We used 24 manually-transcribed trauma resuscitation cases to train, and the remain-ing 6 cases to test our model. We achieved 79.12% accuracy, and showed performance advantages over existing visual-audio systems for critical phases of the process. In addition to language information, we evaluated a multimodal phase prediction structure that also uses audio input. We finally identified the challenges of substituting manual transcription with automatic speech recognition in trauma resuscitation.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.50,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031153,LSTM;deep learning;process phase detection;semantic representation;verbal communication logs,Logic gates;Microphones;Phase detection;Semantics;Speech;Surgery,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
519,CVRT: Cognitive Visual Recognition Tracker,M. Velazquez; Y. Lee,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,31,38,"Studies on visual attention of patients with Alzheimer's disease and Dementia is a promising way for keeping track of the individual patient's image recognition ability over. This research seeks to expand upon the current applications of combining the Android operating system with TensorFlow by providing a visual question answering platform for image analysis. This application, Cognitive Visual Recognition Tracker (CVRT), provides an entry point by which the user can ask questions concerning any image of their choosing, and then receive cumulative metrics over time to better assess any diminishing cognitive ability (i.e. Alzheimer's patients). In this work, recurrent neural networks as well as semantic analysis are leveraged to provide an interactive VQA experience. One of the main objectives of CVRT is for physicians to be able to determine trends from patient data that could either be applicable to the individual patient, or to many patients if an aggregate is formed from many individual datasets. On an individual level, these metrics would provide a way for the physician to monitor daily cognitive capability, whereas on a grander scale, these joint datasets could be used to provide better overall treatment for the disease with the future inclusion of predictive analytics. The final contribution is an interactive metrics platform by which other users can assess the primary user's cognitive capacity based on features of their questioning, and to then provide them with accurate trending or possible remediation plans based on their condition.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.65,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031129,Alzheimer's disease and Dementia;Deep Learning;Visual Question Answering,Conferences;Informatics;Medical services,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
520,Automated EEG-Based Epileptic Seizure Detection Using Deep Neural Networks,J. Birjandtalab; M. Heydarzadeh; M. Nourani,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,552,555,"Millions of people around the world suffer from epilepsy. It is very important to provide a method to efficiently monitor the seizures and alert the caregivers to help patients. It is proven that EEG signals are the best markers for diagnosis of the epileptic seizures. In this paper, we used the frequency domain features (normalized in-band power spectral density) to extract information from EEG signals. We applied a deep learning technique based on multilayer perceptrons to improve the accuracy of seizure detection. The results indicate that our nonlinear technique is able to efficiently and automatically detect seizure and non-seizure episodes with an F-measure accuracy of around 95%.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.55,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031211,Deep Neural Networks;EEG signals;Feature extraction;Multi Layer Perceptron;Seizure detection,Biological neural networks;Electroencephalography;Feature extraction;Multilayer perceptrons;Training,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
521,Medical Concept Normalization for Online User-Generated Texts,K. Lee; S. A. Hasan; O. Farri; A. Choudhary; A. Agrawal,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,462,469,"Social media has become an important tool for sharing content in the last decade. People often talk about their experiences and opinions on different health-related issues e.g. they write reviews on medications, describe symptoms and ask informal questions about various health concerns. Due to the colloquial nature of the languages used in the social media, it is often difficult for an automated system to accurately interpret them for appropriate clinical understanding. To address this challenge, this paper proposes a novel approach for medical concept normalization of user-generated texts to map a health condition described in the colloquial language to a medical concept defined in standard clinical terminologies. We use multiple deep learning architectures such as convolutional neural networks (CNN) and recurrent neural networks (RNN) with input word embeddings trained on various clinical domain-specific knowledge sources. Extensive experiments on two benchmark datasets demonstrate that the proposed models can achieve up to 21.28% accuracy improvements over the existing models when we use the combination of all knowledge sources to learn neural embeddings.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.59,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031195,deep learning;medical concept normalization;social media,Drugs;Hair;Hidden Markov models;Medical diagnostic imaging;Pain;Recurrent neural networks;Social network services,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
522,Deep Learning Based Recognition of Meltdown in Autistic Kids,V. S. P. Patnam; F. T. George; K. George; A. Verma,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,391,396,"Children with autism often experience sudden meltdowns which not only makes the moment tough for the caretakers/parents but also make the children hurt themselves physically. Studies have discovered that children with autistic spectrum disorder exhibit certain actions through which we can anticipate mutilating meltdowns in them. The objective of our project is to build a system that can recognize such kind of actions using deep learning techniques thereby, notifying the caretakers/parents so that they can get the situation under control in lesser time. Using deep learning RCNNs, we can train the system faster yet reliable because unlike all the machine learning algorithms, deep learning algorithms are more efficient and have more scope into future. We have trained a classifier on images that are gathered from videos and reliable internet sources with most predictive gestures, through which we can detect the meltdowns more precisely. We have trained a model that validated the accuracy by ~93% which is accompanied by a loss/train classifier with a minimal 0.4% loss. Functional testing was done through feeding the deep neural network with chosen actions performed by five individuals that resulted in an accuracy of ~92% in all cases, which can assure the real-time usage of the system.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.35,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031180,Autistic spectrum disorder;Convolution Neural Network;Graphics Processing Unit;deep learning;inference;training classifiers,Autism;Convolution;Databases;Ear;Machine learning;Neural networks;Training,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
523,Extracting Drug-Drug Interactions with Word and Character-Level Recurrent Neural Networks,R. Kavuluru; A. Rios; T. Tran,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914.0,2017,,,5,12,"Drug-drug interactions (DDIs) are known to be responsible for nearly a third of all adverse drug reactions. Hence several current efforts focus on extracting signal from EMRs to prioritize DDIs that need further exploration. To this end, being able to extract explicit mentions of DDIs in free text narratives is an important task. In this paper, we explore recurrent neural network (RNN) architectures to detect and classify DDIs from unstructured text using the DDIExtraction dataset from the SemEval 2013 (task 9) shared task. Our methods are in line with those used in other recent deep learning efforts for relation extraction including DDI extraction. However, to our knowledge, we are the first to investigate the potential of character-level RNNs (Char-RNNs) for DDI extraction (and relation extraction in general). Furthermore, we explore a simple but effective model bootstrapping method to (a). build model averaging ensembles, (b). derive confidence intervals around mean micro-F scores (MMF), and (c). assess the average behavior of our methods. Without any rule based filtering of negative examples, a popular heuristic used by most earlier efforts, we achieve an MMF of 69.13. By adding simple replicable heuristics to filter negative instances we are able to achieve an MMF of 70.38. Furthermore, our best ensembles produce micro F-scores of 70.81 (without filtering) and 72.13 (with filtering), which are superior to metrics reported in published results. Although Char-RNNs turnout to be inferior to regular word based RNN models in overall comparisons, we find that ensembling models from both architectures results in nontrivial gains over simply using either alone, indicating that they complement each other.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.15,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031125,drug-drug interactions;recurrent neural networks;relation classification,Computer architecture;Drugs;Kernel;Logic gates;Recurrent neural networks;Support vector machines,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
524,Big Social Data Analytics for Public Health: Predicting Facebook Post Performance Using Artificial Neural Networks and Deep Learning,N. Straton; R. R. Mukkamala; R. Vatrapu,,2017 IEEE International Congress on Big Data (BigData Congress),20170911.0,2017,,,89,96,"Facebook ""post popularity"" analysis is fundamental for differentiating between relevant posts and posts with low user engagement and consequently their characteristics. This research study aims at health and care organizations to improve information dissemination on social media platforms by reducing clutter and noise. At the same time, it will help users navigate through vast amount of information in direction of the relevant health and care content. Furthermore, study explores prediction of popularity of healthcare posts on the largest social media platform Facebook. Methodology is presented in this paper to predict user engagement based on eleven characteristics of the post: Post Type, Hour Span, Facebook Wall Category, Level, Country, isHoliday, Season, Created Year, Month, Day of the Week, Time of the Day. Finally, post performance prediction is conducted using Artificial Neural Networks (ANN) and Deep Neural Networks (DNN). Different network topology measures are used to achieve best accuracy prediction followed by examples and discussion on why DNN might not be optimal technique for the given data set.",,Electronic:978-1-5386-1996-4; POD:978-1-5386-1997-1; USB:978-1-5386-1995-7,10.1109/BigDataCongress.2017.21,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029313,Artificial Neural Network (ANN);Deep Neural Network (DNN);Negative Entropy;Post Performance;Purity,Data analysis;Entropy;Facebook;Neural networks;Public healthcare,,,,,,,,,,25-30 June 2017,,IEEE,IEEE Conference Publications
525,Deep Temporal Multimodal Fusion for Medical Procedure Monitoring using Wearable Sensors,E. A. Bernal; X. Yang; Q. Li; J. Kumar; S. Madhvanath; P. Ramesh; R. Bala,"Decision Support and Machine Intelligence, United Technologies Research Center, 129535 East Hartford, Connecticut United States (e-mail: eabernal@gmail.com)",IEEE Transactions on Multimedia,,2017,PP,99.0,1,1,"Process monitoring and verification have a wide range of uses in the medical and healthcare fields. Currently, such tasks are often carried out by a trained specialist, which makes them expensive, inefficient, and time-consuming. Recent advances in automated video- and multimodal-data-based action and activity recognition have made it possible to reduce the extent of manual intervention required to effectively carry out process supervision tasks. In this paper, we propose algorithms for automated egocentric human action and activity recognition from multimodal data, with a target application of monitoring and assisting a user perform a multi-step medical procedure. We propose a supervised deep multimodal fusion framework that relies on concurrent processing of motion data acquired with wearable sensors and video data acquired with an egocentric or body-mounted camera. We demonstrate the effectiveness of the algorithm on a public multimodal dataset and conclude that automated process monitoring via the use of multiple heterogeneous sensors is a viable alternative to its manual counterpart. Furthermore, we demonstrate that the application of previously proposed adaptive sampling schemes to the video processing branch of the multimodal framework results in significant performance improvements.",1520-9210;15209210,,10.1109/TMM.2017.2726187,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976382,Wearable sensors;action and activity recognition;deep learning;deep temporal fusion;egocentric vision;hand localization;medical procedures;multimodal fusion,Activity recognition;Biomedical monitoring;Data integration;Medical services;Monitoring;Wearable sensors,,,,,,,,,20170712.0,,,IEEE,IEEE Early Access Articles
526,FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition,H. Ding; S. K. Zhou; R. Chellappa,"Univ. of Maryland, College Park, MD, USA",2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),20170629.0,2017,,,118,126,"Relatively small data sets available for expression recognition research make the training of deep networks very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pretrained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization results show that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu- CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.",,Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7,10.1109/FG.2017.23,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961731,,Convolution;Distribution functions;Face;Face recognition;Image recognition;Neurons;Training,convolution;data visualisation;emotion recognition;face recognition;learning (artificial intelligence),CK+;FaceNet2ExpNet;Oulu- CASIA;SFEW;TFD;convolutional layer training;deep convolutional neural networks;deep face recognition net;deep network training;expression recognition;static images;visualization,,,,,,,,May 30 2017-June 3 2017,,IEEE,IEEE Conference Publications
527,The chatbot feels you - a counseling service using emotional response generation,Dongkeon Lee; Kyo-Joong Oh; Ho-Jin Choi,"School of Computing, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320.0,2017,,,437,440,"Early study tries to use chatbot for counseling services. They changed drinking habit of who being consulted by leading them via intervene chatbot. However, the application did not concerned about psychiatric status through continuous conversation with user monitoring. Furthermore, they had no ethical judgment method that about the intervention of the chatbot. We argue that more reasonable and continuous emotion recognition will make better mental healthcare experiment. It will be more proper clinical psychiatric consolation in ethical view as well. This paper suggests a introduce a novel chatbot system for psychiatric counseling service. Our system understands content of conversation based on recent natural language processing (NLP) methods with emotion recognition. It senses emotional flow through the continuous observation of conversation. Also, we generate personalized counseling response from user input, to do this, we use additional constrains to generation model for the proper response generation which can detect conversational context, user emotion and expected reaction.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881752,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881752,conversational service;deep learning;response generation,Context;Data models;Emotion recognition;Employee welfare;Medical services;Natural language processing;Probabilistic logic,emotion recognition;ethical aspects;health care;natural language processing;psychology,NLP;clinical psychiatric consolation;continuous conversation observation;continuous emotion recognition;conversation content;drinking habit;emotional flow;emotional response generation;ethical view;expected reaction;intervene chatbot;mental healthcare experiment;natural language processing;personalized counseling response;psychiatric counseling service;user emotion;user input,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
528,Deep Decision Network for Multi-class Image Classification,V. N. Murthy; V. Singh; T. Chen; R. Manmatha; D. Comaniciu,"Sch. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212.0,2016,,,2240,2248,"In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.246,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780615,,Airplanes;Covariance matrices;Decision trees;Machine learning;Symmetric matrices;Training;Vegetation,image classification;learning (artificial intelligence);trees (mathematics),CIFAR-100;DDN;deep decision network;deep learning network;multiclass image classification;tree-like structured network,,,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
529,Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy,J. Liang; R. Lu; C. Zhang; F. Wang,"Dept. of Autom., Tsinghua Univ., Beijing, China",2016 IEEE International Conference on Healthcare Informatics (ICHI),20161208.0,2016,,,184,191,"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt ""auras"". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.",,Electronic:978-1-5090-6117-4; POD:978-1-5090-6118-1,10.1109/ICHI.2016.27,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776343,,Brain models;Electroencephalography;Epilepsy;Feature extraction;Machine learning;Training,brain;electroencephalography;knowledge management;learning (artificial intelligence);medical disorders;medical signal processing;neural nets,American Epilepsy Society Seizure Prediction Challenge;EEG datasets;EEG signals;brain disorder;deep learning techniques;deep neural network pretraining;electroencephalography recordings;epilepsy patients;knowledge transfer strategy;medical data resource utilization;seizure prediction;speech analysis;spontaneous seizures;transfer learning strategy,,,,,,,,4-7 Oct. 2016,,IEEE,IEEE Conference Publications
530,Voice Pathology Detection Using Deep Learning: a Preliminary Study,P. Harar; J. B. Alonso-Hernandezy; J. Mekyska; Z. Galaz; R. Burget; Z. Smekal,"Department of Telecommunications Brno University of Technology, Technicka 10, 61600 Brno, Czech Republic",2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI),20170724.0,2017,,,1,4,This paper describes a preliminary investigation of Voice Pathology Detection using Deep Neural Networks (DNN). We used voice recordings of sustained vowel /a/ produced at normal pitch from German corpus Saarbruecken Voice Database (SVD). This corpus contains voice recordings and electroglottograph signals of more than 2 000 speakers. The idea behind this experiment is the use of convolutional layers in combination with recurrent Long-Short-Term-Memory (LSTM) layers on raw audio signal. Each recording was split into 64 ms Hamming windowed segments with 30 ms overlap. Our trained model achieved 71.36% accuracy with 65.04% sensitivity and 77.67% specificity on 206 validation files and 68.08% accuracy with 66.75% sensitivity and 77.89% specificity on 874 testing files. This is a promising result in favor of this approach because it is comparable to similar previously published experiment that used different methodology. Further investigation is needed to achieve the state-of-the-art results.,,Electronic:978-1-5386-0850-0; POD:978-1-5386-0851-7,10.1109/IWOBI.2017.7985525,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985525,,Convolution;Databases;Feature extraction;Pathology;Support vector machines;Testing;Training,audio signal processing;bioelectric phenomena;electric impedance measurement;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;speech;speech processing,German corpus Saarbruecken voice database;Hamming windowed segments;audio signal;convolutional layers;deep learning;deep neural networks;electroglottograph signals;long-short-term-memory layers;sustained vowel;voice pathology detection;voice recordings,,,,,,,,10-12 July 2017,,IEEE,IEEE Conference Publications
531,Deep Learning for Automated Extraction of Primary Sites from Cancer Pathology Reports,J. Qiu; H. J. Yoon; P. A. Fearn; G. D. Tourassi,"University of Tennessee, Knoxville, TN, 37996, and Health Data Sciences Institute, Oak Ridge National Laboratory, Oak Ridge, TN 37831 (e-mail: jqiu1@utk.edu)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study we investigated deep learning and a convolutional neural network (CNN), for extracting ICDO- 3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro and macro-F score increases of up to 0.132 and 0.226 respectively when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.",2168-2194;21682194,,10.1109/JBHI.2017.2700722,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918552,Deep learning;convolutional neural network;information extraction;natural language processing;pathology reports;primary cancer site,,,,,,,,,,20170503.0,,,IEEE,IEEE Early Access Articles
532,Deep learning based Nucleus Classification in pancreas histological images,Y. H. Chang; G. Thibault; O. Madin; V. Azimi; C. Meyers; B. Johnson; J. Link; A. Margolin; J. W. Gray,"Oregon Health and Science University (OHSU), Portland, United States of America",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,672,675,"Tumor specimens contain a variety of healthy cells as well as cancerous cells, and this heterogeneity underlies resistance to various cancer therapies. But this problem has not been thoroughly investigated until recently. Meanwhile, technological breakthroughs in imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples, and modern machine learning approaches including deep learning have been shown to produce encouraging results by finding hidden structures and make accurate predictions. In this paper, we propose a Deep learning based Nucleus Classification (DeepNC) approach using paired histopathology and immunofluorescence images (for label), and demonstrate its classification prediction power. This method can solve current issue on discrepancy between genomic- or transcriptomic-based and pathology-based tumor purity estimates by improving histological evaluation. We also explain challenges in training a deep learning model for huge dataset.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036914,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036914,Deep Learning;Histopathology;Immunofluorescence;Segmentation,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
533,Epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images,Y. Huang; H. ZHENG; C. LIU; X. Ding; G. Rohde,"Electrical Engineering Department and Biomedical Engineering Department, University of Virginia, VA, U.S.A.",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99.0,1,1,"Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.",2168-2194;21682194,,10.1109/JBHI.2017.2691738,CCF-TENCENT; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893702,convolutional neural networks;domain adaptation;epitheliumstroma classification;histopathological image analysis;transfer learning,Adaptation models;Feature extraction;Image analysis;Kernel;Machine learning;Neural networks;Training,,,,,,,,,20170406.0,,,IEEE,IEEE Early Access Articles
534,Mixed Neural Network Approach for Temporal Sleep Stage Classification,H. Dong; A. Supratak; W. Pan; C. Wu; P. M. Matthews; Y. Guo,"Department of Computing, Imperial College London, London, SW7 2AZ, UK.",IEEE Transactions on Neural Systems and Rehabilitation Engineering,,2017,PP,99.0,1,1,"This paper proposes a practical approach to addressing limitations posed by using of single-channel electroencephalography (EEG) for sleep stage classification. EEG-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages at lower cost in the home. However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which are both uncomfortable and difficult to position. Previous studies of sleep stage scoring that used only frontal electrodes with a hierarchical decision tree motivated this paper, in which we have taken advantage of rectifier neural network for detecting hierarchical features and long short-term memory (LSTM) network for sequential data learning to optimize classification performance with single-channel recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electrooculogram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm than is available from existing approaches with vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical.",1534-4320;15344320,,10.1109/TNSRE.2017.2733220,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995122,Deep learning;EEG signal;Long short-term memory;Sleep stage classification;electroencephalography,Electrodes;Electroencephalography;Electrooculography;Feature extraction;Sleep;Standards,,,,,,,,,20170728.0,,,IEEE,IEEE Early Access Articles
535,Evaluations of deep convolutional neural networks for automatic identification of malaria infected cells,Y. Dong; Z. Jiang; H. Shen; W. David Pan; L. A. Williams; V. V. B. Reddy; W. H. Benjamin; A. W. Bryan,"Dept. of Electrical and Computer Engineering, University of Alabama in Huntsville, Huntsville, AL 35899, USA",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413.0,2017,,,101,104,"This paper studied automatic identification of malaria infected cells using deep learning methods. We used whole slide images of thin blood stains to compile an dataset of malaria-infected red blood cells and non-infected cells, as labeled by a group of four pathologists. We evaluated three types of well-known convolutional neural networks, including the LeNet, AlexNet and GoogLeNet. Simulation results showed that all these deep convolution neural networks achieved classification accuracies of over 95%, higher than the accuracy of about 92% attainable by using the support vector machine method. Moreover, the deep learning methods have the advantage of being able to automatically learn the features from the input data, thereby requiring minimal inputs from human experts for automated malaria diagnosis.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897215,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897215,,Diseases;Feature extraction;Machine learning;Neural networks;Support vector machines;Testing;Training,biology computing;cellular biophysics;diseases;neural nets;support vector machines,AlexNet;GoogLeNet;cell automatic identification;deep convolutional neural networks;deep learning;malaria infected cells;support vector machine;thin blood stains;whole slide images,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
536,Automatic Feature Learning Method for Detection of Retinal Landmarks,B. Al-Bander; W. Al-Nuaimy; M. A. Al-Taee; A. Al-Ataby; Y. Zheng,,2016 9th International Conference on Developments in eSystems Engineering (DeSE),20170518.0,2016,,,13,18,"This paper presents an automatic deep learning method for location detection of important retinal landmarks, the fovea and optic disc (OD) in digital fundus retinal images with the potential for use in an automated screening and grading system. The proposed method is based on deep convolutional neural networks (CNN) and does not depend the visual appearance or anatomical features of the retinal landmarks. It comprises convolution, max-pooling, fully connected and dropout layers as well as an output layer. The CNN is trained using an existing dataset images along with their annotated locations of the foveal and OD centres. Performance of the network is evaluated using Root Mean Square Error (RMSE). The developed feature learning-based approach presents promising system for retinal landmarks detection.",,Electronic:978-1-5090-5487-9; POD:978-1-5090-5488-6,10.1109/DeSE.2016.4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930616,Automatic feature learning;automated grading;convolutional neural network;deep learning;retinal landmarks,Adaptive optics;Feature extraction;Neural networks;Optical filters;Optical imaging;Retina;Training,,,,,,,,,,Aug. 31 2016-Sept. 2 2016,,IEEE,IEEE Conference Publications
537,Cancer Classification Based on Microarray Gene Expression Data Using Deep Learning,P. Guillen; J. Ebalunode,"Center for Adv. Comput. & Data Syst., Univ. of Houston, Houston, TX, USA",2016 International Conference on Computational Science and Computational Intelligence (CSCI),20170320.0,2016,,,1403,1405,"The classification of cancer is a major research topic in Medicine. Cancer microarray data normally contains a small number of samples, which have a large number of gene expression levels as features, however, makes the classification quite challenging. Using a deep learning algorithm based on multilayer perceptron, we show that classification performance at least as good as published results can be obtained for cancer classification.",,Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1,10.1109/CSCI.2016.0270,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881563,cancer;classification;deep learning;machine learning;microarray gene expression,Cancer;Computer architecture;Gene expression;Machine learning;Multilayer perceptrons;Training;Tumors,bioinformatics;cancer;genetics;multilayer perceptrons;pattern classification,cancer classification;deep learning algorithm;microarray gene expression data;multilayer perceptron,,,,,,,,15-17 Dec. 2016,,IEEE,IEEE Conference Publications
538,A predictive model of gene expression using a deep learning framework,Rui Xie; A. Quitadamo; J. Cheng; Xinghua Shi,"Department of Computer Science, University of Missouri at Columbia, 65201, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,676,681,"With an unprecedented amount of data available, it is important to explore new methods for developing predictive models to mine this data for scientific discoveries. In this study, we propose a deep learning regression model based on MultiLayer Perceptron and Stacked Denoising Auto-encoder (MLP-SAE) to predict gene expression from genotypes of genetic variation. Specifically, we use a stacked denoising auto-encoder to train our regression model in order to extract useful features, and utilize the multilayer perceptron for backpropagation. We further improve our model by adding a dropout technique to prevent overfitting. Our results on a real genomic dataset show that our MLP-SAE model with dropout outperform Lasso, Random Forests, and MLP-SAE without dropout. Our study provides a new application of deep learning in mining genomics data, and demonstrates that deep learning has great potentials in building predictive models to help understand biological systems.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822599,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822599,,Data models;Gene expression;Machine learning;Mathematical model;Predictive models;Training,backpropagation;biology computing;data mining;feature extraction;genetics;genomics;multilayer perceptrons;regression analysis,MLP-SAE model;backpropagation;biological systems;deep learning regression model;dropout technique;feature extraction;gene expression;genetic variation genotypes;genomic dataset;genomics data mining;multilayer perceptron;stacked denoising autoencoder,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
539,"Learning structure in gene expression data using deep architectures, with an application to gene clustering",A. Gupta; H. Wang; M. Ganapathiraju,"Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, USA",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,1328,1335,"Genes play a central role in all biological processes. DNA microarray technology has made it possible to study the expression behavior of thousands of genes in one go. Often, gene expression data is used to generate features for supervised and unsupervised learning tasks. At the same time, advances in the field of deep learning have made available a plethora of architectures. In this paper, we use deep architectures pre-trained in an unsupervised manner using denoising autoencoders as a preprocessing step for a popular unsupervised learning task. Denoising autoencoders (DA) can be used to learn a compact representation of input, and have been used to generate features for further supervised learning tasks. We propose that our deep architectures can be treated as empirical versions of Deep Belief Networks (DBNs). We use our deep architectures to regenerate gene expression time series data for two different data sets. We test our hypothesis on two popular datasets for the unsupervised learning task of clustering and find promising improvements in performance.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359871,autoencoders;deep learning;gene clustering;gene expression,Noise reduction,genetics;learning (artificial intelligence);neural nets;pattern clustering,DNA microarray technology;Deep Belief Networks;biological processes;deep architectures;deep learning;denoising autoencoders;gene clustering;gene expression time series data;learning structure;learning tasks,,,,20.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
540,A Deep Learning Model for Epigenomic Studies,G. Lo Bosco; R. Rizzo; A. Fiannaca; M. La Rosa; A. Urso,"Dept. of Math. & Comput. Sci. UNIPA, Univ. of Palermo Palermo, Palermo, Italy",2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),20170424.0,2016,,,688,692,"Epigenetics is the study of heritable changes in gene expression that does not involve changes to the underlying DNA sequence, i.e. a change in phenotype not involved by a change in genotype. At least three main factor seems responsible for epigenetic change including DNA methylation, histone modification and non-coding RNA, each one sharing having the same property to affect the dynamic of the chromatin structure by acting on Nucleosomes position. A nucleosome is a DNA-histone complex, where around 150 base pairs of double-stranded DNA is wrapped. The role of nucleosomes is to pack the DNA into the nucleus of the Eukaryote cells, to form the Chromatin. Nucleosome positioning plays an important role in gene regulation and several studies shows that distinct DNA sequence features have been identified to be associated with nucleosome presence. Starting from this suggestion, the identification of nucleosomes on a genomic scale has been successfully performed by DNA sequence features representation and classical supervised classification methods such as Support Vector Machines, Logistic regression and so on. Taking in consideration the successful application of the deep neural networks on several challenging classification problems, in this paper we want to study how deep learning network can help in the identification of nucleosomes.",,Electronic:978-1-5090-5698-9; POD:978-1-5090-5699-6,10.1109/SITIS.2016.115,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907542,Classification;Deep Learning;Nucleosome Positioning,Bioinformatics;DNA;Genomics;Kernel;Machine learning;Neural networks;Support vector machines,biology computing;genomics;learning (artificial intelligence);molecular biophysics;neural nets,DNA methylation;DNA sequence;Eukaryote cells;deep learning model;deep learning network;deep neural networks;epigenomic studies;gene expression;histone modification;noncoding RNA;nucleosomes,,,,,,,,Nov. 28 2016-Dec. 1 2016,,IEEE,IEEE Conference Publications
541,A deep learning model for predicting transcription factor binding location at single nucleotide resolution,S. Salekin; J. M. Zhang; Y. Huang,"Electrical and Computer Engineering Department, University of Texas at San Antonio, 1 UTSA Circle, San Antonio, TX 78249, USA",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413.0,2017,,,57,60,"Transcriptional regulation by transcription factors (TFs) plays a pivotal role in controlling the gene expression. However, understanding the mechanism through which the transcription factors regulate the gene expression is a challenging task. This is primarily hindered by the low specificity in identifying transcription factor binding sites (TFBS). The emergence of the ChIP-exonuclease (ChIP-exo) method enables the detection of TFBS at single nucleotide sensitivity, providing us an opportunity to study the detailed mechanisms of TF regulation. Nevertheless, there is still a lack of computational tools that can also provide single base pair (bp) resolution prediction of TFBS. In this paper, we propose DeepSNR, a Deep Learning algorithm for Single Nucleotide Resolution prediction of transcription factor binding site. Our proposed method is inspired by the similarity between predicting the specific binding location from input nucleotide sequence and image segmentation. Particularly, we adopted the deconvolution network (deconvNet); a deep learning model designed for image segmentation, and developed a TFBS specific deconvNet architecture constructed on top of `DeepBind'. We trained a deconvNet for predicting CTCF binding sites using the data from ChIP-exo experiments. The proposed algorithm achieved median precision and recall of 87% and 77% respectively, significantly outperforming motif search based algorithms such as MatInspector.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897204,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897204,,Algorithm design and analysis;Bioinformatics;DNA;Deconvolution;Genomics;Machine learning;Training,biological techniques;biology computing;deconvolution;enzymes;genetics;image segmentation;learning (artificial intelligence);molecular biophysics,CTCF binding sites;ChIP-exonuclease method;DeepBind;DeepSNR;TFBS specific deconvNet architecture;deconvolution network;deep learning model;gene expression;image segmentation;input nucleotide sequence;single nucleotide resolution;single nucleotide sensitivity;transcription factor binding sites,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
542,DP-miRNA: An improved prediction of precursor microRNA using deep learning model,J. Thomas; S. Thomas; L. Sael,"Department of Computer Science, Stony Brook University, NY 11794, USA",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320.0,2017,,,96,99,"MicroRNA (miRNA) are small non-coding RNAs regulating gene expression at the post-transcriptional level. Detecting miRNA in a genome is challenging experimentally and results vary depending on their cellular environment. These limitations inspire the development of knowledge-based prediction method. This paper proposes a deep learning based classification model for predicting precursor miRNA sequence that contains the miRNA sequence. The feature set consists of sequence features, folding measures, stem-loop features and statistical features. We evaluate the performance of the proposed method on human dataset. The deep neural network based classification outperformed support vector machine, neural network, naive Bayes classifiers, k-nearest neighbors, random forests as well as hybrid systems combining SVM and genetic algorithm.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881722,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881722,,Computer science;Machine learning;Neural networks;Predictive models;Proteins;Support vector machines;Training,Bayes methods;RNA;biology computing;cellular biophysics;genetic algorithms;knowledge based systems;learning (artificial intelligence);neural nets;random processes;statistical analysis;support vector machines,DP-miRNA;Naive Bayes classifiers;SVM;cellular environment;deep learning based classification model;deep learning model;deep neural network based classification;folding measures;gene expression;genetic algorithm;genome miRNA detection;human dataset;hybrid systems;k-nearest neighbors;knowledge-based prediction method;noncoding RNA;post-transcriptional level;precursor microRNA prediction;random forests;sequence features;statistical features;stem-loop features;support vector machine,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
543,Multimodal Deep Boltzmann Machines for feature selection on gene expression data,A. F. Syafiandini; I. Wasito; S. Yazid; A. Fitriawan; M. Amien,"Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia",2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS),20170309.0,2016,,,407,412,"In this paper, multimodal Deep Boltzmann Machines (DBM) is employed to learn important genes (biomarkers) on gene expression data from human carcinoma colorectal. The learning process involves gene expression data and several patient phenotypes such as lymph node and distant metastasis occurrence. The proposed framework in this paper uses multimodal DBM to train records with metastasis occurrence. Later, the trained model is tested using records with no metastasis occurrence. After that, Mean Squared Error (MSE) is measured from the reconstructed and the original gene expression data. Genes are ranked based on the MSE value. The first gene has the highest MSE value. After that, k-means clustering is performed using various number of genes. Features that give the highest purity index are considered as the important genes. The important genes obtained from the proposed framework and two sample t-test are being compared. From the accuracy of metastasis classification, the proposed framework gives higher results compared to the top genes from two sample t-test.",,Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7,10.1109/ICACSIS.2016.7872733,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872733,gene expression;microarray;multimodal DBM,Biomarkers;Gene expression;Lymph nodes;Metastasis;Support vector machines;Testing;Training,Boltzmann machines;bioinformatics;feature selection;genetics;learning (artificial intelligence);mean square error methods;pattern classification,DBM;MSE;biomarkers;feature selection;gene expression data;human carcinoma colorectal;k-means clustering;learning process;mean squared error;metastasis classification;multimodal deep Boltzmann machines;patient phenotypes,,,,,,,,15-16 Oct. 2016,,IEEE,IEEE Conference Publications
544,Cancer subtype identification using deep learning approach,A. F. Syafiandini; I. Wasito; S. Yazid; A. Fitriawan; M. Amien,"Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia","2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA)",20170228.0,2016,,,108,112,"In this paper, a framework using deep learning approach is proposed to identify two subtypes of human colorectal carcinoma cancer. The identification process uses information from gene expression and clinical data which is obtained from data integration process. One of deep learning architecture, multimodal Deep Boltzmann Machines (DBM) is used for data integration process. The joint representation gene expression and clinical is later used as Restricted Boltzmann Machines (RBM) input for cancer subtype identification. Kaplan Meier survival analysis is employed to evaluate the identification result. The curves on survival plot obtained from Kaplan Meier analysis are tested using three statistic tests to ensure that there is a significant difference between those curves. According to Log Rank, Generalized Wilcoxon and Tarone-Ware, the two groups of patients with different cancer subtypes identified using the proposed framework are significantly different.",,Electronic:978-1-5090-2323-3; POD:978-1-5090-2324-0; USB:978-1-5090-2322-6,10.1109/IC3INA.2016.7863033,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863033,RBM;cancer subtype;deep learning;multimodal DBM,Computer architecture;Data integration;Data mining;Gene expression;Machine learning;Metastasis,Boltzmann machines;cancer;data integration;learning (artificial intelligence);medical computing;statistical analysis,Kaplan Meier survival analysis;Log Rank;RBM;Tarone-Ware;cancer subtype identification;clinical data;data integration process;deep learning approach;gene expression;generalized Wilcoxon;human colorectal carcinoma cancer;restricted Boltzmann machines,,,,,,,,3-5 Oct. 2016,,IEEE,IEEE Conference Publications
545,Inferring Gene Regulatory Networks by Combining Supervised and Unsupervised Methods,T. Turki; J. T. L. Wang; I. Rajikhan,"Comput. Sci. Dept., King Abdulaziz Univ., Jeddah, Saudi Arabia",2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),20170202.0,2016,,,140,145,"Supervised methods for inferring gene regulatory networks (GRNs) perform well with good training data. However, when training data is absent, these methods are not applicable. Unsupervised methods do not need training data but their accuracy is low. In this paper, we combine supervised and unsupervised methods to infer GRNs using time-series gene expression data. Specifically, we use results obtained from unsupervised methods to train supervised methods. Since the results contain noise, we develop a data cleaning algorithm to remove noise, hence improving the quality of the training data. These refined training data are then used to guide classifiers including support vector machines and deep learning tools to infer GRNs through link prediction. Experimental results on several data sets demonstrate the good performance of the classifiers and the effectiveness of our data cleaning algorithm.",,Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6,10.1109/ICMLA.2016.0031,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838135,data mining;machine learning;network inference;systems biology,Classification algorithms;Cleaning;Gene expression;Inference algorithms;Mathematical model;Training;Training data,bioinformatics;data handling;genetics;network theory (graphs);pattern classification;time series;unsupervised learning,GRN;classifiers;data cleaning;deep learning tools;gene regulatory networks;link prediction;noise removal;support vector machines;time-series gene expression data;unsupervised methods,,,,,,,,18-20 Dec. 2016,,IEEE,IEEE Conference Publications
546,DeepEnhancer: Predicting enhancers by convolutional neural networks,Xu Min; Ning Chen; Ting Chen; Rui Jiang,"MOE Key Laboratory of Bioinformatics, Bioinformatics Division and Center for Synthetic & Systems Biology, TNLIST, Department of Automation, Tsinghua University, Beijing 100084, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,637,644,"Enhancers are crucial to the understanding of mechanisms underlying gene transcriptional regulation. Although having been successfully applied in such projects as ENCODE and Roadmap to generate landscape of enhancers in human cell lines, high-throughput biological experimental techniques are still costly and time consuming for even larger scale identification of enhancers across a variety of tissues under different disease status, making computational identification of enhancers indispensable. In this paper, we propose a computational framework, named DeepEnhancer, to classify enhancers from background genomic sequences. We construct convolutional neural networks of various architectures and compare the classification performance with traditional sequence-based classifiers. We first train the deep learning model on the FANTOM5 permissive enhancer dataset, and then fine-tune the model on ENCODE cell type-specific enhancer datasets by adopting the transfer learning strategy. Experimental results demonstrate that DeepEnhancer has superior efficiency and effectiveness in classification tasks, and the use of max-pooling and batch normalization is beneficial to higher accuracy. To make our approach more understandable, we propose a strategy to visualize the convolutional kernels as sequence logos and compare them against the JASPAR database using TOMTOM. In summary, DeepEnhancer allows researchers to train highly accurate deep models and will be broadly applicable in computational biology.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822593,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822593,,Bioinformatics;Computer architecture;DNA;Feature extraction;Genomics;Kernel;Machine learning,biological tissues;biology computing;diseases;genetics;genomics;learning (artificial intelligence);neural nets,DeepEnhancer;ENCODE cell type-specific enhancer datasets;FANTOM5 permissive enhancer dataset;JASPAR database;Roadmap;TOMTOM;background genomic sequences;batch normalization;classification performance;classification tasks;computational biology;computational framework;computational identification;convolutional kernels;convolutional neural networks;deep learning model;disease status;gene transcriptional regulation;high-throughput biological experimental techniques;human cell lines;max-pooling;sequence logos;tissues;traditional sequence-based classifiers;transfer learning strategy,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
547,DeepSplice: Deep classification of novel splice junctions revealed by RNA-seq,Yi Zhang; Xinan Liu; J. N. MacLeod; Jinze Liu,"Department of Computer Science, University of Kentucky, Lexington, 40506, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,330,333,"Alternative splicing (AS) is a regulated process that enables the production of multiple mRNA transcripts from a single multi-exon gene. The availability of large-scale RNA-seq datasets has made it possible to predict splice junctions, as well as splice sites through spliced alignment to the reference genome. This greatly enhances the capability to decipher gene structures and explore the diversity of splicing variants. However, existing ab initio aligners are vulnerable to false positive spliced alignments as a result of sequence errors and random sequence matches. These spurious alignments can lead to a significant set of false positive splice junction predictions, confusing downstream analyses of splice variant detection and abundance estimation. In this work, we illustrate that splice junction sequence characteristics can be ascertained from experimental data with deep learning techniques. We employ deep convolutional neural networks for a novel splice junction classification tool named DeepSplice that (i) outperforms state-of-the-art methods for predicting splice sites, (ii) shows high computational efficiency and (iii) can be applied to self-defined training data by users.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822541,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822541,Alternative Splicing;Deep Learning;RNA-seq;Splice Junctions,Bioinformatics;Biological neural networks;Convolution;Genomics;Junctions;Neurons;Splicing,RNA;biological techniques;biology computing;genomics;learning (artificial intelligence);neural nets,DeepSplice method;RNA-sequence;alternative splicing;deep classification;deep convolutional neural networks;deep learning techniques;false positive spliced alignments;gene structures;large-scale RNA-seq datasets;mRNA transcripts;random sequence matches;sequence errors;single multiexon gene;splice junctions;splice variant detection,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
548,Towards recognition of protein function based on its structure using deep convolutional networks,A. Tavanaei; A. S. Maida; A. Kaniymattam; R. Loganantharaj,"The Center for Advanced Computer Studies, Bio-inspired AI Lab, University of Louisiana at Lafayette, 70504, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,145,149,"This paper proposes a novel method for protein function recognition using deep learning. Recently, deep convolutional neural networks (DCNNs) demonstrated high performances in many areas of pattern recognition. Protein function is often associated with its tertiary structure denoting the active domain of a protein. This investigation develops a novel DCNN for protein functionality recognition based on its tertiary structure. Two rounds of experiments are performed. The initial experiment on tertiary protein structure alignment shows promising performances (94% accuracy rate) such that it shows the model robustness against rotations, local translations, and scales of the 3D structure. With these results, the main experiments contain five different datasets obtained by similarity measures between pairs of gene ontology terms. The experimental results for protein function recognition on selected datasets show 87.6% and 80.7% maximum and average accuracy rates respectively. The initial success of the DCNN in tertiary protein structure recognition supports further investigations with respect to tertiary protein retrieval and pattern mining on large scale problems.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822509,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822509,,Feature extraction;Machine learning;Protein engineering;Proteins;Solid modeling;Three-dimensional displays;Visualization,bioinformatics;biological techniques;genetics;learning (artificial intelligence);macromolecules;molecular biophysics;molecular configurations;neural nets;ontologies (artificial intelligence);pattern recognition;proteins,deep convolutional neural networks;deep learning;gene ontology terms;pattern mining;pattern recognition;protein function recognition;tertiary protein retrieval;tertiary protein structure alignment,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
549,Layerwise feature selection in Stacked Sparse Auto-Encoder for tumor type prediction,V. Singh; N. Baranwal; R. K. Sevakula; N. K. Verma; Y. Cui,"Dept. of Electrical Engineering, Indian Institute of Technology Kanpur, India",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119.0,2016,,,1542,1548,"Transcriptome data has been proved to be very valuable for clinical applications, such as diagnosis and prognosis of various cancers. In this paper, we present layer-wise feature selection in conjunction with stacked sparse auto-encoders (SSAE), a deep learning strategy for tumor classification with gene expression data. While SSAE learns high-level features from data, performing feature selection in every layer is a heuristic to obtain relevant features at every stage and also to assist in reducing the computation during fine-tuning procedure. The data in the new feature representation is finally used by classifier(s) to perform Tumor detection. The algorithm was tested on 36 datasets from the GEMLeR repository and w.r.t. AUC (Area under ROC curve) performance, it was found to outperform the GEMLeR benchmark results on 35 datasets (tied on the other dataset).",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822750,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822750,cancer classification;deep learning;feature selection;sparse auto-encoder,Benchmark testing;Classification algorithms;Support vector machines,cancer;feature selection;genetics;medical computing;tumours,GEMLeR repository;cancer diagnosis;cancer prognosis;deep learning strategy;gene expression data;layer-wise feature selection;stacked sparse autoencoder;transcriptome data;tumor classification;tumor detection;tumor type prediction,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
550,Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis,W. Zhang; R. Li; T. Zeng; Q. Sun; S. Kumar; J. Ye; S. Ji,"Wenlu Zhang is with the Department of Computer Science, Old Dominion University, Norfolk, VA, 23529.(email: wzhang@cs.odu.edu)",IEEE Transactions on Big Data,,2017,PP,99.0,1,1,"A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila, texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.",,,10.1109/TBDATA.2016.2573280,10.13039/100000002 - National Institutes of Health; 10.13039/100000145 - Division of Information and Intelligent Systems; 10.13039/100000153 - Division of Biological Infrastructure; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480825,Deep learning;bioinformatics;image analysis;multi-task learning;transfer learning,Biological system modeling;Computational modeling;Data models;Feature extraction;Gene expression;Training,,,,,,,,,20160530.0,,,IEEE,IEEE Early Access Articles
551,Learning of Generic Vision Features Using Deep CNN,K. N. D.; B. S. P.,"Dept. Of Comput. Sci., Amrita Sch. of Eng., Coimbatore, India",2015 Fifth International Conference on Advances in Computing and Communications (ICACC),20160317.0,2015,,,54,57,"Eminence of learning algorithm applied for computer vision tasks depends on the features engineered from image. It's premise that different representations can interweave and ensnare most of the elucidative genes that are responsible for variations in images, be it rigid, affine or projective. Hence researches give at most attention in hand-engineering features that capture these variations. But problem is, we need subtle domain knowledge to do that. Thereby making researchers elude epitome of representations. Hence learning algorithms never reach their full potential. In recent times there has been a shift from hand-crafting features to representation learning. The resulting features are not only optimal but also generic as in they can be used as off the shelf features for visual recognition tasks. In this paper we design and experiment with a basic deep convolution neural nets for learning generic vision features with an variant of convolving kernels. They operate by giving importance to individual uncorrelated color channels in a color model by convolving each channel with channel specific kernels. We were able to achieve considerable improvement in performance even when using smaller dataset.",,Electronic:978-1-4673-6994-7; POD:978-1-4673-6995-4,10.1109/ICACC.2015.52,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433775,Convolution Neural networks;Deep learning;Feature learning;Image classification;Supervised learning,Convolution;Feature extraction;Kernel;Linearity;Neural networks;Neurons;Training,computer vision;learning (artificial intelligence);neural nets,basic deep convolution neural nets;channel specific kernels;color channels;computer vision tasks;convolving kernels;deep CNN;elucidative genes;generic vision features;hand-crafting features;hand-engineering features;learning algorithms;representation learning;visual recognition tasks,,,,27.0,,,,2-4 Sept. 2015,,IEEE,IEEE Conference Publications
552,Deep Convolutional Neural Networks for Multi-instance Multi-task Learning,T. Zeng; S. Ji,"Sch. of Electr. Eng. & Comput. Sci., Washington State Univ., Pullman, WA, USA",2015 IEEE International Conference on Data Mining,20160107.0,2015,,,579,588,"Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.",1550-4786;15504786,Electronic:978-1-4673-9504-5; POD:978-1-4673-9505-2,10.1109/ICDM.2015.92,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373362,Deep learning;bioinformatics;multi-instance learning;multi-task learning;transfer learning,Biological system modeling;Brain models;Data models;Gene expression;Standards,image classification;image representation;learning (artificial intelligence);neural nets,MIMT-CNN problem;bag level representations;complex multiple-to-multiple relations;deep convolutional neural networks;deep leaning model;image representation;mouse brain gene expression pattern annotation data;multiinstance multilabel tasks;multiinstance multitask convolutional neural networks;multiinstance multitask learning;single-instance single-label image classification tasks;single-label single-task data sets;transfer learning;ultimate multilabel predictions,,1.0,,30.0,,,,14-17 Nov. 2015,,IEEE,IEEE Conference Publications
553,Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets,M. K. K. Leung; A. Delong; B. Alipanahi; B. J. Frey,"Dept. of Electr. & Comput. Eng., Univ. of Toronto, Toronto, ON, Canada",Proceedings of the IEEE,20151218.0,2016,104,1.0,176,197,"In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.",0018-9219;00189219,,10.1109/JPROC.2015.2494198,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347331,Computational biology;deep learning;genetic variants;genome analysis;genome biology;genomic medicine;machine learning;precision medicine,Big data;Bioinformatics;Diseases;Genomics;Machine learning;Medical treatment,DNA;cellular biophysics;diseases;genetics;genomics;learning (artificial intelligence);medical computing;medicine,DNA variations;cell variables;deep learning;disease risks;gene expression;gene splicing;genomic medicine;high-throughput measurement;large-scale data sets;machine learning;nucleic acids;proteins,,7.0,,231.0,,,20151204.0,Jan. 2016,,IEEE,IEEE Journals & Magazines
554,Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach,M. Liang; Z. Li; T. Chen; J. Zeng,"Department of Mathematical Sciences, Tsinghua University, Beijing, P. R. China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,20150805.0,2015,12,4.0,928,937,"Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personali- ed cancer therapy.",1545-5963;15455963,,10.1109/TCBB.2014.2377729,National Basic Research Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977954,Multi-platform cancer data analysis;clinical data;genomic data;identification of cancer subtypes;multimodal deep belief network;restricted Boltzmann machine,Bioinformatics;Cancer;Computational biology;DNA;Data analysis;Data models;Genomics,RNA;belief networks;cancer;data analysis;feature extraction;genetics;genomics;medical computing;molecular biophysics;pattern clustering;tumours;unsupervised learning,DNA methylation;advancing personalized therapy;cancer data analysis;cancer pathogenesis;cancer patient clustering;cancer subtype identification;complex cross-modality correlations;contrastive divergence;cross-modality correlations;disease pathogenesis;gene expression;high-throughput sequencing technologies;input modality;integrative clustering approaches;integrative data analysis;integrative data analysis approach;intramodality correlations;intrinsic statistical properties;joint latent model;key genes;latent feature extraction;machine learning model;miR-29a;miRNA expression;multimodal DBN based data analysis;multimodal DBN model;multimodal deep belief network;multimodal deep learning approach;multiplatform cancer data;multiplatform genomic data;multiple input modalities;ovarian cancer patients;personalized cancer therapy;practical learning algorithm;tumor samples;unsupervised manner,Computational Biology;Gene Expression Profiling;Humans;Kaplan-Meier Estimate;Machine Learning;MicroRNAs;Neoplasms,9.0,,30.0,,,20141205.0,July-Aug. 1 2015,,IEEE,IEEE Journals & Magazines
555,Multi-level gene/MiRNA feature selection using deep belief nets and active learning,R. Ibrahim; N. A. Yousri; M. A. Ismail; N. M. El-Makky,"Computer and Systems Engineering Department, Alexandria University, Alexandria 21544, Egypt",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106.0,2014,,,3957,3960,"Selecting the most discriminative genes/miRNAs has been raised as an important task in bioinformatics to enhance disease classifiers and to mitigate the dimensionality curse problem. Original feature selection methods choose genes/miRNAs based on their individual features regardless of how they perform together. Considering group features instead of individual ones provides a better view for selecting the most informative genes/miRNAs. Recently, deep learning has proven its ability in representing the data in multiple levels of abstraction, allowing for better discrimination between different classes. However, the idea of using deep learning for feature selection is not widely used in the bioinformatics field yet. In this paper, a novel multi-level feature selection approach named MLFS is proposed for selecting genes/miRNAs based on expression profiles. The approach is based on both deep and active learning. Moreover, an extension to use the technique for miRNAs is presented by considering the biological relation between miRNAs and genes. Experimental results show that the approach was able to outperform classical feature selection methods in hepatocellular carcinoma (HCC) by 9%, lung cancer by 6% and breast cancer by around 10% in F1-measure. Results also show the enhancement in F1-measure of our approach over recently related work in [1] and [2].",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6944490,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944490,,Accuracy;Bioinformatics;Breast cancer;Gene expression;Lungs;Training,RNA;bioinformatics;cancer;cellular biophysics;data structures;feature selection;genetics;genomics;learning (artificial intelligence);lung;molecular biophysics,F1-measure;active learning;bioinformatics;breast cancer;data representation;deep belief nets;dimensionality curse problem;disease classifier enhancement;hepatocellular carcinoma;lung cancer;multilevel gene-MiRNA feature selection,,3.0,,16.0,,,,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
556,MiRTDL: A Deep Learning Approach for miRNA Target Prediction,S. Cheng; M. Guo; C. Wang; X. Liu; Y. Liu; X. Wu,"School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,20161207.0,2016,13,6.0,1161,1169,"MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.",1545-5963;15455963,,10.1109/TCBB.2015.2510002,Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362158,Constraint relaxation;convolutional neural network;miRNA;target prediction,Diseases;Machine learning;Matched filters;Neural networks;Prediction algorithms;RNA;Support vector machines,RNA;bioinformatics;biological techniques;diseases;genetics;learning (artificial intelligence);molecular biophysics;neural nets,CNN;MiRTDL;balanced training dataset;convolutional neural network;deep learning approach;diseases;essential information;genes;miRNA regulatory mechanism;miRNA target pairs;miRNA target prediction algorithm;miRTDL;microRNA;real targets,,1.0,,,,,20151222.0,November 1 2016,,IEEE,IEEE Journals & Magazines
557,Gold classification of COPDGene cohort based on deep learning,J. Ying; J. Dutta; N. Guo; L. Xia; A. Sitek; Q. Li; Q. Li,"Nuclear Medicine and Molecular Imaging Radiology Department, Massachusetts General Hospital, Boston, MA, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519.0,2016,,,2474,2478,"This study aims to employ deep learning for the development of an automatic classifier for the severity of chronic obstructive pulmonary disease (COPD) in patients. A three-layer deep belief network (DBN) with two hidden layers and one visible layer was employed to generate a model for classification, and the model's robustness against exacerbation was analyzed. Subjects from the COPDGene cohort were staged using the GOLD 2011 guidelines. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 97.2% by using a 10-fold cross validation experiment. The most sensitive features as revealed by the DBN weights were consistent with the clinical consensus as per previous studies and clinical diagnosis rules. In summary, we demonstrate that the DBN is a competitive tool for exacerbation risk assessment for patients suffering from, COPD.",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7472122,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472122,Chronic Obstructive Pulmonary Disease (COPD);Deep Belief Networks (DBNs);Global Initiative for Chronic Obstructive Lung Disease (GOLD);classification;deep learning,Diseases;Feature extraction;Gold;Lungs;Machine learning;Medical diagnostic imaging;Training,belief networks;biology computing;diseases;feature selection;learning (artificial intelligence);optimisation;pattern classification,COPD gene cohort;DBN weights;automatic gold classification;chronic obstructive pulmonary disease;deep learning;exacerbation risk assessment;feature selection;parameter optimization;three-layer deep belief network,,,,19.0,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
558,Big data analytics in genomics: The point on Deep Learning solutions,F. Celesti; A. Celesti; L. Carnevale; A. Galletta; S. Campo; A. Romano; P. Bramanti; M. Villari,"Department of Biomedical and Dental Sciences and Morphofunctional Images, University of Messina, Italy",2017 IEEE Symposium on Computers and Communications (ISCC),20170904.0,2017,,,306,309,"Nowadays, Next Generation Sequeencing (NGS) is a catch-all term used to describe different modern DNA sequencing applications that produce big genomics data that can be analysed in a faster fashion than in the past. For this reason, NGS requires more and more sophisticated algorithms and high-performance parallel processing systems able to analyse and extract knowledge from a huge amount of genomics and molecular data. In this context, researchers are beginning to look at emerging deep learning algorithms able to perform efficient big data analytics. In this paper, we analyse and classify the major current deep learning solutions that allow biotechnology researchers to perform big genomics data analytics. Moreover, by means of a taxonomic analysis, we provide a clear picture of the current state of the art also discussing future challenges.",,Electronic:978-1-5386-1629-1; POD:978-1-5386-1630-7; USB:978-1-5386-1628-4,10.1109/ISCC.2017.8024547,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024547,Cloud computing;DNA sequencing;Genomics;NGS;big data;biotechnology;deep learning,Bioinformatics;DNA;Genomics;Machine learning;Proteins;Sequential analysis,,,,,,,,,,3-6 July 2017,,IEEE,IEEE Conference Publications
559,Using convolutional neural networks to explore the microbiome,D. Reiman; A. Metwally; Y. Dai,"Bioinformatics Program of the Department of Bioengineering, University of Illinois at Chicago, 60612, USA",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914.0,2017,,,4269,4272,"The microbiome has been shown to have an impact on the development of various diseases in the host. Being able to make an accurate prediction of the phenotype of a genomic sample based on its microbial taxonomic abundance profile is an important problem for personalized medicine. In this paper, we examine the potential of using a deep learning framework, a convolutional neural network (CNN), for such a prediction. To facilitate the CNN learning, we explore the structure of abundance profiles by creating the phylogenetic tree and by designing a scheme to embed the tree to a matrix that retains the spatial relationship of nodes in the tree and their quantitative characteristics. The proposed CNN framework is highly accurate, achieving a 99.47% of accuracy based on the evaluation on a dataset 1967 samples of three phenotypes. Our result demonstrated the feasibility and promising aspect of CNN in the classification of sample phenotype.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037799,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037799,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
560,Probabilistic Graphical Models and Deep Belief Networks for Prognosis of Breast Cancer,M. Khademi; N. S. Nedialkov,"Dept. of Comput. & Software, McMaster Univ., Hamilton, ON, Canada",2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),20160303.0,2015,,,727,732,"We propose a probabilistic graphical model (PGM) for prognosis and diagnosis of breast cancer. PGMs are suitable for building predictive models in medical applications, as they are powerful tools for making decisions under uncertainty from big data with missing attributes and noisy evidence. Previous work relied mostly on clinical data to create a predictive model. Moreover, practical knowledge of an expert was needed to build the structure of a model, which may not be accurate. In our opinion, since cancer is basically a genetic disease, the integration of microarray and clinical data can improve the accuracy of a predictive model. However, since microarray data is high-dimensional, including genomic variables may lead to poor results for structure and parameter learning due to the curse of dimensionality and small sample size problems. We address these problems by applying manifold learning and a deep belief network (DBN) to microarray data. First, we construct a PGM and a DBN using clinical and microarray data, and extract the structure of the clinical model automatically by applying a structure learning algorithm to the clinical data. Then, we integrate these two models using softmax nodes. Extensive experiments using real-world databases, such as METABRIC and NKI, show promising results in comparison to Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN) classifiers, for classifying tumors and predicting events like recurrence and metastasis.",,Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7,10.1109/ICMLA.2015.196,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424407,breast cancer;deep belief networks;microarray data;probabilistic graphical models,Approximation algorithms;Breast cancer;Manifolds;Probabilistic logic;Prognostics and health management;Training,bioinformatics;cancer;genomics;graph theory;learning (artificial intelligence);pattern classification;probability;tumours,DBN;METABRIC database;NKI database;PGM;SVM classifier;breast cancer diagnosis;breast cancer prognosis;clinical data integration;curse-of-dimensionality;deep-belief networks;genetic disease;genomic variables;high-dimensional microarray data;k-NN classifier;k-nearest neighbors classifier;manifold learning;medical applications;metastasis event prediction;microarray data integration;parameter learning;predictive model;predictive model accuracy improvement;probabilistic graphical model;recurrence event prediction;softmax nodes;structure learning;structure learning algorithm;support vector machine classifier;tumors,,,,20.0,,,,9-11 Dec. 2015,,IEEE,IEEE Conference Publications
561,The effective diagnosis of schizophrenia by using multi-layer RBMs deep networks,Chen Qiao; D. D. Lin; Shao-Long Cao; Yu-Ping Wang,"School of Mathematics and Statistics, Xi'an Jiaotong University, 710049, China",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,603,606,"Schizophrenia is one of the most prevalent mental diseases, and is considered to be caused by the interplay of a number of genetic factors. In this paper, by constructing a multilayer restricted Boltzmann machines (RBMs) deep network, we use the genomic data (i.e., SNP data) for unsupervised feature learning and disease diagnosis of schizophrenia. In order to obtain some more accurate diagnosis results by RBMs, firstly, we transform the SNP data into binary sequences, and then by training the multi-layer RBMs deep network on unlabeled data, the multi-level abstract features of the genomic data are obtained and stored in the network. Finally, by adding a linear classifier to the top of the multi-layer RBMs deep network, the classification results on the testing data are gained. The results show that the average performance of this method is better than that of other methods, e.g., SVM (including linear SVM as well as SVM with multilayer perceptron kernel), sparse representations based classifier and k-nearest neighbors method. It is indicated that the multi-layer RBMs deep network can extract deep hierarchical representations of the genomic data, and then promises a more comprehensive approach for the mental disease diagnosis.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359751,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359751,,Bioinformatics;Genomics;Quality control;Support vector machines,Boltzmann machines;binary sequences;diseases;genomics;patient diagnosis;support vector machines;unsupervised learning,SNP data;SVM;binary sequence;genetic factor;genomic data;k-nearest neighbor method;linear classifier;mental disease diagnosis;multi-layer restricted Boltzmann machines deep network;multilayer RBM deep network;schizophrenia disease diagnosis;sparse representation-based classifier;unsupervised feature learning,,,,16.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
562,Ensemble of deep long short term memory networks for labelling origin of replication sequences,U. Singh; S. Chauhan; A. Krishnamachari; L. Vig,"School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India",2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),20151207.0,2015,,,1,7,"Advancement in sequence data generation technologies are churning out voluminous omics data and posing a massive challenge to annotate the biological functional features. Sequence data from the well studied model organism Saccharomyces cerevisiae has been commonly used to test and validate in silico prediction methods. DNA replication is a critical step in the cellular process and the sequence location where this process originates in the genomic landscape is generally referred as origin of replication. In this paper we investigate the application bidirectional Long Short Term (LSTM) Networks to predict origin of replication sequences. Long Short Term Memory (LSTM) networks have recently been shown to yield state of the art performance in speech recognition, and music generation. These networks are capable of learning long term patterns via the use of multiplication gates. This paper utilizes Deep bidirectional LSTM for prediction of origin of replication sequences belonging to the organism Saccharomyces cerevisiae. Results demonstrate that LSTMs outperform the commonly used machine learning classifiers such as Support Vector Machine (SVM), Random Forest (RF), Artificial Neural Network (ANN), and Hidden Markov Model (HMM). An important additional advantage of LSTMs is that they work directly on the sequences and obviate the need for hand coded features.",,Electronic:978-1-4673-8273-1; POD:978-1-4673-8274-8,10.1109/DSAA.2015.7344871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344871,,Bioinformatics;Biological cells;Computer architecture;DNA;Genomics;Hidden Markov models;Logic gates,DNA;biology computing;genomics;recurrent neural nets,DNA replication;Saccharomyces cerevisiae;bidirectional long short term memory;biological functional feature;cellular process;deep bidirectional LSTM;deep long short term memory network;genomic landscape;multiplication gates;replication sequences;sequence data generation technology;voluminous omics data,,,,24.0,,,,19-21 Oct. 2015,,IEEE,IEEE Conference Publications
563,Boosting compound-protein interaction prediction by deep learning,Kai Tian; Mingyu Shao; Shuigeng Zhou; Jihong Guan,"Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, 200433, China",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217.0,2015,,,29,34,"The identification of interactions between compounds and proteins plays an important role in network pharmacology and drug discovery. However, experimentally identifying compound-protein interactions (CPIs) is generally expensive and time-consuming, computational approaches are thus introduced. Among these, machine-learning based methods have achieved a considerable success. However, due to the nonlinear and imbalanced nature of biological data, many machine learning approaches have their own limitations. Recently, deep learning techniques show advantages over many state-of-the-art machine learning methods in many applications. In this study, we aim at improving the performance of CPI prediction based on deep learning, and propose a method called DL-CPI (the abbreviation of Deep Learning for Compound-Protein Interactions prediction), which employs deep neural network (DNN) to effectively learn the representations of compound-protein pairs. Extensive experiments show that DL-CPI can learn useful features of compound-protein pairs by a layerwise abstraction, and thus achieves better prediction performance than existing methods on both balanced and imbalanced datasets.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359651,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359651,Compound-protein interaction;Deep learning;Deep neural network (DNN),Bioinformatics;Genomics;Radio frequency;Training,biology computing;drugs;health care;learning (artificial intelligence);neural nets;proteins,DL-CPI;compound-protein interaction prediction;deep learning techniques;deep neural network;drug discovery;layerwise abstraction;machine-learning;network pharmacology,,1.0,,22.0,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
