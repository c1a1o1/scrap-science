Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,MeSH Terms,Article Citation Count,Patent Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Integrating Online and Offline Three-Dimensional Deep Learning for Automated Polyp Detection in Colonoscopy Videos,L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Journal of Biomedical and Health Informatics,20170520,2017,21,1,65,75,"Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.",2168-2194;21682194,,10.1109/JBHI.2016.2637004,Hong Kong Special Administrative Region; Shenzhen Science and Technology Program; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776845,Automated polyp detection;colonoscopy video;computer-aided diagnosis;convolutional neural networks (CNNs);deep learning,Cancer;Colonoscopy;Feature extraction;MIMICs;Shape;Three-dimensional displays;Videos,cancer;endoscopes;learning (artificial intelligence);medical image processing;neural nets;video signal processing,3D fully convolutional network;automated polyp detection;colonoscopy videos;colorectal cancer diagnosis;colorectal cancer prevention;offline three-dimensional deep learning integration framework;online three-dimensional deep learning integration framework;spatiotemporal features,,,,,,,20161207,Jan. 2017,,IEEE,IEEE Journals & Magazines
Automatic Detection and Classification of Colorectal Polyps by Transferring Low-Level CNN Features From Nonmedical Domain,R. Zhang; Y. Zheng; T. W. C. Mak; R. Yu; S. H. Wong; J. Y. W. Lau; C. C. Y. Poon,"Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR",IEEE Journal of Biomedical and Health Informatics,20170520,2017,21,1,41,47,"Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.",2168-2194;21682194,,10.1109/JBHI.2016.2635662,"Chow Yuk Ho Technology Centre for Innovative Medicine; Hong Kong Innovation and Technology Fund, Shaw Endoscopy Center; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769237,Colorectal cancer;deep learning;health informatics;polyp diagnosis,Biomedical imaging;Cancer;Colonoscopy;Feature extraction;Machine learning;Neurons;Training,biological tissues;biomedical optical imaging;cancer;endoscopes,NBI endoscopic polyp images;adenomatous colorectal polyps;cancer deaths;colonoscopy;colorectal cancer;colorectal polyps automatic detection;colorectal polyps classification;deep convolutional neural network;endoscopic images;endoscopic nonpolyp images;fully automatic algorithm;human visual observation;hyperplasia;hyperplastic colorectal polyps;learning application;low-level CNN features;narrowband imaging endoscopy;nonmedical datasets;optical magnification;polyp histology;polypectomy;tissue;white-light endoscopy,,,,,,,20161205,Jan. 2017,,IEEE,IEEE Journals & Magazines
Comparison of hand-craft feature based SVM and CNN based deep learning framework for automatic polyp classification,Y. Shin; I. Balasingham,"Department Electronics and Telecommunications at Norwegian University of Science and Technology (NTNU), Trondheim, Norway",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,3277,3280,"Colonoscopy is a standard method for screening polyps by highly trained physicians. Miss-detected polyps in colonoscopy are potential risk factor for colorectal cancer. In this study, we investigate an automatic polyp classification framework. We aim to compare two different approaches named hand-craft feature method and convolutional neural network (CNN) based deep learning method. Combined shape and color features are used for hand craft feature extraction and support vector machine (SVM) method is adopted for classification. For CNN approach, three convolution and pooling based deep learning framework is used for classification purpose. The proposed framework is evaluated using three public polyp databases. From the experimental results, we have shown that the CNN based deep learning framework shows better classification performance than the hand-craft feature based methods. It achieves over 90% of classification accuracy, sensitivity, specificity and precision.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037556,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037556,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Combining Convolutional and Recurrent Neural Networks for Human Skin Detection,H. Zuo; H. Fan; E. Blasch; H. Ling,"Department of Chemical Equipment and Control Engineering, China University of Petroleum, Qingdao, China",IEEE Signal Processing Letters,20170209,2017,24,3,289,293,"Skin detection from images, typically used as a preprocessing step, has a wide range of applications such as dermatology diagnostics, human computer interaction designs, and etc. It is a challenging problem due to many factors such as variation in pigment melanin, uneven illumination, and differences in ethnicity geographics. Besides, age and gender introduce additional difficulties to the detection process. It is hard to determine whether a single pixel is skin or nonskin without considering the context. An efficient traditional hand-engineered skin color detection algorithm requires extensive work by domain experts. Recently, deep learning algorithms, especially convolutional neural networks (CNNs), have achieved great success in pixel-wise labeling tasks. However, CNN-based architectures are not sufficient for modeling the relationship between pixels and their neighbors. In this letter, we integrate recurrent neural networks (RNNs) layers into the fully convolutional neural networks (FCNs), and develop an end-to-end network for human skin detection. In particular, FCN layers capture generic local features, while RNN layers model the semantic contextual dependencies in images. Experimental results on the COMPAQ and ECU skin datasets validate the effectiveness of the proposed approach, where RNN layers enhance the discriminative power of skin detection in complex background situations.",1070-9908;10709908,,10.1109/LSP.2017.2654803,U.S. National Science Foundation; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004543 - China Scholarship Council; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820144,Convolutional neural networks (CNNs);recurrent neural networks (RNNs);skin classification;skin detection;skin segmentation,Convolution;Detection algorithms;Image color analysis;Lighting;Recurrent neural networks;Skin,image colour analysis;image recognition;recurrent neural nets;skin,COMPAQ skin datasets;ECU skin datasets;FCN layers;complex background situations;convolutional neural networks;deep learning algorithms;dermatology diagnostics;discriminative power;domain experts;end-to-end network;ethnicity geographics;generic local features;hand-engineered skin color detection;human computer interaction;human skin detection;pigment melanin;pixel-wise labeling tasks;preprocessing step;recurrent neural networks;semantic contextual dependency;uneven illumination,,,,,,,20170117,March 2017,,IEEE,IEEE Journals & Magazines
Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction,R. Li; T. Zeng; H. Peng; S. Ji,"School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA",IEEE Transactions on Medical Imaging,20170628,2017,36,7,1533,1541,"Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.",0278-0062;02780062,,10.1109/TMI.2017.2679713,10.13039/100000001 - National Science Foundation; 10.13039/100007588 - Washington State University; 10.13039/100009980 - Old Dominion University; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874113,BigNeuron;Deep learning;image denoising;image segmentation;neuron reconstruction,Convolution;Image reconstruction;Image segmentation;Microscopy;Morphology;Neurons;Three-dimensional displays,biomedical optical imaging;brain;image denoising;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optical microscopy,3-D convolutional neural networks;3-D microscopy images;3-D neuron reconstruction;3-D neuron structure;CNN architecture;brain anatomy;brain wiring;deep learning segmentation;digital reconstruction;digital tracing;discontinued segments;image segmentation methods;neurite patterns;neuronal microscopy images;neuronal voxels;noise removal;optical microscopy images;organisms;preprocessing step;reconstruction algorithms;reversing engineering;tracing performance;volumetric images;voxel-wise segmentation maps,,,,,,,20170308,July 2017,,IEEE,IEEE Journals & Magazines
Hybrid deep learning for Reflectance Confocal Microscopy skin images,P. Kaur; K. J. Dana; G. O. Cula; M. C. Mack,"Department of Electrical and Computer Engineering, Rutgers University, NJ, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424,2016,,,1466,1471,"Reflectance Confocal Microscopy (RCM) is used for evaluation of human skin disorders and the effects of skin treatments by imaging the skin layers at different depths. Traditionally, clinical experts manually categorize the images captured into different skin layers. This time-consuming labeling task impedes the convenient analysis of skin image datasets. In recent automated image recognition tasks, deep learning with convolutional neural nets (CNN) has achieved remarkable results. However in many clinical settings, training data is often limited and insufficient for CNN training. For recognition of RCM skin images, we demonstrate that a CNN trained on a moderate size dataset leads to low accuracy. We introduce a hybrid deep learning approach which uses traditional texton-based feature vectors as input to train a deep neural network. This hybrid method uses fixed filters in the input layer instead of tuned filters, yet superior performance is achieved. Our dataset consists of 1500 images from 15 RCM stacks belonging to six different categories of skin layers. We show that our hybrid deep learning approach performs with a test accuracy of 82% compared with 51% for CNN. We also compare the results with additional proposed methods for RCM image recognition and show improved accuracy.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899844,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899844,,Epidermis;Histograms;Image recognition;Libraries;Machine learning;Neural networks,convolution;data analysis;filtering theory;image recognition;learning (artificial intelligence);medical image processing;microscopy;vectors;visual databases,CNN training;RCM;automated image recognition tasks;clinical experts;convolutional neural nets;fixed filters;human skin disorders;hybrid deep learning approach;reflectance confocal microscopy skin images;skin image datasets;skin layers;traditional texton-based feature vectors,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
Human induced pluripotent stem cell region recognition in microscopy images using Convolutional Neural Networks,Y. H. Chang; K. Abe; H. Yokota; K. Sudo; Y. Nakamura; C. Y. Lin; M. D. Tsai,"Department of Information and Computer Engineering, Chung-Yuan Christian University, Chung-Li, 32023, Taiwan",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,4058,4061,"We present a deep learning architecture Convolutional Neural Networks (CNNs) for automatic classification and recognition of reprogramming and reprogrammed human Induced Pluripotent Stem (iPS) cell regions in microscopy images. The differentiated cells that possibly undergo reprogramming to iPS cells can be detected by this method for screening reagents or culture conditions in iPS induction. The learning results demonstrate that our CNNs can achieve the Top-1 and Top-2 error rates of 9.2% and 0.84%, respectively, to produce probability maps for the automatic analysis. The implementation results show that this automatic method can successfully detect and localize the human iPS cell formation, thereby yield a potential tool for helping iPS cell culture.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037747,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037747,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images,Y. T. Su; Y. Lu; M. Chen; A. A. Liu,"School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China.",IEEE Access,,2017,PP,99,1,1,"We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a Convolutional Neural Network (CNN) and a Long Short Term Memory network (LSTM) to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data we validate the proposed method on include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 dataset, and the F score of 96.5% on the C3H10 dataset. The results on both datasets outperform traditional graph model based approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of CNN-LSTM network.",,,10.1109/ACCESS.2017.2745544,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019789,biomedical imaging;computer vision;machine learning;mitosis detection;stem cell,Computer architecture;Feature extraction;Hidden Markov models;Machine learning;Microscopy;Spatiotemporal phenomena;Visualization,,,,,,,,,20170829,,,IEEE,IEEE Early Access Articles
An efficient method for neuronal tracking in electron microscopy images,L. Yin; C. Xiao; Q. Xie; X. Chen; L. Shen; H. Han,"Computational Mathematics, Hubei University, Wuhan, 430062, China",2017 IEEE International Conference on Mechatronics and Automation (ICMA),20170824,2017,,,1865,1870,"With the introduction of deep learning, a wave of artificial intelligence research has been set off again. Scientists focus on brain-inspired intelligence, namely, try to get inspiration from the brain nervous system and cognitive behavior mechanism, to develop intelligent computing models as well as algorithms with stronger information representation, processing and learning ability. So, the study of neurons and the connections between neurons of brain are needed. One major obstacle of reconstruction lies in segmenting and tracking neuronal processes. Electron microscopy is producing neurons images rapidly. In response, we propose an efficient method for neuronal tracking in electron microscopy images to help scientists reconstruct complex neurons. First, we track neurons by kernelized correlation filter to get candidate neuron; then we calculate overlap area and distance of the contours between two consecutive images to get final neuron. We evaluate the performance of our method on a public electron microscopy dataset. The method is superior in accuracy and efficiency.",,CD:978-1-5090-6757-2; Electronic:978-1-5090-6759-6; POD:978-1-5090-6760-2; Paper:978-1-5090-6758-9,10.1109/ICMA.2017.8016102,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016102,3D reconstruction;correlation filters;electron microscopy;neuron;tracking,Correlation;Electron microscopy;Image segmentation;Neurons;Target tracking,,,,,,,,,,6-9 Aug. 2017,,IEEE,IEEE Conference Publications
MIMO-Net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images,S. E. A. Raza; L. Cheung; D. Epstein; S. Pelengaris; M. Khan; N. M. Rajpoot,"Department of Computer Science, University of Warwick, Coventry, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,337,340,"We propose a novel multiple-input multiple-output convolution neural network (MIMO-Net) for cell segmentation in fluorescence microscopy images. The proposed network trains the network parameters using multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The MIMO-Net allows us to deal with variable intensity cell boundaries and highly variable cell size in the mouse pancreatic tissue by adding extra convolutional layers which bypass the max-pooling operation. The results show that our method outperforms state-of-the-art deep learning based approaches for segmentation.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950532,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950532,Cell Segmentation;Deep Learning;Fluorescence Microscopy,Biomembranes;Computer architecture;Convolution;Image segmentation;Machine learning;Microprocessors;Microscopy,biomedical optical imaging;fluorescence;image resolution;image segmentation;medical image processing;neural nets;optical microscopy,cell segmentation;deep learning based approaches;fluorescence microscopy images;mouse pancreatic tissue;multipleinput multipleoutput convolution neural network;multiresolution deconvolution filters;variable intensity cell boundaries,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Deep convolutional neural networks for detecting secondary structures in protein density maps from cryo-electron microscopy,R. Li; D. Si; T. Zeng; S. Ji; J. He,"Department of Computer Science, Old Dominion University, Norfolk, Virginia 23529, United States of America",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,41,46,"The detection of secondary structure of proteins using three dimensional (3D) cryo-electron microscopy (cryo-EM) images is still a challenging task when the spatial resolution of cryo-EM images is at medium level (5-10Å). Prior researches focused on the usage of local features that may not capture the global information of image objects. In this study, we propose to use deep learning methods to extract high representative global features and then automatically detect secondary structures of proteins. In particular, we build a convolutional neural network (CNN) classifier that predicts the probability of label for every individual voxel in 3D cryo-EM image with respect to the secondary structure elements of proteins such as α-helix, β-sheet and background. To effectively incorporate the 3D spatial information in protein structures, we propose to perform 3D convolutions in the convolutional layers of CNNs. We show that the proposed CNN classifier can outperform existing SVM method on identifying the secondary structure elements of proteins from 3D cryo-EM medium resolution images.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822490,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822490,,Computational modeling;Convolution;Deconvolution;Image resolution;Protein engineering;Proteins;Three-dimensional displays,biology computing;electron microscopy;feature extraction;image resolution;molecular biophysics;molecular configurations;neural nets;probability;proteins;support vector machines,α-helix;β-sheet;3D convolutions;3D cryo-EM image;3D spatial information;CNN classifier;SVM;convolutional layers;convolutional neural network classifier;deep convolutional neural networks;deep learning methods;high-representative global feature extraction;image objects;local features;probability;protein density maps;secondary structure detection;spatial resolution;three-dimensional cryo-electron microscopy images,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks,J. T. Kwak; S. M. Hewitt,"Department of Computer Science and Engineering, Sejong University, Seoul, Korea 05006.",IEEE Access,,2017,PP,99,1,1,"In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei were utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training dataset including 73 benign and 89 cancer samples and validated on the testing dataset comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison to the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value<5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.",,,10.1109/ACCESS.2017.2747838,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8023758,artificial neural network;cancer detection;computer-aided diagnosis;microscopy;pattern recognition,Biological tissues;Kernel;Machine learning;Neurons;Pathology;Prostate cancer,,,,,,,,,20170831,,,IEEE,IEEE Early Access Articles
Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning,M. I. Razzak; S. Naz,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824,2017,,,801,807,"Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.111,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014845,Blood Sample Analysis;ELM;KWFLICM;RBC;cell morphology;image analysis,Blood;Diseases;Feature extraction;Image color analysis;Image segmentation;Microscopy;Shape,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
Crowdsourcing for Chromosome Segmentation and Deep Classification,M. Sharma; O. Saha; A. Sriraman; R. Hebbalaguppe; L. Vig; S. Karande,,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),20170824,2017,,,786,793,"Metaphase chromosome analysis is one of the primary techniques utilized in cytogenetics. Observations of chromosomal segments or translocations during metaphase can indicate structural changes in the cell genome, and is often used for diagnostic purposes. Karyotyping of the chromosomes micro-photographed under metaphase is done by characterizing the individual chromosomes in cell spread images. Currently, considerable effort and time is spent to manually segment out chromosomes from cell images, and classifying the segmented chromosomes into one of the 24 types, or for diseased cells to one of the known translocated types. Segmenting out the chromosomes in such images can be especially laborious and is often done manually, if there are overlapping chromosomes in the image which are not easily separable by image processing techniques. Many techniques have been proposed to automate the segmentation and classification of chromosomes from spread images with reasonable accuracy, but given the criticality of the domain, a human in the loop is often still required. In this paper, we present a method to segment out and classify chromosomes for healthy patients using a combination of crowdsourcing, preprocessing and deep learning, wherein the non-expert crowd from CrowdFlower is utilized to segment out the chromosomes from the cell image, which are then straightened and fed into a (hierarchical) deep neural network for classification. Experiments are performed on 400 real healthy patient images obtained from a hospital. Results are encouraging and promise to significantly reduce the cognitive burden of segmenting and karyotyping chromosomes.",,Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3,10.1109/CVPRW.2017.109,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014843,,Biological cells;Conferences;Crowdsourcing;Image segmentation;Machine learning;Microscopy;Pipelines,,,,,,,,,,21-26 July 2017,,IEEE,IEEE Conference Publications
Semantic segmentation of microscopic images of H&E stained prostatic tissue using CNN,J. Isaksson; I. Arvidsson; K. Åaström; A. Heyden,"Lund University, Centre for Mathematical Sciences, Lund, Sweden",2017 International Joint Conference on Neural Networks (IJCNN),20170703,2017,,,1252,1256,"There is a need for an automatic Gleason scoring system that can be used for prostate cancer diagnosis. Today the diagnoses are determined by pathologists manually, which is both a complex and a time-consuming task. To reduce the pathologists' workload, but also to reduce variations between different pathologists, an automatic classification system would be of great use. Some previous works have aimed for this, but still more work needs to be done. It is probable that such a tool would benefit from having access to individually segmented, pathologically relevant objects from the images. Therefore, we have developed an algorithm for semantic segmentation of the microscopic images of H&E stained prostate tissue into Background, Stroma, Epithelial Cytoplasm and Nuclei. This algorithm is based on deep learning, or more specifically a convolutional neural network. The network design is inspired by architectures that previously have been proved successful in different applications. It consists of a contracting and an expanding part, which are symmetrical. We have reached an accuracy of 80 %, as measured by the mean of the intersection over union, for segmentation into four classes. Previous works have only investigated nuclei segmentation, and our network performed similar but for the more challenging task of four class segmentation.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7965996,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965996,,Cancer;Gold;Image segmentation;Microscopy;Neural networks;Semantics;Standards,biological tissues;cancer;image classification;image segmentation;medical image processing;neural nets;patient diagnosis,CNN;H&E stained prostatic tissue;automatic Gleason scoring system;automatic classification system;epithelial cytoplasm;microscopic image semantic segmentation;prostate cancer diagnosis,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
Automating Papanicolaou Test Using Deep Convolutional Activation Feature,J. Hyeon; H. J. Choi; K. N. Lee; B. D. Lee,"Sch. of Comput., KAIST, Daejeon, South Korea",2017 18th IEEE International Conference on Mobile Data Management (MDM),20170703,2017,,,382,385,"Cervical cancer is the women's fourth most common cancer worldwide, with 266,000 deaths in a year. Cervical cancer can be diagnosed by the Papanicolaou test. In this test, a cytopathologist observes a microscopic image of the cervix cells and decides whether the patient is abnormal or not. According to research, the accuracy of the cervical cytology is reported as 89.7%. Because it is associated with the patient's life, it is important to improve the accuracy of this test. Many systems have been proposed to help judge experts to improve the accuracy of tests in the medical field, but development has been limited to areas where there are cleanly quantified test data. In this paper, we design and train a model to automatically classify the normal/abnormal state of cervical cells from microscopic images by using a convolutional neural network and several machine learning classifiers. As a result, the support vector machine achieves the highest performance with 78% F1 score.",,Electronic:978-1-5386-3932-0; POD:978-1-5386-3933-7,10.1109/MDM.2017.66,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962484,Cervical Cancer;Cervical Cancer Screening Test;Convolutional Neural Network;Deep Convolutional Activation Feature;Papanicolaou Test,Biological neural networks;Cervical cancer;Feature extraction;Microscopy;Support vector machines,biomedical optical imaging;cancer;cellular biophysics;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;microscopes;support vector machines,F1 score;Papanicolaou test automation;automatic abnormal state classification;automatic normal state classification;cervical cancer;cervical cytology;cervix cells;convolutional neural network;deep convolutional activation feature;machine learning classifiers;medical field;microscopic image;support vector machine,,,,,,,,May 29 2017-June 1 2017,,IEEE,IEEE Conference Publications
Mitosis Detection in Phase Contrast Microscopy Image Sequences of Stem Cell Populations: A Critical Review,A. A. Liu; Y. Lu; M. Chen; Y. Su,"Electronics Information Engineering, Tianjin University, Tianjin, Tianjin China 300072 (e-mail: anan0422@gmail.com)",IEEE Transactions on Big Data,,2017,PP,99,1,1,"Detecting mitosis from cell population is a fundamental problem in many biological researches and biomedical applications. In modern researches, advanced imaging technologies have been applied to generate large amount of microscope images of cells. However, detecting all mitotic cells from these images with human eye is tedious and time-consuming. In recent years, several approaches have been proposed to help humans finish this job automatically with high efficiency and accuracy. In this review paper, we first described some commonly used datasets for mitosis detection, and then discussed different kinds of methods for mitosis detection, like tracking based methods, tracking free methods, hybrid methods, and the most recently proposed works based on deep learning architecture. We compared these methods on same datasets, and found that deep learning based approaches have achieved a great improvement in performance. At last, we discussed the future possible approaches on mitosis detection, to combine the success of previous works and the advantage of big data in modern researches. Considering expertise is highly required in biomedical area, we will further discuss the possibility to learn information from biomedical big data with less expert annotation.",,,10.1109/TBDATA.2017.2721438,China Scholarship Council; Elite Scholar Program of Tianjin University; National Natural Science Foundation of China; Tianjin Research Program of Application Foundation and Advanced Technology; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962189,big data;biomedical image;computer vision;microscopy image;mitosis detection;stem cell,Big Data;Computer architecture;Image sequences;Microscopy;Sociology;Statistics;Stem cells,,,,,,,,,20170629,,,IEEE,IEEE Early Access Articles
Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods,G. Carneiro; T. Peng; C. Bayer; N. Navab,"Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia",IEEE Transactions on Medical Imaging,20170628,2017,36,7,1405,1417,"In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitativ- results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.",0278-0062;02780062,,10.1109/TMI.2017.2677479,10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Experienced Researchers; 10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Postdoctoral Researchers; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869416,Microscopy;deep learning;high-order loss functions;structured output learning;weakly-supervised training,Biomedical imaging;Cancer;Computational modeling;Manuals;Medical treatment;Training;Tumors,biomedical optical imaging;cancer;fluorescence;image classification;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;tumours,HE images;IF images;acutely hypoxic regions;automatic quantification;baseline method;cancer patients;chronically hypoxic regions;clinical annotations;estimated annotations;fixed grid;fully-convolutional neural network;hematoxylin and eosin stained images;high resolution immunofluorescence images;high-order loss function;histological specimen;hypoxia-modified therapies;input images;latent structured output learning formulation;machine learning-based methodologies;majority classifier;manual annotations;multimodal microscopy images;naive method;normoxic regions;pixel-wise labeling;standard cancer treatments;standardized measurement;tumor hypoxia;tumour hypoxia;weakly-supervised learning methods;weakly-supervised structured output classification problem,,,,,,,20170302,July 2017,,IEEE,IEEE Journals & Magazines
Malaria Parasite Detection From Peripheral Blood Smear Images Using Deep Belief Networks,D. Bibin; M. S. Nair; P. Punitha,"Department of Research and Development Centre, Bharathiar University, Coimbatore, India",IEEE Access,20170619,2017,5,,9099,9108,"In this paper, we propose a novel method to identify the presence of malaria parasites in human peripheral blood smear images using a deep belief network (DBN). This paper introduces a trained model based on a DBN to classify 4100 peripheral blood smear images into the parasite or non-parasite class. The proposed DBN is pre-trained by stacking restricted Boltzmann machines using the contrastive divergence method for pre-training. To train the DBN, we extract features from the images and initialize the visible variables of the DBN. A concatenated feature of color and texture is used as a feature vector in this paper. Finally, the DBN is discriminatively fine-tuned using a backpropagation algorithm that computes the probability of class labels. The optimum size of the DBN architecture used in this paper is 484-600-600-600-600-2, in which the visible layer has 484 nodes and the output layer has two nodes with four hidden layers containing 600 hidden nodes in every layer. The proposed method has performed significantly better than the other state-of-the-art methods with an F-score of 89.66%, a sensitivity of 97.60%, and specificity of 95.92%. This paper is the first application of a DBN for malaria parasite detection in human peripheral blood smear images.",2169-3536;21693536,,10.1109/ACCESS.2017.2705642,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7931565,Deep learning;contrastive divergence;deep belief network;discriminative training;malaria parasite detection;restricted Boltzmann machine,Blood;Computer architecture;Diseases;Feature extraction;Image color analysis;Microscopy;Training,Boltzmann machines;backpropagation;belief networks;biology computing;blood;feature extraction;image classification;learning (artificial intelligence),DBN;backpropagation algorithm;contrastive divergence method;deep belief networks;feature extraction;feature vector;human peripheral blood smear images;malaria parasite detection;restricted Boltzmann machines,,,,,,,20170518,2017,,IEEE,IEEE Journals & Magazines
Deep residual Hough voting for mitotic cell detection in histopathology images,T. Wollmann; K. Rohr,"University of Heidelberg, BIOQUANT, IPMB, and DKFZ Heidelberg, Dept. Bioinformatics and Functional Genomics, Biomedical Computer Vision Group, Im Neuenheimer Feld 267, 69120, Germany",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,341,344,"Cell detection in microscopy images is a common and challenging task. We propose a new approach for mitotic cell detection in histopathology images, which is based on a Deep Residual Network architecture combined with Hough voting. We propose a voting layer for neural networks and introduce a novel loss function. Our approach is learned from scratch using cell centroids and the original images. We benchmarked our approach on the challenging AMIDA13 dataset containing histology images of invasive breast carcinoma. It turned out that our approach achieved competitive results.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950533,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950533,Deep Learning;Dilated Convolution;Hough transform;Microscopy;Residual Network,Biomedical imaging;Computer architecture;Convolution;Microprocessors;Neural networks;Training;Transforms,Hough transforms;biological organs;cellular biophysics;learning (artificial intelligence);medical image processing;neural net architecture,AMIDA13 dataset;cell centroids;deep residual Hough voting;deep residual network architecture;histopathology images;invasive breast carcinoma;microscopy images;mitotic cell detection,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Deep learning for magnification independent breast cancer histopathology image classification,N. Bayramoglu; J. Kannala; J. Heikkilä,"Center for Machine Vision and Signal Analysis, University of Oulu, Finland",2016 23rd International Conference on Pattern Recognition (ICPR),20170424,2016,,,2440,2445,"Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7900002,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900002,,Breast cancer;Databases;Microscopy;Pathology;Training;Training data,cancer;computer vision;image classification;learning (artificial intelligence);medical image processing;neural nets,BreaKHis dataset;breast tissues;computer vision;convolutional neural networks;deep learning;diagnostic pathology workflow;digital imaging techniques;image classification;machine learning;magnification independent breast cancer histopathology image classification;microscopic analysis;multitask CNN;single task CNN,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks,L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Transactions on Medical Imaging,20170331,2017,36,4,994,1004,"Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.",0278-0062;02780062,,10.1109/TMI.2016.2642839,Research Grants Council of the Hong Kong Special Administrative Region; 10.13039/501100003453 - Guangdong Natural Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699,Automated melanoma recognition;fully convolutional neural networks;residual learning;skin lesion analysis;very deep convolutional neural networks,Biomedical imaging;Feature extraction;Image segmentation;Lesions;Malignant tumors;Skin;Training data,biomedical optical imaging;cancer;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;skin,automated melanoma recognition;classification network;deep convolutional neural networks;deep residual networks;dermoscopy images;fully convolutional residual network;low-level hand-crafted features;medical image analysis;skin lesion segmentation;training data,,,,,,,20161221,April 2017,,IEEE,IEEE Journals & Magazines
Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation,A. Fakhry; T. Zeng; S. Ji,"Department of Computer Science, Old Dominion University, Norfolk, VA, USA",IEEE Transactions on Medical Imaging,20170201,2017,36,2,447,456,"Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.",0278-0062;02780062,,10.1109/TMI.2016.2613019,"10.13039/100000153 - National Science Foundation, Old Dominion University, and Washington State University; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638,Brain circuit reconstruction;deconvolutional networks;deep learning;electron microscopy;image segmentation;residual learning,Convolution;Deconvolution;Feature extraction;Image reconstruction;Image segmentation;Predictive models;Three-dimensional displays,brain;deconvolution;electron microscopy;image segmentation;medical image processing;neural nets;neurophysiology,3D neurite segmentation;EM image segmentation;anatomical connection reconstruction;brain electron microscopy;circuit mapping;contextual information;dense output prediction;electron microscopy images;full-resolution features;full-resolution predictions;hand-crafted features;human-level performance;information pathways;neurite segmentation challenge dataset;neurons;residual deconvolutional networks,,,,,,,20160923,Feb. 2017,,IEEE,IEEE Journals & Magazines
CNN-based image analysis for malaria diagnosis,Z. Liang; A. Powell; I. Ersoy; M. Poostchi; K. Silamut; K. Palaniappan; P. Guo; M. A. Hossain; A. Sameer; R. J. Maude; J. X. Huang; S. Jaeger; G. Thoma,"School of Information Technology, York University, Toronto, ON, M3J1P3, Canada",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,493,496,"Malaria is a major global health threat. The standard way of diagnosing malaria is by visually examining blood smears for parasite-infected red blood cells under the microscope by qualified technicians. This method is inefficient and the diagnosis depends on the experience and the knowledge of the person doing the examination. Automatic image recognition technologies based on machine learning have been applied to malaria blood smears for diagnosis before. However, the practical performance has not been sufficient so far. This study proposes a new and robust machine learning model based on a convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. In a ten-fold cross-validation based on 27,578 single cell images, the average accuracy of our new 16-layer CNN model is 97.37%. A transfer learning model only achieves 91.99% on the same images. The CNN model shows superiority over the transfer learning model in all performance indicators such as sensitivity (96.99% vs 89.00%), specificity (97.75% vs 94.98%), precision (97.73% vs 95.12%), F1 score (97.36% vs 90.24%), and Matthews correlation coefficient (94.75% vs 85.25%).",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822567,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822567,computer-aided diagnosis;convolutional neural network;deep learning;machine learning;malaria,Blood;Data models;Diseases;Machine learning;Mathematical model;Microscopy;Training,blood;cellular biophysics;diseases;image recognition;learning (artificial intelligence);medical image processing;neural nets,CNN-based image analysis;Matthews correlation coefficient;automatic image recognition technology;convolutional neural network;machine learning model;malaria diagnosis;parasite-infected red blood cell;single cell classification;single cell images,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
Cell proposal network for microscopy image analysis,S. U. Akram; J. Kannala; L. Eklund; J. Heikkilä,"Center for Machine Vision Research, University of Oulu, Finland",2016 IEEE International Conference on Image Processing (ICIP),20160819,2016,,,3199,3203,"Robust cell detection plays a key role in the development of reliable methods for automated analysis of microscopy images. It is a challenging problem due to low contrast, variable fluorescence, weak boundaries, conjoined and overlapping cells, causing most cell detection methods to fail in difficult situations. One approach for overcoming these challenges is to use cell proposals, which enable the use of more advanced features from ambiguous regions and/or information from adjacent frames to make better decisions. However, most current methods rely on simple proposal generation and scoring methods, which limits the performance they can reach. In this paper, we propose a convolutional neural network based method which generates cell proposals to facilitate cell detection, segmentation and tracking. We compare our method against commonly used proposal generation and scoring methods and show that our method generates significantly better proposals, and achieves higher final recall and average precision.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532950,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532950,cell detection;cell proposals;cell tracking;deep learning;fully convolutional network,Feature extraction;Image analysis;Image segmentation;Microscopy;Proposals;Shape;Training,image segmentation;medical image processing;neural nets,cell proposal network;cell segmentation;cell tracking;convolutional neural network based method;microscopy image analysis;robust cell detection,,1,,16,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
Membrane segmentation via active learning with deep networks,U. Gaur; M. Kourakis; E. Newman-Smith; W. Smith; B. S. Manjunath,"Department of Computer Science, University of California Santa Barbara",2016 IEEE International Conference on Image Processing (ICIP),20160819,2016,,,1943,1947,"Segmentation is a key component of several bio-medical image processing systems. Recently, segmentation methods based on supervised learning such as deep convolutional networks have enjoyed immense success for natural image datasets and biological datasets alike. These methods require large volumes of data to avoid overfitting which limits their applicability. In this work, we present a transfer learning mechanism based on active learning which allows us to utilize pre-trained deep networks for segmenting new domains with limited labelled data. We introduce a novel optimization criterion to allow feedback on the most uncertain, yet abundant image patterns thus provisioning for an expert in the loop albeit with minimum amount of guidance. Our experiments demonstrate the effectiveness of the proposed method in improving segmentation performance with very limited labelled data.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532697,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532697,Active Learning;Deep Networks;Image Segmentation;Transfer Learning,Computer architecture;Image segmentation;Microprocessors;Microscopy;Optimization;Training;Uncertainty,convolution;image segmentation;learning (artificial intelligence);optimisation,active learning;deep convolutional networks;deep networks;membrane segmentation;optimization criterion;supervised learning;transfer learning mechanism,,,,17,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
Multi-loss convolutional networks for gland analysis in microscopy,A. BenTaieb; J. Kawahara; G. Hamarneh,"Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,642,645,"Manual tissue diagnosis is the most prevalent approach to cancer diagnosis. However, it mainly relies on a subjective visual quantification of specific morphometric features, which often leads to a relatively limited reproducibility among experts. In most computational techniques proposed to automate the diagnostic procedure, accurate segmentation is paramount as a precursor to the extraction of relevant morphometric features. Since the ultimate goal of segmentation is generally classification, yet a given class imparts an expected tissue appearance beneficial to segmentation, we pose the problem of automatic tissue analysis as the joint task of segmentation and classification. We propose a novel multi-objective learning method that optimizes a single unified deep fully convolutional neural network with two distinct loss functions. We illustrate our reasoning on the task of colon adenocarcinomas diagnosis and show how glands' classification can facilitate their segmentation by adding class-specific spatial priors. The final classification also benefits from this joint learning framework yielding an improvement of 6% over classification-only models.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493349,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493349,Classification;Deep Learning;Histopathology;Segmentation,Cancer;Colon;Feature extraction;Glands;Image segmentation;Training;Tumors,cancer;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation,automatic tissue analysis;cancer diagnosis;colon adenocarcinoma diagnosis;gland classification;image segmentation;microscopy;morphometric feature extraction;multiloss convolutional networks;multiobjective learning method;optimization,,,,6,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Structure-based assessment of cancerous mitochondria using deep networks,M. Mishra; S. Schmitt; L. Wang; M. K. Strasser; C. Marr; N. Navab; H. Zischka; T. Peng,"Computer Aided Medical Procedures (CAMP), Technische Universitaet Muenchen, Germany",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,545,548,"Mitochondrial functions are essential for cell survival. Pathologic situations, e.g. cancer, can impair mitochondrial function which is frequently reflected by an altered morphology. So far, feature description of mitochondrial structure in cancer remains largely qualitative. In this study, we propose a learning-based approach to quantitatively assess the structure of mitochondria isolated from liver tumor cell lines using convolutional neural network (CNN). Besides achieving a high classification accuracy on isolated mitochondria from healthy tissue and different tumor cell lines which the CNN model was trained on, CNN is also able to classify unseen tumor cell lines, which suggests its superior capability to capture the intrinsic structural transition from healthy to tumor mitochondria.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493327,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493327,Mitochondria;convolutional neural network;deep learning;electron microscopy,Indexes;Liver;Standards;Support vector machines;Training;Tumors,cancer;cellular biophysics;learning (artificial intelligence);liver;medical image processing;microorganisms;neurophysiology;tumours,CNN model;altered morphology;cancer;cancerous mitochondria;cell survival;convolutional neural network;deep networks;healthy tumor mitochondria;high classification accuracy;learning-based approach;liver tumor cell lines;mitochondria isolated structure;mitochondrial functions;mitochondrial structure;pathologic situations;structural transition;structure-based assessment;tumor cell lines,,,,10,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions,G. Carneiro; T. Peng; C. Bayer; N. Navab,"Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia",2015 IEEE International Conference on Computer Vision (ICCV),20160218,2015,,,648,656,"We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.",,Electronic:978-1-4673-8391-2; POD:978-1-4673-8392-9,10.1109/ICCV.2015.81,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410438,,Cancer;Computer vision;Manuals;Microscopy;Support vector machines;Training;Tumors,cancer;graph theory;medical image processing;neural nets;support vector machines;tumours,DCNN model;FLSSVM;cancer treatment;deep convolutional neural network;flexible graph;flexible latent structure support vector machine;high-order loss function;latent graph;malignant tumour;microcirculatory supply units;microscopy imaging;weakly-supervised structured output learning,,,,37,,,,7-13 Dec. 2015,,IEEE,IEEE Conference Publications
Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy,B. Jiang; X. Wang; J. Luo; X. Zhang; Y. Xiong; H. Pang,"Guangzhou Inst. of Biomed. & Health, Guangzhou, China","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)",20160215,2015,,,122,126,"The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.",,Electronic:978-1-4673-7723-2; POD:978-1-4673-7724-9,10.1109/IMCCC.2015.33,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405812,bright-field microscopy;convolutional neural networks;deep learning;machine learning;non-invasive;non-labelled;trans-differentiated neural progenitor cells,Biological neural networks;Electronic mail;Feature extraction;Image recognition;Machine learning;Microscopy;Morphology,cellular biophysics;image classification;medical image processing;neural nets;optical microscopy,NPC classification;automatic recognition;bright-field microscopic imaging analysis;convolutional neural networks;image pre-processing;nonNPC classification;trans-differentiated neural progenitor cells,,,,17,,,,18-20 Sept. 2015,,IEEE,IEEE Conference Publications
Deep learning for healthcare decision making with EMRs,Z. Liang; G. Zhang; J. X. Huang; Q. V. Hu,"School of Information Technology, York University, Toronto, ON, M3J1P3, Canada",2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20150115,2014,,,556,559,"Computer aid technology is widely applied in decision-making and outcome assessment of healthcare delivery, in which modeling knowledge and expert experience is technically important. However, the conventional rule-based models are incapable of capturing the underlying knowledge because they are incapable of simulating the complexity of human brains and highly rely on feature representation of problem domains. Thus we attempt to apply a deep model to overcome this weakness. The deep model can simulate the thinking procedure of human and combine feature representation and learning in a unified model. A modified version of convolutional deep belief networks is used as an effective training method for large-scale data sets. Then it is tested by two instances: a dataset on hypertension retrieved from a HIS system, and a dataset on Chinese medical diagnosis and treatment prescription from a manual converted electronic medical record (EMR) database. The experimental results indicate that the proposed deep model is able to reveal previously unknown concepts and performs much better than the conventional shallow models.",,Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8,10.1109/BIBM.2014.6999219,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999219,deep belief network;deep learning;restricted Boltzmann machine;syndrome classification;unsupervised feature learning,Brain modeling;Data models;Hypertension;Medical diagnostic imaging;Support vector machines;Training,belief networks;brain;brain-computer interfaces;decision making;electronic health records;health care;patient diagnosis;patient treatment;unsupervised learning,Chinese medical diagnosis;Chinese medical treatment prescription;EMR database;HIS system;belief networks;computer aid technology;decision making;deep learning;electronic medical record database;feature representation;healthcare;human brains;shallow models;training method,,3,,18,,,,2-5 Nov. 2014,,IEEE,IEEE Conference Publications
Staged Inference using Conditional Deep Learning for energy efficient real-time smart diagnosis,M. Parsa; P. Panda; S. Sen; K. Roy,"School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907, USA",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,78,81,"Recent progress in biosensor technology and wearable devices has created a formidable opportunity for remote healthcare monitoring systems as well as real-time diagnosis and disease prevention. The use of data mining techniques is indispensable for analysis of the large pool of data generated by the wearable devices. Deep learning is among the promising methods for analyzing such data for healthcare applications and disease diagnosis. However, the conventional deep neural networks are computationally intensive and it is impractical to use them in real-time diagnosis with low-powered on-body devices. We propose Staged Inference using Conditional Deep Learning (SICDL), as an energy efficient approach for creating healthcare monitoring systems. For smart diagnostics, we observe that all diagnoses are not equally challenging. The proposed approach thus decomposes the diagnoses into preliminary analysis (such as healthy vs unhealthy) and detailed analysis (such as identifying the specific type of cardio disease). The preliminary diagnosis is conducted real-time with a low complexity neural network realized on the resource-constrained on-body device. The detailed diagnosis requires a larger network that is implemented remotely in cloud and is conditionally activated only for detailed diagnosis (unhealthy individuals). We evaluated the proposed approach using available physiological sensor data from Physionet databases, and achieved 38% energy reduction in comparison to the conventional deep learning approach.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036767,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036767,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database,C. Y. Hung; W. C. Chen; P. T. Lai; C. H. Lin; C. C. Lee,"Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,3110,3113,"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037515,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037515,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Oro Vision: Deep Learning for Classifying Orofacial Diseases,R. Anantharaman; V. Anantharaman; Y. Lee,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,39,45,"This experiment is an attempt to apply deep learning techniques to orofacial image analysis. Health promotion is recognized as a viable approach to preventing diseases and disorders and promoting changes in health behaviors or practices. Each year, oral cancer kills more people in the US than does cervical cancer, malignant melanoma, or Hodgkin's disease. A first line of defense against oral diseases is an orofacial selfexamination. The goal of this experiment titled ""Oro Vision"" is to provide an assessment tool for field workers to perform initial examinations of orofacial diseases, using a camera enabled mobile phone. For this experiment, we chose to implement Oro Vision to detect mouth sores. The goal is to extend this model to identify several other Oral diseases such as Thrush, Leukoplakia, Lichenplanus, etc. One variety of mouth sore, referred to as the ""cold sore"" is highly contagious and an infected person can easily pass on the infection to another person just through skin to skin contact. ""Oro Vision"" is implemented as an HTML5 mobile responsive web app that can be accessed through any mobile or standard browser. Oro Vision uses deep learning to train a model and subsequently uses this trained model to distinguish a cold sore from a canker sore. In addition, an accurate diagnosis by a trained healthcare professional is required before any kind of treatment is discussed since several other conditions of the mouth including oral cancer may mimic canker sores.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.69,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031130,clarifai;deep learning;mouth sore;retrained inception,Cancer;Machine learning;Mobile communication;Mouth;Tools,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Continuous Assessment of Children’s Emotional States Using Acoustic Analysis,Y. Gong; C. Poellabauer,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,171,178,"Emotional and behavioral disorders (EBD) are a widespread healthcare concern in children and adolescents. Prevention and early intervention are the most powerful tools in ameliorating the problem, and therefore, timely and accurate detection of abnormal emotional patterns is of vital importance. In this paper, we propose a system that detects second-level emotional states of children using hour-level audio recordings. The proposed system consists of an audio segmentation and speaker tracking front-end along with an emotion recognition back-end. Supervised support vector machine is used in the front-end to improve its robustness to short and inconsistent child speech pattern and end-to-end deep learning is used in the emotion recognition back-end to improve its robustness to noise and segmentation error. We further demonstrate the potential of the proposed system as an automated emotion analysis tool.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.53,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031145,children emotion recognition;continuous emotion assessment;deep neural networks;emotional and behavioral disorders,Acoustics;Emotion recognition;Medical treatment;Pediatrics;Speaker recognition;Speech;Speech recognition,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
A Deep-Learning-Based Method of Estimating Water Intake,Y. Yamada; T. Saito; S. Kawasaki; D. Iketa; M. Katagiri; M. Nishimura; H. Mineno,,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),20170911,2017,2,,96,101,"In Japan, which has become a very aged society, the increasing burden of nursing care is an issue. Services and systems related to automatic recording of healthcare management of elderly people have been proposed in order to reduce the burden of nursing care. Water intake is one of the items necessary for healthcare management of elderly people. However, it is not currently automated, which is a burden on caregivers. In the case of the conventional method, the swallowing sound is used for estimating the water intake. However, the estimation error for each subject is large. Accuracy of estimated water intake is improved by using deep learning. Specifically, three features, namely, mel frequency cepstral coefficient (MFCC), duration of water intake, and a RASTA filter auditory spectrum, are extracted from a subject's swallowing sound (which is thought to be highly correlated with water intake). A method of estimating water intake, which considers abstract features that are difficult for people to find, is proposed and verified. It is experimentally shown that RMSE of water intake estimated by the proposed method using deep learning is reduced compared with that estimated by conventional methods.",0730-3157;07303157,Electronic:978-1-5386-0367-3; POD:978-1-5386-0368-0,10.1109/COMPSAC.2017.14,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029900,deep learning;estimation;healthcare;water intake,Feature extraction;Hidden Markov models;Lungs;Machine learning;Medical services;Microphones;Senior citizens,,,,,,,,,,4-8 July 2017,,IEEE,IEEE Conference Publications
Classification of various daily behaviors using deep learning and smart watch,M. C. Kwon; M. Ju; S. Choi,"Dept. of Secured Smart Electric Vehicle, Kookmin University, Seoul, Korea 02707",2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN),20170727,2017,,,735,740,"In traditional healthcare and therapy, human behavior has been classified into only two categories: specific behavior and active behavior. As internet of things and wearable devices become popular, however, it is necessary to classify human behavior into more various categories for providing useful services. In this paper, we propose a novel classification scheme that classifies human behavior into 11 different categories including active and inactive activities in daily life. We collect data with smart watch and use deep learning model with a neural network for the classification. Extensive evaluation shows that various daily human behavior can be classified with 99.24% accuracy, and that the classification of human behavior can be used for various services.",,Electronic:978-1-5090-4749-9; POD:978-1-5090-4750-5; USB:978-1-5090-4748-2,10.1109/ICUFN.2017.7993888,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7993888,classification;deep learning;human behavior;internet of things;smart watch,Acceleration;Feature extraction;Games;Intelligent sensors;Machine learning;Medical services,Internet of Things;behavioural sciences computing;medical computing;neural nets;patient monitoring;patient treatment;pattern classification;wearable computers,Internet of things;active behavior;classification scheme;daily behaviors classification;daily life;deep learning;healthcare;human behavior;inactive activities;neural network;smart watch;specific behavior;therapy;wearable devices,,,,,,,,4-7 July 2017,,IEEE,IEEE Conference Publications
Deep Features Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network,M. Chen; X. Shi; Y. Zhang; D. Wu; M. Guizani,"School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, HuBei China (e-mail: minchen@ieee.org)",IEEE Transactions on Big Data,,2017,PP,99,1,1,"At present, computed tomography (CT) are widely used to assist diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) is an extremely important research field in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it evaluates that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during of artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.",,,10.1109/TBDATA.2017.2717439,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954012,Convolutional autoencoder neural network;Feature learning;Hand-craft feature;Lung nodule;Unsupervised learning,Biomedical imaging;Computed tomography;Convolutional codes;Feature extraction;Image analysis;Lungs;Training,,,,,,,,,20170620,,,IEEE,IEEE Early Access Articles
Automated 5-year mortality prediction using deep learning and radiomics features from chest computed tomography,G. Carneiro; L. Oakden-Rayner; A. P. Bradley; J. Nascimento; L. Palmer,"Australian Centre for Visual Technologies, The University of Adelaide, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,130,134,"In this paper, we propose new prognostic methods that predict 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on two state-of-the-art deep learning models extended to 3-D inputs, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection and extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning models produces a mean 5-year mortality prediction AUC in [68.8%,69.8%] and accuracy in [64.5%,66.5%], while radiomics produces a mean AUC of 64.6% and accuracy of 64.6%. The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950485,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950485,computed tomography;deep learning;feature learning;five-year mortality;hand-designed features;radiomics,Biomedical imaging;Computed tomography;Fats;Feature extraction;Image segmentation;Machine learning;Training,computerised tomography;feature extraction;geriatrics;health care;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation,3-D inputs;CT image;annotated chest CT;automated 5-year mortality prediction;chest computed tomography;classifier learning process;deep learning models;elderly individuals;feature extraction;hand-crafted radiomics features;mean 5-year mortality prediction AUC;multiple anatomic structures;multistage framework;personalised healthcare;preventive healthcare;prognostic methods;segmentation maps;single optimisation process;unified framework,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Deep learning Parkinson's from smartphone data,C. Stamate; G. D. Magoulas; S. Kueppers; E. Nomikou; I. Daskalopoulos; M. U. Luchini; T. Moussouri; G. Roussos,"Birkbeck College, University of London, UK",2017 IEEE International Conference on Pervasive Computing and Communications (PerCom),20170504,2017,,,31,40,"The cloudUPDRS app is a Class I medical device, namely an active transient non-invasive instrument, certified by the Medicines and Healthcare products Regulatory Agency in the UK for the clinical assessment of the motor symptoms of Parkinson's Disease. The app follows closely the Unified Parkinson's Disease Rating Scale which is the most commonly used protocol in the clinical study of PD; can be used by patients and their carers at home or in the community; and, requires the user to perform a sequence of iterated movements which are recorded by the phone sensors. This paper discusses how the cloudUPDRS system addresses two key challenges towards meeting essential consistency and efficiency requirements, namely: (i) How to ensure high-quality data collection especially considering the unsupervised nature of the test, in particular, how to achieve firm user adherence to the prescribed movements; and (ii) How to reduce test duration from approximately 25 minutes typically required by an experienced patient, to below 4 minutes, a threshold identified as critical to obtain significant improvements in clinical compliance. To address the former, we combine a bespoke design of the user experience tailored so as to constrain context, with a deep learning approach used to identify failures to follow the movement protocol while at the same time limiting false positives to avoid unnecessary repetition. We address the latter by developing a machine learning approach to personalise assessments by selecting those elements of the UPDRS protocol that most closely match individual symptom profiles and thus offer the highest inferential power hence closely estimating the patent's overall UPRDS score.",,Electronic:978-1-5090-4327-9; POD:978-1-5090-4328-6,10.1109/PERCOM.2017.7917848,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917848,,Conferences;Diseases;Machine learning;Market research;Pervasive computing;Protocols;Sensors,cloud computing;learning (artificial intelligence);medical computing;medical disorders;smart phones,Medicines-and-Healthcare products Regulatory Agency;UK;UPDRS protocol;active transient noninvasive instrument;class-I medical device;clinical assessment;cloudUPDRS application;data collection;deep learning approach;false positives;machine learning approach;motor symptoms;smart phone data;test duration reduction;unified Parkinson's disease rating scale;user experience,,,,,,,,13-17 March 2017,,IEEE,IEEE Conference Publications
Semi-automated annotation of signal events in clinical EEG data,S. Yang; S. López; M. Golmohammadi; I. Obeid; J. Picone,"Neural Engineering Data Consortium, Temple University, Philadelphia, Pennsylvania, USA",2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),20170209,2016,,,1,5,"To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation.",,Electronic:978-1-5090-6713-8; POD:978-1-5090-6714-5,10.1109/SPMB.2016.7846855,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846855,,Brain models;Data models;Electroencephalography;Hidden Markov models;Sensitivity;Training,electroencephalography;health care;iterative methods;learning (artificial intelligence);medical signal processing,EEG corpus;clinical EEG data;confidence scores;deep learning;healthcare;high-performance automated decision support systems;initial training iteration;machine learning;signal events semi-automated annotation,,,,,,,,3-3 Dec. 2016,,IEEE,IEEE Conference Publications
The effects of deep network topology on mortality prediction,H. Du; M. M. Ghassemi; M. Feng,"School of Electrical and Electronic Engineering, Nanyang Technology University, Singapore",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,2602,2605,"Deep learning has achieved remarkable results in the areas of computer vision, speech recognition, natural language processing and most recently, even playing Go. The application of deep-learning to problems in healthcare, however, has gained attention only in recent years, and it's ultimate place at the bedside remains a topic of skeptical discussion. While there is a growing academic interest in the application of Machine Learning (ML) techniques to clinical problems, many in the clinical community see little incentive to upgrade from simpler methods, such as logistic regression, to deep learning. Logistic regression, after all, provides odds ratios, p-values and confidence intervals that allow for ease of interpretation, while deep nets are often seen as `black-boxes' which are difficult to understand and, as of yet, have not demonstrated performance levels far exceeding their simpler counterparts. If deep learning is to ever take a place at the bedside, it will require studies which (1) showcase the performance of deep-learning methods relative to other approaches and (2) interpret the relationships between network structure, model performance, features and outcomes. We have chosen these two requirements as the goal of this study. In our investigation, we utilized a publicly available EMR dataset of over 32,000 intensive care unit patients and trained a Deep Belief Network (DBN) to predict patient mortality at discharge. Utilizing an evolutionary algorithm, we demonstrate automated topology selection for DBNs. We demonstrate that with the correct topology selection, DBNs can achieve better prediction performance compared to several bench-marking methods.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7591263,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591263,,Bioinformatics;Genomics;Machine learning;Network topology;Neural networks;Topology;Training,belief networks;electronic health records;evolutionary computation;health care;learning (artificial intelligence);medical computing,EMR dataset;deep belief network;deep network topology;deep-learning methods;electronic medical record;evolutionary algorithm;healthcare;intensive care unit patients;network structure;patient mortality prediction,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
An adaptive deep learning approach for PPG-based identification,V. Jindal; J. Birjandtalab; M. B. Pouyan; M. Nourani,"Quality of Life Technology Laboratory, The University of Texas at Dallas, Richardson, TX 75080",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,6401,6404,"Wearable biosensors have become increasingly popular in healthcare due to their capabilities for low cost and long term biosignal monitoring. This paper presents a novel two-stage technique to offer biometric identification using these biosensors through Deep Belief Networks and Restricted Boltzman Machines. Our identification approach improves robustness in current monitoring procedures within clinical, e-health and fitness environments using Photoplethysmography (PPG) signals through deep learning classification models. The approach is tested on TROIKA dataset using 10-fold cross validation and achieved an accuracy of 96.1%.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7592193,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592193,,Biological system modeling;Biomedical monitoring;Brain modeling;Feature extraction;Machine learning;Neural networks;Training,Boltzmann machines;belief networks;biometrics (access control);biosensors;body sensor networks;health care;learning (artificial intelligence);medical signal processing;patient monitoring;photoplethysmography,10-fold cross validation;Deep Belief Networks;PPG-based identification;Restricted Boltzman Machines;TROIKA dataset;adaptive deep learning approach;biometric identification;biosignal monitoring;clinical environments;deep learning classification models;e-health environments;fitness environments;healthcare;photoplethysmography signals;two-stage technique;wearable biosensors,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
Improving Tuberculosis Diagnostics Using Deep Learning and Mobile Health Technologies among Resource-Poor and Marginalized Communities,Y. Cao; C. Liu; B. Liu; M. J. Brunette; N. Zhang; T. Sun; P. Zhang; J. Peinado; E. S. Garavito; L. L. Garcia; W. H. Curioso,"Dept. of Comput. Sci., Univ. of Massachusetts-Lowell, Lowell, MA, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20160818,2016,,,274,281,"Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Perú, a densely occupied urban community and high-burden TB.",,Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2,10.1109/CHASE.2016.18,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545842,Perú;deep convolutional neural networks;deep learning;diagnosis;mHealth;mobile computing;tuberculosis,Diagnostic radiography;Image databases;Mobile communication;Mobile computing;Mobile handsets;X-ray imaging,diagnostic radiography;diseases;epidemics;image classification;learning (artificial intelligence);medical image processing;mobile computing;neural nets;patient diagnosis;visual databases,African;CNN-based models;Deep Learning;Mobile Health Technologies;South-East Asia;TB diagnosis delay;Tuberculosis Diagnostics;Western Pacific;X-ray image database;automated TB screening;chronic infectious disease;deep convolutional neural network-based models;disease transmission;healthcare;image classification;machine learning;mobile computing;tuberculosis epidemic,,,,,,,,27-29 June 2016,,IEEE,IEEE Conference Publications
Deep learning for human activity recognition: A resource efficient implementation on low-power devices,D. Ravi; C. Wong; B. Lo; G. Z. Yang,"The Hamlyn Centre, Imperial College London, London",2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN),20160721,2016,,,71,76,"Human Activity Recognition provides valuable contextual information for wellbeing, healthcare, and sport applications. Over the past decades, many machine learning approaches have been proposed to identify activities from inertial sensor data for specific applications. Most methods, however, are designed for offline processing rather than processing on the sensor node. In this paper, a human activity recognition technique based on a deep learning methodology is designed to enable accurate and real-time classification for low-power wearable devices. To obtain invariance against changes in sensor orientation, sensor placement, and in sensor acquisition rates, we design a feature generation process that is applied to the spectral domain of the inertial data. Specifically, the proposed method uses sums of temporal convolutions of the transformed input. Accuracy of the proposed approach is evaluated against the current state-of-the-art methods using both laboratory and real world activity datasets. A systematic analysis of the feature generation parameters and a comparison of activity recognition computation times on mobile devices and sensor nodes are also presented.",,Electronic:978-1-5090-3087-3; POD:978-1-5090-3088-0,10.1109/BSN.2016.7516235,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516235,ActiveMiles;Deep Learning;HAR;Low-Power Devices,Convolution;Data mining;Feature extraction;Machine learning;Spectrogram;Time-frequency analysis,convolution;feature extraction;image recognition;learning (artificial intelligence);sensor placement,contextual information;deep learning;feature generation;human activity recognition;inertial sensor data;low-power wearable devices;machine learning;mobile devices;offline processing;resource efficient implementation;sensor acquisition rates;sensor nodes;sensor placement;spectral domain;temporal convolutions,,,,,,,,14-17 June 2016,,IEEE,IEEE Conference Publications
A restricted Boltzmann machine based two-lead electrocardiography classification,Y. Yan; X. Qin; Y. Wu; N. Zhang; J. Fan; L. Wang,"Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN),20151019,2015,,,1,9,"An restricted Boltzmann machine learning algorithm were proposed in the two-lead heart beat classification problem. ECG classification is a complex pattern recognition problem. The unsupervised learning algorithm of restricted Boltzmann machine is ideal in mining the massive unlabelled ECG wave beats collected in the heart healthcare monitoring applications. A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. In this paper a deep belief network was constructed and the RBM based algorithm was used in the classification problem. Under the recommended twelve classes by the ANSI/AAMI EC57: 1998/(R)2008 standard as the waveform labels, the algorithm was evaluated on the two-lead ECG dataset of MIT-BIH and gets the performance with accuracy of 98.829%. The proposed algorithm performed well in the two-lead ECG classification problem, which could be generalized to multi-lead unsupervised ECG classification or detection problems.",2376-8886;23768886,Electronic:978-1-4673-7201-5; POD:978-1-4673-7202-2,10.1109/BSN.2015.7299399,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299399,big data;deep belief network;electrocardiography classification;restricted Boltzmann machine,Accuracy;Data models;Electrocardiography;Feature extraction;Heart beat;Signal processing algorithms;Training,Boltzmann machines;diseases;electrocardiography;learning (artificial intelligence);medical signal processing;signal classification;stochastic processes,MIT-BIH arrhythmia database;RBM learning algorithm;complex pattern recognition;heart healthcare monitoring;massive unlabelled ECG wave beat;multilead unsupervised ECG classification;probability distribution;restricted Boltzmann machine;stochastic artificial neural network;two-lead ECG classification;two-lead ECG dataset;two-lead heart beat classification,,2,,31,,,,9-12 June 2015,,IEEE,IEEE Conference Publications
Inexpensive user tracking using Boltzmann Machines,E. Mocanu; D. C. Mocanu; H. B. Ammar; Z. Zivkovic; A. Liotta; E. Smirnov,"Department of Electrical Engineering, Eindhoven University of Technology, Netherlands","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",20141204,2014,,,1,6,"Inexpensive user tracking is an important problem in various application domains such as healthcare, human-computer interaction, energy savings, safety, robotics, security and so on. Yet, it cannot be easily solved due to its probabilistic nature, high level of abstraction and uncertainties, on the one side, and to the limitations of our current technologies and learning algorithms, on the other side. In this paper, we tackle this problem by using the Multi-integrated Sensor Technology, which comes at a low price. At the same time, we are aiming to address the lightweight learning requirements by investigating Factored Conditional Restricted Boltzmann Machines (FCRBMs), a form of Deep Learning, that has proven to be an efficient and effective machine learning framework. However, due to their construction properties, the conventional FCRBMs are only capable of performing predictions but are not capable of making classification. Herein, we are proposing extended FCRBMs (eFCRBMs), which incorporate a novel classification scheme, to solve this problem. Experiments performed on both artificially generated as well as real-world data demonstrate the effectiveness and efficiency of the proposed technique. We show that eFCRBMs outperform popular approaches including Support Vector Machines, Naive Bayes, AdaBoost, and Gaussian Mixture Models.",1062-922X;1062922X,Electronic:978-1-4799-3840-7; POD:978-1-4799-3841-4; USB:978-1-4799-3839-1,10.1109/SMC.2014.6973875,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973875,,Computers;History;Neurons;Probabilistic logic;Robot sensing systems;TV;Time series analysis,Boltzmann machines;Gaussian processes;image classification;image fusion;learning (artificial intelligence);object detection;object tracking;recurrent neural nets;support vector machines,AdaBoost;Gaussian mixture models;classification scheme;deep learning;energy savings;extended FCRBM;factored conditional restricted Boltzmann machines;healthcare;human-computer interaction;inexpensive user tracking;learning requirements;machine learning framework;multiintegrated sensor technology;naive Bayes;people detection;people tracking;robotics;support vector machines,,4,,17,,,,5-8 Oct. 2014,,IEEE,IEEE Conference Publications
Predictive Modeling of Therapy Decisions in Metastatic Breast Cancer with Recurrent Neural Network Encoder and Multinomial Hierarchical Regression Decoder,Y. Yang; P. A. Fasching; V. Tresp,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,46,55,"The increasing availability of novel health-related data sources —e.g., from molecular analysis, health Apps and electronic health records— might eventually overwhelm the physician, and the community is investigating analytics approaches that might be useful to support clinical decisions. In particular, the success of the latest developments in Deep Learning has demonstrated that machine learning models are capable of handling —and actually profiting from— high dimensional and possibly sequential data. In this work, we propose an encoder-decoder network approach to model the physician's therapy decisions. Our approach also provides physicians with a list of similar historical patient cases to support the recommended decisions. By using a combination of a Recurrent Neural Network Encoder and a Multinomial Hierarchical Regression Decoder, we specifically tackle two common challenges in modeling clinical data:First, the issue of handling episodic data of variable lengths and, second, the need to represent hierarchical decision procedures. We conduct experiments on a large real-world dataset collected from thousands of metastatic breast cancer patients and show that our model outperforms more traditional approaches.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.51,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031131,Decision Support;Encoder-Decoder Framework;Hierarchical Regression;Recurrent Neural Networks,Breast cancer;Decoding;Metastasis;Predictive models;Surgery,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Language-Based Process Phase Detection in the Trauma Resuscitation,Y. Gu; X. Li; S. Chen; H. Li; R. A. Farneth; I. Marsic; R. S. Burd,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,239,247,"Process phase detection has been widely used in surgical process modeling (SPM) to track process progression. These studies mostly used video and embedded sensor data, but spoken language also provides rich semantic information directly related to process progression. We present a long-short term memory (LSTM) deep learning model to predict trauma resuscitation phases using verbal communication logs. We first use an LSTM to extract the sentence meaning representations, and then sequentially feed them into another LSTM to extract the mean-ing of a sentence group within a time window. This information is ultimately used for phase prediction. We used 24 manually-transcribed trauma resuscitation cases to train, and the remain-ing 6 cases to test our model. We achieved 79.12% accuracy, and showed performance advantages over existing visual-audio systems for critical phases of the process. In addition to language information, we evaluated a multimodal phase prediction structure that also uses audio input. We finally identified the challenges of substituting manual transcription with automatic speech recognition in trauma resuscitation.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.50,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031153,LSTM;deep learning;process phase detection;semantic representation;verbal communication logs,Logic gates;Microphones;Phase detection;Semantics;Speech;Surgery,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data,Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,380,385,"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.45,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178,,Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
CVRT: Cognitive Visual Recognition Tracker,M. Velazquez; Y. Lee,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,31,38,"Studies on visual attention of patients with Alzheimer's disease and Dementia is a promising way for keeping track of the individual patient's image recognition ability over. This research seeks to expand upon the current applications of combining the Android operating system with TensorFlow by providing a visual question answering platform for image analysis. This application, Cognitive Visual Recognition Tracker (CVRT), provides an entry point by which the user can ask questions concerning any image of their choosing, and then receive cumulative metrics over time to better assess any diminishing cognitive ability (i.e. Alzheimer's patients). In this work, recurrent neural networks as well as semantic analysis are leveraged to provide an interactive VQA experience. One of the main objectives of CVRT is for physicians to be able to determine trends from patient data that could either be applicable to the individual patient, or to many patients if an aggregate is formed from many individual datasets. On an individual level, these metrics would provide a way for the physician to monitor daily cognitive capability, whereas on a grander scale, these joint datasets could be used to provide better overall treatment for the disease with the future inclusion of predictive analytics. The final contribution is an interactive metrics platform by which other users can assess the primary user's cognitive capacity based on features of their questioning, and to then provide them with accurate trending or possible remediation plans based on their condition.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.65,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031129,Alzheimer's disease and Dementia;Deep Learning;Visual Question Answering,Conferences;Informatics;Medical services,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Automated EEG-Based Epileptic Seizure Detection Using Deep Neural Networks,J. Birjandtalab; M. Heydarzadeh; M. Nourani,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,552,555,"Millions of people around the world suffer from epilepsy. It is very important to provide a method to efficiently monitor the seizures and alert the caregivers to help patients. It is proven that EEG signals are the best markers for diagnosis of the epileptic seizures. In this paper, we used the frequency domain features (normalized in-band power spectral density) to extract information from EEG signals. We applied a deep learning technique based on multilayer perceptrons to improve the accuracy of seizure detection. The results indicate that our nonlinear technique is able to efficiently and automatically detect seizure and non-seizure episodes with an F-measure accuracy of around 95%.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.55,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031211,Deep Neural Networks;EEG signals;Feature extraction;Multi Layer Perceptron;Seizure detection,Biological neural networks;Electroencephalography;Feature extraction;Multilayer perceptrons;Training,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Medical Concept Normalization for Online User-Generated Texts,K. Lee; S. A. Hasan; O. Farri; A. Choudhary; A. Agrawal,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,462,469,"Social media has become an important tool for sharing content in the last decade. People often talk about their experiences and opinions on different health-related issues e.g. they write reviews on medications, describe symptoms and ask informal questions about various health concerns. Due to the colloquial nature of the languages used in the social media, it is often difficult for an automated system to accurately interpret them for appropriate clinical understanding. To address this challenge, this paper proposes a novel approach for medical concept normalization of user-generated texts to map a health condition described in the colloquial language to a medical concept defined in standard clinical terminologies. We use multiple deep learning architectures such as convolutional neural networks (CNN) and recurrent neural networks (RNN) with input word embeddings trained on various clinical domain-specific knowledge sources. Extensive experiments on two benchmark datasets demonstrate that the proposed models can achieve up to 21.28% accuracy improvements over the existing models when we use the combination of all knowledge sources to learn neural embeddings.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.59,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031195,deep learning;medical concept normalization;social media,Drugs;Hair;Hidden Markov models;Medical diagnostic imaging;Pain;Recurrent neural networks;Social network services,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Deep Learning Based Recognition of Meltdown in Autistic Kids,V. S. P. Patnam; F. T. George; K. George; A. Verma,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,391,396,"Children with autism often experience sudden meltdowns which not only makes the moment tough for the caretakers/parents but also make the children hurt themselves physically. Studies have discovered that children with autistic spectrum disorder exhibit certain actions through which we can anticipate mutilating meltdowns in them. The objective of our project is to build a system that can recognize such kind of actions using deep learning techniques thereby, notifying the caretakers/parents so that they can get the situation under control in lesser time. Using deep learning RCNNs, we can train the system faster yet reliable because unlike all the machine learning algorithms, deep learning algorithms are more efficient and have more scope into future. We have trained a classifier on images that are gathered from videos and reliable internet sources with most predictive gestures, through which we can detect the meltdowns more precisely. We have trained a model that validated the accuracy by ~93% which is accompanied by a loss/train classifier with a minimal 0.4% loss. Functional testing was done through feeding the deep neural network with chosen actions performed by five individuals that resulted in an accuracy of ~92% in all cases, which can assure the real-time usage of the system.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.35,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031180,Autistic spectrum disorder;Convolution Neural Network;Graphics Processing Unit;deep learning;inference;training classifiers,Autism;Convolution;Databases;Ear;Machine learning;Neural networks;Training,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Single Sensor Techniques for Sleep Apnea Diagnosis Using Deep Learning,R. K. Pathinarupothi; D. P. J; E. S. Rangan; G. E. A; V. R; K. P. Soman,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,524,529,"A large number of obstructive sleep apnea (OSA) cases are under-diagnosed due unavailability, inconvenience or expense of sleep labs. Hence, an automated detection by applying computational techniques to multivariate signals has already become a well-researched subject. However, the best-known techniques that use various features have not achieved the gold standard of polysomnography (PSG) tests. In this paper, we substantiate the medical conjecture that OSA directly impacts body parameters such as Instantaneous Heart Rate (IHR) and blood oxygen saturation (SpO2). We then use a deep learning technique called LSTM-RNN (long short-term memory recurrent neural networks) to experimentally prove that OSA severity detection can be solely based on either IHR or SpO2 signals, which can be easily, obtained using off-the-shelf non-intrusive wearable single sensors. The results obtained from LSTM-RNN model shows an area under curve (AUC) of 0.98 associated with very high accuracy on a dataset of more than 16,000 apnea non-apnea minutes. These results have encouraged our collaborating doctors to further come up with a diagnostic protocol that is based on LSTM-RNN, SpO2, and IHR, thereby increasing the chances of larger adoption among medical community.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.37,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031206,Instantaneous Heart Rate and SpO2;LSTM-RNN;Obstructive sleep apnea,Electrocardiography;Heart rate variability;Medical diagnostic imaging;Sleep apnea;Time series analysis,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Extracting Drug-Drug Interactions with Word and Character-Level Recurrent Neural Networks,R. Kavuluru; A. Rios; T. Tran,,2017 IEEE International Conference on Healthcare Informatics (ICHI),20170914,2017,,,5,12,"Drug-drug interactions (DDIs) are known to be responsible for nearly a third of all adverse drug reactions. Hence several current efforts focus on extracting signal from EMRs to prioritize DDIs that need further exploration. To this end, being able to extract explicit mentions of DDIs in free text narratives is an important task. In this paper, we explore recurrent neural network (RNN) architectures to detect and classify DDIs from unstructured text using the DDIExtraction dataset from the SemEval 2013 (task 9) shared task. Our methods are in line with those used in other recent deep learning efforts for relation extraction including DDI extraction. However, to our knowledge, we are the first to investigate the potential of character-level RNNs (Char-RNNs) for DDI extraction (and relation extraction in general). Furthermore, we explore a simple but effective model bootstrapping method to (a). build model averaging ensembles, (b). derive confidence intervals around mean micro-F scores (MMF), and (c). assess the average behavior of our methods. Without any rule based filtering of negative examples, a popular heuristic used by most earlier efforts, we achieve an MMF of 69.13. By adding simple replicable heuristics to filter negative instances we are able to achieve an MMF of 70.38. Furthermore, our best ensembles produce micro F-scores of 70.81 (without filtering) and 72.13 (with filtering), which are superior to metrics reported in published results. Although Char-RNNs turnout to be inferior to regular word based RNN models in overall comparisons, we find that ensembling models from both architectures results in nontrivial gains over simply using either alone, indicating that they complement each other.",,Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3,10.1109/ICHI.2017.15,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031125,drug-drug interactions;recurrent neural networks;relation classification,Computer architecture;Drugs;Kernel;Logic gates;Recurrent neural networks;Support vector machines,,,,,,,,,,23-26 Aug. 2017,,IEEE,IEEE Conference Publications
Big Social Data Analytics for Public Health: Predicting Facebook Post Performance Using Artificial Neural Networks and Deep Learning,N. Straton; R. R. Mukkamala; R. Vatrapu,,2017 IEEE International Congress on Big Data (BigData Congress),20170911,2017,,,89,96,"Facebook ""post popularity"" analysis is fundamental for differentiating between relevant posts and posts with low user engagement and consequently their characteristics. This research study aims at health and care organizations to improve information dissemination on social media platforms by reducing clutter and noise. At the same time, it will help users navigate through vast amount of information in direction of the relevant health and care content. Furthermore, study explores prediction of popularity of healthcare posts on the largest social media platform Facebook. Methodology is presented in this paper to predict user engagement based on eleven characteristics of the post: Post Type, Hour Span, Facebook Wall Category, Level, Country, isHoliday, Season, Created Year, Month, Day of the Week, Time of the Day. Finally, post performance prediction is conducted using Artificial Neural Networks (ANN) and Deep Neural Networks (DNN). Different network topology measures are used to achieve best accuracy prediction followed by examples and discussion on why DNN might not be optimal technique for the given data set.",,Electronic:978-1-5386-1996-4; POD:978-1-5386-1997-1; USB:978-1-5386-1995-7,10.1109/BigDataCongress.2017.21,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029313,Artificial Neural Network (ANN);Deep Neural Network (DNN);Negative Entropy;Post Performance;Purity,Data analysis;Entropy;Facebook;Neural networks;Public healthcare,,,,,,,,,,25-30 June 2017,,IEEE,IEEE Conference Publications
Deep Temporal Multimodal Fusion for Medical Procedure Monitoring using Wearable Sensors,E. A. Bernal; X. Yang; Q. Li; J. Kumar; S. Madhvanath; P. Ramesh; R. Bala,"Decision Support and Machine Intelligence, United Technologies Research Center, 129535 East Hartford, Connecticut United States (e-mail: eabernal@gmail.com)",IEEE Transactions on Multimedia,,2017,PP,99,1,1,"Process monitoring and verification have a wide range of uses in the medical and healthcare fields. Currently, such tasks are often carried out by a trained specialist, which makes them expensive, inefficient, and time-consuming. Recent advances in automated video- and multimodal-data-based action and activity recognition have made it possible to reduce the extent of manual intervention required to effectively carry out process supervision tasks. In this paper, we propose algorithms for automated egocentric human action and activity recognition from multimodal data, with a target application of monitoring and assisting a user perform a multi-step medical procedure. We propose a supervised deep multimodal fusion framework that relies on concurrent processing of motion data acquired with wearable sensors and video data acquired with an egocentric or body-mounted camera. We demonstrate the effectiveness of the algorithm on a public multimodal dataset and conclude that automated process monitoring via the use of multiple heterogeneous sensors is a viable alternative to its manual counterpart. Furthermore, we demonstrate that the application of previously proposed adaptive sampling schemes to the video processing branch of the multimodal framework results in significant performance improvements.",1520-9210;15209210,,10.1109/TMM.2017.2726187,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976382,Wearable sensors;action and activity recognition;deep learning;deep temporal fusion;egocentric vision;hand localization;medical procedures;multimodal fusion,Activity recognition;Biomedical monitoring;Data integration;Medical services;Monitoring;Wearable sensors,,,,,,,,,20170712,,,IEEE,IEEE Early Access Articles
FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition,H. Ding; S. K. Zhou; R. Chellappa,"Univ. of Maryland, College Park, MD, USA",2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),20170629,2017,,,118,126,"Relatively small data sets available for expression recognition research make the training of deep networks very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pretrained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization results show that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu- CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.",,Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7,10.1109/FG.2017.23,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961731,,Convolution;Distribution functions;Face;Face recognition;Image recognition;Neurons;Training,convolution;data visualisation;emotion recognition;face recognition;learning (artificial intelligence),CK+;FaceNet2ExpNet;Oulu- CASIA;SFEW;TFD;convolutional layer training;deep convolutional neural networks;deep face recognition net;deep network training;expression recognition;static images;visualization,,,,,,,,May 30 2017-June 3 2017,,IEEE,IEEE Conference Publications
Automated assessment of endometrium from transvaginal ultrasound using Deep Learned Snake,N. Singhal; S. Mukherjee; C. Perrey,"GE Global Research, Bangalore, India",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,283,286,Endometrium assessment via thickness measurement is commonly performed in routine gynecological ultrasound examination for assessing the reproductive health of patients undergoing fertility related treatments and endometrium cancer screening in women with post-menopausal bleeding. This paper introduces a fully automated technique for endometrium thickness measurement from three-dimensional transvaginal ultrasound (TVUS) images. The algorithm combines the robustness of deep neural networks with the more interpretable level set method for segmentation. We propose a hybrid variational curve propagation model which embeds a deep-learned endometrium probability map in the segmentation energy functional. This solution provides approximately 30% performance improvement over a contemporary supervised learning method on a database of 59 TVUS images and the thickness measurement is found to be within ±2mm of the manual measurement in 87% of the cases.,,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950520,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950520,Endometrium;deep learning;level set;segmentation;ultrasound;uterus,Image segmentation;Level set;Shape;Thickness measurement;Three-dimensional displays;Training;Ultrasonic imaging,biomedical measurement;biomedical ultrasonics;cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines;thickness measurement,3D TVUS images;3D transvaginal ultrasound images;automated endometrium assessment;deep learned snake;deep neural networks;deep-learned endometrium probability map;endometrium cancer screening;endometrium thickness measurement;gynecological ultrasound examination;image segmentation;post-menopausal bleeding;supervised learning method;variational curve propagation model,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Ultrasound Standard Plane Detection Using a Composite Neural Network Framework,H. Chen; L. Wu; Q. Dou; J. Qin; S. Li; J. Z. Cheng; D. Ni; P. A. Heng,"Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong",IEEE Transactions on Cybernetics,20170520,2017,47,6,1576,1586,"Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.",2168-2267;21682267,,10.1109/TCYB.2017.2685080,"National Basic Research Program of China, 973 Program; National Natural Science Foundation of China; Research Grants Council of Hong Kong Special Administrative Region; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445,Convolutional neural network (CNN);deep learning;knowledge transfer;recurrent neural network (RNN);standard plane;ultrasound (US),Biomedical imaging;Feature extraction;Fetus;Machine learning;Standards;Training data;Videos,,,,,,,,,20170330,June 2017,,IEEE,IEEE Journals & Magazines
Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks,H. Chen; D. Ni; J. Qin; S. Li; X. Yang; T. Wang; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong",IEEE Journal of Biomedical and Health Informatics,20170520,2015,19,5,1627,1636,"Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.",2168-2194;21682194,,10.1109/JBHI.2015.2425041,Hong Kong Innovation and Technology Fund; Research Grants Council of Hong Kong; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943,Convolutional neural network (CNN);Ultrasound;convolutional neural network;deep learning;domain transfer;knowledge transfer;standard plane;ultrasound (US),Biomedical imaging;Dictionaries;Feature extraction;Informatics;Standards;Training;Videos,biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics,FASP localization;US videos;automatic standard plane localization;classification performance;domain transferred deep convolutional neural network;fetal abdominal standard plane;fetal ultrasound;high-level features;learning-based approach;low-level features;medical image computing problems;natural images;overfitting problem;task-specific CNN;transfer learning strategy;ultrasound videos,"0;Abdomen;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;ROC Curve;Ultrasonography, Prenatal",32,,37,,,20150421,Sept. 2015,,IEEE,IEEE Journals & Magazines
The chatbot feels you - a counseling service using emotional response generation,Dongkeon Lee; Kyo-Joong Oh; Ho-Jin Choi,"School of Computing, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320,2017,,,437,440,"Early study tries to use chatbot for counseling services. They changed drinking habit of who being consulted by leading them via intervene chatbot. However, the application did not concerned about psychiatric status through continuous conversation with user monitoring. Furthermore, they had no ethical judgment method that about the intervention of the chatbot. We argue that more reasonable and continuous emotion recognition will make better mental healthcare experiment. It will be more proper clinical psychiatric consolation in ethical view as well. This paper suggests a introduce a novel chatbot system for psychiatric counseling service. Our system understands content of conversation based on recent natural language processing (NLP) methods with emotion recognition. It senses emotional flow through the continuous observation of conversation. Also, we generate personalized counseling response from user input, to do this, we use additional constrains to generation model for the proper response generation which can detect conversational context, user emotion and expected reaction.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881752,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881752,conversational service;deep learning;response generation,Context;Data models;Emotion recognition;Employee welfare;Medical services;Natural language processing;Probabilistic logic,emotion recognition;ethical aspects;health care;natural language processing;psychology,NLP;clinical psychiatric consolation;continuous conversation observation;continuous emotion recognition;conversation content;drinking habit;emotional flow;emotional response generation;ethical view;expected reaction;intervene chatbot;mental healthcare experiment;natural language processing;personalized counseling response;psychiatric counseling service;user emotion;user input,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
Feature Fusion for Denoising and Sparse Autoencoders: Application to Neuroimaging Data,A. Moussavi-Khalkhali; M. Jamshidi; S. Wijemanne,"Dept. of Electr. & Comput. Eng., Univ. of Texas at San Antonio, San Antonio, TX, USA",2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),20170202,2016,,,605,610,"Although there is no cure to date, Alzheimer's disease detection in early stages has a significant impact on the patient's life in terms of cost, the progress, and helping to plan in advance for an appropriate healthcare in the life ahead as well as providing clinical etiologies for further research. This paper discusses implementing a feature fusion method utilizing sparse and denoising autoencoders to reveal the stage of Alzheimer's disease. Four cohorts consisted of individuals with Alzheimer's disease, late mild cognitive impairment, early mild cognitive impairment, and normal control groups are classified using multinomial logistic regression fueled by the fusion of high-level and low-level features. The high-level features are extracted from the stacked autoencoders. The results show that feature fusion enhance the performance of typical autoencoders. However, the performance of feature fusion using denoising autoencoders is superior to that of the sparse training of autoencoders in terms of overall accuracy, precision, and recall.",,Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6,10.1109/ICMLA.2016.0106,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838210,Alzheimer's disease stage detection;deep learning;feature fusion;sacked sparse autoencoders;stacked denoising autoencoders,Alzheimer's disease;Classification algorithms;Feature extraction;Magnetic resonance imaging;Noise reduction;Training,diseases;feature extraction;image coding;image denoising;image fusion;medical image processing;neurophysiology;regression analysis,Alzheimer disease detection;clinical etiologies;denoising autoencoders;feature fusion method;high-level features;low-level features;mild cognitive impairment;multinomial logistic regression;neuroimaging data;normal control groups;sparse autoencoder training;stacked autoencoders,,,,,,,,18-20 Dec. 2016,,IEEE,IEEE Conference Publications
Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding,Z. Zhu; C. Yin; B. Qian; Y. Cheng; J. Wei; F. Wang,"Xi'an Jiaotong Univ., Xi'an, China",2016 IEEE 16th International Conference on Data Mining (ICDM),20170202,2016,,,749,758,"Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. Aproper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is the Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector based representation for patients, which typically requires aggregation of medical events over a certain time period. As aconsequence, the temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture, and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines.",,Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9,10.1109/ICDM.2016.0086,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837899,Deep Matching;Medical Concept Embedding;Patient Similarity,Context;Diseases;Medical diagnostic imaging;Natural language processing;Neural networks,electronic health records;health care;patient care;pattern matching;unsupervised learning,EHR;clinical similarities;convolutional neural network architecture;deep architecture;electronic health records;healthcare informatics;medical concept embedding;medical interpretations;patient similarity evaluation;patient similarity measurement;patient similarity research;real-world clinical data;temporal longitudinal patient EHR matching;vector based representation,,,,,,,,12-15 Dec. 2016,,IEEE,IEEE Conference Publications
Deep Decision Network for Multi-class Image Classification,V. N. Murthy; V. Singh; T. Chen; R. Manmatha; D. Comaniciu,"Sch. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA",2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),20161212,2016,,,2240,2248,"In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.",,Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8,10.1109/CVPR.2016.246,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780615,,Airplanes;Covariance matrices;Decision trees;Machine learning;Symmetric matrices;Training;Vegetation,image classification;learning (artificial intelligence);trees (mathematics),CIFAR-100;DDN;deep decision network;deep learning network;multiclass image classification;tree-like structured network,,,,,,,,27-30 June 2016,,IEEE,IEEE Conference Publications
Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy,J. Liang; R. Lu; C. Zhang; F. Wang,"Dept. of Autom., Tsinghua Univ., Beijing, China",2016 IEEE International Conference on Healthcare Informatics (ICHI),20161208,2016,,,184,191,"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt ""auras"". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.",,Electronic:978-1-5090-6117-4; POD:978-1-5090-6118-1,10.1109/ICHI.2016.27,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776343,,Brain models;Electroencephalography;Epilepsy;Feature extraction;Machine learning;Training,brain;electroencephalography;knowledge management;learning (artificial intelligence);medical disorders;medical signal processing;neural nets,American Epilepsy Society Seizure Prediction Challenge;EEG datasets;EEG signals;brain disorder;deep learning techniques;deep neural network pretraining;electroencephalography recordings;epilepsy patients;knowledge transfer strategy;medical data resource utilization;seizure prediction;speech analysis;spontaneous seizures;transfer learning strategy,,,,,,,,4-7 Oct. 2016,,IEEE,IEEE Conference Publications
Fetal facial standard plane recognition via very deep convolutional networks,Z. Yu; D. Ni; S. Chen; S. Li; T. Wang; B. Lei,"School of Biomedical Engineering, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,627,630,"The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3×3) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590780,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590780,,Computer architecture;Convolution;Image recognition;Imaging;Standards;Training;Ultrasonic imaging,biomedical ultrasonics;face recognition;learning (artificial intelligence);medical image processing;neural nets;obstetrics,DCNN;FFSP dataset;US examination;automatic FFSP recognition method;convolution filters;deep learning approach;fetal facial standard plane recognition;hand-crafted visual features;ultrasound images;very deep convolutional networks,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
Learning a multiscale patch-based representation for image denoising in X-RAY fluoroscopy,Y. Matviychuk; B. Mailhé; X. Chen; Q. Wang; A. Kiraly; N. Strobel; M. Nadar,"Siemens Healthcare, Medical Imaging Technologies, Princeton, NJ, USA",2016 IEEE International Conference on Image Processing (ICIP),20160819,2016,,,2330,2334,"Denoising is an indispensable step in processing low-dose X-ray fluoroscopic images that requires development of specialized high-quality algorithms able to operate in near real-time. We address this problem with an efficient deep learning approach based on the process-centric view of traditional iterative thresholding methods. We develop a novel trainable patch-based multiscale framework for sparse image representation. In a computationally efficient way, it allows us to accurately reconstruct important image features on multiple levels of decomposition with patch dictionaries of reduced size and complexity. The flexibility of the chosen machine learning approach allows us to tailor the learned basis for preserving important structural information in the image and noticeably minimize the amount of artifacts. Our denoising results obtained with real clinical data demonstrate significant quality improvement and are computed much faster in comparison with the BM3D algorithm.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532775,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532775,Radiography;iterative algorithms;neural networks,Approximation algorithms;Dictionaries;Image reconstruction;Image representation;Iterative methods;Neural networks;Noise reduction,diagnostic radiography;feature extraction;image denoising;image reconstruction;image representation;image segmentation;iterative methods;learning (artificial intelligence);medical image processing,BM3D algorithm;X-ray fluoroscopy;artifact minimization;clinical data;complexity;deep learning approach;image decomposition;image denoising;image feature reconstruction;iterative thresholding method;low-dose X-ray fluoroscopic image;machine learning;multiscale patch-based representation learning;patch dictionary;sparse image representation;structural information preservation;trainable patch-based multiscale framework,,1,,38,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
Real-time 2D/3D registration via CNN regression,S. Miao; Z. J. Wang; Y. Zheng; R. Liao,"Electrical and Computer Engineering, University of British Columbia, Canada",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,1430,1434,"In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493536,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493536,2-D/3-D Registration;Convolutional Neural Network;Deep Learning;Image Guided Intervention,Biomedical imaging;Feature extraction;Neural networks;Real-time systems;Solid modeling;Training;X-ray imaging,,,,,,18,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing,F. C. Ghesu; E. Krubasik; B. Georgescu; V. Singh; Y. Zheng; J. Hornegger; D. Comaniciu,"Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA",IEEE Transactions on Medical Imaging,20160429,2016,35,5,1217,1228,"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% o- er the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.",0278-0062;02780062,,10.1109/TMI.2016.2538802,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426845,Deep learning;image parsing;marginal space learning;sparse representations;three-dimensional (3D) object detection and segmentation,Context;Feature extraction;Image segmentation;Machine learning;Robustness;Shape;Three-dimensional displays,biomedical ultrasonics;feature extraction;image classification;image sampling;image segmentation;learning (artificial intelligence);medical image processing;pattern clustering;probability;ultrasonic imaging,3D context;DL-based active shape model;anatomical object detection;anatomical pose estimation;annotated image databases;aortic valve;automated feature design;boundary delineation;clinical workflow;clustered high-probability regions;computational efficiency;deep learning network architectures;deep learning systems;diagnosis;extensive dataset;full 3D data detection;full 3D data segmentation;hierarchical marginal spaces;learning classifiers;machine learning methods;marginal space deep learning;nonrigid object boundary;object localization;object parametrization;parametrized representations;patient stratification;representative image features;restricted affine transformation;run-time performance;scanning high-dimensional parametric spaces;scanning hypotheses;segmentation support;sparse adaptive data sampling patterns;therapy planning;two-step learning problem;ultrasound;volumetric medical image data parsing,,9,,46,,,20160307,May 2016,,IEEE,IEEE Journals & Magazines
Multi-Instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition,Z. Yan; Y. Zhan; Z. Peng; S. Liao; Y. Shinagawa; S. Zhang; D. N. Metaxas; X. S. Zhou,"Department of Computer Science, Rutgers University, Piscataway, NJ, USA",IEEE Transactions on Medical Imaging,20160429,2016,35,5,1332,1343,"In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. “Bodypart identity” of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.",0278-0062;02780062,,10.1109/TMI.2016.2524985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398101,CNN;discriminative local information discovery;multi-instance;multi-stage,Algorithm design and analysis;DICOM;Image analysis;Image recognition;Machine learning;Three-dimensional displays,cardiology;computerised tomography;face recognition;image classification;learning (artificial intelligence);medical image processing,aorta arch slice;body-part recognition;cardiac slice;convolutional neural network;discriminative information;discriminative local anatomies;discriminative local appearances;global image context;human faces;human identity information;image classification problem;image recognition problems;image-level classifier;large scale CT dataset;local image information;local image patches;mediastinum region;multiinstance deep learning;multiinstance learning fashion;multistage deep learning framework;prelearned CNN;pretrain stage;synthetic dataset;transversal slice,,10,,51,,,20160203,May 2016,,IEEE,IEEE Journals & Magazines
A CNN Regression Approach for Real-Time 2D/3D Registration,S. Miao; Z. J. Wang; R. Liao,"Department of Electrical and Computer Engineering, University of British Columbia, Vancouver",IEEE Transactions on Medical Imaging,20160429,2016,35,5,1352,1363,"In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.",0278-0062;02780062,,10.1109/TMI.2016.2521800,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393571,2-D/3-D registration;convolutional neural network;deep learning;image guided intervention,Attenuation;Biomedical imaging;Computed tomography;Feature extraction;Real-time systems;X-ray imaging,diagnostic radiography;feature extraction;image reconstruction;image registration;iterative methods;medical image processing;neural nets;optimisation;regression analysis,3D pose-indexed features;CNN regression approach;CNN regressors;X-ray images;automatic feature extraction step;complex regression task;convolutional neural network regression approach;digitally reconstructed radiograph;formation parameters;intensity-based 2D-3D registration technology;intensity-based methods;iterative optimization;memory footprint;multiple simpler subtasks;optimization-based methods;real-time 2D-3D registration;scalar-valued metric function;transformation parameters,,4,,31,,,20160126,May 2016,,IEEE,IEEE Journals & Magazines
Temporal Pattern and Association Discovery of Diagnosis Codes Using Deep Learning,S. Mehrabi; S. Sohn; D. Li; J. J. Pankratz; T. Therneau; J. L. S. Sauver; H. Liu; M. Palakal,"Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA",2015 International Conference on Healthcare Informatics,20151210,2015,,,408,416,"Longitudinal health records contain data on patients' visits, condition, treatment, and test results representing progression of their health status over time. In poorly understood patient populations, such data are particularly helpful in characterizing disease progression and early detection. In this work we developed a deep learning algorithm for temporal pattern discovery over Rochester Epidemiology Project data. We modeled each patient's records as a matrix of temporal clinical events with ICD9 and HCUP CSS diagnosis codes as rows and years of diagnosis as columns. Patients aged 18 or younger at the time of diagnosis were selected. A deep Boltzmann machine network with three hidden layers was constructed with each patient's diagnosis matrix values as visible nodes. The final weights of the network model were analyzed as the common features among patients' records.",,Electronic:978-1-4673-9548-9; POD:978-1-4673-9549-6,10.1109/ICHI.2015.58,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349719,Deep Learning;Rochester Epidemiology Project;Temporal Pattern Discovery,Cascading style sheets;Diseases;Machine learning;Medical diagnostic imaging;Sociology;Statistics,Boltzmann machines;data mining;electronic health records;learning (artificial intelligence);matrix algebra;patient diagnosis,HCUP CSS diagnosis;ICD9 diagnosis code;Rochester Epidemiology Project data;deep Boltzmann machine network;deep learning;deep learning algorithm;disease progression;longitudinal health record;patient diagnosis matrix value;temporal association discovery;temporal pattern discovery,,4,,45,,,,21-23 Oct. 2015,,IEEE,IEEE Conference Publications
Voice Pathology Detection Using Deep Learning: a Preliminary Study,P. Harar; J. B. Alonso-Hernandezy; J. Mekyska; Z. Galaz; R. Burget; Z. Smekal,"Department of Telecommunications Brno University of Technology, Technicka 10, 61600 Brno, Czech Republic",2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI),20170724,2017,,,1,4,This paper describes a preliminary investigation of Voice Pathology Detection using Deep Neural Networks (DNN). We used voice recordings of sustained vowel /a/ produced at normal pitch from German corpus Saarbruecken Voice Database (SVD). This corpus contains voice recordings and electroglottograph signals of more than 2 000 speakers. The idea behind this experiment is the use of convolutional layers in combination with recurrent Long-Short-Term-Memory (LSTM) layers on raw audio signal. Each recording was split into 64 ms Hamming windowed segments with 30 ms overlap. Our trained model achieved 71.36% accuracy with 65.04% sensitivity and 77.67% specificity on 206 validation files and 68.08% accuracy with 66.75% sensitivity and 77.89% specificity on 874 testing files. This is a promising result in favor of this approach because it is comparable to similar previously published experiment that used different methodology. Further investigation is needed to achieve the state-of-the-art results.,,Electronic:978-1-5386-0850-0; POD:978-1-5386-0851-7,10.1109/IWOBI.2017.7985525,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985525,,Convolution;Databases;Feature extraction;Pathology;Support vector machines;Testing;Training,audio signal processing;bioelectric phenomena;electric impedance measurement;learning (artificial intelligence);medical signal detection;medical signal processing;neural nets;speech;speech processing,German corpus Saarbruecken voice database;Hamming windowed segments;audio signal;convolutional layers;deep learning;deep neural networks;electroglottograph signals;long-short-term-memory layers;sustained vowel;voice pathology detection;voice recordings,,,,,,,,10-12 July 2017,,IEEE,IEEE Conference Publications
Deep Learning for Automated Extraction of Primary Sites from Cancer Pathology Reports,J. Qiu; H. J. Yoon; P. A. Fearn; G. D. Tourassi,"University of Tennessee, Knoxville, TN, 37996, and Health Data Sciences Institute, Oak Ridge National Laboratory, Oak Ridge, TN 37831 (e-mail: jqiu1@utk.edu)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99,1,1,"for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study we investigated deep learning and a convolutional neural network (CNN), for extracting ICDO- 3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro and macro-F score increases of up to 0.132 and 0.226 respectively when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.",2168-2194;21682194,,10.1109/JBHI.2017.2700722,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918552,Deep learning;convolutional neural network;information extraction;natural language processing;pathology reports;primary cancer site,,,,,,,,,,20170503,,,IEEE,IEEE Early Access Articles
Chest pathology detection using deep learning with non-medical training,Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan,"The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723,2015,,,294,297,"In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871,CNN;Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks,Biomedical imaging;Diagnostic radiography;Feature extraction;Machine learning;Pathology;Visualization;X-rays,convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets,AUC;CNN algorithm;CNN deep architecture classification;CNN learning;GIST feature;ImageNet;area under curve;chest X-ray image dataset;chest pathology detection;chest radiograph;convolutional neural network;deep learning;domain specific representation;general medical image recognition task;high level image representation learning;large scale nonmedical image database;mid level image representation learning;nonmedical learning;nonmedical training;pathology identification;pathology type,,10,,15,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
Deep learning based Nucleus Classification in pancreas histological images,Y. H. Chang; G. Thibault; O. Madin; V. Azimi; C. Meyers; B. Johnson; J. Link; A. Margolin; J. W. Gray,"Oregon Health and Science University (OHSU), Portland, United States of America",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,672,675,"Tumor specimens contain a variety of healthy cells as well as cancerous cells, and this heterogeneity underlies resistance to various cancer therapies. But this problem has not been thoroughly investigated until recently. Meanwhile, technological breakthroughs in imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples, and modern machine learning approaches including deep learning have been shown to produce encouraging results by finding hidden structures and make accurate predictions. In this paper, we propose a Deep learning based Nucleus Classification (DeepNC) approach using paired histopathology and immunofluorescence images (for label), and demonstrate its classification prediction power. This method can solve current issue on discrepancy between genomic- or transcriptomic-based and pathology-based tumor purity estimates by improving histological evaluation. We also explain challenges in training a deep learning model for huge dataset.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036914,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036914,Deep Learning;Histopathology;Immunofluorescence;Segmentation,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images,T. H. Song; V. Sanchez; H. EIDaly; N. M. Rajpoot,"Department of Computer Science, University of Warwick, UK",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,1040,1043,"Automated cell detection is a critical step for a number of computer-assisted pathology related image analysis algorithm. However, automated cell detection is complicated due to the variable cytomorphological and histological factors associated with each cell. In order to efficiently resolve the challenge of automated cell detection, deep learning strategies are widely applied and have recently been shown to be successful in histopathological images. In this paper, we concentrate on bone marrow trephine biopsy images and propose a hybrid deep autoencoder (HDA) network with Curvature Gaussian model for efficient and precise bone marrow hematopoietic stem cell detection via related high-level feature correspondence. The accuracy of our proposed method is up to 94%, outperforming other supervised and unsupervised detection approaches.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950694,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950694,Autoencoder;Bone marrow;Deep learning;Nuclei Detection,Biological system modeling;Biopsy;Bones;Decoding;Feature extraction;Shape;Training,Gaussian processes;bone;cellular biophysics;learning (artificial intelligence);medical image processing,automated cell detection;bone marrow hematopoietic stem cell detection;bone marrow trephine biopsy images;curvature Gaussian model;deep learning;hybrid deep autoencoder,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis,S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou,"ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland",IEEE Journal of Biomedical and Health Informatics,20170520,2017,21,1,76,84,"Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.",2168-2194;21682194,,10.1109/JBHI.2016.2636929,Bern University Hospital; 10.13039/501100001711 - Swiss National Science Foundation (SNSF); ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792,Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning,Biomedical imaging;Computed tomography;Databases;Knowledge engineering;Lungs;Machine learning;Training,biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets,CT images;computed tomography;computer-aided diagnosis;convolutional neural networks;fused knowledge compression;interstitial lung disease diagnosis;lung pattern analysis;lung tissue data;medical image analysis;multisource transfer learning;texture classification;texture databases,,,,,,,20161207,Jan. 2017,,IEEE,IEEE Journals & Magazines
Evaluation of feature descriptors for cancerous tissue recognition,P. Stanitsas; A. Cherian; Xinyan Li; A. Truskinovsky; V. Morellas; N. Papanikolopoulos,"Department of Computer Science and Engineering, University of Minnesota, USA",2016 23rd International Conference on Pattern Recognition (ICPR),20170424,2016,,,1490,1495,"Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.",,Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9,10.1109/ICPR.2016.7899848,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848,,Cancer;Covariance matrices;Feature extraction;Geometry;Histograms;Image color analysis;Symmetric matrices,cancer;computer vision;feature extraction;image representation;image texture;medical image processing;neural nets;object recognition,CAD;CKD;RCD;breast carcinomas;cancerous tissue recognition;computer vision;computer-aided diagnosis;convolutional neural networks;covariance based descriptors;covariance kernel descriptor;feature descriptors;feature representation;image descriptors;myometrial leiomyosarcomas;pathologists;prostate carcinomas;region covariance descriptors;texture recognition,,,,,,,,4-8 Dec. 2016,,IEEE,IEEE Conference Publications
Epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images,Y. Huang; H. ZHENG; C. LIU; X. Ding; G. Rohde,"Electrical Engineering Department and Biomedical Engineering Department, University of Virginia, VA, U.S.A.",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99,1,1,"Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.",2168-2194;21682194,,10.1109/JBHI.2017.2691738,CCF-TENCENT; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893702,convolutional neural networks;domain adaptation;epitheliumstroma classification;histopathological image analysis;transfer learning,Adaptation models;Feature extraction;Image analysis;Kernel;Machine learning;Neural networks;Training,,,,,,,,,20170406,,,IEEE,IEEE Early Access Articles
Advanced deep learning for blood vessel segmentation in retinal fundus images,Lua Ngo; Jae-Ho Han,"Dept. Brain and Cognitive Engineering, Korea University, Seoul, South Korea",2017 5th International Winter Conference on Brain-Computer Interface (BCI),20170220,2017,,,91,92,"Rising of deep learning methodologies draws huge attention to their application in image processing and classification. Catching up the trends, this study briefly presents state-of-the-art of deep learning applications in medical imaging interfered with achievements of blood vessel segmentation methods in neurosensory retinal fundus images. Successful segmentation based on deep learning offers advantage in diagnosing ophthalmological disease or pathology.",,Electronic:978-1-5090-5096-3; POD:978-1-5090-5097-0,10.1109/IWW-BCI.2017.7858169,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858169,Biomedical optical imaging;blood vessels;fundus images;image segmentation;medical image processing,,blood vessels;image segmentation;learning (artificial intelligence);medical image processing,advanced deep learning methodologies;blood vessel segmentation methods;image classification;image processing;ophthalmological disease;pathology;retinal fundus images,,,,,,,,9-11 Jan. 2017,,IEEE,IEEE Conference Publications
Customizing CNNs for blood vessel segmentation from fundus images,S. K. Vengalil; N. Sinha; S. S. S. Kruthiventi; R. V. Babu,"International Institute of Information Technology, Bangalore, India",2016 International Conference on Signal Processing and Communications (SPCOM),20161117,2016,,,1,4,"For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 × 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.",,Electronic:978-1-5090-1746-1; POD:978-1-5090-1747-8,10.1109/SPCOM.2016.7746702,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746702,,Biomedical imaging;Blood vessels;Image resolution;Image segmentation;Pathology;Testing;Training,blood vessels;convolution;diseases;image colour analysis;image resolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets,CNN;DRIVE database;HRF databases;ROC curve;automatic screening;blood vessel network;blood vessel segmentation;convolutional neural network;deep learning framework;eye diseases;fundal images;fundus images;gray scale image;image resolution;semantic image segmentation,,,,,,,,12-15 June 2016,,IEEE,IEEE Conference Publications
Automatic Lumbar Vertebrae Detection Based on Feature Fusion Deep Learning for Partial Occluded C-arm X-ray Images,Y. Li; W. Liang; Y. Zhang; H. An; J. Tan,"Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,647,650,"Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590785,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590785,,Computational modeling;Feature extraction;Kernel;Machine learning;Pathology;Surgery;X-ray imaging,computational complexity;computerised tomography;image fusion;image segmentation;image texture;learning (artificial intelligence);medical image processing;neural nets;prosthetics;surgery,CNN;DRR;FFDL;Gabor kernel;IG-MISS;ROI;abnormal pathological condition;automatic lumbar vertebrae detection;automatic segmentation;computational complexity;convolutional neural network model;feature fusion deep learning model;image-guided minimally invasive spine surgery;imaging angle;lumbar vertebrae X-ray images;lumbar vertebrae contour;lumbar vertebrae texture;partial occluded C-arm X-ray images;sobel kernel;surgical implants;training data,,1,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images,K. Sirinukunwattana; S. E. A. Raza; Y. W. Tsang; D. R. J. Snead; I. A. Cree; N. M. Rajpoot,"Department of Computer Science, University of Warwick, Coventry, UK",IEEE Transactions on Medical Imaging,20160429,2016,35,5,1196,1206,"Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.",0278-0062;02780062,,10.1109/TMI.2016.2525803,10.13039/100008982 - Qatar National Research Fund; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399414,Convolutional neural network;deep learning;histology image analysis;nucleus detection,Cancer;Computer architecture;Feature extraction;Machine learning;Microprocessors;Shape;Tumors,biological organs;biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;probability;tumours,NEP;SC-CNN;cancerous tissue;cell nuclei classification;cell nuclei detection;cellular heterogeneity;colorectal adenocarcinoma images;dataset;eosin stain;high-probability values;highest average F1 score;histopathology images;joint detection;locality sensitive deep learning;neighboring ensemble predictor;quantitative analysis;routine colon cancer histology images;spatially constrained convolutional neural network;standard hematoxylin;tissue constituents;whole-slide images,,16,,38,,,20160204,May 2016,,IEEE,IEEE Journals & Magazines
Retinal vessel landmark detection using deep learning and hessian matrix,T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang,"Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China",2015 8th International Congress on Image and Signal Processing (CISP),20160218,2015,,,387,392,"The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.",,Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2,10.1109/CISP.2015.7407910,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910,Deep learning;Hessian response;Image registration;Landmark detection;The probability map;The retinal image;Vessel Segmentation,Feature extraction;Image color analysis;Image registration;Image segmentation;Machine learning;Neural networks;Retina,Hessian matrices;eye;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery,Hessian matrix;bifurcation points;crossover points;curvature extreme points;deep learning;feature detection;image noise;multimodel retinal image;multiscale Hessian response;ophthalmological surgery;pathology;retinal image probability map;retinal image registration;retinal vessel landmark detection;vessel segmentation,,,,9,,,,14-16 Oct. 2015,,IEEE,IEEE Conference Publications
An Automatic Learning-Based Framework for Robust Nucleus Segmentation,F. Xing; Y. Xie; L. Yang,"Department of Electrical and Computer Engineering, University of Florida, Gainesville",IEEE Transactions on Medical Imaging,20160202,2016,35,2,550,566,"Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.",0278-0062;02780062,,10.1109/TMI.2015.2481436,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274740,Deep convolutional neural network;nucleus segmentation;sparse representation,Breast cancer;Computational modeling;Image color analysis;Image segmentation;Robustness;Shape;Tumors,cancer;diagnostic radiography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;probability;tumours,automated nucleus segmentation;automatic learning-based framework;automatic morphological feature computation;brain tumor;breast cancer;computer-aided image analysis;deep CNN model;deep convolutional neural network model;diseases;early detection;feature learning characteristic;high level shape prior modeling;histopathology imaging;histopathology specimens;iterative region merging approach;large-scale pathology image datasets;local repulsive deformable model;pancreatic neuroendocrine tumor;probability map;quantitative analysis;robust nucleus segmentation;robust selection-based sparse shape model;staining histopathology images;tissue,,7,,83,,,20150923,Feb. 2016,,IEEE,IEEE Journals & Magazines
Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images,J. Xu; L. Xiang; Q. Liu; H. Gilmore; J. Wu; J. Tang; A. Madabhushi,"Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China",IEEE Transactions on Medical Imaging,20160104,2016,35,1,119,130,"Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.",0278-0062;02780062,,10.1109/TMI.2015.2458702,; 10.13039/100000062 - National Institute of Diabetes and Digestive and Kidney Diseases; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353,Automated nuclei detection;breast cancer histopathology;deep learning;digital pathology;feature representation learning;stacked sparse autoencoder,Breast cancer;Decoding;Feature extraction;Image color analysis;Pathology;Training,biological tissues;cancer;image classification;image coding;image representation;image resolution;learning (artificial intelligence);medical image processing,Nottingham histologic score system;automated grading;automated nuclear detection;average area under Precision-Recall curve;breast cancer tissue specimens;computer assisted pathology related image analysis algorithms;deep learning strategy;high resolution digitized pathology images;high-level features;high-resolution breast cancer histopathological images;image classifier;image patch representation;nuclei detection;pixel intensity;sliding window operation;stacked sparse autoencoder,,18,,43,,,20150720,Jan. 2016,,IEEE,IEEE Journals & Magazines
A comparative study for chest radiograph image retrieval using binary texture and deep learning classification,Y. Anavi; I. Kogan; E. Gelbart; O. Geva; H. Greenspan,"Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105,2015,,,2940,2943,"In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the “descriptor”-based approach); the second approach (“classification”-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7319008,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319008,,Biomedical imaging;Feature extraction;Heart;Machine learning;Measurement;Pathology;Support vector machines,diagnostic radiography;image classification;image retrieval;image texture;learning (artificial intelligence);medical image processing;probability;support vector machines,CNN;SVM classifier;X-ray image retrieval;binary features;binary texture;chest pathology retrieval;chest radiograph image retrieval;classification-based approach;decision values;deep learning classification;deep learning features;descriptor-based approach;image descriptors;pair-wise classification;probability descriptor;query image;texture features,,3,,12,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
Mixed Neural Network Approach for Temporal Sleep Stage Classification,H. Dong; A. Supratak; W. Pan; C. Wu; P. M. Matthews; Y. Guo,"Department of Computing, Imperial College London, London, SW7 2AZ, UK.",IEEE Transactions on Neural Systems and Rehabilitation Engineering,,2017,PP,99,1,1,"This paper proposes a practical approach to addressing limitations posed by using of single-channel electroencephalography (EEG) for sleep stage classification. EEG-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages at lower cost in the home. However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which are both uncomfortable and difficult to position. Previous studies of sleep stage scoring that used only frontal electrodes with a hierarchical decision tree motivated this paper, in which we have taken advantage of rectifier neural network for detecting hierarchical features and long short-term memory (LSTM) network for sequential data learning to optimize classification performance with single-channel recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electrooculogram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm than is available from existing approaches with vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical.",1534-4320;15344320,,10.1109/TNSRE.2017.2733220,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995122,Deep learning;EEG signal;Long short-term memory;Sleep stage classification;electroencephalography,Electrodes;Electroencephalography;Electrooculography;Feature extraction;Sleep;Standards,,,,,,,,,20170728,,,IEEE,IEEE Early Access Articles
Nuclei segmentation in histopathology images using deep neural networks,P. Naylor; M. Laé; F. Reyal; T. Walter,"MINES ParisTech, PSL Research University, CBIO - Centre de Bioinformatique, 77300 Fontainebleau, France",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,933,936,"Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950669,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950669,Breast Cancer;Cellular Phenotyping;Convolutional Neural Networks;Deep Learning;Digital Pathology;Histopathology;Nuclei Segmentation,Cancer;Computer architecture;Image segmentation;Machine learning;Microprocessors;Neural networks;Semantics,biomedical optical imaging;cancer;cellular biophysics;image segmentation;medical image processing;neural nets;probability;tumours,automatic segmentation;cancer diagnosis;cancer prognosis;cell nuclei;cellular phenotypes;complex image data;deep neural networks;digital pathology;fully automated workflow;histopathology image data;image data set;manually annotated images;nuclei segmentation;pathologists;posterior probability maps;scientific community;split jointly segmented nuclei;stained tumor sections;tissular phenotypes,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images,B. Ehteshami Bejnordi; J. Lin; B. Glass; M. Mullooly; G. L. Gierach; M. E. Sherman; N. Karssemeijer; J. van der Laak; A. H. Beck,"Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, Netherlands",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,929,932,"Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950668,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950668,Breast Cancer;Convolutional Neural Networks;Digital pathology;Tumor Associated Stroma,Breast cancer;Feature extraction;Training;Tumors,biomedical optical imaging;cancer;cellular biophysics;learning (artificial intelligence);mammography;medical image processing;patient diagnosis;tumours,breast cancer diagnosis;breast carcinomas diagnosis;breast tissue biopsies;convolutional neural networks;deep learning-based assessment;epithelial cells;epithelial regions;epithelial tissue architecture;hematoxyli-and-eosin stained breast specimens;histopathology images;tumor-associated stroma,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Wide residual networks for mitosis detection,E. Zerhouni; D. Lányi; M. Viana; M. Gabrani,"IBM Research Zurich, S&#x00E4;umerstrasse 4, 8803 R&#x00FC;schlikon, Switzerland",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,924,928,"One of the most important prognostic markers to assess proliferation activity of breast tumors is estimating the number of mitotic figures in H&E stained tissue. We propose the use of a recently published convolutional neural network architecture, Wide Residual Networks, for mitosis detection in breast histology images. The model is trained to classify each pixel of on an image using as context a patch centered on the pixel. We apply post-processing on the network output in order to filter out noise and select true mitosis. Finally, we combine the output of several networks using majority vote. Our approach ranked 2nd in the MICCAI TUPAC 2016 competition for mitosis detection, outperforming most other contestants by a significant margin.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950667,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950667,Mitotic activity;convolutional network;deep learning;tumor proliferation;wide-residual network,Image processing;Machine learning;Neural networks;Pathology;Shape;Testing;Training,cellular biophysics;diagnostic radiography;image classification;image filtering;mammography;medical image processing;neural nets;tumours,breast histology images;breast tumors;convolutional neural network architecture;hematoxylin-and-eosin-stained-tissue;image classification;image filtering;mitosis detection;mitotic figures;proliferation activity;wide residual networks,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Disease grading of heterogeneous tissue using convolutional autoencoder,E. Zerhouni; B. Prisacari; Q. Zhong; P. Wild; M. Gabrani,"IBM Research-Z&#x00FC;rich, Saeumerstrasse 4, 8803 Rueschlikon, Switzerland",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,596,599,"One of the main challenges of histological image analysis is the high dimensionality of the images. This can be addressed via summarizing techniques or feature engineering. However, such approaches can limit the performance of subsequent machine learning models, particularly when dealing with highly heterogeneous tissue samples. One possible alternative is to employ unsupervised learning to determine the most relevant features automatically. In this paper, we propose a method of generating representative image signatures that are robust to tissue heterogeneity. At the core of our approach lies a novel deep-learning based mechanism to simultaneously produce representative image features as well as perform dictionary learning to further reduce dimensionality. By integrating this mechanism in a broader framework for disease grading, we show significant improvement in terms of grading accuracy compared to alternative local feature extraction methods.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950591,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950591,Convolutional Autoencoder;Dimensionality Reduction;Image Description;Tissue Heterogeneity,Dictionaries;Diseases;Feature extraction;Image color analysis;Image reconstruction;Morphology;Training,biological tissues;convolutional codes;diseases;feature extraction;image coding;learning (artificial intelligence);medical image processing,convolutional autoencoder;deep-learning;dictionary learning;disease grading;feature engineering;feature extraction;heterogeneous tissue;histological image analysis;image features;machine learning;tissue heterogeneity;unsupervised learning,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
The importance of stain normalization in colorectal tissue classification with convolutional networks,F. Ciompi; O. Geessink; B. E. Bejnordi; G. S. de Souza; A. Baidoshvili; G. Litjens; B. van Ginneken; I. Nagtegaal; J. van der Laak,"Dept. of Pathology, Radboud University Medical Center, Nijmegen, Netherlands",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,160,163,"The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950492,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950492,Colorectal Cancer;Deep learning;Digital pathology,Algorithm design and analysis;Biomarkers;Blood;Cancer;Image color analysis;Training;Tumors,biological tissues;cancer;image classification;medical image processing;neural nets,ConvNets;colorectal tissue classification;convolutional networks;hematoxylin-eosin stained histopathology images;imaging biomarkers;stain normalization,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
DeepPap: Deep Convolutional Networks for Cervical Cell Classification,L. Zhang; L. Lu; I. Nogues; R. Summers; S. Liu; J. Yao,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA.(email:ling.zhang3@nih.gov)",IEEE Journal of Biomedical and Health Informatics,,2017,PP,99,1,1,"Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into ”abnormal” and ”normal” categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells – without prior segmentation – based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.",2168-2194;21682194,,10.1109/JBHI.2017.2705583,10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065,Cell classification;Cervical cytology;Deep learning;Neural networks;Pap smear,Feature extraction;Image segmentation;Imaging;Informatics;Neural networks;Testing;Training,,,,,,,,,20170519,,,IEEE,IEEE Early Access Articles
Evaluations of deep convolutional neural networks for automatic identification of malaria infected cells,Y. Dong; Z. Jiang; H. Shen; W. David Pan; L. A. Williams; V. V. B. Reddy; W. H. Benjamin; A. W. Bryan,"Dept. of Electrical and Computer Engineering, University of Alabama in Huntsville, Huntsville, AL 35899, USA",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413,2017,,,101,104,"This paper studied automatic identification of malaria infected cells using deep learning methods. We used whole slide images of thin blood stains to compile an dataset of malaria-infected red blood cells and non-infected cells, as labeled by a group of four pathologists. We evaluated three types of well-known convolutional neural networks, including the LeNet, AlexNet and GoogLeNet. Simulation results showed that all these deep convolution neural networks achieved classification accuracies of over 95%, higher than the accuracy of about 92% attainable by using the support vector machine method. Moreover, the deep learning methods have the advantage of being able to automatically learn the features from the input data, thereby requiring minimal inputs from human experts for automated malaria diagnosis.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897215,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897215,,Diseases;Feature extraction;Machine learning;Neural networks;Support vector machines;Testing;Training,biology computing;cellular biophysics;diseases;neural nets;support vector machines,AlexNet;GoogLeNet;cell automatic identification;deep convolutional neural networks;deep learning;malaria infected cells;support vector machine;thin blood stains;whole slide images,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
Deep convolutional neural network for survival analysis with pathological images,X. Zhu; J. Yao; J. Huang,"Department of Computer Science and Engineering, The University of Texas at Arlington, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,544,547,"Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822579,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822579,Deep learning;Lung cancer;Pathological images;Survival analysis,Analytical models;Data models;Feature extraction;Hazards;Lungs;Pathology;Predictive models,cancer;feature extraction;lung;medical image processing;neural nets,DeepConvSurv;NLST lung cancer data;cox proportional hazard model;deep convolutional neural network;hand-crafted feature;medical imaging technology;national lung screening trial;pathological image;survival analysis;survival prediction;unstructured medical image,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling,A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers,"Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA",IEEE Transactions on Image Processing,20161124,2017,26,1,386,399,"Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.",1057-7149;10577149,,10.1109/TIP.2016.2624198,10.13039/100000098 - Intramural Research Program of the National Institutes of Health Clinical Center; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966,Abdominal computed tomography (CT);cascaded random forest;deep convolutional neural networks;dense image patch labeling;pancreas segmentation,Computed tomography;Image segmentation;Labeling;Liver;Pancreas;Shape,biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability,CT slice image decomposition;abdominal computed tomography scans;bottom-up approach;cascaded superpixels;computer-aided diagnosis;deep convolutional neural network classification;deep image patch labeling;dense patch labeling;disjoint boundary-preserving superpixels;image histogram;image location;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach;pancreas class probability maps;pancreas segmentation;pathology detection;probability features;quantitative imaging analysis;random forest classification;superpixel classification;surgical assistance;texture features,,,,,,,20161101,Jan. 2017,,IEEE,IEEE Journals & Magazines
Segmenting Retinal Blood Vessels With Deep Neural Networks,P. Liskowski; K. Krawiec,"Institute of Computing Science, Poznan University of Technology, Poland",IEEE Transactions on Medical Imaging,20161103,2016,35,11,2369,2380,"The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.",0278-0062;02780062,,10.1109/TMI.2016.2546227,10.13039/501100005632 - Narodowe Centrum Bada? i Rozwoju; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440871,Classification;deep learning;feature learning;fundus;neural networks;retina;retinopathy;structured prediction;vessel segmentation,Biomedical imaging;Blood vessels;Convolution;Databases;Image segmentation;Neural networks;Pathology,blood vessels;eye;image classification;image segmentation;medical image processing;neural nets;sensitivity analysis;unsupervised learning,CHASE databases;DRIVE databases;ROC curve;STARE databases;central vessel reflex;deep neural networks;diagnostic factor;fundus imaging;gamma corrections;geometric transformations;global contrast normalization;hemorrhages;human eye;image classification;microaneurysms;nontrivial task;ophthalmology;retinal blood vessel segmentation;structured prediction;supervised segmentation;vascular network;zero-phase whitening,,7,,,,,20160324,Nov. 2016,,IEEE,IEEE Journals & Magazines
Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography,A. Lahiri; A. G. Roy; D. Sheet; P. K. Biswas,"Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology Kharagpur, India",2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20161018,2016,,,1340,1343,"Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.",1557-170X;1557170X,Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8,10.1109/EMBC.2016.7590955,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590955,,Biomedical imaging;Feature extraction;Image segmentation;Kernel;Retinal vessels;Standards;Training,biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;feature extraction;image classification;image coding;image denoising;image segmentation;medical image processing;unsupervised learning,2-level fusion;DRIVE dataset;Kappa agreement coefficient;SoftMax classifier;architectural revision;autoencoders fosters diversity;automated segmentation;bootstrap samples;cardiovascular diseases;classification task;computed aided diagnosis;deep neural ensemble;diabetic retinopathy;first level training;hypertensive disorders;label-free angiography;label-free fundus images;learning dictionary;maximum average accuracy;medical image analysis;network architectures;noisy background;ophthalmic pathologies;physical appearance;retinal blood vessels;retinal vessel segmentation;sparsely trained denoised stacked autoencoder;standard deviation;unsupervised hierarchical feature learning;visual kernels,,,,,,,,16-20 Aug. 2016,,IEEE,IEEE Conference Publications
Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation,A. Mansoor; J. J. Cerrolaza; R. Idrees; E. Biggs; M. A. Alsharid; R. A. Avery; M. G. Linguraru,"Children's National Health System, Washington",IEEE Transactions on Medical Imaging,20160729,2016,35,8,1856,1865,"Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.",0278-0062;02780062,,10.1109/TMI.2016.2535222,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420737,Anterior visual pathway;MRI;intensity normalization;partitioned statistical model;shape model;sparse learning,Biomedical optical imaging;Machine learning;Magnetic resonance imaging;Optical imaging;Pathology;Robustness;Shape,biomedical MRI;curvelet transforms;eye;image segmentation;image sequences;medical image processing;paediatrics;statistical analysis,Dice similarity coefficient;MRI sequences;anatomic structures;anterior visual pathway segmentation;conditional space deep learning;cranial nerve systems;deep learning feature representation;deep learning guided partitioned shape model;joint partitioned statistical shape model;low-grade gliomas;optic nerve;pathologic AVP segmentation;pediatric subjects;robust statistical model;shape localization method;sparse appearance-based learning approach;volumetric multiscale curvelet transform-based intensity normalization method,,3,,,,,20160226,Aug. 2016,,IEEE,IEEE Journals & Magazines
Automated mitosis detection with deep regression networks,H. Chen; X. Wang; P. A. Heng,"Department of Computer Science and Engineering, The Chinese University of Hong Kong",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,1204,1207,"Mitosis counting is one of the strongest prognostic markers for invasive breast cancer diagnosis. Clinical visual examination on histology slides by pathologists is tedious, error-prone and time-consuming. Furthermore, with the advent of whole slide imaging for high-throughput digitization, a large quantity of histology images need to be analyzed. Therefore, automated mitosis detection methods are highly demanded in clinical practice. In this paper, we proposed a deep regression network (DRN) to meet these challenges. It consisted of a downsampling path for extracting the high level information and an upsampling path for outputting the score map with original input size, thus it can be trained in an end-to-end way. In addition, we transferred knowledge learned from cross domains to mitigate the issue of insufficient medical training data. Experimental results on the benchmark dataset 2012ICPR Mitosis Detection Challenge demonstrated the efficacy of our approach, which achieved comparable or better performance than the state-of-the-art methods.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493482,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493482,Mitosis detection;convolutional neural network;deep learning;regression,Benchmark testing;Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Pathology;Training,biological tissues;cancer;learning (artificial intelligence);medical image processing;regression analysis,automated mitosis detection;benchmark dataset 2012 ICPR Mitosis Detection Challenge;clinical visual examination;deep regression network;deep regression networks;downsampling path;high level information;high-throughput digitization;histology imaging;histology slides;invasive breast cancer diagnosis;medical training data;prognostic markers;upsampling path;whole slide imaging,,,,20,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Handcrafted features with convolutional neural networks for detection of tumor cells in histology images,M. N. Kashif; S. E. A. Raza; K. Sirinukunwattana; M. Arif; N. Rajpoot,"Department of Electrical Engineering, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,1029,1032,"Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493441,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493441,Convolutional Neural Network;Digital Pathology;Scattering Transform;Tumor Nuclei Detection,Feature extraction;Image color analysis;Neural networks;Scattering;Standards;Transforms;Tumors,cellular biophysics;feature extraction;medical image processing;neural nets;tumours,cancer histology images;handcrafted features;nonlinear invariant texture features;scattering transform;spatially constrained convolutional neural networks;tumor cell detection,,,,13,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Retinal vessel segmentation via deep learning network and fully-connected conditional random fields,H. Fu; Y. Xu; D. W. K. Wong; J. Liu,"Ocular Imaging Department, Institute for Infocomm Research, Agency for Science, Technology and Research (A&#8727;STAR), Singapore",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,698,701,"Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493362,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493362,Conditional Random Fields;Convolutional Neural Networks;Vessel segmentation,Computer architecture;Image segmentation;Machine learning;Neural networks;Pathology;Retinal vessels,blood vessels;eye;image segmentation;medical image processing;neural nets,DRIVE dataset;STARE dataset;binary vessel segmentation;boundary detection;convolutional neural networks;deep learning network;fully-connected conditional random fields;fundus image;pathological region;retinal vessel segmentation;vessel probability map,,3,,18,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation,H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers,"Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA",IEEE Transactions on Medical Imaging,20160503,2016,35,5,1170,1181,"Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.",0278-0062;02780062,,10.1109/TMI.2015.2482920,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156,Computer aided diagnosis;artificial neural networks;computed tomography;deep learning;machine learning;medical diagnostic imaging;multi-layer neural network;object detection,Colonic polyps;Computed tomography;Feature extraction;Lymph nodes;Three-dimensional displays;Training,computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability,classification probability;colonic polyp detection;computed tomography;computer-aided detection;deep convolutional neural network classifier training;false positives;lymph node detection;medical imaging;random rotations;random translations;random view aggregation;scale transformations;sclerotic metastasis detection;two-tiered coarse-to-fine cascade framework,,11,,60,,,20150928,May 2016,,IEEE,IEEE Journals & Magazines
Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation,Y. Xu; Z. Jia; Y. Ai; F. Zhang; M. Lai; E. I. C. Chang,"Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beihang University, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20150806,2015,,,947,951,"We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.",1520-6149;15206149,Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1,10.1109/ICASSP.2015.7178109,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109,classification;deep convolutional activation features;deep learning;feature learning;segmentation,Biomedical imaging;Feature extraction;Image segmentation;Support vector machines;Training;Training data;Tumors,brain;feature extraction;image classification;image segmentation;medical image processing;tumours,CNN activations;ImageNet;MICCAI 2014 Brain Tumor Digital Pathology Challenge;brain tumor histopathology;deep convolutional activation features;features extraction;image classification;image dimensions;image segmentation,,3,,23,,,,19-24 April 2015,,IEEE,IEEE Conference Publications
Deep learning of feature representation with multiple instance learning for medical image analysis,Y. Xu; T. Mo; Q. Feng; P. Zhong; M. Lai; E. I. C. Chang,"State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20140714,2014,,,1626,1630,"This paper studies the effectiveness of accomplishing high-level tasks with a minimum of manual annotation and good feature representations for medical images. In medical image analysis, objects like cells are characterized by significant clinical features. Previously developed features like SIFT and HARR are unable to comprehensively represent such objects. Therefore, feature representation is especially important. In this paper, we study automatic extraction of feature representation through deep learning (DNN). Furthermore, detailed annotation of objects is often an ambiguous and challenging task. We use multiple instance learning (MIL) framework in classification training with deep learning features. Several interesting conclusions can be drawn from our work: (1) automatic feature learning outperforms manual feature; (2) the unsupervised approach can achieve performance that's close to fully supervised approach (93.56%) vs. (94.52%); and (3) the MIL performance of coarse label (96.30%) outweighs the supervised performance of fine label (95.40%) in supervised deep learning features.",1520-6149;15206149,Electronic:978-1-4799-2893-4; POD:978-1-4799-2894-1; USB:978-1-4799-2892-7,10.1109/ICASSP.2014.6853873,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853873,deep learning;feature learning;multiple instance learning;supervised;un-supervised,Biomedical imaging;Cancer;Feature extraction;Manuals;Supervised learning;Training;Vectors,feature extraction;image representation;learning (artificial intelligence);medical image processing,DNN;HARR features;MIL framework;SIFT features;automatic feature representation extraction;classification training;clinical features;feature representation;manual annotation;medical image analysis;multiple instance learning;supervised deep learning features;unsupervised approach,,14,,26,,,,4-9 May 2014,,IEEE,IEEE Conference Publications
A Deep Learning Method for Microaneurysm Detection in Fundus Images,J. Shan; L. Li,"Dept. of Comput. Sci., Pace Univ. New York City, New York, NY, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20160818,2016,,,357,358,"Diabetic Retinopathy (DR) is the leading cause of blindness in the working-age population. Microaneurysms (MAs), due to leakage from retina blood vessels, are the early signs of DR. However, automated MA detection is complicated because of the small size of MA lesions and the low contrast between the lesion and its retinal background. Recently deep learning (DL) strategies have been used for automatic feature extraction and classification problems, especially for image analysis. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a DL strategy, is presented for MA detection in fundus images. Small image patches are generated from the original fundus images. The SSAE learns high-level features from pixel intensities alone in order to identify distinguishing features of MA. The high-level features learned by SSAE are fed into a classifier to categorize each image patch as MA or non-MA. The public benchmark DIARETDB is utilized to provide the training/testing data and ground truth. Among the 89 images, totally 2182 image patches with MA lesions, serve as positive data, and another 6230 image patches without MA lesions are generated by a randomly sliding window operation, to serve as negative data. Without any blood vessel removal or complicated preprocessing operations, SSAE learned directly from the raw image patches, and automatically extracted the distinguishing features to classify the patches using Softmax Classifier. By employing the fine-tuning operation, an improved F-measure 91.3% and an average area under the ROC curve (AUC) 96.2% were achieved using 10-fold cross-validation.",,Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2,10.1109/CHASE.2016.12,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545864,automated microaneurysm detection;deep learning;diabetic retinopathy;feature representation;stacked sparse autoencoder,Biomedical image processing;Conferences;Feature extraction;Lesions;Machine learning,blood vessels;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing,10-fold cross-validation;DIARETDB public benchmark;DR;F-measure;SSAE;automated MA detection;automatic feature extraction;blindness;classification problems;deep learning method;diabetic retinopathy;fine-tuning operation;fundus images;image analysis;microaneurysm detection;microaneurysms;randomly sliding window operation;retina blood vessels;softmax classifier;stacked sparse autoencoder;working-age population,,1,,,,,,27-29 June 2016,,IEEE,IEEE Conference Publications
Deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images,D. Maji; A. Santara; S. Ghosh; D. Sheet; P. Mitra,"Indian Institute of Technology Kharagpur, WB 721302, India",2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20151105,2015,,,3029,3032,"Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195.",1094-687X;1094687X,DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5,10.1109/EMBC.2015.7319030,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319030,Computational imaging;deep learning;denoising auto-encoder;random forests;vessel detection,Biomedical imaging;Image analysis;Radio frequency;Retinal vessels;Vegetation,biomedical optical imaging;blood vessels;diseases;eye;image denoising;learning (artificial intelligence);medical image processing;vision defects,DNN response;DRIVE database;ROC curve;blood vessels;computational imaging framework;deep network;deep neural network;disease diagnosis;fundus color imaging;large-scale screening;learning based hybrid architecture;pathological damage;periodic screening;random forest hybrid architecture;retinal vessel detection;sparse trained denoising autoencoders;unsupervised learning;vision impairment,,1,,16,,,,25-29 Aug. 2015,,IEEE,IEEE Conference Publications
Multi-stage segmentation of the fovea in retinal fundus images using fully Convolutional Neural Networks,S. Sedai; R. Tennakoon; P. Roy; K. Cao; R. Garnavi,"IBM Research - Australia, Melbourne, VIC, Australia",2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),20170619,2017,,,1083,1086,"The fovea is one of the most important anatomical landmarks in the eye and its localization is required in automated analysis of retinal diseases due to its role in sharp central vision. In this paper, we propose a two-stage deep learning framework for accurate segmentation of the fovea in retinal colour fundus images. In the first stage, coarse segmentation is performed to localize the fovea in the fundus image. The location information from the first stage is then used to perform fine-grained segmentation of the fovea region in the second stage. The proposed method performs end-to-end pixelwise segmentation by creating a deep learning model based on fully convolutional neural networks, which does not require the prior knowledge of the location of other retinal structures such as optic disc (OD) and vasculature geometry. We demonstrate the effectiveness of our method on a dataset with 400 retinal images with average localization error of 14 ± 7 pixels.",,Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5,10.1109/ISBI.2017.7950704,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950704,Convolution neural network;Fovea Segmentation;Retinal Imaging,Convolution;Image segmentation;Machine learning;Neural networks;Optical imaging;Retina;Training,biomedical optical imaging;eye;image segmentation;medical image processing;neural nets,end-to-end pixelwise segmentation;fovea;fully convolutional neural networks;multistage segmentation;optic disc;retinal colour fundus images;retinal structures;two-stage deep learning framework;vasculature geometry,,,,,,,,18-21 April 2017,,IEEE,IEEE Conference Publications
Automatic Feature Learning Method for Detection of Retinal Landmarks,B. Al-Bander; W. Al-Nuaimy; M. A. Al-Taee; A. Al-Ataby; Y. Zheng,,2016 9th International Conference on Developments in eSystems Engineering (DeSE),20170518,2016,,,13,18,"This paper presents an automatic deep learning method for location detection of important retinal landmarks, the fovea and optic disc (OD) in digital fundus retinal images with the potential for use in an automated screening and grading system. The proposed method is based on deep convolutional neural networks (CNN) and does not depend the visual appearance or anatomical features of the retinal landmarks. It comprises convolution, max-pooling, fully connected and dropout layers as well as an output layer. The CNN is trained using an existing dataset images along with their annotated locations of the foveal and OD centres. Performance of the network is evaluated using Root Mean Square Error (RMSE). The developed feature learning-based approach presents promising system for retinal landmarks detection.",,Electronic:978-1-5090-5487-9; POD:978-1-5090-5488-6,10.1109/DeSE.2016.4,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930616,Automatic feature learning;automated grading;convolutional neural network;deep learning;retinal landmarks,Adaptive optics;Feature extraction;Neural networks;Optical filters;Optical imaging;Retina;Training,,,,,,,,,,Aug. 31 2016-Sept. 2 2016,,IEEE,IEEE Conference Publications
Augmenting data when training a CNN for retinal vessel segmentation: How to warp?,A. Oliveira; S. Pereira; C. A. Silva,"CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal",2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG),20170330,2017,,,1,4,"The retinal vascular condition is a trustworthy biomarker of several ophthalmologic and cardiovascular diseases, so automatic vessel segmentation is a crucial step to diagnose and monitor these problems. Deep Learning models have recently revolutionized the state-of-the-art in several fields, since they can learn features with multiple levels of abstraction from the data itself. However, these methods can easily fall into overfitting, since a huge number of parameters must be learned. Having bigger datasets may act as regularization and lead to better models. Yet, acquiring and manually annotating images, especially in the medical field, can be a long and costly procedure. Hence, when using regular datasets, people heavily need to apply artificial data augmentation. In this work, we use a fully convolutional neural network capable of reaching the state-of-the-art. Also, we investigate the benefits of augmenting data with new samples created by warping retinal fundus images with nonlinear transformations. Our results hint that may be possible to halve the amount of data, while maintaining the same performance.",,Electronic:978-1-5090-4801-4; POD:978-1-5090-4802-1,10.1109/ENBENG.2017.7889443,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889443,Convolutional neural network;Data augmentation;Retinal blood vessel segmentation,Data mining;Image segmentation;Neural networks;Retinal vessels;Training;Two dimensional displays,biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets;vision defects,CNN;artificial data augmentation;automatic vessel segmentation;biomarker;cardiovascular diseases;deep Learning models;fully convolutional neural network;medical field;nonlinear transformations;ophthalmologic diseases;regular datasets;retinal fundus images;retinal vascular condition;retinal vessel segmentation,,,,,,,,16-18 Feb. 2017,,IEEE,IEEE Conference Publications
Deep vessel tracking: A generalized probabilistic approach via deep learning,A. Wu; Z. Xu; M. Gao; M. Buty; D. J. Mollura,"Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892",2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI),20160616,2016,,,1363,1367,"Analysis of vascular geometry is important in many medical imaging applications, such as retinal, pulmonary, and cardiac investigations. In order to make reliable judgments for clinical usage, accurate and robust segmentation methods are needed. Due to the high complexity of biological vasculature trees, manual identification is often too time-consuming and tedious to be used in practice. To design an automated and computerized method, a major challenge is that the appearance of vasculatures in medical images has great variance across modalities and subjects. Therefore, most existing approaches are specially designed for a particular task, lacking the flexibility to be adapted to other circumstances. In this paper, we present a generic approach for vascular structure identification from medical images, which can be used for multiple purposes robustly. The proposed method uses the state-of-the-art deep convolutional neural network (CNN) to learn the appearance features of the target. A Principal Component Analysis (PCA)-based nearest neighbor search is then utilized to estimate the local structure distribution, which is further incorporated within the generalized probabilistic tracking framework to extract the entire connected tree. Qualitative and quantitative results over retinal fundus data demonstrate that the proposed framework achieves comparable accuracy as compared with state-of-the-art methods, while efficiently producing more information regarding the candidate tree structure.",,Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9,10.1109/ISBI.2016.7493520,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493520,Deep Learning;Generalized Probabilistic Tracking;Nearest Neighbor Search;Principal Component Analysis;Vascular Structure,Biomedical imaging;Dictionaries;Image segmentation;Machine learning;Probabilistic logic;Robustness,,,,1,,10,,,,13-16 April 2016,,IEEE,IEEE Conference Publications
Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images,M. J. J. P. van Grinsven; B. van Ginneken; C. B. Hoyng; T. Theelen; C. I. Sánchez,"Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands",IEEE Transactions on Medical Imaging,20160429,2016,35,5,1273,1284,"Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.",0278-0062;02780062,,10.1109/TMI.2016.2526689,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401052,Convolutional neural network;deep learning;hemorrhage;selective sampling,Biomedical imaging;Databases;Hemorrhaging;Image analysis;Image color analysis;Observers;Training,biomedical optical imaging;blood;computer vision;image classification;image colour analysis;image sampling;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis,CNN learning process;CNN training iteration;color fundus images;computer vision applications;deep learning network architectures;dynamically selecting misclassified negative samples;fast convolutional neural network training;hemorrhage detection;independent test set;medical image analysis tasks;receiver operating characteristics curve;selective data sampling;selective sampling method,,7,,48,,,20160208,May 2016,,IEEE,IEEE Journals & Magazines
Detection of exudates in fundus photographs using convolutional neural networks,P. Prentašić; S. Lončarić,"University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Image Processing Group 10000, Croatia",2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA),20151026,2015,,,188,192,"Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.",1845-5921;18455921,Electronic:978-1-4673-8032-4; POD:978-1-4673-8033-1,10.1109/ISPA.2015.7306056,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306056,,Convolution;Diabetes;Neural networks;Optical imaging;Optical sensors;Retina;Retinopathy,biomedical optical imaging;colour photography;diseases;eye;image colour analysis;medical image processing;neural nets;object detection,color fundus photographs;convolutional neural networks;deep neural networks;diabetic retinopathy;early diagnosis;exudate detection;machine learning technique;preventable blindness,,,,31,,,,7-9 Sept. 2015,,IEEE,IEEE Conference Publications
Using deep learning for robustness to parapapillary atrophy in optic disc segmentation,R. Srivastava; J. Cheng; D. W. K. Wong; J. Liu,"Institute for Infocomm Research, Singapore 138632",2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI),20150723,2015,,,768,771,"Optic Disc (OD) segmentation from retinal fundus images is important for many applications such as detecting other optic structures and early detection of glaucoma. One of the major problems in segmenting OD is the presence of Para-papillary Atrophy (PPA) which in many cases looks similar to the OD. Researchers have used many different features to distinguish between PPA and OD, however each of these features has some limitation or the other. In this paper, we propose to use a deep neural network for OD segmentation which can learn features to distinguish PPA from OD. Using simple image intensity based features, the proposed method has the least mean overlapping error (9.7%) among the state-of-the-art works for OD segmentation in fundus images with PPA.",1945-7928;19457928,Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0,10.1109/ISBI.2015.7163985,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163985,Optic Disc segmentation;deep learning;parapapillary atrophy;retinal fundus images,Adaptive optics;Atrophy;Feature extraction;Image segmentation;Optical imaging;Retina;Training,eye;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology,deep learning;deep neural network;glaucoma detection;image intensity based features;least mean overlapping error;optic disc segmentation;optic structures;parapapillary atrophy;retinal fundus images,,1,,10,,,,16-19 April 2015,,IEEE,IEEE Conference Publications
Image quality classification for DR screening using deep learning,F. Yu; J. Sun; A. Li; J. Cheng; C. Wan; J. Liu,"Nanjing University of Aeronautics and Astronautics, China",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,664,667,"The quality of input images significantly affects the outcome of automated diabetic retinopathy (DR) screening systems. Unlike the previous methods that only consider simple low-level features such as hand-crafted geometric and structural features, in this paper we propose a novel method for retinal image quality classification (IQC) that performs computational algorithms imitating the working of the human visual system. The proposed algorithm combines unsupervised features from saliency map and supervised features coming from convolutional neural networks (CNN), which are fed to an SVM to automatically detect high quality vs poor quality retinal fundus images. We demonstrate the superior performance of our proposed algorithm on a large retinal fundus image dataset and the method could achieve higher accuracy than other methods. Although retinal images are used in this study, the methodology is applicable to the image quality assessment and enhancement of other types of medical images.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036912,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036912,convolutional neural networks;image quality classification;saliency map,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Deep tessellated retinal image detection using Convolutional Neural Networks,X. Lyu; H. Li; Y. Zhen; X. Ji; S. Zhang,"Computer Graphics and Imaging Lab, College of Computer Science and Technology, Zhejiang University, Hangzhou, China",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,676,680,"Tessellation in fundus is not only a visible feature for aged-related and myopic maculopathy but also confuse retinal vessel segmentation. The detection of tessellated images is an inevitable processing in retinal image analysis. In this work, we propose a model using convolutional neural network for detecting tessellated images. The input to the model is pre-processed fundus image, and the output indicate whether this photograph has tessellation or not. A database with 12,000 colour retinal images is collected to evaluate the classification performance. The best tessellation classifier achieves accuracy of 97.73% and AUC value of 0.9659 using pretrained GoogLeNet and transfer learning technique.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8036915,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036915,Tessellated fundus;convolutional neural network;tessellation detection;transfer learning,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Convolutional neural networks for deep feature learning in retinal vessel segmentation,A. F. Khalaf; I. A. Yassine; A. S. Fahmy,"Systems and Biomedical Engineering Department, Faculty of Engineering, Cairo University",2016 IEEE International Conference on Image Processing (ICIP),20161208,2016,,,385,388,"Analysis of retinal vessels in fundus images provides a valuable tool for characterizing many retinal and systemic diseases. Accurate automatic segmentation of these vessels is usually required as an essential analysis step. In this work, we propose a new formulation of deep Convolutional Neural Networks that allows simple and accurate segmentation of the retinal vessels. A major modification in this work is to reduce the intra-class variance by formulating the problem as a Three-class problem that differentiates: large vessels, small vessels, and background areas. In addition, different sizes of the convolutional kernels have been studied and it was found that a combination of kernels with different sizes achieve the best sensitivity and specificity. The proposed method was tested using DRIVE dataset and it showed superior performance compared to several other state of the art methods. The segmentation sensitivity, specificity and accuracy were found to be 83.97%, 95.62% and 94.56% respectively.",,Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3,10.1109/ICIP.2016.7532384,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532384,Convolutional Neural Networks;Deep Learning;Pattern Classification;Retinal Blood Vessel Segmentation,,diseases;image segmentation;medical image processing;neural nets,DRIVE dataset;convolutional neural networks;deep feature learning;fundus images;retinal blood vessel segmentation;retinal diseases;retinal vessel analysis;segmentation sensitivity;systemic diseases,,,,,,,,25-28 Sept. 2016,,IEEE,IEEE Conference Publications
HCNN: Heterogeneous Convolutional Neural Networks for Comorbid Risk Prediction with Electronic Health Records,J. Zhang; J. Gong; L. Barnes,"Dept. of Syst. & Inf. Eng., Univ. of Virginia, Charlottesville, VA, USA","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",20170817,2017,,,214,221,"The increasing adoption of electronic health record (EHR) systems has brought tremendous opportunities in medicine enabling more personalized prognostic models. However, most work to date has investigated the binary classification problem for predicting the onset of one chronic disease, but little attention has been given to assessing risk of developing comorbidities that are major causes of morbidity and mortality. For example, type 2 diabetes and chronic kidney disease frequently accompany congestive heart failure. This paper is motivated by the problem of predicting comorbid diseases and aims to answer the following question: can we predict the comorbid risk using a patient's medical history? We propose a new predictive learning framework, Heterogeneous Convolutional Neural Network (HCNN), that represents EHRs as graphs with heterogeneous attributes (e.g. diagnoses, procedures, and medication), and then develop a novel deep learning methodology for risk prediction of multiple comorbid diseases. The main innovation of the framework is that it defines the distance between the heterogeneous attributes of the graph representation extracted from the EHR and develops an appropriate learning infrastructure that is a composition of sparse convolutional layers and local pooling steps that match with the local structure of the space of the heterogeneous attributes. As a result, the new method is capable of capturing features about the relationships between heterogeneous attributes of the graphs. Through a comparative study on patient EHR data, HCNN achieves better performance than traditional convolutional neural networks on the risk prediction of comorbid diseases.",,Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5,10.1109/CHASE.2017.80,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010635,Electronic Health Records;Heterogeneous Convolution;Neural Networks;Risk Prediction,Convolution;Correlation;Diseases;Feature extraction;Machine learning;Medical diagnostic imaging;Neural networks,diseases;electronic health records;feedforward neural nets;learning (artificial intelligence);pattern classification;risk management,EHR systems;HCNN;binary classification problem;chronic disease;comorbid diseases;comorbid risk prediction;deep learning methodology;electronic health records;graph representation;heterogeneous attributes;heterogeneous convolutional neural networks;learning infrastructure;local pooling steps;personalized prognostic models;predictive learning framework;sparse convolutional layers,,,,,,,,17-19 July 2017,,IEEE,IEEE Conference Publications
Classification of radiology reports using neural attention models,B. Shin; F. H. Chokshi; T. Lee; J. D. Choi,"Mathematics and Computer Science, Emory University, Atlanta, GA 30322",2017 International Joint Conference on Neural Networks (IJCNN),20170703,2017,,,4363,4370,"The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure “black-box” models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.",,Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9,10.1109/IJCNN.2017.7966408,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966408,,Computational modeling;Convolution;Machine learning;Neural networks;Radiology;Sentiment analysis;Support vector machines,computerised tomography;convolution;data analysis;electronic health records;neural nets;pattern classification;radiology,CNN;EHR;attention analysis;convolutional neural networks;double-annotated dataset;electronic health record;neural attention mechanism;neural attention models;radiology head computed tomography reports classification,,,,,,,,14-19 May 2017,,IEEE,IEEE Conference Publications
A regularized deep learning approach for clinical risk prediction of acute coronary syndrome using electronic health records,Z. Huang; W. Dong; H. Duan; J. Liu,"College of Biomedical Engineering and Instrument Science of Zhejiang University, Hangzhou, Zhejiang China 310008 (e-mail: zhengxing.h@gmail.com)",IEEE Transactions on Biomedical Engineering,,2017,PP,99,1,1,"Objective: Acute coronary syndrome (ACS), as a common and severe cardiovascular disease, is a leading cause of death and the principal cause of serious long-term disability globally. Clinical risk prediction of ACS is important for early intervention and treatment. Existing ACS risk scoring models are based mainly on a small set of hand-picked risk factors and often dichotomize predictive variables to simplify the score calculation [1-3]. Methods: This study develops a regularized stacked denoising auto-encoder (SDAE) model to stratify clinical risks of ACS patients from a large volume of electronic health records (EHR). To capture characteristics of patients at similar risk levels, and preserve the discriminating information across different risk levels, two constraints are added on SDAE to make the reconstructed feature representations contain more risk information of patients, which contribute to a better clinical risk prediction result. Results: We validate our approach on a real clinical dataset consisting of 3,464 ACS patient samples. The performance of our approach for predicting ACS risk remains robust and reaches 0.868 and 0.73 in terms of both AUC and Accuracy, respectively. Conclusions: The obtained results show that the proposed approach achieves a competitive performance compared to state-of-the-art models in dealing with the clinical risk prediction problem. In addition, our approach can extract informative risk factors of ACS via a reconstructive learning strategy. Some of these extracted risk factors are not only consistent with existing medical domain knowledge, but also contain suggestive hypotheses that could be validated by further investigations in the medical domain.",0018-9294;00189294,,10.1109/TBME.2017.2731158,National Key Research and Development Program of China; National Nature Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990180,Acute coronary syndrome;Clinical risk prediction;Deep learning;Electronic Health Record;Stacked denoising auto-encoder,,,,,,,,,,20170724,,,IEEE,IEEE Early Access Articles
Cancer Classification Based on Microarray Gene Expression Data Using Deep Learning,P. Guillen; J. Ebalunode,"Center for Adv. Comput. & Data Syst., Univ. of Houston, Houston, TX, USA",2016 International Conference on Computational Science and Computational Intelligence (CSCI),20170320,2016,,,1403,1405,"The classification of cancer is a major research topic in Medicine. Cancer microarray data normally contains a small number of samples, which have a large number of gene expression levels as features, however, makes the classification quite challenging. Using a deep learning algorithm based on multilayer perceptron, we show that classification performance at least as good as published results can be obtained for cancer classification.",,Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1,10.1109/CSCI.2016.0270,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881563,cancer;classification;deep learning;machine learning;microarray gene expression,Cancer;Computer architecture;Gene expression;Machine learning;Multilayer perceptrons;Training;Tumors,bioinformatics;cancer;genetics;multilayer perceptrons;pattern classification,cancer classification;deep learning algorithm;microarray gene expression data;multilayer perceptron,,,,,,,,15-17 Dec. 2016,,IEEE,IEEE Conference Publications
A predictive model of gene expression using a deep learning framework,Rui Xie; A. Quitadamo; J. Cheng; Xinghua Shi,"Department of Computer Science, University of Missouri at Columbia, 65201, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,676,681,"With an unprecedented amount of data available, it is important to explore new methods for developing predictive models to mine this data for scientific discoveries. In this study, we propose a deep learning regression model based on MultiLayer Perceptron and Stacked Denoising Auto-encoder (MLP-SAE) to predict gene expression from genotypes of genetic variation. Specifically, we use a stacked denoising auto-encoder to train our regression model in order to extract useful features, and utilize the multilayer perceptron for backpropagation. We further improve our model by adding a dropout technique to prevent overfitting. Our results on a real genomic dataset show that our MLP-SAE model with dropout outperform Lasso, Random Forests, and MLP-SAE without dropout. Our study provides a new application of deep learning in mining genomics data, and demonstrates that deep learning has great potentials in building predictive models to help understand biological systems.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822599,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822599,,Data models;Gene expression;Machine learning;Mathematical model;Predictive models;Training,backpropagation;biology computing;data mining;feature extraction;genetics;genomics;multilayer perceptrons;regression analysis,MLP-SAE model;backpropagation;biological systems;deep learning regression model;dropout technique;feature extraction;gene expression;genetic variation genotypes;genomic dataset;genomics data mining;multilayer perceptron;stacked denoising autoencoder,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
"Learning structure in gene expression data using deep architectures, with an application to gene clustering",A. Gupta; H. Wang; M. Ganapathiraju,"Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, USA",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217,2015,,,1328,1335,"Genes play a central role in all biological processes. DNA microarray technology has made it possible to study the expression behavior of thousands of genes in one go. Often, gene expression data is used to generate features for supervised and unsupervised learning tasks. At the same time, advances in the field of deep learning have made available a plethora of architectures. In this paper, we use deep architectures pre-trained in an unsupervised manner using denoising autoencoders as a preprocessing step for a popular unsupervised learning task. Denoising autoencoders (DA) can be used to learn a compact representation of input, and have been used to generate features for further supervised learning tasks. We propose that our deep architectures can be treated as empirical versions of Deep Belief Networks (DBNs). We use our deep architectures to regenerate gene expression time series data for two different data sets. We test our hypothesis on two popular datasets for the unsupervised learning task of clustering and find promising improvements in performance.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359871,autoencoders;deep learning;gene clustering;gene expression,Noise reduction,genetics;learning (artificial intelligence);neural nets;pattern clustering,DNA microarray technology;Deep Belief Networks;biological processes;deep architectures;deep learning;denoising autoencoders;gene clustering;gene expression time series data;learning structure;learning tasks,,,,20,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
A Deep Learning Model for Epigenomic Studies,G. Lo Bosco; R. Rizzo; A. Fiannaca; M. La Rosa; A. Urso,"Dept. of Math. & Comput. Sci. UNIPA, Univ. of Palermo Palermo, Palermo, Italy",2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),20170424,2016,,,688,692,"Epigenetics is the study of heritable changes in gene expression that does not involve changes to the underlying DNA sequence, i.e. a change in phenotype not involved by a change in genotype. At least three main factor seems responsible for epigenetic change including DNA methylation, histone modification and non-coding RNA, each one sharing having the same property to affect the dynamic of the chromatin structure by acting on Nucleosomes position. A nucleosome is a DNA-histone complex, where around 150 base pairs of double-stranded DNA is wrapped. The role of nucleosomes is to pack the DNA into the nucleus of the Eukaryote cells, to form the Chromatin. Nucleosome positioning plays an important role in gene regulation and several studies shows that distinct DNA sequence features have been identified to be associated with nucleosome presence. Starting from this suggestion, the identification of nucleosomes on a genomic scale has been successfully performed by DNA sequence features representation and classical supervised classification methods such as Support Vector Machines, Logistic regression and so on. Taking in consideration the successful application of the deep neural networks on several challenging classification problems, in this paper we want to study how deep learning network can help in the identification of nucleosomes.",,Electronic:978-1-5090-5698-9; POD:978-1-5090-5699-6,10.1109/SITIS.2016.115,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907542,Classification;Deep Learning;Nucleosome Positioning,Bioinformatics;DNA;Genomics;Kernel;Machine learning;Neural networks;Support vector machines,biology computing;genomics;learning (artificial intelligence);molecular biophysics;neural nets,DNA methylation;DNA sequence;Eukaryote cells;deep learning model;deep learning network;deep neural networks;epigenomic studies;gene expression;histone modification;noncoding RNA;nucleosomes,,,,,,,,Nov. 28 2016-Dec. 1 2016,,IEEE,IEEE Conference Publications
A deep learning model for predicting transcription factor binding location at single nucleotide resolution,S. Salekin; J. M. Zhang; Y. Huang,"Electrical and Computer Engineering Department, University of Texas at San Antonio, 1 UTSA Circle, San Antonio, TX 78249, USA",2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI),20170413,2017,,,57,60,"Transcriptional regulation by transcription factors (TFs) plays a pivotal role in controlling the gene expression. However, understanding the mechanism through which the transcription factors regulate the gene expression is a challenging task. This is primarily hindered by the low specificity in identifying transcription factor binding sites (TFBS). The emergence of the ChIP-exonuclease (ChIP-exo) method enables the detection of TFBS at single nucleotide sensitivity, providing us an opportunity to study the detailed mechanisms of TF regulation. Nevertheless, there is still a lack of computational tools that can also provide single base pair (bp) resolution prediction of TFBS. In this paper, we propose DeepSNR, a Deep Learning algorithm for Single Nucleotide Resolution prediction of transcription factor binding site. Our proposed method is inspired by the similarity between predicting the specific binding location from input nucleotide sequence and image segmentation. Particularly, we adopted the deconvolution network (deconvNet); a deep learning model designed for image segmentation, and developed a TFBS specific deconvNet architecture constructed on top of `DeepBind'. We trained a deconvNet for predicting CTCF binding sites using the data from ChIP-exo experiments. The proposed algorithm achieved median precision and recall of 87% and 77% respectively, significantly outperforming motif search based algorithms such as MatInspector.",,Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0,10.1109/BHI.2017.7897204,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897204,,Algorithm design and analysis;Bioinformatics;DNA;Deconvolution;Genomics;Machine learning;Training,biological techniques;biology computing;deconvolution;enzymes;genetics;image segmentation;learning (artificial intelligence);molecular biophysics,CTCF binding sites;ChIP-exonuclease method;DeepBind;DeepSNR;TFBS specific deconvNet architecture;deconvolution network;deep learning model;gene expression;image segmentation;input nucleotide sequence;single nucleotide resolution;single nucleotide sensitivity;transcription factor binding sites,,,,,,,,16-19 Feb. 2017,,IEEE,IEEE Conference Publications
DP-miRNA: An improved prediction of precursor microRNA using deep learning model,J. Thomas; S. Thomas; L. Sael,"Department of Computer Science, Stony Brook University, NY 11794, USA",2017 IEEE International Conference on Big Data and Smart Computing (BigComp),20170320,2017,,,96,99,"MicroRNA (miRNA) are small non-coding RNAs regulating gene expression at the post-transcriptional level. Detecting miRNA in a genome is challenging experimentally and results vary depending on their cellular environment. These limitations inspire the development of knowledge-based prediction method. This paper proposes a deep learning based classification model for predicting precursor miRNA sequence that contains the miRNA sequence. The feature set consists of sequence features, folding measures, stem-loop features and statistical features. We evaluate the performance of the proposed method on human dataset. The deep neural network based classification outperformed support vector machine, neural network, naive Bayes classifiers, k-nearest neighbors, random forests as well as hybrid systems combining SVM and genetic algorithm.",,Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9,10.1109/BIGCOMP.2017.7881722,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881722,,Computer science;Machine learning;Neural networks;Predictive models;Proteins;Support vector machines;Training,Bayes methods;RNA;biology computing;cellular biophysics;genetic algorithms;knowledge based systems;learning (artificial intelligence);neural nets;random processes;statistical analysis;support vector machines,DP-miRNA;Naive Bayes classifiers;SVM;cellular environment;deep learning based classification model;deep learning model;deep neural network based classification;folding measures;gene expression;genetic algorithm;genome miRNA detection;human dataset;hybrid systems;k-nearest neighbors;knowledge-based prediction method;noncoding RNA;post-transcriptional level;precursor microRNA prediction;random forests;sequence features;statistical features;stem-loop features;support vector machine,,,,,,,,13-16 Feb. 2017,,IEEE,IEEE Conference Publications
Multimodal Deep Boltzmann Machines for feature selection on gene expression data,A. F. Syafiandini; I. Wasito; S. Yazid; A. Fitriawan; M. Amien,"Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia",2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS),20170309,2016,,,407,412,"In this paper, multimodal Deep Boltzmann Machines (DBM) is employed to learn important genes (biomarkers) on gene expression data from human carcinoma colorectal. The learning process involves gene expression data and several patient phenotypes such as lymph node and distant metastasis occurrence. The proposed framework in this paper uses multimodal DBM to train records with metastasis occurrence. Later, the trained model is tested using records with no metastasis occurrence. After that, Mean Squared Error (MSE) is measured from the reconstructed and the original gene expression data. Genes are ranked based on the MSE value. The first gene has the highest MSE value. After that, k-means clustering is performed using various number of genes. Features that give the highest purity index are considered as the important genes. The important genes obtained from the proposed framework and two sample t-test are being compared. From the accuracy of metastasis classification, the proposed framework gives higher results compared to the top genes from two sample t-test.",,Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7,10.1109/ICACSIS.2016.7872733,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872733,gene expression;microarray;multimodal DBM,Biomarkers;Gene expression;Lymph nodes;Metastasis;Support vector machines;Testing;Training,Boltzmann machines;bioinformatics;feature selection;genetics;learning (artificial intelligence);mean square error methods;pattern classification,DBM;MSE;biomarkers;feature selection;gene expression data;human carcinoma colorectal;k-means clustering;learning process;mean squared error;metastasis classification;multimodal deep Boltzmann machines;patient phenotypes,,,,,,,,15-16 Oct. 2016,,IEEE,IEEE Conference Publications
Cancer subtype identification using deep learning approach,A. F. Syafiandini; I. Wasito; S. Yazid; A. Fitriawan; M. Amien,"Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia","2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA)",20170228,2016,,,108,112,"In this paper, a framework using deep learning approach is proposed to identify two subtypes of human colorectal carcinoma cancer. The identification process uses information from gene expression and clinical data which is obtained from data integration process. One of deep learning architecture, multimodal Deep Boltzmann Machines (DBM) is used for data integration process. The joint representation gene expression and clinical is later used as Restricted Boltzmann Machines (RBM) input for cancer subtype identification. Kaplan Meier survival analysis is employed to evaluate the identification result. The curves on survival plot obtained from Kaplan Meier analysis are tested using three statistic tests to ensure that there is a significant difference between those curves. According to Log Rank, Generalized Wilcoxon and Tarone-Ware, the two groups of patients with different cancer subtypes identified using the proposed framework are significantly different.",,Electronic:978-1-5090-2323-3; POD:978-1-5090-2324-0; USB:978-1-5090-2322-6,10.1109/IC3INA.2016.7863033,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863033,RBM;cancer subtype;deep learning;multimodal DBM,Computer architecture;Data integration;Data mining;Gene expression;Machine learning;Metastasis,Boltzmann machines;cancer;data integration;learning (artificial intelligence);medical computing;statistical analysis,Kaplan Meier survival analysis;Log Rank;RBM;Tarone-Ware;cancer subtype identification;clinical data;data integration process;deep learning approach;gene expression;generalized Wilcoxon;human colorectal carcinoma cancer;restricted Boltzmann machines,,,,,,,,3-5 Oct. 2016,,IEEE,IEEE Conference Publications
Inferring Gene Regulatory Networks by Combining Supervised and Unsupervised Methods,T. Turki; J. T. L. Wang; I. Rajikhan,"Comput. Sci. Dept., King Abdulaziz Univ., Jeddah, Saudi Arabia",2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),20170202,2016,,,140,145,"Supervised methods for inferring gene regulatory networks (GRNs) perform well with good training data. However, when training data is absent, these methods are not applicable. Unsupervised methods do not need training data but their accuracy is low. In this paper, we combine supervised and unsupervised methods to infer GRNs using time-series gene expression data. Specifically, we use results obtained from unsupervised methods to train supervised methods. Since the results contain noise, we develop a data cleaning algorithm to remove noise, hence improving the quality of the training data. These refined training data are then used to guide classifiers including support vector machines and deep learning tools to infer GRNs through link prediction. Experimental results on several data sets demonstrate the good performance of the classifiers and the effectiveness of our data cleaning algorithm.",,Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6,10.1109/ICMLA.2016.0031,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838135,data mining;machine learning;network inference;systems biology,Classification algorithms;Cleaning;Gene expression;Inference algorithms;Mathematical model;Training;Training data,bioinformatics;data handling;genetics;network theory (graphs);pattern classification;time series;unsupervised learning,GRN;classifiers;data cleaning;deep learning tools;gene regulatory networks;link prediction;noise removal;support vector machines;time-series gene expression data;unsupervised methods,,,,,,,,18-20 Dec. 2016,,IEEE,IEEE Conference Publications
DeepEnhancer: Predicting enhancers by convolutional neural networks,Xu Min; Ning Chen; Ting Chen; Rui Jiang,"MOE Key Laboratory of Bioinformatics, Bioinformatics Division and Center for Synthetic & Systems Biology, TNLIST, Department of Automation, Tsinghua University, Beijing 100084, China",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,637,644,"Enhancers are crucial to the understanding of mechanisms underlying gene transcriptional regulation. Although having been successfully applied in such projects as ENCODE and Roadmap to generate landscape of enhancers in human cell lines, high-throughput biological experimental techniques are still costly and time consuming for even larger scale identification of enhancers across a variety of tissues under different disease status, making computational identification of enhancers indispensable. In this paper, we propose a computational framework, named DeepEnhancer, to classify enhancers from background genomic sequences. We construct convolutional neural networks of various architectures and compare the classification performance with traditional sequence-based classifiers. We first train the deep learning model on the FANTOM5 permissive enhancer dataset, and then fine-tune the model on ENCODE cell type-specific enhancer datasets by adopting the transfer learning strategy. Experimental results demonstrate that DeepEnhancer has superior efficiency and effectiveness in classification tasks, and the use of max-pooling and batch normalization is beneficial to higher accuracy. To make our approach more understandable, we propose a strategy to visualize the convolutional kernels as sequence logos and compare them against the JASPAR database using TOMTOM. In summary, DeepEnhancer allows researchers to train highly accurate deep models and will be broadly applicable in computational biology.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822593,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822593,,Bioinformatics;Computer architecture;DNA;Feature extraction;Genomics;Kernel;Machine learning,biological tissues;biology computing;diseases;genetics;genomics;learning (artificial intelligence);neural nets,DeepEnhancer;ENCODE cell type-specific enhancer datasets;FANTOM5 permissive enhancer dataset;JASPAR database;Roadmap;TOMTOM;background genomic sequences;batch normalization;classification performance;classification tasks;computational biology;computational framework;computational identification;convolutional kernels;convolutional neural networks;deep learning model;disease status;gene transcriptional regulation;high-throughput biological experimental techniques;human cell lines;max-pooling;sequence logos;tissues;traditional sequence-based classifiers;transfer learning strategy,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
DeepSplice: Deep classification of novel splice junctions revealed by RNA-seq,Yi Zhang; Xinan Liu; J. N. MacLeod; Jinze Liu,"Department of Computer Science, University of Kentucky, Lexington, 40506, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,330,333,"Alternative splicing (AS) is a regulated process that enables the production of multiple mRNA transcripts from a single multi-exon gene. The availability of large-scale RNA-seq datasets has made it possible to predict splice junctions, as well as splice sites through spliced alignment to the reference genome. This greatly enhances the capability to decipher gene structures and explore the diversity of splicing variants. However, existing ab initio aligners are vulnerable to false positive spliced alignments as a result of sequence errors and random sequence matches. These spurious alignments can lead to a significant set of false positive splice junction predictions, confusing downstream analyses of splice variant detection and abundance estimation. In this work, we illustrate that splice junction sequence characteristics can be ascertained from experimental data with deep learning techniques. We employ deep convolutional neural networks for a novel splice junction classification tool named DeepSplice that (i) outperforms state-of-the-art methods for predicting splice sites, (ii) shows high computational efficiency and (iii) can be applied to self-defined training data by users.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822541,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822541,Alternative Splicing;Deep Learning;RNA-seq;Splice Junctions,Bioinformatics;Biological neural networks;Convolution;Genomics;Junctions;Neurons;Splicing,RNA;biological techniques;biology computing;genomics;learning (artificial intelligence);neural nets,DeepSplice method;RNA-sequence;alternative splicing;deep classification;deep convolutional neural networks;deep learning techniques;false positive spliced alignments;gene structures;large-scale RNA-seq datasets;mRNA transcripts;random sequence matches;sequence errors;single multiexon gene;splice junctions;splice variant detection,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
Towards recognition of protein function based on its structure using deep convolutional networks,A. Tavanaei; A. S. Maida; A. Kaniymattam; R. Loganantharaj,"The Center for Advanced Computer Studies, Bio-inspired AI Lab, University of Louisiana at Lafayette, 70504, USA",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,145,149,"This paper proposes a novel method for protein function recognition using deep learning. Recently, deep convolutional neural networks (DCNNs) demonstrated high performances in many areas of pattern recognition. Protein function is often associated with its tertiary structure denoting the active domain of a protein. This investigation develops a novel DCNN for protein functionality recognition based on its tertiary structure. Two rounds of experiments are performed. The initial experiment on tertiary protein structure alignment shows promising performances (94% accuracy rate) such that it shows the model robustness against rotations, local translations, and scales of the 3D structure. With these results, the main experiments contain five different datasets obtained by similarity measures between pairs of gene ontology terms. The experimental results for protein function recognition on selected datasets show 87.6% and 80.7% maximum and average accuracy rates respectively. The initial success of the DCNN in tertiary protein structure recognition supports further investigations with respect to tertiary protein retrieval and pattern mining on large scale problems.",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822509,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822509,,Feature extraction;Machine learning;Protein engineering;Proteins;Solid modeling;Three-dimensional displays;Visualization,bioinformatics;biological techniques;genetics;learning (artificial intelligence);macromolecules;molecular biophysics;molecular configurations;neural nets;ontologies (artificial intelligence);pattern recognition;proteins,deep convolutional neural networks;deep learning;gene ontology terms;pattern mining;pattern recognition;protein function recognition;tertiary protein retrieval;tertiary protein structure alignment,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
Layerwise feature selection in Stacked Sparse Auto-Encoder for tumor type prediction,V. Singh; N. Baranwal; R. K. Sevakula; N. K. Verma; Y. Cui,"Dept. of Electrical Engineering, Indian Institute of Technology Kanpur, India",2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20170119,2016,,,1542,1548,"Transcriptome data has been proved to be very valuable for clinical applications, such as diagnosis and prognosis of various cancers. In this paper, we present layer-wise feature selection in conjunction with stacked sparse auto-encoders (SSAE), a deep learning strategy for tumor classification with gene expression data. While SSAE learns high-level features from data, performing feature selection in every layer is a heuristic to obtain relevant features at every stage and also to assist in reducing the computation during fine-tuning procedure. The data in the new feature representation is finally used by classifier(s) to perform Tumor detection. The algorithm was tested on 36 datasets from the GEMLeR repository and w.r.t. AUC (Area under ROC curve) performance, it was found to outperform the GEMLeR benchmark results on 35 datasets (tied on the other dataset).",,Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9,10.1109/BIBM.2016.7822750,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822750,cancer classification;deep learning;feature selection;sparse auto-encoder,Benchmark testing;Classification algorithms;Support vector machines,cancer;feature selection;genetics;medical computing;tumours,GEMLeR repository;cancer diagnosis;cancer prognosis;deep learning strategy;gene expression data;layer-wise feature selection;stacked sparse autoencoder;transcriptome data;tumor classification;tumor detection;tumor type prediction,,,,,,,,15-18 Dec. 2016,,IEEE,IEEE Conference Publications
Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis,W. Zhang; R. Li; T. Zeng; Q. Sun; S. Kumar; J. Ye; S. Ji,"Wenlu Zhang is with the Department of Computer Science, Old Dominion University, Norfolk, VA, 23529.(email: wzhang@cs.odu.edu)",IEEE Transactions on Big Data,,2017,PP,99,1,1,"A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila, texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.",,,10.1109/TBDATA.2016.2573280,10.13039/100000002 - National Institutes of Health; 10.13039/100000145 - Division of Information and Intelligent Systems; 10.13039/100000153 - Division of Biological Infrastructure; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480825,Deep learning;bioinformatics;image analysis;multi-task learning;transfer learning,Biological system modeling;Computational modeling;Data models;Feature extraction;Gene expression;Training,,,,,,,,,20160530,,,IEEE,IEEE Early Access Articles
Learning of Generic Vision Features Using Deep CNN,K. N. D.; B. S. P.,"Dept. Of Comput. Sci., Amrita Sch. of Eng., Coimbatore, India",2015 Fifth International Conference on Advances in Computing and Communications (ICACC),20160317,2015,,,54,57,"Eminence of learning algorithm applied for computer vision tasks depends on the features engineered from image. It's premise that different representations can interweave and ensnare most of the elucidative genes that are responsible for variations in images, be it rigid, affine or projective. Hence researches give at most attention in hand-engineering features that capture these variations. But problem is, we need subtle domain knowledge to do that. Thereby making researchers elude epitome of representations. Hence learning algorithms never reach their full potential. In recent times there has been a shift from hand-crafting features to representation learning. The resulting features are not only optimal but also generic as in they can be used as off the shelf features for visual recognition tasks. In this paper we design and experiment with a basic deep convolution neural nets for learning generic vision features with an variant of convolving kernels. They operate by giving importance to individual uncorrelated color channels in a color model by convolving each channel with channel specific kernels. We were able to achieve considerable improvement in performance even when using smaller dataset.",,Electronic:978-1-4673-6994-7; POD:978-1-4673-6995-4,10.1109/ICACC.2015.52,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433775,Convolution Neural networks;Deep learning;Feature learning;Image classification;Supervised learning,Convolution;Feature extraction;Kernel;Linearity;Neural networks;Neurons;Training,computer vision;learning (artificial intelligence);neural nets,basic deep convolution neural nets;channel specific kernels;color channels;computer vision tasks;convolving kernels;deep CNN;elucidative genes;generic vision features;hand-crafting features;hand-engineering features;learning algorithms;representation learning;visual recognition tasks,,,,27,,,,2-4 Sept. 2015,,IEEE,IEEE Conference Publications
Deep Convolutional Neural Networks for Multi-instance Multi-task Learning,T. Zeng; S. Ji,"Sch. of Electr. Eng. & Comput. Sci., Washington State Univ., Pullman, WA, USA",2015 IEEE International Conference on Data Mining,20160107,2015,,,579,588,"Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.",1550-4786;15504786,Electronic:978-1-4673-9504-5; POD:978-1-4673-9505-2,10.1109/ICDM.2015.92,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373362,Deep learning;bioinformatics;multi-instance learning;multi-task learning;transfer learning,Biological system modeling;Brain models;Data models;Gene expression;Standards,image classification;image representation;learning (artificial intelligence);neural nets,MIMT-CNN problem;bag level representations;complex multiple-to-multiple relations;deep convolutional neural networks;deep leaning model;image representation;mouse brain gene expression pattern annotation data;multiinstance multilabel tasks;multiinstance multitask convolutional neural networks;multiinstance multitask learning;single-instance single-label image classification tasks;single-label single-task data sets;transfer learning;ultimate multilabel predictions,,1,,30,,,,14-17 Nov. 2015,,IEEE,IEEE Conference Publications
Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets,M. K. K. Leung; A. Delong; B. Alipanahi; B. J. Frey,"Dept. of Electr. & Comput. Eng., Univ. of Toronto, Toronto, ON, Canada",Proceedings of the IEEE,20151218,2016,104,1,176,197,"In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.",0018-9219;00189219,,10.1109/JPROC.2015.2494198,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347331,Computational biology;deep learning;genetic variants;genome analysis;genome biology;genomic medicine;machine learning;precision medicine,Big data;Bioinformatics;Diseases;Genomics;Machine learning;Medical treatment,DNA;cellular biophysics;diseases;genetics;genomics;learning (artificial intelligence);medical computing;medicine,DNA variations;cell variables;deep learning;disease risks;gene expression;gene splicing;genomic medicine;high-throughput measurement;large-scale data sets;machine learning;nucleic acids;proteins,,7,,231,,,20151204,Jan. 2016,,IEEE,IEEE Journals & Magazines
Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach,M. Liang; Z. Li; T. Chen; J. Zeng,"Department of Mathematical Sciences, Tsinghua University, Beijing, P. R. China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,20150805,2015,12,4,928,937,"Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personali- ed cancer therapy.",1545-5963;15455963,,10.1109/TCBB.2014.2377729,National Basic Research Program of China; 10.13039/501100001809 - National Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977954,Multi-platform cancer data analysis;clinical data;genomic data;identification of cancer subtypes;multimodal deep belief network;restricted Boltzmann machine,Bioinformatics;Cancer;Computational biology;DNA;Data analysis;Data models;Genomics,RNA;belief networks;cancer;data analysis;feature extraction;genetics;genomics;medical computing;molecular biophysics;pattern clustering;tumours;unsupervised learning,DNA methylation;advancing personalized therapy;cancer data analysis;cancer pathogenesis;cancer patient clustering;cancer subtype identification;complex cross-modality correlations;contrastive divergence;cross-modality correlations;disease pathogenesis;gene expression;high-throughput sequencing technologies;input modality;integrative clustering approaches;integrative data analysis;integrative data analysis approach;intramodality correlations;intrinsic statistical properties;joint latent model;key genes;latent feature extraction;machine learning model;miR-29a;miRNA expression;multimodal DBN based data analysis;multimodal DBN model;multimodal deep belief network;multimodal deep learning approach;multiplatform cancer data;multiplatform genomic data;multiple input modalities;ovarian cancer patients;personalized cancer therapy;practical learning algorithm;tumor samples;unsupervised manner,Computational Biology;Gene Expression Profiling;Humans;Kaplan-Meier Estimate;Machine Learning;MicroRNAs;Neoplasms,9,,30,,,20141205,July-Aug. 1 2015,,IEEE,IEEE Journals & Magazines
Multi-level gene/MiRNA feature selection using deep belief nets and active learning,R. Ibrahim; N. A. Yousri; M. A. Ismail; N. M. El-Makky,"Computer and Systems Engineering Department, Alexandria University, Alexandria 21544, Egypt",2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20141106,2014,,,3957,3960,"Selecting the most discriminative genes/miRNAs has been raised as an important task in bioinformatics to enhance disease classifiers and to mitigate the dimensionality curse problem. Original feature selection methods choose genes/miRNAs based on their individual features regardless of how they perform together. Considering group features instead of individual ones provides a better view for selecting the most informative genes/miRNAs. Recently, deep learning has proven its ability in representing the data in multiple levels of abstraction, allowing for better discrimination between different classes. However, the idea of using deep learning for feature selection is not widely used in the bioinformatics field yet. In this paper, a novel multi-level feature selection approach named MLFS is proposed for selecting genes/miRNAs based on expression profiles. The approach is based on both deep and active learning. Moreover, an extension to use the technique for miRNAs is presented by considering the biological relation between miRNAs and genes. Experimental results show that the approach was able to outperform classical feature selection methods in hepatocellular carcinoma (HCC) by 9%, lung cancer by 6% and breast cancer by around 10% in F1-measure. Results also show the enhancement in F1-measure of our approach over recently related work in [1] and [2].",1094-687X;1094687X,Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6,10.1109/EMBC.2014.6944490,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944490,,Accuracy;Bioinformatics;Breast cancer;Gene expression;Lungs;Training,RNA;bioinformatics;cancer;cellular biophysics;data structures;feature selection;genetics;genomics;learning (artificial intelligence);lung;molecular biophysics,F1-measure;active learning;bioinformatics;breast cancer;data representation;deep belief nets;dimensionality curse problem;disease classifier enhancement;hepatocellular carcinoma;lung cancer;multilevel gene-MiRNA feature selection,,3,,16,,,,26-30 Aug. 2014,,IEEE,IEEE Conference Publications
MiRTDL: A Deep Learning Approach for miRNA Target Prediction,S. Cheng; M. Guo; C. Wang; X. Liu; Y. Liu; X. Wu,"School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China",IEEE/ACM Transactions on Computational Biology and Bioinformatics,20161207,2016,13,6,1161,1169,"MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.",1545-5963;15455963,,10.1109/TCBB.2015.2510002,Natural Science Foundation of China; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362158,Constraint relaxation;convolutional neural network;miRNA;target prediction,Diseases;Machine learning;Matched filters;Neural networks;Prediction algorithms;RNA;Support vector machines,RNA;bioinformatics;biological techniques;diseases;genetics;learning (artificial intelligence);molecular biophysics;neural nets,CNN;MiRTDL;balanced training dataset;convolutional neural network;deep learning approach;diseases;essential information;genes;miRNA regulatory mechanism;miRNA target pairs;miRNA target prediction algorithm;miRTDL;microRNA;real targets,,1,,,,,20151222,November 1 2016,,IEEE,IEEE Journals & Magazines
Gold classification of COPDGene cohort based on deep learning,J. Ying; J. Dutta; N. Guo; L. Xia; A. Sitek; Q. Li; Q. Li,"Nuclear Medicine and Molecular Imaging Radiology Department, Massachusetts General Hospital, Boston, MA, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",20160519,2016,,,2474,2478,"This study aims to employ deep learning for the development of an automatic classifier for the severity of chronic obstructive pulmonary disease (COPD) in patients. A three-layer deep belief network (DBN) with two hidden layers and one visible layer was employed to generate a model for classification, and the model's robustness against exacerbation was analyzed. Subjects from the COPDGene cohort were staged using the GOLD 2011 guidelines. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 97.2% by using a 10-fold cross validation experiment. The most sensitive features as revealed by the DBN weights were consistent with the clinical consensus as per previous studies and clinical diagnosis rules. In summary, we demonstrate that the DBN is a competitive tool for exacerbation risk assessment for patients suffering from, COPD.",,Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3,10.1109/ICASSP.2016.7472122,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472122,Chronic Obstructive Pulmonary Disease (COPD);Deep Belief Networks (DBNs);Global Initiative for Chronic Obstructive Lung Disease (GOLD);classification;deep learning,Diseases;Feature extraction;Gold;Lungs;Machine learning;Medical diagnostic imaging;Training,belief networks;biology computing;diseases;feature selection;learning (artificial intelligence);optimisation;pattern classification,COPD gene cohort;DBN weights;automatic gold classification;chronic obstructive pulmonary disease;deep learning;exacerbation risk assessment;feature selection;parameter optimization;three-layer deep belief network,,,,19,,,,20-25 March 2016,,IEEE,IEEE Conference Publications
Big data analytics in genomics: The point on Deep Learning solutions,F. Celesti; A. Celesti; L. Carnevale; A. Galletta; S. Campo; A. Romano; P. Bramanti; M. Villari,"Department of Biomedical and Dental Sciences and Morphofunctional Images, University of Messina, Italy",2017 IEEE Symposium on Computers and Communications (ISCC),20170904,2017,,,306,309,"Nowadays, Next Generation Sequeencing (NGS) is a catch-all term used to describe different modern DNA sequencing applications that produce big genomics data that can be analysed in a faster fashion than in the past. For this reason, NGS requires more and more sophisticated algorithms and high-performance parallel processing systems able to analyse and extract knowledge from a huge amount of genomics and molecular data. In this context, researchers are beginning to look at emerging deep learning algorithms able to perform efficient big data analytics. In this paper, we analyse and classify the major current deep learning solutions that allow biotechnology researchers to perform big genomics data analytics. Moreover, by means of a taxonomic analysis, we provide a clear picture of the current state of the art also discussing future challenges.",,Electronic:978-1-5386-1629-1; POD:978-1-5386-1630-7; USB:978-1-5386-1628-4,10.1109/ISCC.2017.8024547,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024547,Cloud computing;DNA sequencing;Genomics;NGS;big data;biotechnology;deep learning,Bioinformatics;DNA;Genomics;Machine learning;Proteins;Sequential analysis,,,,,,,,,,3-6 July 2017,,IEEE,IEEE Conference Publications
Using convolutional neural networks to explore the microbiome,D. Reiman; A. Metwally; Y. Dai,"Bioinformatics Program of the Department of Bioengineering, University of Illinois at Chicago, 60612, USA",2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),20170914,2017,,,4269,4272,"The microbiome has been shown to have an impact on the development of various diseases in the host. Being able to make an accurate prediction of the phenotype of a genomic sample based on its microbial taxonomic abundance profile is an important problem for personalized medicine. In this paper, we examine the potential of using a deep learning framework, a convolutional neural network (CNN), for such a prediction. To facilitate the CNN learning, we explore the structure of abundance profiles by creating the phylogenetic tree and by designing a scheme to embed the tree to a matrix that retains the spatial relationship of nodes in the tree and their quantitative characteristics. The proposed CNN framework is highly accurate, achieving a 99.47% of accuracy based on the evaluation on a dataset 1967 samples of three phenotypes. Our result demonstrated the feasibility and promising aspect of CNN in the classification of sample phenotype.",,Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8,10.1109/EMBC.2017.8037799,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037799,,,,,,,,,,,,11-15 July 2017,,IEEE,IEEE Conference Publications
Probabilistic Graphical Models and Deep Belief Networks for Prognosis of Breast Cancer,M. Khademi; N. S. Nedialkov,"Dept. of Comput. & Software, McMaster Univ., Hamilton, ON, Canada",2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),20160303,2015,,,727,732,"We propose a probabilistic graphical model (PGM) for prognosis and diagnosis of breast cancer. PGMs are suitable for building predictive models in medical applications, as they are powerful tools for making decisions under uncertainty from big data with missing attributes and noisy evidence. Previous work relied mostly on clinical data to create a predictive model. Moreover, practical knowledge of an expert was needed to build the structure of a model, which may not be accurate. In our opinion, since cancer is basically a genetic disease, the integration of microarray and clinical data can improve the accuracy of a predictive model. However, since microarray data is high-dimensional, including genomic variables may lead to poor results for structure and parameter learning due to the curse of dimensionality and small sample size problems. We address these problems by applying manifold learning and a deep belief network (DBN) to microarray data. First, we construct a PGM and a DBN using clinical and microarray data, and extract the structure of the clinical model automatically by applying a structure learning algorithm to the clinical data. Then, we integrate these two models using softmax nodes. Extensive experiments using real-world databases, such as METABRIC and NKI, show promising results in comparison to Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN) classifiers, for classifying tumors and predicting events like recurrence and metastasis.",,Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7,10.1109/ICMLA.2015.196,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424407,breast cancer;deep belief networks;microarray data;probabilistic graphical models,Approximation algorithms;Breast cancer;Manifolds;Probabilistic logic;Prognostics and health management;Training,bioinformatics;cancer;genomics;graph theory;learning (artificial intelligence);pattern classification;probability;tumours,DBN;METABRIC database;NKI database;PGM;SVM classifier;breast cancer diagnosis;breast cancer prognosis;clinical data integration;curse-of-dimensionality;deep-belief networks;genetic disease;genomic variables;high-dimensional microarray data;k-NN classifier;k-nearest neighbors classifier;manifold learning;medical applications;metastasis event prediction;microarray data integration;parameter learning;predictive model;predictive model accuracy improvement;probabilistic graphical model;recurrence event prediction;softmax nodes;structure learning;structure learning algorithm;support vector machine classifier;tumors,,,,20,,,,9-11 Dec. 2015,,IEEE,IEEE Conference Publications
The effective diagnosis of schizophrenia by using multi-layer RBMs deep networks,Chen Qiao; D. D. Lin; Shao-Long Cao; Yu-Ping Wang,"School of Mathematics and Statistics, Xi'an Jiaotong University, 710049, China",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217,2015,,,603,606,"Schizophrenia is one of the most prevalent mental diseases, and is considered to be caused by the interplay of a number of genetic factors. In this paper, by constructing a multilayer restricted Boltzmann machines (RBMs) deep network, we use the genomic data (i.e., SNP data) for unsupervised feature learning and disease diagnosis of schizophrenia. In order to obtain some more accurate diagnosis results by RBMs, firstly, we transform the SNP data into binary sequences, and then by training the multi-layer RBMs deep network on unlabeled data, the multi-level abstract features of the genomic data are obtained and stored in the network. Finally, by adding a linear classifier to the top of the multi-layer RBMs deep network, the classification results on the testing data are gained. The results show that the average performance of this method is better than that of other methods, e.g., SVM (including linear SVM as well as SVM with multilayer perceptron kernel), sparse representations based classifier and k-nearest neighbors method. It is indicated that the multi-layer RBMs deep network can extract deep hierarchical representations of the genomic data, and then promises a more comprehensive approach for the mental disease diagnosis.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359751,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359751,,Bioinformatics;Genomics;Quality control;Support vector machines,Boltzmann machines;binary sequences;diseases;genomics;patient diagnosis;support vector machines;unsupervised learning,SNP data;SVM;binary sequence;genetic factor;genomic data;k-nearest neighbor method;linear classifier;mental disease diagnosis;multi-layer restricted Boltzmann machines deep network;multilayer RBM deep network;schizophrenia disease diagnosis;sparse representation-based classifier;unsupervised feature learning,,,,16,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
Ensemble of deep long short term memory networks for labelling origin of replication sequences,U. Singh; S. Chauhan; A. Krishnamachari; L. Vig,"School of Computational and Integrative Sciences, Jawaharlal Nehru University, New Delhi, India",2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),20151207,2015,,,1,7,"Advancement in sequence data generation technologies are churning out voluminous omics data and posing a massive challenge to annotate the biological functional features. Sequence data from the well studied model organism Saccharomyces cerevisiae has been commonly used to test and validate in silico prediction methods. DNA replication is a critical step in the cellular process and the sequence location where this process originates in the genomic landscape is generally referred as origin of replication. In this paper we investigate the application bidirectional Long Short Term (LSTM) Networks to predict origin of replication sequences. Long Short Term Memory (LSTM) networks have recently been shown to yield state of the art performance in speech recognition, and music generation. These networks are capable of learning long term patterns via the use of multiplication gates. This paper utilizes Deep bidirectional LSTM for prediction of origin of replication sequences belonging to the organism Saccharomyces cerevisiae. Results demonstrate that LSTMs outperform the commonly used machine learning classifiers such as Support Vector Machine (SVM), Random Forest (RF), Artificial Neural Network (ANN), and Hidden Markov Model (HMM). An important additional advantage of LSTMs is that they work directly on the sequences and obviate the need for hand coded features.",,Electronic:978-1-4673-8273-1; POD:978-1-4673-8274-8,10.1109/DSAA.2015.7344871,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344871,,Bioinformatics;Biological cells;Computer architecture;DNA;Genomics;Hidden Markov models;Logic gates,DNA;biology computing;genomics;recurrent neural nets,DNA replication;Saccharomyces cerevisiae;bidirectional long short term memory;biological functional feature;cellular process;deep bidirectional LSTM;deep long short term memory network;genomic landscape;multiplication gates;replication sequences;sequence data generation technology;voluminous omics data,,,,24,,,,19-21 Oct. 2015,,IEEE,IEEE Conference Publications
Cells classification with deep learning,A. Sezer; U. Çekmez,"Bilgisayar M&#x00FC;hendisligi B&#x00F6;l&#x00FC;m&#x00FC;, Y&#x0131;ld&#x0131;z Teknik &#x00DC;niversitesi, 34220 Istanbul, T&#x00FC;rkiye",2017 25th Signal Processing and Communications Applications Conference (SIU),20170629,2017,,,1,4,"Proteomic analysis is a rapidly developing research field that has recently been used in the diagnosis and treatment of various diseases by analyzing the structure and functions of protein patterns in the cell. Numerous computer based decision support mechanisms implemented in this context have mostly used special image processing techniques until now. Recently, high performance self-learning deep learning methods have taken place in the classification studies over the conventional methods examining the structural features of the patterns, shapes and the texture properties in the images. In this study, different intracellular patterns of HeLa cells taken by the microscope used in the testing of pattern analysis and the output is compared by classifying these patterns by using both deep learning methods and bag-of-features method. As a result of the experiments, it is seen that the success of the proposed deep learning model has a very high performance in classifying compared to the existing models and bag-of-features technique.",,Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3,10.1109/SIU.2017.7960647,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960647,classification;convolutional deep neural networks;deep learning;genome;protein;proteomic analysis,Bioinformatics;Dogs;Genomics;Machine learning;Neural networks;Pattern recognition;Proteins,cellular biophysics;diseases;image classification;image texture;learning (artificial intelligence);medical image processing;patient diagnosis;patient treatment;proteins,HeLa cell intracellular patterns;cell classification;computer-based decision support mechanisms;high-performance self-learning deep learning;image processing;pattern structural features;proteomic analysis;texture properties,,,,,,,,15-18 May 2017,,IEEE,IEEE Conference Publications
Detecting Cardiovascular Disease from Mammograms With Deep Learning,J. Wang; H. Ding; F. A. Bidgoli; B. Zhou; C. Iribarren; S. Molloi; P. Baldi,"Department of Computer Science, Institute for Genomics and Bioinformatics, University of California at Irvine, Irvine, CA, USA",IEEE Transactions on Medical Imaging,20170501,2017,36,5,1172,1181,"Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected in mammograms, can be useful risk markers associated with the disease. We investigate the feasibility of automated and accurate detection of BACs in mammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-response receiver operating characteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection in mammograms to help identify and assess patients with cardiovascular risks.",0278-0062;02780062,,10.1109/TMI.2017.2655486,"10.13039/100000001 - National Science Foundation; 10.13039/100000050 - National Heart, Lung, and Blood Institute (Bethesda, MD) to CI and SM; 10.13039/100006785 - Google Faculty Research Award under Grant; ",http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827150,Breast arterial calcification (BAC);coronary artery disease;deep learning;mammography,Arteries;Breast;Calcium;Diseases;Machine learning;Mammography;Neural networks,cardiovascular system;diseases;learning (artificial intelligence);mammography;medical image processing;neural nets,12-layer convolutional neural network;BAC detection;FROC analysis;breast arterial calcifications;calcium mass quantification analysis;cardiovascular disease detection;coronary artery disease;death;deep learning;digital mammograms;patch based procedure,,,,,,,20170119,May 2017,,IEEE,IEEE Journals & Magazines
Boosting compound-protein interaction prediction by deep learning,Kai Tian; Mingyu Shao; Shuigeng Zhou; Jihong Guan,"Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, 200433, China",2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),20151217,2015,,,29,34,"The identification of interactions between compounds and proteins plays an important role in network pharmacology and drug discovery. However, experimentally identifying compound-protein interactions (CPIs) is generally expensive and time-consuming, computational approaches are thus introduced. Among these, machine-learning based methods have achieved a considerable success. However, due to the nonlinear and imbalanced nature of biological data, many machine learning approaches have their own limitations. Recently, deep learning techniques show advantages over many state-of-the-art machine learning methods in many applications. In this study, we aim at improving the performance of CPI prediction based on deep learning, and propose a method called DL-CPI (the abbreviation of Deep Learning for Compound-Protein Interactions prediction), which employs deep neural network (DNN) to effectively learn the representations of compound-protein pairs. Extensive experiments show that DL-CPI can learn useful features of compound-protein pairs by a layerwise abstraction, and thus achieves better prediction performance than existing methods on both balanced and imbalanced datasets.",,Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1,10.1109/BIBM.2015.7359651,,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359651,Compound-protein interaction;Deep learning;Deep neural network (DNN),Bioinformatics;Genomics;Radio frequency;Training,biology computing;drugs;health care;learning (artificial intelligence);neural nets;proteins,DL-CPI;compound-protein interaction prediction;deep learning techniques;deep neural network;drug discovery;layerwise abstraction;machine-learning;network pharmacology,,1,,22,,,,9-12 Nov. 2015,,IEEE,IEEE Conference Publications
Multi-Layer and Recursive Neural Networks for Metagenomic Classification,G. Ditzler; R. Polikar; G. Rosen,"Department of Electrical & Computer Engineering, Drexel University, Philadelphia",IEEE Transactions on NanoBioscience,20150828,2015,14,6,608,616,"Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.",1536-1241;15361241,,10.1109/TNB.2015.2461219,Department of Energy; 10.13039/100000001 - National Science Foundation; ,http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432,Comparative metagenomics;metagenomics;microbiome;neural networks,Feature extraction;Machine learning;Nanobioscience;Neural networks;Organisms;Training;Vegetation,DNA;genomics;image classification;image representation;learning (artificial intelligence);medical image processing;molecular biophysics;multilayer perceptrons;natural language processing;neurophysiology,classification accuracy;data structure;deep belief network;deep learning methods;generalization performance;hierarchical representations;image classification;language modeling;machine learning;machine learning community;metagenomic classification;metagenomic data analysis;metagenomic literature;multilayer perceptron;multilayer-recursive neural networks;natural language processing;neural networks;nonlinear feature representations;prediction algorithm;predictive metagenomic analysis;unsupervised fashion,Algorithms;Metagenomics;Microbiota;Neural Networks (Computer),4,,44,,,20150824,Sept. 2015,,IEEE,IEEE Journals & Magazines
