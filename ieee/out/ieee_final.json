[{"Title":"Multi-Scale Rotation-Invariant Convolutional Neural Networks for Lung Texture Classification","Description":"Q. Wang,  Y. Zheng,  g. yang,  W. Jin,  X. Chen,  y. yin","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"We propose a new Multi-scale Rotation-invariant Convolutional Neural Network (MRCNN) model for classifying various lung tissue types on high-resolution computed tomography (HRCT). MRCNN employs Gabor-local binary pattern (Gabor-LBP) which introduces a good property in image analysis - invariance to image scales and rotations. In addition, we offer an approach to deal with the problems caused by imbalanced number of samples between different classes in most of the existing works, accomplished by changing the overlapping size between the adjacent patches. Experimental results on a public Interstitial Lung Disease (ILD) database show a superior performance of the proposed method to state-of-the-art.","email":["shdyn2000@mail.sdu.edu.cn","jie@gmail.com","gpyang@sdu.edu.cn","dong@sdufe.edu.cn","xjchen@suda.edu.cn","ylyin@sdu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7883849","source":"ieee","year":2017,"key":"7b8ec9d1-e85f-4e80-bc3d-ed9eeb2b225f","use":1,"doi":"10.1109\/JBHI.2017.2685586"},{"Title":"Adrenal lesions detection on low-contrast CT images using fully convolutional networks with multi-scale integration","Description":"L. Bi,  J. Kim,  T. Su,  M. Fulham,  D. Feng,  G. Ning","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Adrenal lesions include a wide variety of benign and malignant neoplasms of the adrenal gland, and are seen in up to 5% of computed tomography (CT) examinations of the abdomen. Better identification of these lesions is important for effective management and patient prognosis. Detection on low-contrast CT images, however, even for experienced physicians can be difficult and error-prone, because the lesions are often problematic to be separated from the normal surrounding structures. Existing lesion detection techniques have problems in identifying and differentiating low-contrast tumors, which is related to their use of low-level features rather than high level of semantics. Hence we propose an automated approach using fully convolutional networks (FCNs) and multi-scale integration to detect adrenal lesion on low-contrast CT scans. The architecture of FCNs includes deep, coarse, semantic information and shallow, fine, appearance information in a hierarchical manner and it enables the encoding of image-wide location and semantics, which are desirable characteristics for adrenal lesion detection. We also propose a multi-scale integration with a superpixel based random walk (MI-SRW) approach to refine the lesion boundaries on different scales. The MI-SRW technique enables us to constrain the spatial and appearance consistency and then use complementary information provided on different scales to detect adrenal lesions of various sizes and characteristics. We used 38 adrenal lesions detected on low-contrast CT and compared our approach to existing `state-of-the-art' methods and found that our approach had superior detection performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950660","source":"ieee","year":2017,"key":"460c590e-50ee-42e0-86bd-ff4ae88abbb9","use":1,"doi":"10.1109\/ISBI.2017.7950660"},{"Title":"Anatomy-specific classification of medical images using deep convolutional nets","Description":"H. R. Roth,  C. T. Lee,  H. C. Shin,  A. Seff,  L. Kim,  J. Yao,  L. Lu,  R. M. Summers","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. \u201cDeep learning\u201d methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163826","source":"ieee","year":2015,"key":"4548e43a-cfc1-4498-ab07-e8404aff1104","use":1,"doi":"10.1109\/ISBI.2015.7163826"},{"Title":"Deep Convolutional Neural Network for Inverse Problems in Imaging","Description":"K. H. Jin,  M. T. McCann,  E. Froustey,  M. Unser","ShortDetails":"IEEE Transactions on Image Processing. 2017","abstract":"In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 \u00d7 512 image on the GPU.","email":["holger.roth@nih.govorrms"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7949028","source":"ieee","year":2017,"key":"f5ef134f-bd46-49bf-a5bc-d0d95325c0b0","use":1,"doi":"10.1109\/TIP.2017.2713099"},{"Title":"Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?","Description":"N. Tajbakhsh,  J. Y. Shin,  S. R. Gurudu,  R. T. Hurst,  C. B. Kendall,  M. B. Gotway,  J. Liang","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.","email":["tajbakhsh@asu.edu","sejong@asu.edu","gurudu.suryakanth@mayo.edu","hurst.r@mayo.edu","christopher@mayo.edu","gotway.michael@mayo.edu","jianming.liang@asu.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7426826","source":"ieee","year":2016,"key":"1dca0532-e410-4f5f-b0a7-98e7f45c4ff9","use":1,"doi":"10.1109\/TMI.2016.2535302"},{"Title":"Size and Texture-Based Classification of Lung Tumors with 3D CNNs","Description":"Z. Luo,  M. A. Brubaker,  M. Brudno","ShortDetails":"2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017","abstract":"In this paper, we explore the use of current deep learning methods in the field of computer-aided diagnosis (CAD). Specifically we propose the use of 3D convolutional neural nets (CNN) in classifying lung nodules based off of their appearance in CT scans. We explore the choices of network architectures, learning parameters and problem formulations. Comparing these results to other methods we show that the proposed method has close to perfect performance on the publicly available LIDC dataset, achieving an AUC of 0:9685 and a false positive rate of 0:46% with a true positive rate of 90% where the ground truth is the expert opinion of a radiologist.","email":["zhihao.luo@columbia.edu","mab@eecs.yorku.ca","brudno@cs.toronto.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7926678","source":"ieee","year":2017,"key":"ed565085-8423-4579-8732-bed6fb2a0389","use":1,"doi":"10.1109\/WACV.2017.95"},{"Title":"Lung nodule detection in CT images using deep convolutional neural networks","Description":"R. Golan,  C. Jacob,  J. Denzinger","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"Early detection of lung nodules in thoracic Computed Tomography (CT) scans is of great importance for the successful diagnosis and treatment of lung cancer. Due to improvements in screening technologies, and an increased demand for their use, radiologists are required to analyze an ever increasing amount of image data, which can affect the quality of their diagnoses. Computer-Aided Detection (CADe) systems are designed to assist radiologists in this endeavor. Here, we present a CADe system for the detection of lung nodules in thoracic CT images. Our system is based on (1) the publicly available Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) database, which contains 1018 thoracic CT scans with nodules of different shape and size, and (2) a deep Convolutional Neural Network (CNN), which is trained, using the back-propagation algorithm, to extract valuable volumetric features from the input data and detect lung nodules in sub-volumes of CT images. Considering only those test nodules that have been annotated by four radiologists, our CADe system achieves a sensitivity (true positive rate) of 78.9% with 20 false positives (FPs) per scan, or a sensitivity of 71.2% with 10 FPs per scan. This is achieved without using any segmentation or additional FP reduction procedures, both of which are commonly used in other CADe systems. Furthermore, our CADe system is validated on a larger number of lung nodules compared to other studies, which increases the variation in their appearance, and therefore, makes their detection by a CADe system more challenging.","email":["grotem@ucalgary.ca","cjacob@ucalgary.ca","denzinge@cpsc.ucalgary.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727205","source":"ieee","year":2016,"key":"94ba9ab3-dab6-400f-995d-7fdd12a1fde8","use":1,"doi":"10.1109\/IJCNN.2016.7727205"},{"Title":"Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN)","Description":"H. Chen,  Y. Zhang,  M. K. Kalra,  F. Lin,  Y. Chen,  P. Liao,  J. Zhou,  G. Wang","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.","email":["pubs-permissions@ieee.org.","linfeng@scu.edu.cn","huchen@scu.edu.cn","zhoujl@scu.edu.cn","yzhang@scu.edu.cn","mkalra@mgh.harvard.edu","chenyang.list@seu.edu.cn","universe6527@163.com","wangg6@rpi.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7947200","source":"ieee","year":2017,"key":"a4409c44-eee3-49aa-bdf3-7a59c61d100b","use":1,"doi":"10.1109\/TMI.2017.2715284"},{"Title":"Automatic segmentation of the left ventricle in cardiac CT angiography using convolutional neural networks","Description":"M. Zreik,  T. Leiner,  B. D. de Vos,  R. W. van Hamersvelt,  M. A. Viergever,  I. I\u0161gum","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Accurate delineation of the left ventricle (LV) is an important step in evaluation of cardiac function. In this paper, we present an automatic method for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation is performed in two stages. First, a bounding box around the LV is detected using a combination of three convolutional neural networks (CNNs). Subsequently, to obtain the segmentation of the LV, voxel classification is performed within the defined bounding box using a CNN. The study included CCTA scans of sixty patients, fifty scans were used to train the CNNs for the LV localization, five scans were used to train LV segmentation and the remaining five scans were used for testing the method. Automatic segmentation resulted in the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1 mm. The results demonstrate that automatic segmentation of the LV in CCTA scans using voxel classification with convolutional neural networks is feasible.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493206","source":"ieee","year":2016,"key":"ddf8c60c-0683-417d-a164-172f9e85537c","use":1,"doi":"10.1109\/ISBI.2016.7493206"},{"Title":"Classification of radiology reports using neural attention models","Description":"B. Shin,  F. H. Chokshi,  T. Lee,  J. D. Choi","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure \u201cblack-box\u201d models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.","email":["bonggun.shin@emory.edu","fchoksh@emory.edu","timlee1028@gmail.com","jinho.choi@emory.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7966408","source":"ieee","year":2017,"key":"dfff45b8-c2e9-46ca-8a6a-804375deb727","use":1,"doi":"10.1109\/IJCNN.2017.7966408"},{"Title":"TumorNet: Lung nodule characterization using multi-view Convolutional Neural Network with Gaussian Process","Description":"S. Hussein,  R. Gillies,  K. Cao,  Q. Song,  U. Bagci","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Characterization of lung nodules as benign or malignant is one of the most important tasks in lung cancer diagnosis, staging and treatment planning. While the variation in the appearance of the nodules remains large, there is a need for a fast and robust computer aided system. In this work, we propose an end-to-end trainable multi-view deep Convolutional Neural Network (CNN) for nodule characterization. First, we use median intensity projection to obtain a 2D patch corresponding to each dimension. The three images are then concatenated to form a tensor, where the images serve as different channels of the input image. In order to increase the number of training samples, we perform data augmentation by scaling, rotating and adding noise to the input image. The trained network is used to extract features from the input image followed by a Gaussian Process (GP) regression to obtain the malignancy score. We also empirically establish the significance of different high level nodule attributes such as calcification, sphericity and others for malignancy determination. These attributes are found to be complementary to the deep multi-view CNN features and a significant improvement over other methods is obtained.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950686","source":"ieee","year":2017,"key":"f87b8b0e-3295-4fef-99f2-e800e8cf9b45","use":1,"doi":"10.1109\/ISBI.2017.7950686"},{"Title":"Ischemic stroke identification based on EEG and EOG using ID convolutional neural network and batch normalization","Description":"E. P. Giri,  M. I. Fanany,  A. M. Arymurthy,  S. K. Wijaya","ShortDetails":"2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS). 2016","abstract":"In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of ID Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72). The feature used in our study were only 24 handcrafted feature with simple feature extraction process.","email":["4epgthebest@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7872780","source":"ieee","year":2016,"key":"f89ee53a-6639-42a7-95ea-14727194ef3b","use":1,"doi":"10.1109\/ICACSIS.2016.7872780"},{"Title":"Lung nodule detection in CT using 3D convolutional neural networks","Description":"X. Huang,  J. Shan,  V. Vaidya","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"We propose a new computer-aided detection system that uses 3D convolutional neural networks (CNN) for detecting lung nodules in low dose computed tomography. The system leverages both a priori knowledge about lung nodules and confounding anatomical structures and data-driven machine-learned features and classifier. Specifically, we generate nodule candidates using a local geometric-model-based filter and further reduce the structure variability by estimating the local orientation. The nodule candidates in the form of 3D cubes are fed into a deep 3D convolutional neural network that is trained to differentiate nodule and non-nodule inputs. We use data augmentation techniques to generate a large number of training examples and apply regularization to avoid overfitting. On a set of 99 CT scans, the proposed system achieved state-of-the-art performance and significantly outperformed a similar hybrid system that uses conventional shallow learning. The experimental results showed benefits of using a priori models to reduce the problem space for data-driven machine learning of complex deep neural networks. The results also showed the advantages of 3D CNN over 2D CNN in volumetric medical image analysis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950542","source":"ieee","year":2017,"key":"8066a1b4-bdee-41f1-b2cc-27d9db138b45","use":1,"doi":"10.1109\/ISBI.2017.7950542"},{"Title":"Combining deep neural network and traditional image features to improve survival prediction accuracy for lung cancer patients from diagnostic CT","Description":"R. Paul,  S. H. Hawkins,  L. O. Hall,  D. B. Goldgof,  R. J. Gillies","ShortDetails":"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2016","abstract":"Lung cancer is caused by abnormal and uncontrolled growth of cells in the lungs and the mortality rate of lung cancer is the highest among all types of cancer. It can be identified and treated with the help of computed tomography (CT) images. For an automated classifier, identifying good features from an image is a key concern. Deep feature extraction using pre-trained convolutional neural networks has been successful for some image domains recently. In our study, we apply a pre-trained convolutional neural network (CNN) to extract deep features from lung cancer CT images and then train classifiers to predict short and long term survivors. The best accuracy of 77.5% was with a cropping approach using a decision tree classifier in a leave one out cross validation with ten features chosen using symmetric uncertainty feature ranking. We mixed extracted deep neural network features along with quantitative (traditional image) features and obtained the best accuracy of 82.5% with a nearest neighbor classifier in a leave one out cross validation using the symmetric uncertainty feature ranking algorithm.","email":["arahulp@mail.usf.edu","shhawkins@mail.usf.edu","lohall@mail.usf.edu","goldgof@mail.usf.edu","brobert.gillies@moffitt.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7844626","source":"ieee","year":2016,"key":"5cd82f94-9ccb-4643-b4c9-19d80f385cf0","use":1,"doi":"10.1109\/SMC.2016.7844626"},{"Title":"Application of big data analytics for automated estimation of CT image quality","Description":"M. D. Naeemi,  J. Ren,  N. Hollcroft,  A. M. Alessio,  S. Roychowdhury","ShortDetails":"2016 IEEE International Conference on Big Data (Big Data). 2016","abstract":"With the increasing applications of Big Data analytics in medical image processing systems, there has been a growing need for quantitative medical image quality assessment techniques. Specifically for computed tomography (CT) images, quantitative image assessment can allow for benchmarking image processing methods and optimization of image acquisition parameters. In this work, large volumes of CT images from phantoms and patients are analyzed using 3 data models that vary in their implementation time complexities. The goal here is to identify the optimal method that scales across data set variabilities for predictive modeling of CT image quality (CTIQ). The first two models rely on spatial segmentation of regions-of-interest (ROIs) and estimate CTIQs in terms of segmented pixel variabilities. The third, convolutional neural network (CNN) model relies on error back-propagation from the training set of images to learn the regions indicative of CTIQ. We observe that for 70\/30 data split, the average multi-class classification accuracies for CTIQ prediction using the 3 data models range from 73.6-100% and 50-100% for the phantom and patient CT images, respectively. Using variance of pixels within the segmented ROIs as a CTIQ classification parameter, the spatial segmentation data models are found to be more generalizable that the CNN model. However, the CNN model is found to be more suitable for CT image texture classification in the absence of structural variabilities. Our analysis demonstrates that spatial ROI segmentation data models are consistent CTIQ estimators while the CNN models are consistent identifiers of structural similarities for CT image data sets.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7841003","source":"ieee","year":2016,"key":"04c9a7f2-2865-41a1-bb87-f031b0cb3de8","use":1,"doi":"10.1109\/BigData.2016.7841003"},{"Title":"Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection","Description":"Q. Dou,  H. Chen,  L. Yu,  J. Qin,  P. A. Heng","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2017","abstract":"Objective: False positive reduction is one of the most crucial components in an automated pulmonary nodule detection system, which plays an important role in lung cancer diagnosis and early treatment. The objective of this paper is to effectively address the challenges in this task and therefore to accurately discriminate the true nodules from a large number of candidates. Methods: We propose a novel method employing three-dimensional (3-D) convolutional neural networks (CNNs) for false positive reduction in automated pulmonary nodule detection from volumetric computed tomography (CT) scans. Compared with its 2-D counterparts, the 3-D CNNs can encode richer spatial information and extract more representative features via their hierarchical architecture trained with 3-D samples. More importantly, we further propose a simple yet effective strategy to encode multilevel contextual information to meet the challenges coming with the large variations and hard mimics of pulmonary nodules. Results: The proposed framework has been extensively validated in the LUNA16 challenge held in conjunction with ISBI 2016, where we achieved the highest competition performance metric (CPM) score in the false positive reduction track. Conclusion: Experimental results demonstrated the importance and effectiveness of integrating multilevel contextual information into 3-D CNN framework for automated pulmonary nodule detection in volumetric CT data. Significance: While our method is tailored for pulmonary nodule detection, the proposed framework is general and can be easily extended to many other 3-D object detection tasks from volumetric medical images, where the targeting objects have large variations and are accompanied by a number of hard mimics.","email":["qdou@cse.cuhk.edu.hk","pubs-permissions@ieee.org.","64@5x5x364","1x1x164@5x5x364","5x5x115064@5x5x364","2x2x164@5x5x364","5x5x325064@5x5x364","2x2x264@5x5x364"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7576695","source":"ieee","year":2017,"key":"cdd4ecf7-5dac-47c7-94be-a5c67584d7e8","use":1,"doi":"10.1109\/TBME.2016.2613502"},{"Title":"Low-dose CT denoising with convolutional neural network","Description":"H. Chen,  Y. Zhang,  W. Zhang,  P. Liao,  K. Li,  J. Zhou,  G. Wang","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"To reduce the potential radiation risk, low-dose CT has attracted much attention. However, simply lowering the radiation dose will lead to significant deterioration of the image quality. In this paper, we propose a noise reduction method for low-dose CT via deep neural network without accessing original projection data. A deep convolutional neural network is trained to transform low-dose CT images towards normal-dose CT images, patch by patch. Visual and quantitative evaluation demonstrates a competing performance of the proposed method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950488","source":"ieee","year":2017,"key":"c4957561-c33e-4669-90b0-0c58598fe833","use":1,"doi":"10.1109\/ISBI.2017.7950488"},{"Title":"ConvNet-Based Localization of Anatomical Structures in 3-D Medical Images","Description":"B. D. de Vos,  J. M. Wolterink,  P. A. de Jong,  T. Leiner,  M. A. Viergever,  I. I\u0161gum","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7862905","source":"ieee","year":2017,"key":"013bd3ad-9d3d-44f9-9be2-1a8f6b71a5ba","use":1,"doi":"10.1109\/TMI.2017.2673121"},{"Title":"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning","Description":"H. C. Shin,  H. R. Roth,  M. Gao,  L. Lu,  Z. Xu,  I. Nogues,  J. Yao,  D. Mollura,  R. M. Summers","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7404017","source":"ieee","year":2016,"key":"bf684dd0-065a-4866-98e9-aeb9bbb420d0","use":1,"doi":"10.1109\/TMI.2016.2528162"},{"Title":"Generative Adversarial Networks for Noise Reduction in Low-Dose CT","Description":"J. M. Wolterink,  T. Leiner,  M. A. Viergever,  I. Isgum","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Noise is inherent to low-dose CT acquisition. We propose to train a convolutional neural network (CNN) jointly with an adversarial CNN to estimate routine-dose CT images from low-dose CT images and hence reduce noise. A generator CNN was trained to transform low-dose CT images into routine-dose CT images using voxel-wise loss minimization. An adversarial discriminator CNN was simultaneously trained to distinguish the output of the generator from routinedose CT images. The performance of this discriminator was used as an adversarial loss for the generator. Experiments were performed using CT images of an anthropomorphic phantom containing calcium inserts, as well as patient non-contrast-enhanced cardiac CT images. The phantom and patients were scanned at 20% and 100% routine clinical dose. Three training strategies were compared: the first used only voxel-wise loss, the second combined voxel-wise loss and adversarial loss, and the third used only adversarial loss. The results showed that training with only voxel-wise loss resulted in the highest peak signal-to-noise ratio with respect to reference routine-dose images. However, the CNNs trained with adversarial loss captured image statistics of routine-dose images better. Noise reduction improved quantification of low-density calcified inserts in phantom CT images and allowed coronary calcium scoring in low-dose patient CT images with high noise levels. Testing took less than 10 seconds per CT volume. CNN-based low-dose CT noise reduction in the image domain is feasible. Training with an adversarial network improves the CNN\u2019s ability to generate images with an appearance similar to that of reference routine-dose CT images.","email":["pubs-permissions@ieee.org.","j.m.wolterink@umcutrecht.nl."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7934380","source":"ieee","year":2017,"key":"68724252-d9f7-4a06-ab22-11b3aac63233","use":1,"doi":"10.1109\/TMI.2017.2708987"},{"Title":"A deep learning based approach to classification of CT brain images","Description":"X. W. Gao,  R. Hui","ShortDetails":"2016 SAI Computing Conference (SAI). 2016","abstract":"This study explores the applicability of the state of the art of deep learning convolutional neural network (CNN) to the classification of CT brain images, aiming at bring images into clinical applications. Towards this end, three categories are clustered, which contains subjects' data with either Alzheimer's disease (AD) or lesion (e.g. tumour) or normal ageing. Specifically, due to the characteristics of CT brain images with larger thickness along depth (z) direction (~5mm), both 2D and 3D CNN are employed in this research. The fusion is therefore conducted based on both 2D CT images along axial direction and 3D segmented blocks with the accuracy rates are 88.8%, 76.7% and 95% for classes of AD, lesion and normal respectively, leading to an average of 86.8%.","email":["x.gao@mdx.ac.uk","Huirui2002@163.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7555958","source":"ieee","year":2016,"key":"95402f2a-cb85-4213-8f79-532779f3b27f","use":1,"doi":"10.1109\/SAI.2016.7555958"},{"Title":"Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network","Description":"M. Anthimopoulos,  S. Christodoulidis,  L. Ebner,  A. Christe,  S. Mougiakakou","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 \u00d7 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO\/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","email":["pubs-permissions@ieee.org.","marios.anthimopoulos@artorg.unibe.ch","stergios.christodoulidis@artorg.unibe","stavroula.mougiakakou@artorg.unibe.ch","andreas.christe@insel.ch","lukas.ebner@insel.ch"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7422082","source":"ieee","year":2016,"key":"65109af7-53c1-46b3-a397-e9a94f86637c","use":1,"doi":"10.1109\/TMI.2016.2535865"},{"Title":"Segmentation of Pulmonary CT Image by Using Convolutional Neural Network Based on Membership Function","Description":"J. Xu,  H. Liu","ShortDetails":"2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC). 2017","abstract":"The accurate segmentation of pulmonary CT images is of great significance to clinical computer-aided diagnosis and treatment. In order to avoid the explicit extraction of image features and improve the efficiency of image segmentation and reduce the influence of human factors on the segmentation results, this paper proposes a method of segmenting pulmonary CT image based on membership function convolution neural network (MFCNN). First, the method uses pulmonary CT image filtered by the Gaussian as the input data of the convolution neural network. Then, that uses the improved convolution neural network to achieve the initial segmentation of the image. Finally, the final segmentation result is obtained by setting the threshold based on this paper method. After experimental comparison, this paper demonstrates the feasibility and effectiveness of convolution neural network in the segmentation of pulmonary CT images.","email":["xxxuuujjjuuunnn@126.com","liuh_lh@126.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8005794","source":"ieee","year":2017,"key":"e9a0a42f-0c1e-4eb9-bd87-9f81126e8bfe","use":1,"doi":"10.1109\/CSE-EUC.2017.42"},{"Title":"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","Description":"S. Christodoulidis,  M. Anthimopoulos,  L. Ebner,  A. Christe,  S. Mougiakakou","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7776792","source":"ieee","year":2017,"key":"b32b3df7-577b-48f0-bc50-48d04d1f02f3","use":1,"doi":"10.1109\/JBHI.2016.2636929"},{"Title":"Deep-learning strategy for pulmonary artery-vein classification of non-contrast CT images","Description":"P. Nardelli,  D. Jimenez-Carretero,  D. Bermejo-Pel\u00e1ez,  M. J. Ledesma-Carbayo,  F. N. Rahaghi,  R. S. J. Est\u00e9par","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Artery-vein classification on pulmonary computed tomography (CT) images is becoming of high interest in the scientific community due to the prevalence of pulmonary vascular disease that affects arteries and veins through different mechanisms. In this work, we present a novel approach to automatically segment and classify vessels from chest CT images. We use a scale-space particle segmentation to isolate vessels, and combine a convolutional neural network (CNN) to graph-cut (GC) to classify the single particles. Information about proximity of arteries to airways is learned by the network by means of a bronchus enhanced image. The methodology is evaluated on the superior and inferior lobes of the right lung of twenty clinical cases. Comparison with manual classification and a Random Forests (RF) classifier is performed. The algorithm achieves an overall accuracy of 87% when compared to manual reference, which is higher than the 73% accuracy achieved by RF.","email":["gios.christodoulidis@artorg.unibe.ch","marios.anthimopoulos@artorg.unibe.ch","lukas.ebner@insel.ch","andreas.christe@insel.ch","stavroula.mougiakakou@artorg.unibe.ch"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950543","source":"ieee","year":2017,"key":"9c11f381-cfa5-4277-ace7-f9be4d63f312","use":1,"doi":"10.1109\/ISBI.2017.7950543"},{"Title":"Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines","Description":"G. van Tulder,  M. de Bruijne","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.","email":["pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7401039","source":"ieee","year":2016,"key":"61504234-3fb9-43d0-b134-8b16f34b493b","use":1,"doi":"10.1109\/TMI.2016.2526687"},{"Title":"Off-the-shelf convolutional neural network features for pulmonary nodule detection in computed tomography scans","Description":"B. van Ginneken,  A. A. A. Setio,  C. Jacobs,  F. Ciompi","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Convolutional neural networks (CNNs) have emerged as the most powerful technique for a range of different tasks in computer vision. Recent work suggested that CNN features are generic and can be used for classification tasks outside the exact domain for which the networks were trained. In this work we use the features from one such network, OverFeat, trained for object detection in natural images, for nodule detection in computed tomography scans. We use 865 scans from the publicly available LIDC data set, read by four thoracic radiologists. Nodule candidates are generated by a state-of-the-art nodule detection system. We extract 2D sagittal, coronal and axial patches for each nodule candidate and extract 4096 features from the penultimate layer of OverFeat and classify these with linear support vector machines. We show for various configurations that the off-the-shelf CNN features perform surprisingly well, but not as good as the dedicated detection system. When both approaches are combined, significantly better results are obtained than either approach alone. We conclude that CNN features have great potential to be used for detection tasks in volumetric medical data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163869","source":"ieee","year":2015,"key":"9b961773-837d-47cc-9a01-7d3b5c4c88a3","use":1,"doi":"10.1109\/ISBI.2015.7163869"},{"Title":"Atherosclerotic vascular calcification detection and segmentation on low dose computed tomography scans using convolutional neural networks","Description":"K. Chellamuthu,  J. Liu,  J. Yao,  M. Bagheri,  L. Lu,  V. Sandfort,  R. M. Summers","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"We propose an automated platform for extra-coronary calcification detection on low dose CT scans. We utilize faster regional convolutional neural networks (R-CNN) to directly detect calcifications at the lesion-level without performing vessel extraction. To segment detected calcifications at the voxel-level, we employ holistically nested edge detection (HED). CT scans of 112 vasculitis patients and 3219 images with labeled calcifications were used to develop and evaluate our method. By employing a two-class faster R-CNN, the average precision (AP) increased from 49.2% to 84.4% for calcification detection. In addition, sensitivity of 85.0% at 1 false positive per image was observed. The Dice Similarity Coefficient (DSC) for calcification segmentation using HED (0.83\u00b10.08) was significantly better (p\u226a0.01) than the traditional threshold-based method (0.59\u00b10.26).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950544","source":"ieee","year":2017,"key":"e8b72726-5fbb-4d62-b19f-07f37d51ad65","use":1,"doi":"10.1109\/ISBI.2017.7950544"},{"Title":"Convolutional neural networks for lung cancer screening in computed tomography (CT) scans","Description":"P. Rao,  N. A. Pereira,  R. Srinivasan","ShortDetails":"2016 2nd International Conference on Contemporary Computing and Informatics (IC3I). 2016","abstract":"Diagnosis and cure of cancer has been one of the biggest challenges faced by mankind in the last few decades. Early detection of cancer would facilitate in saving millions of lives across the globe every year. This paper presents an approach which uses a Convolutional Neural Network (CNNs) to classify tumours seen in lung cancer screening computed tomography scans as malignant or benign. CNNs have special properties such as spatial invariance, and allow for multiple feature extraction. When such layers are cascaded, leading to Deep CNNs, it has been shown widely that the accuracy of prediction increases dramatically. In this work, we have designed a CNN suitable for the analysis of CT scans with tumours, using domain knowledge from both medicine and neural networks. The results show that the accuracy of classification for our network performs better than both the traidtional neural networks, and also existing CNNs built for image classification purposes.","email":["prajwaljpj@gmail.com","nishalpereira@gmail.com","raghuram@msrit.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7918014","source":"ieee","year":2016,"key":"c642c7a5-cd18-4525-b3f7-1f2a103a7765","use":1,"doi":"10.1109\/IC3I.2016.7918014"},{"Title":"Deep Features Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network","Description":"M. Chen,  X. Shi,  Y. Zhang,  D. Wu,  M. Guizani","ShortDetails":"IEEE Transactions on Big Data. 2017","abstract":"At present, computed tomography (CT) are widely used to assist diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) is an extremely important research field in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it evaluates that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during of artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.","email":["yin.zhang.cn@ieee.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7954012","source":"ieee","year":2017,"key":"d067afa4-16e1-48ae-9a68-5a2022b213dc","use":1,"doi":"10.1109\/TBDATA.2017.2717439"},{"Title":"Colitis detection on computed tomography using regional convolutional neural networks","Description":"J. Liu,  D. Wang,  Z. Wei,  L. Lu,  L. Kim,  E. Turkbey,  R. M. Summers","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Colitis is inflammation of the colon that is frequently associated with infection and immune compromise. The wall of a colon afflicted with colitis is much thicker than normal. Colitis can be debilitating or life threatening, and early detection is essential to initiate proper treatment. In this work, we apply high-capacity convolutional neural net-works (CNNs) to bottom-up region proposals to detect potential colitis on CT scans. Our method first generates around 3000 category-independent region proposals for each slice of the input CT scan using selective search. Then, a fixed-length feature vector is extracted from each region proposal using a CNN. Finally, each region proposal is classified and assigned a confidence score with a linear SVM. We applied the detection method to 448 images from 56 CT scans of patients with colitis for evaluation. The detection system achieved 85% sensitivity at 1 false positive per image.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493402","source":"ieee","year":2016,"key":"6323d584-1b3c-4584-a17d-7489402e245e","use":1,"doi":"10.1109\/ISBI.2016.7493402"},{"Title":"Detecting Anatomical Landmarks From Limited Medical Imaging Data Using Two-Stage Task-Oriented Deep Neural Networks","Description":"J. Zhang,  M. Liu,  D. Shen","ShortDetails":"IEEE Transactions on Image Processing. 2017","abstract":"One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961205","source":"ieee","year":2017,"key":"c0c550d2-3a04-4d88-9f40-312d13b77e7d","use":1,"doi":"10.1109\/TIP.2017.2721106"},{"Title":"A CNN Regression Approach for Real-Time 2D\/3D Registration","Description":"S. Miao,  Z. J. Wang,  R. Liao","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D\/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D\/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7393571","source":"ieee","year":2016,"key":"44acb4cb-d1e0-465f-abc2-9441707db2d3","use":1,"doi":"10.1109\/TMI.2016.2521800"},{"Title":"Convolutional neural networks for predicting molecular profiles of non-small cell lung cancer","Description":"D. Yu,  M. Zhou,  F. Yang,  D. Dong,  O. Gevaert,  Z. Liu,  J. Shi,  J. Tian","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Quantitative imaging biomarkers identification has become a powerful tool for predictive diagnosis given increasingly available clinical imaging data. In parallel, molecular profiles have been well documented in non-small cell lung cancers (NSCLCs). However, there has been limited studies on leveraging the two major sources for improving lung cancer computer-aided diagnosis. In this paper, we investigate the problem of predicting molecular profiles with CT imaging arrays in NSCLC. In particular, we formulate a discriminative convolutional neural network to learn deep features for predicting epidermal growth factor receptor (EGFR) mutation states that are associated with cancer cell growth. We evaluated our approach on two independent datasets including a discovery set with 595 patients (Datset1) and a validation set with 89 patients (Dataset2). Extensive experimental results demonstrated that the learned CNN-based features are effective in predicting EGFR mutation states (AUC=0.828, ACC=76.16%) on Dataset1, and it further demonstrated generalized predictive performance (AUC=0.668, ACC=67.55%) on Dataset2.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950585","source":"ieee","year":2017,"key":"5b690cd9-8c40-4cf9-ae4a-eb6e77c6830d","use":1,"doi":"10.1109\/ISBI.2017.7950585"},{"Title":"Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks","Description":"A. A. A. Setio,  F. Ciompi,  G. Litjens,  P. Gerke,  C. Jacobs,  S. J. van Riel,  M. M. W. Wille,  M. Naqibullah,  C. I. S\u00e1nchez,  B. van Ginneken","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7422783","source":"ieee","year":2016,"key":"e4f56643-961d-4f58-885e-ae70cb1f4d82","use":1,"doi":"10.1109\/TMI.2016.2536809"},{"Title":"A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling","Description":"A. Farag,  L. Lu,  H. R. Roth,  J. Liu,  E. Turkbey,  R. M. Summers","ShortDetails":"IEEE Transactions on Image Processing. 2017","abstract":"Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model\/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by \u201cleave-one-patient-out\u201d), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus \u2265 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7\u00b113.0% versus 52.51\u00b120.84% in dice coefficients.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727966","source":"ieee","year":2017,"key":"975475a1-8261-41b7-b3a8-e1efc59adbd8","use":1,"doi":"10.1109\/TIP.2016.2624198"},{"Title":"Self supervised deep representation learning for fine-grained body part recognition","Description":"P. Zhang,  F. Wang,  Y. Zheng","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Difficulty on collecting annotated medical images leads to lack of enough supervision and makes discrimination tasks challenging. However, raw data, e.g., spatial context information from 3D CT images, even without annotation, may contain rich useful information. In this paper, we exploit spatial context information as a source of supervision to solve discrimination tasks for fine-grained body part recognition with conventional 3D CT and MR volumes. The proposed pipeline consists of two steps: 1) pre-train a convolutional network for an auxiliary task of 2D slices ordering in a self-supervised manner; 2) transfer and fine-tune the pre-trained network for fine-grained body part recognition. Without any use of human annotation in the first stage, the pre-trained network can still outperform CNN trained from scratch on CT as well as M-R data. Moreover, by comparing with pre-trained CNN from ImageNet, we discover that the distance between source and target tasks plays a crucial role in transfer learning. Our experiments demonstrate that our approach can achieve high accuracy with a slice location estimation error of only a few slices on CT and MR data. To the best of our knowledge, our work is the first attempt studying the problem of robust body part recognition at a continuous level.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950587","source":"ieee","year":2017,"key":"fa9fb874-8ecb-4b01-b2da-f19e96cab92f","use":1,"doi":"10.1109\/ISBI.2017.7950587"},{"Title":"Classification of thyroid nodules in ultrasound images using deep model based transfer learning and hybrid features","Description":"T. Liu,  S. Xie,  J. Yu,  L. Niu,  W. Sun","ShortDetails":"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017","abstract":"Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful semantic features to the classification. Firstly, a CNN model trained with a massive natural dataset is transferred to the ultrasound image domain, to generate semantic deep features and handle the small sample problem. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Local Binary Patterns (LBP) together, to form a hybrid feature space. Finally, a positive-sample-first majority voting and a feature-selected based strategy are employed for the hybrid classification. Experimental results on 1037 images show that the accuracy of our proposed method is 0.931, which outperformed other relative methods by over 10%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7952290","source":"ieee","year":2017,"key":"ad38f85f-f4a7-4a5a-be1b-080433cf903c","use":1,"doi":"10.1109\/ICASSP.2017.7952290"},{"Title":"Automatic 3D ultrasound segmentation of the first trimester placenta using deep learning","Description":"P. Looney,  G. N. Stevenson,  K. H. Nicolaides,  W. Plasencia,  M. Molloholli,  S. Natsis,  S. L. Collins","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Placental volume measured with 3D ultrasound in the first trimester has been shown to be correlated to adverse pregnancy outcomes. This could potentially be used as a screening test to predict the \u201cat risk\u201d pregnancy. However, manual segmentation whilst previously shown to be accurate and repeatable is very time consuming and semi-automated methods still require operator input. To generate a screening tool, fully automated placental segmentation is required. In this work, a deep convolutional neural network (cNN), DeepMedic, was trained using the output of the semi-automated Random Walker method as ground truth. 300 3D ultrasound scans of first trimester placentas were used to train, validate and test the cNN. Compared against the semi-automated segmentation, resultant median (1<sup>st<\/sup> Quartile, 3<sup>rd<\/sup> Quartile) Dice Similarity Coefficient was 0.73 (0.66, 0.76). The median (1<sup>st<\/sup> Quartile, 3<sup>rd<\/sup> Quartile) Hausdorff distance was 27 mm (18 mm, 36 mm). We present the first attempt at using a deep cNN for segmentation of 3D ultrasound of the placenta. This work shows that feasible results compared to ground truth were obtained that could form the basis of a fully automatic segmentation method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950519","source":"ieee","year":2017,"key":"80031293-51bc-4749-8551-512f0504dd15","use":1,"doi":"10.1109\/ISBI.2017.7950519"},{"Title":"Hybrid approach for automatic segmentation of fetal abdomen from ultrasound images using deep learning","Description":"H. Ravishankar,  S. M. Prabhu,  V. Vaidya,  N. Singhal","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"In this paper, we propose a hybrid approach combining traditional texture analysis methods with deep learning for the automatic detection and measurement of abdominal contour from 2-D fetal ultrasound images. Following a learning-based procedure for region of interest (ROI) localization to segment the abdominal boundary, we show that convolutional neural networks (CNNs) outperform other state-of-the-art texture features and conventional classifiers, in addressing the binary classification problem of distinguishing between abdomen versus non-abdomen regions. However, we obtain significantly better segmentation results in identifying the best ROI containing fetal abdomen, when the predictions from CNN are combined with those from gradient boosting machine (GBM) using histogram of oriented gradient (HOG) features. We trained our method on a set of 70 images and tested them on another distinct set of 70 images. We obtained a mean DICE similarity coefficient of 0.90, which shows excellent overlap with the ground truth. We report that the mean computed gestational age difference between our segmentation results and the ground truth, is within two weeks for 90% (and within one week for 70%) of the testing cases.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493382","source":"ieee","year":2016,"key":"84d80cbf-ef9d-4d2d-99b0-1dedc9c911eb","use":1,"doi":"10.1109\/ISBI.2016.7493382"},{"Title":"Automatic fetal body and amniotic fluid segmentation from fetal ultrasound images by encoder-decoder network with inner layers","Description":"Y. Li,  R. Xu,  J. Ohya,  H. Iwata","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"This paper explores the effectiveness of applying a deep learning based method to segment the amniotic fluid and fetal tissues in fetal ultrasound (US) images. The deeply learned model firstly encodes the input image into down scaled feature maps by convolution and pooling structures, then up-scale the feature maps to confidence maps by corresponded un-pooling and convolution layers. Additional convolution layers with 1\u00d71 sized kernels are adopted to enhance the feature representations, which could be used to further improve the discriminative learning of our model. We effectively update the weights of the network by fine-tuning on part of the layers from a pre-trained model. By conducting experiments using clinical data, the feasibility of our proposed approach is compared and discussed. The result proves that this work achieves satisfied results for segmentation of specific anatomical structures from US images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037116","source":"ieee","year":2017,"key":"cdbf876e-77de-4f06-9a74-b969755cec6c","use":1,"doi":"10.1109\/EMBC.2017.8037116"},{"Title":"Automated embolic signal detection using Deep Convolutional Neural Network","Description":"P. Sombune,  P. Phienphanich,  S. Phuechpanpaisal,  S. Muengtaweepongsa,  A. Ruamthanthong,  C. Tantibundhit","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"This work investigated the potential of Deep Neural Network in detection of cerebral embolic signal (ES) from transcranial Doppler ultrasound (TCD). The resulting system is aimed to couple with TCD devices in diagnosing a risk of stroke in real-time with high accuracy. The Adaptive Gain Control (AGC) approach developed in our previous study is employed to capture suspected ESs in real-time. By using spectrograms of the same TCD signal dataset as that of our previous work as inputs and the same experimental setup, Deep Convolutional Neural Network (CNN), which can learn features while training, was investigated for its ability to bypass the traditional handcrafted feature extraction and selection process. Extracted feature vectors from the suspected ESs are later determined whether they are of an ES, artifact (AF) or normal (NR) interval. The effectiveness of the developed system was evaluated over 19 subjects going under procedures generating emboli. The CNN-based system could achieve in average of 83.0% sensitivity, 80.1% specificity, and 81.4% accuracy, with considerably much less time consumption in development. The certainly growing set of training samples and computational resources will contribute to high performance. Besides having potential use in various clinical ES monitoring settings, continuation of this promising study will benefit developments of wearable applications by leveraging learnable features to serve demographic differentials.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037577","source":"ieee","year":2017,"key":"08d78d95-b04f-445f-bd63-1ac93ba8ef6a","use":1,"doi":"10.1109\/EMBC.2017.8037577"},{"Title":"Automated Breast Ultrasound Lesions Detection using Convolutional Neural Networks","Description":"M. H. Yap,  G. Pons,  J. Mart\u00ed,  S. Ganau,  M. Sent\u00eds,  R. Zwiggelaar,  A. K. Davison,  R. Mart\u00ed","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Breast lesion detection using ultrasound imaging is considered an important step of Computer-Aided Diagnosis systems. Over the past decade, researchers have demonstrated the possibilities to automate the initial lesion detection. However, the lack of a common dataset impedes research when comparing the performance of such algorithms. This paper proposes the use of deep learning approaches for breast ultrasound lesion detection and investigates three different methods: a Patch-based LeNet, a U-Net, and a transfer learning approach with a pretrained FCN-AlexNet. Their performance is compared against four state-of-the-art lesion detection algorithms (i.e. Radial Gradient Index, Multifractal Filtering, Rule-based Region Ranking and Deformable Part Models). In addition, this paper compares and contrasts two conventional ultrasound image datasets acquired from two different ultrasound systems. Dataset A comprises 306 (60 malignant and 246 benign) images and Dataset B comprises 163 (53 malignant and 110 benign) images. To overcome the lack of public datasets in this domain, Dataset B will be made available for research purposes. The results demonstrate an overall improvement by the deep learning approaches when assessed on both datasets in terms of True Positive Fraction, False Positives per image, and F-measure.","email":["m.yap@mmu.ac.uk.","gponsro@uoc.edu.","robert.marti@udg.edu.","MSentis@tauli.cat.","rrz@aber.ac.uk.","adrian.davison@manchester.ac.uk."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8003418","source":"ieee","year":2017,"key":"23e4dbf1-e0e7-4bec-9c85-30d1cbacf666","use":1,"doi":"10.1109\/JBHI.2017.2731873"},{"Title":"Deep Learning on Sparse Manifolds for Faster Object Segmentation","Description":"J. C. Nascimento,  G. Carneiro","ShortDetails":"IEEE Transactions on Image Processing. 2017","abstract":"We propose a new combination of deep belief networks and sparse manifold learning strategies for the 2D segmentation of non-rigid visual objects. With this novel combination, we aim to reduce the training and inference complexities while maintaining the accuracy of machine learning-based non-rigid segmentation methodologies. Typical non-rigid object segmentation methodologies divide the problem into a <italic>rigid detection<\/italic> followed by a <italic>non-rigid segmentation<\/italic>, where the low dimensionality of the rigid detection allows for a robust training (i.e., a training that does not require a vast amount of annotated images to estimate robust appearance and shape models) and a fast search process during inference. Therefore, it is desirable that the dimensionality of this rigid transformation space is as small as possible in order to enhance the advantages brought by the aforementioned division of the problem. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection space. Furthermore, we propose the use of deep belief networks to allow for a training process that can produce robust appearance models without the need of large annotated training sets. We test our approach in the segmentation of the left ventricle of the heart from ultrasound images and lips from frontal face images. Our experiments show that the use of sparse manifolds and deep belief networks for the rigid detection stage leads to segmentation results that are as accurate as the current state of the art, but with lower search complexity and training processes that require a small amount of annotated training data.","email":["jan@isr.ist.utl.pt."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7973159","source":"ieee","year":2017,"key":"a5099dee-1a89-494e-a087-e65dc25865e2","use":1,"doi":"10.1109\/TIP.2017.2725582"},{"Title":"Automated assessment of endometrium from transvaginal ultrasound using Deep Learned Snake","Description":"N. Singhal,  S. Mukherjee,  C. Perrey","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Endometrium assessment via thickness measurement is commonly performed in routine gynecological ultrasound examination for assessing the reproductive health of patients undergoing fertility related treatments and endometrium cancer screening in women with post-menopausal bleeding. This paper introduces a fully automated technique for endometrium thickness measurement from three-dimensional transvaginal ultrasound (TVUS) images. The algorithm combines the robustness of deep neural networks with the more interpretable level set method for segmentation. We propose a hybrid variational curve propagation model which embeds a deep-learned endometrium probability map in the segmentation energy functional. This solution provides approximately 30% performance improvement over a contemporary supervised learning method on a database of 59 TVUS images and the thickness measurement is found to be within \u00b12mm of the manual measurement in 87% of the cases.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950520","source":"ieee","year":2017,"key":"54f3332d-5777-42f0-be3e-750da487f79a","use":1,"doi":"10.1109\/ISBI.2017.7950520"},{"Title":"Liver Fibrosis Classification Based on Transfer Learning and FCNet for Ultrasound Images","Description":"D. Meng,  L. Zhang,  G. Cao,  W. Cao,  G. Zhang,  B. Hu","ShortDetails":"IEEE Access. 2017","abstract":"Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.","email":["mengdan90@163.com","gtcao@sei.ecnu.edu.cn","zsmj@hotmail.com","wmcao@szu.edu.cn","gxzhang@sei.ecnu.edu.cn","stephen@163.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7890483","source":"ieee","year":2017,"key":"7aa21208-ba83-47cf-a2ab-a25ed3427715","use":1,"doi":"10.1109\/ACCESS.2017.2689058"},{"Title":"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","Description":"K. Lekadir,  A. Galimzianova,  \u00c0. Betriu,  M. del Mar Vila,  L. Igual,  D. L. Rubin,  E. Fern\u00e1ndez,  P. Radeva,  S. Napel","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","email":["klekadir@cvc.uab.es"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7752798","source":"ieee","year":2017,"key":"3daa4798-8f08-4695-a860-aa663ed36820","use":1,"doi":"10.1109\/JBHI.2016.2631401"},{"Title":"Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks","Description":"H. Chen,  D. Ni,  J. Qin,  S. Li,  X. Yang,  T. Wang,  P. A. Heng","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2015","abstract":"Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.","email":["ie.haochen@gmail.com","nidong@szu.edu.cn","jqin@szu.edu.cn","lishengli63@126.com","pheng@cse.cuhk.edu.hk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7090943","source":"ieee","year":2015,"key":"2f4ae966-7c74-4fb8-923e-301ac4d845fa","use":1,"doi":"10.1109\/JBHI.2015.2425041"},{"Title":"A Deep Convolutional Neural Network Based Framework for Automatic Fetal Facial Standard Plane Recognition","Description":"Z. Yu,  E. L. Tan,  D. Ni,  J. Qin,  S. Chen,  S. Li,  B. Lei,  T. Wang","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Ultrasound imaging has become a prevalent examination method in prenatal diagnosis. Accurate acquisition of fetal facial standard plane (FFSP) is the most important precondition for subsequent diagnosis and measurement. In the past few years, considerable effort has been devoted to FFSP recognition using various hand-crafted features, but the recognition performance is still unsatisfactory due to the high intra-class variation of FFSPs and the high degree of visual similarity between FFSPs and other non-FFSPs. To improve the recognition performance, we propose a method to automatically recognize FFSP via a deep convolutional neural network (DCNN) architecture. The proposed DCNN consists of 16 convolutional layers with small 3\u00d73 size kernels and three fully connected layers. A global average pooling (GAP) is adopted in the last pooling layer to significantly reduce network parameters, which alleviates the overfitting problems and improves the performance under limited training data. Both the transfer learning strategy and a data augmentation technique tailored for FFSP are implemented to further boost the recognition performance. Extensive experiments demonstrate the advantage of our proposed method over traditional approaches and the effectiveness of DCNN to recognize FFSP for clinical diagnosis.","email":["leiby@szu.edu.cn","tfwang@szu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7930382","source":"ieee","year":2017,"key":"52f2341e-c951-4287-bb1d-47e2ad043473","use":1,"doi":"10.1109\/JBHI.2017.2705031"},{"Title":"Deep learning of submerged body images from 2D sonar sensor based on convolutional neural network","Description":"S. Lee","ShortDetails":"2017 IEEE Underwater Technology (UT). 2017","abstract":"Given the harsh working conditions such as high-speed flow rate, turbid watch, and steep terrain, it is a very challenging task to find submerged bodies in disaster site occurred at sea or river or for the military purpose. Therefore, if it is possible to utilize the unmanned robot, such as the USV(Unmanned Surface Vehicle) and UUV (Unmanned Underwater Vehicle) for the navigational operation of these special purpose, it has a great effect. Underwater ultrasound image information is pretty difficult to make the geometric modeling of submerged body due to heavy noise on its characteristics. This study presents the robust method of submerged body recognition based on the CNN(Convolutional Neural Network), which is one of the deep learning approach.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7890309","source":"ieee","year":2017,"key":"abf64f11-6d27-4616-b812-f52f08c1fb85","use":1,"doi":"10.1109\/UT.2017.7890309"},{"Title":"Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images","Description":"Y. Zhang,  M. T. C. Ying,  L. Yang,  A. T. Ahuja,  D. Z. Chen","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Ultrasound as a well-established imaging modality is widely used in imaging lymph nodes for clinical diagnosis and disease analysis. Quantitative analysis of lymph node features, morphology, and relations can provide valuable information for diagnosis and immune system studies. For such analysis, it is necessary to first accurately segment the lymph node areas in ultrasound images. In this paper, we develop a new deep learning method, called Coarse-to-Fine Stacked Fully Convolutional Nets (CFS-FCN), for automatically segmenting lymph nodes in ultrasound images. Our method consists of multiple stages of FCN modules. We train the CFS-FCN model to learn the segmentation knowledge from a coarse-to-fine, simple-to-complex manner. A data set of 80 ultrasound images containing both normal and diseased lymph nodes is used in our experiments, which show that our method considerably outperforms the state-of-the-art deep learning methods for lymph node segmentation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822557","source":"ieee","year":2016,"key":"a40c80b6-ae17-4739-be93-57b0aabeee73","use":1,"doi":"10.1109\/BIBM.2016.7822557"},{"Title":"Fetal facial standard plane recognition via very deep convolutional networks","Description":"Z. Yu,  D. Ni,  S. Chen,  S. Li,  T. Wang,  B. Lei","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3\u00d73) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590780","source":"ieee","year":2016,"key":"825af611-c6cb-4f1a-a92f-a03e81804cd2","use":1,"doi":"10.1109\/EMBC.2016.7590780"},{"Title":"Multi-atlas segmentation using manifold learning with deep belief networks","Description":"J. C. Nascimento,  G. Carneiro","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"This paper proposes a novel combination of manifold learning with deep belief networks for the detection and segmentation of left ventricle (LV) in 2D \u2014 ultrasound (US) images. The main goal is to reduce both training and inference complexities while maintaining the segmentation accuracy of machine learning based methods for non-rigid segmentation methodologies. The manifold learning approach used can be viewed as an atlas-based segmentation. It partitions the data into several patches. Each patch proposes a segmentation of the LV that somehow must be fused. This is accomplished by a deep belief network (DBN) multi-classifier that assigns a weight for each patch LV segmentation. The approach is thus threefold: (i) it does not rely on a single segmentation, (ii) it provides a great reduction in the rigid detection phase that is performed at lower dimensional space comparing with the initial contour space, and (iii) DBN's allows for a training process that can produce robust appearance models without the need of large annotated training sets.","email":["sejiny3@kongju.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493403","source":"ieee","year":2016,"key":"1e11e00b-d238-47e2-94be-3e10d8baad53","use":1,"doi":"10.1109\/ISBI.2016.7493403"},{"Title":"Describing ultrasound video content using deep convolutional neural networks","Description":"Y. Gao,  M. A. Maraci,  J. A. Noble","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"We address the task of object recognition in obstetric ultrasound videos using deep Convolutional Neural Networks (CNNs). A transfer learning based design is presented to study the transferability of features learnt from natural images to ultrasound image object recognition which on the surface is a very different problem. Our results demonstrate that CNNs initialised with large-scale pre-trained networks outperform those directly learnt from small-scale ultrasound data (91.5% versus 87.9%), in terms of object identification.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493384","source":"ieee","year":2016,"key":"223066e3-d979-4b12-8c32-5c7a6bc87af5","use":1,"doi":"10.1109\/ISBI.2016.7493384"},{"Title":"Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing","Description":"F. C. Ghesu,  E. Krubasik,  B. Georgescu,  V. Singh,  Y. Zheng,  J. Hornegger,  D. Comaniciu","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% o- er the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","email":["orin.c.ghesu@fau.de","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7426845","source":"ieee","year":2016,"key":"baeb389f-79cf-4fdd-b7a2-6f775d5a8e3c","use":1,"doi":"10.1109\/TMI.2016.2538802"},{"Title":"A novel method with a deep network and directional edges for automatic detection of a fetal head","Description":"S. Nie,  J. Yu,  P. Chen,  J. Zhang,  Y. Wang","ShortDetails":"2015 23rd European Signal Processing Conference (EUSIPCO). 2015","abstract":"In this paper, we propose a novel method for the automatic detection of fetal head in 2D ultrasound images. Fetal head detection has been a challenging task, as the ultrasound images usually have poor quality, the structures contained in the images are complex, and the gray scale distribution is highly variable. Our approach is based on a deep belief network and a modified circle detection method. The whole process can be divided into two steps: first, a deep learning architecture is applied to search the whole image and determine the result patch that contains the entire fetal head; second, a modified circle detection method is used along with Hough transform to detect the position and size of the fetal head. In order to validate our method, experiments are performed on both synthetic data and clinic ultrasound data. A good performance of the proposed method is shown in the paper.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7362464","source":"ieee","year":2015,"key":"b59439d9-9c56-45df-808a-8688cdf90002","use":1,"doi":"10.1109\/EUSIPCO.2015.7362464"},{"Title":"Mapping between ultrasound and vowel speech using DNN framework","Description":"X. Zheng,  J. Wei,  W. Lu,  Q. Fang,  J. Dang","ShortDetails":"The 9th International Symposium on Chinese Spoken Language Processing. 2014","abstract":"Building up the mapping between articulatory movements and corresponding speech could great facility the speech training and speech aid for voiceless patients. In this paper, we propose a deep learning framework for building up a mapping between articulatory information and corresponding speech, which were recorded by ultrasound system. The dataset includes six Chinese vowels. We use Bimodal Deep Autoencoder algorithm based on RBM to learn the relationship between speech and articulation, the weights matrix of representation of them. Speech and ultrasound images have been reconstructed using the extracted features. The reconstruction error of articulation by our method is less than that of PCA based approach. The reconstructed speech is similar to the original one. We propose a mapping from ultrasound tongue image to acoustic signal with a revised Denoising Autoencoder, the results show that it is a promising approach. In contrast, another experiment is conducted to synthesize the ultrasound tongue image from the speech, but the result should be improved.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6936700","source":"ieee","year":2014,"key":"f1675620-d487-40e9-9998-908fc6810fe7","use":1,"doi":"10.1109\/ISCSLP.2014.6936700"},{"Title":"Small Sample Deep Learning for Newborn Gestational Age Estimation","Description":"M. T. Torres,  M. F. Valstar,  C. Henry,  C. Ward,  D. Sharkey","ShortDetails":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). 2017","abstract":"A baby's gestational age determines whether or not they are preterm, which helps clinicians decide on suitable post-natal treatment. The most accurate dating methods use Ultrasound Scan (USS) machines, but these machines are expensive, require trained personnel and cannot always be deployed to remote areas. In the absence of USS, the Ballard Score can be used, which is a manual postnatal dating method. However, this method is highly subjective and results can vary widely depending on the experience of the rater. In this paper, we present an automatic system for postnatal gestational age estimation aimed to be deployed on mobile phones, using small sets of images of a newborn's face, foot and ear. We present a novel two-stage approach that makes the most out of Convolutional Neural Networks trained on small sets of images to predict broad classes of gestational age, and then fuse the outputs of these discrete classes with a baby's weight to make fine-grained predictions of gestational age. On a purpose=collected dataset of 88 babies, experiments show that our approach attains an expected error of 6 days and is three times more accurate than the manual postnatal method (Ballard). Making use of images improves predictions by 30% compared to using weight only. This indicates that even with a very small set of data, our method is a viable candidate for postnatal gestational age estimation in areas were USS is not available.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961726","source":"ieee","year":2017,"key":"351c16ad-87b3-462b-a38f-eca2ae400dd6","use":1,"doi":"10.1109\/FG.2017.19"},{"Title":"Automated characterization of the fetal heart in ultrasound images using fully convolutional neural networks","Description":"V. Sundaresan,  C. P. Bridge,  C. Ioannou,  J. A. Noble","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automatic analysis of fetal echocardiography screening images could aid in the identification of congenital heart diseases. The first step towards automatic fetal echocardiography analysis is locating the fetal heart in an image and identifying the viewing (imaging) plane. This is highly challenging since the fetal heart is small with relatively indistinct anatomical structural appearance. This is further compounded by the presence of artefacts in ultrasound images. Herein we provide a state-of-art solution for detecting the fetal heart and classifying each individual frame as belonging to one of the standard viewing planes using fully convolutional neural networks (FCNs). Our FCN model achieves a classification error rate of 23.48% on real-world clinical ultrasound data. We also present comparative performance for analysis of different FCN architectures.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950609","source":"ieee","year":2017,"key":"4fee85ee-dd9a-47c5-af08-b0db35426554","use":1,"doi":"10.1109\/ISBI.2017.7950609"},{"Title":"Feature selection and thyroid nodule classification using transfer learning","Description":"T. Liu,  S. Xie,  Y. Zhang,  J. Yu,  L. Niu,  W. Sun","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful and specific features to the classification. A CNN model trained with ImageNet data is transferred to the ultrasound image domain, to generate semantic deep features under small sample condition. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Scale Invariant Feature Transform (SIFT) together to form a hybrid feature space. Furthermore, to make the general deep features more pertinent to our problem, a feature subset selection process is employed for the hybrid nodule classification, followed by a detailed discussion on the influence of feature number and feature composition method. Experimental results on 1037 images show that the accuracy of our proposed method is 0.929, which outperforms other relative methods by over 10%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950707","source":"ieee","year":2017,"key":"715c3303-f6d4-4ffd-9cf5-8856a7cccf8c","use":1,"doi":"10.1109\/ISBI.2017.7950707"},{"Title":"Classification of breast lesions using cross-modal deep learning","Description":"O. Hadad,  R. Bakalo,  R. Ben-Ari,  S. Hashoul,  G. Amit","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automatic detection and classification of lesions in medical images is a desirable goal, with numerous clinical applications. In breast imaging, multiple modalities such as X-ray, ultrasound and MRI are often used in the diagnostic workflow. Training robust classifiers for each modality is challenging due to the typically small size of the available datasets. We propose to use cross-modal transfer learning to improve the robustness of the classifiers. We demonstrate the potential of this approach on a problem of identifying masses in breast MRI images, using a network that was trained on mammography images. Comparison between cross-modal and cross-domain transfer learning showed that the former improved the classification performance, with overall accuracy of 0.93 versus 0.90, while the accuracy of de-novo training was 0.94. Using transfer learning within the medical imaging domain may help to produce standard pre-trained shared models, which can be utilized to solve a variety of specific clinical problems.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950480","source":"ieee","year":2017,"key":"d309add6-8eee-4dc8-b8cb-9b9fb6370531","use":1,"doi":"10.1109\/ISBI.2017.7950480"},{"Title":"Handcrafted features vs ConvNets in 2D echocardiographic images","Description":"C. Raynaud,  H. Langet,  M. S. Amzulescu,  E. Saloux,  H. Bertrand,  P. Allain,  P. Piro","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In this paper, we address the problem of automated pose classification and segmentation of the left ventricle (LV) in 2D echocardiographic images. For this purpose, we compare two complementary approaches. The first one is based on engineering ad-hoc features according to the traditional machine learning paradigm. Namely, we extract phase features to build an unsupervised LV pose estimator, as well as a global image descriptor for view type classification. We also apply the Supervised Descent Method (SDM) to iteratively refine the LV contour. The second approach follows the deep learning framework, where a Convolutional Network (ConvNet) learns the visual features automatically. Our experiments on a large database of apical sequences show that the two approaches yield comparable results on view classification, but SDM outperforms ConvNet on LV segmentation at a significantly lower training computational cost.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950712","source":"ieee","year":2017,"key":"de5609a3-4f81-4da5-a37e-0dda9b9fe4e4","use":1,"doi":"10.1109\/ISBI.2017.7950712"},{"Title":"Detection of lumen and media-adventitia borders in IVUS images using sparse auto-encoder neural network","Description":"S. Su,  Z. Gao,  H. Zhang,  Q. Lin,  W. K. Hau,  S. Li","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"This paper describes an artificial neural network (ANN) method that employs a feature-learning algorithm to detect the lumen and MA borders in intravascular ultrasound (IVUS) images. Three types of imaging features including spatial, neighboring, and gradient features were used as the input features to the neural network, and then the different vascular layers were distinguished using two sparse autoencoders and one softmax classifier. To smooth the lumen and MA borders detected by the ANN method, we used the active contour model. The performance of our approach was compared with the manual drawing method and another existing method on 538 IVUS images from six subjects. Results showed that our approach had a high correlation (r = 0.9284 ~ 0.9875 for all measurements) and good agreement (bias = 0.0148 ~ 0.4209 mm) with the manual drawing method, and small detection error (lumen border: 0.0928\u00b10.0935 mm, MA border: 0.1056\u00b10.1088 mm). The average time to process each image was 14\u00b14.6 seconds. The obtained results indicate that our proposed approach can be used to efficiently and accurately detect the lumen and MA borders in IVUS images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950713","source":"ieee","year":2017,"key":"a0812406-f555-43cb-ad67-e70ba263d5f1","use":1,"doi":"10.1109\/ISBI.2017.7950713"},{"Title":"Feature extraction using multimodal convolutional neural networks for visual speech recognition","Description":"E. Tatulli,  T. Hueber","ShortDetails":"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017","abstract":"This article addresses the problem of continuous speech recognition from visual information only, without exploiting any audio signal. Our approach combines a video camera and an ultrasound imaging system for monitoring simultaneously the speaker's lips and the movement of the tongue. We investigate the use of convolutional neural networks (CNN) to extract visual features directly from the raw ultrasound and video images. We propose different architectures among which a multimodal CNN processing jointly the two visual modalities. Combined with an HMM-GMM decoder, the CNN-based approach outperforms our previous baseline based on Principal Component Analysis. Importantly, the recognition accuracy is only 4% lower than the one obtained when decoding the audio signal, which makes it a good candidate for a practical visual speech recognition system.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7952701","source":"ieee","year":2017,"key":"effe348f-c257-4bd2-9e5d-bf340afc8d0f","use":1,"doi":"10.1109\/ICASSP.2017.7952701"},{"Title":"Coronary luminal and wall mask prediction using convolutional neural network","Description":"Y. Hong,  Y. M. Hong,  Y. Jang,  S. Kim,  B. Jeon,  S. Jung,  S. Ha,  D. Han,  H. Shim,  H. J. Chang","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"A significant amount of research has been done on the segmentation of coronary arteries. However, the resulting automated boundary delineation is still not suitable for clinical utilization. The convolutional neural network was driving advances in the medical image processing. We propose the brief convolutional network (BCN) that automatically produces the labeled mask with the luminal and wall boundaries of the coronary artery. We utilized 50 patients of CCTA - intravascular ultrasound matched image data sets. Training and testing were performed on 40 and 10 patient data sets, respectively. The prediction of luminal and wall mask was performed using stacked BCN on the each image view: axial, coronal, and sagittal of straightened curved planar reformation. We defined the vector that includes probability from BCN result on each image view and proposed amplified probability. We used an Adaptive Boost regressor with an extremely randomized tree regressor to determine the label for unknown probability vector.","email":["pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950696","source":"ieee","year":2017,"key":"b93e1823-d82b-48df-931f-b747ba15a2e9","use":1,"doi":"10.1109\/ISBI.2017.7950696"},{"Title":"Ultrasound Standard Plane Detection Using a Composite Neural Network Framework","Description":"H. Chen,  L. Wu,  Q. Dou,  J. Qin,  S. Li,  J. Z. Cheng,  D. Ni,  P. A. Heng","ShortDetails":"IEEE Transactions on Cybernetics. 2017","abstract":"Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7890445","source":"ieee","year":2017,"key":"6240cc26-0b20-4dc5-ae04-1085c9723f53","use":1,"doi":"10.1109\/TCYB.2017.2685080"},{"Title":"Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks","Description":"Q. Dou,  H. Chen,  L. Yu,  L. Zhao,  J. Qin,  D. Wang,  V. C. Mok,  L. Shi,  P. A. Heng","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7403984","source":"ieee","year":2016,"key":"6096b186-ffd3-43de-8f25-693f6e91b3ec","use":1,"doi":"10.1109\/TMI.2016.2528129"},{"Title":"Retinal vessel landmark detection using deep learning and hessian matrix","Description":"T. Fang,  R. Su,  L. Xie,  Q. Gu,  Q. Li,  P. Liang,  T. Wang","ShortDetails":"2015 8th International Congress on Image and Signal Processing (CISP). 2015","abstract":"The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7407910","source":"ieee","year":2015,"key":"53ce4575-cf5c-4386-a5fc-84ca5de92020","use":1,"doi":"10.1109\/CISP.2015.7407910"},{"Title":"A supervised method using convolutional neural networks for retinal vessel delineation","Description":"Q. Li,  L. Xie,  Q. Zhang,  S. Qi,  P. Liang,  H. Zhang,  T. Wang","ShortDetails":"2015 8th International Congress on Image and Signal Processing (CISP). 2015","abstract":"Retinal vessel delineation is a hot research topic owing to its importance in a lot of clinic application. Several methods have been proposed in the past decades. Here we will present a new supervised method for retinal vessel segmentation. The method is designed to explore the complex relationship between retinal images and their corresponding vessel label maps. Specifically, in order to build a model describing the direct transformation from retinal image to vessel map, we introduce a deep convolutional neural network (abbreviation as CNN), which has strong enough induction ability. For the purpose of constructing the whole vessel probability map, we also design a synthesis method. Our method shows better performance on DRIVE dataset than state-of-the-art of reported approaches in the light of sensitivity (abbreviation as Se), specificity (abbreviation as Sp) and accuracy (abbreviation as Acc). Our proposed method has great potential to be applied in existing computer-assisted diagnostic system of ophthalmologic diseases. Meanwhile, the method may offer a novel, general computing framework for segmentation in other fields.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7407916","source":"ieee","year":2015,"key":"8eface67-ce3b-47b2-8d58-b56f7c61bcda","use":1,"doi":"10.1109\/CISP.2015.7407916"},{"Title":"A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images","Description":"Q. Li,  B. Feng,  L. Xie,  P. Liang,  H. Zhang,  T. Wang","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.","email":["lql@szu.edu.cn","feng.bowei@163.com","xielinpei@email.szu.edu.cn","liangping@szu.edu.cn","tfwang@szu.edu.cn.","isaac_zhs@126.com","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7161344","source":"ieee","year":2016,"key":"dbf113b8-f1f9-4127-a28f-daa085e0f46d","use":1,"doi":"10.1109\/TMI.2015.2457891"},{"Title":"Accurate Segmentation of Cervical Cytoplasm and Nuclei Based on Multiscale Convolutional Network and Graph Partitioning","Description":"Y. Song,  L. Zhang,  S. Chen,  D. Ni,  B. Lei,  T. Wang","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2015","abstract":"In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.","email":["leiby@szu.edu.cn","tfwang@szu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7103332","source":"ieee","year":2015,"key":"fe6fe85f-b6d2-4eab-8157-07bf6669b087","use":1,"doi":"10.1109\/TBME.2015.2430895"},{"Title":"A deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei","Description":"Y. Song,  L. Zhang,  S. Chen,  D. Ni,  B. Li,  Y. Zhou,  B. Lei,  T. Wang","ShortDetails":"2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 2014","abstract":"In this paper, a superpixel and convolution neural network (CNN) based segmentation method is proposed for cervical cancer cell segmentation. Since the background and cytoplasm contrast is not relatively obvious, cytoplasm segmentation is first performed. Deep learning based on CNN is explored for region of interest detection. A coarse-to-fine nucleus segmentation for cervical cancer cell segmentation and further refinement is also developed. Experimental results show that an accuracy of 94.50% is achieved for nucleus region detection and a precision of 0.9143\u00b10.0202 and a recall of 0.8726\u00b10.0008 are achieved for nucleus cell segmentation. Furthermore, our comparative analysis also shows that the proposed method outperforms the related methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6944230","source":"ieee","year":2014,"key":"55d2300d-23d1-4b4d-95ff-7622dd431455","use":1,"doi":"10.1109\/EMBC.2014.6944230"},{"Title":"Non-rigid Segmentation Using Sparse Low Dimensional Manifolds and Deep Belief Networks","Description":"J. C. Nascimento,  G. Carneiro","ShortDetails":"2014 IEEE Conference on Computer Vision and Pattern Recognition. 2014","abstract":"In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the search procedure is onducted directly on a sparse low-dimensional manifold, guided by the classification results computed from a deep belief network. Our main contribution is the fact that we do not rely on the typical sub-division of segmentation tasks into rigid detection and non-rigid delineation. Instead, the non-rigid segmentation is performed directly, where points in the sparse low-dimensional can be mapped to an explicit contour representation in image space. Our proposal shows significantly smaller search and training complexities given that the dimensionality of the manifold is much smaller than the dimensionality of the search spaces for rigid detection and non-rigid delineation aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database. Our experiments show that the use of sparse low dimensional manifolds reduces the search and training complexities of current segmentation approaches without a significant impact on the segmentation accuracy shown by state-of-the-art approaches.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6909438","source":"ieee","year":2014,"key":"dbee8666-6220-4aae-a234-7cfb99543ad8","use":1,"doi":"10.1109\/CVPR.2014.44"},{"Title":"Image quality classification for DR screening using deep learning","Description":"F. Yu,  J. Sun,  A. Li,  J. Cheng,  C. Wan,  J. Liu","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"The quality of input images significantly affects the outcome of automated diabetic retinopathy (DR) screening systems. Unlike the previous methods that only consider simple low-level features such as hand-crafted geometric and structural features, in this paper we propose a novel method for retinal image quality classification (IQC) that performs computational algorithms imitating the working of the human visual system. The proposed algorithm combines unsupervised features from saliency map and supervised features coming from convolutional neural networks (CNN), which are fed to an SVM to automatically detect high quality vs poor quality retinal fundus images. We demonstrate the superior performance of our proposed algorithm on a large retinal fundus image dataset and the method could achieve higher accuracy than other methods. Although retinal images are used in this study, the methodology is applicable to the image quality assessment and enhancement of other types of medical images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036912","source":"ieee","year":2017,"key":"a059d7d7-a8e1-4cf6-91df-ca0fb0803ac3","use":1,"doi":"10.1109\/EMBC.2017.8036912"},{"Title":"Analysis of Disfluencies for automatic detection of Mild Cognitive Impartment: a deep learning approach","Description":"K. Lopez-de-Ipina,  U. Martinez-de-Lizarduy,  P. M. Calvo,  B. Beitia,  J. Garcia-Melero,  M. Ecay-Torres,  A. Estanga,  M. Faundez-Zanuy","ShortDetails":"2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI). 2017","abstract":"The so-called Mild Cognitive Impairment (MCI) or cognitive loss appears in a previous stage before Alzheimer's Disease (AD), but it does not seem sufficiently severe to interfere in independent abilities of daily life, so it usually does not receive an appropriate diagnosis. Its detection is a challenging issue to be addressed by medical specialists. This work presents a novel proposal based on automatic analysis of speech and disfluencies aimed at supporting MCI diagnosis. The approach includes deep learning by means of Convolutional Neural Networks (CNN) and non-linear multifeature modelling. Moreover, to select the most relevant features non-parametric Mann-Whitney U-testt and Support Vector Machine Attribute (SVM) evaluation are used.","email":["faundez@tecnocampus.cat"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7985526","source":"ieee","year":2017,"key":"7cfe2356-72e4-48fb-bb2a-409075af6bba","use":1,"doi":"10.1109\/IWOBI.2017.7985526"},{"Title":"Cell classification using convolutional neural networks in medical hyperspectral imagery","Description":"Xiang Li,  W. Li,  Xiaodong Xu,  Wei Hu","ShortDetails":"2017 2nd International Conference on Image, Vision and Computing (ICIVC). 2017","abstract":"Hyperspectral imaging is a rising imaging modality in the field of medical applications, and the combination of both spectral and spatial information provides wealth information for cell classification. In this paper, deep convolutional neural network (CNN) is employed to achieve blood cell discrimination in medical hyperspectral images (MHSI). As a deep learning architecture, CNNs are expected to get more discriminative and semantic features, which effect classification accuracy to a certain extent. Experimental results based on two real medical hyperspectral image data sets demonstrate that cell classification using CNNs is effective. In addition, compared to traditional support vector machine (SVM), the proposed method, which jointly exploits spatial and spectral features, can achieve better classification performance, showcasing the CNN-based methods' tremendous potential for accurate medical hyperspectral data classification.","email":["liwei089@ieee.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7984606","source":"ieee","year":2017,"key":"310fb8e5-7fda-416b-84c0-35cb33685984","use":1,"doi":"10.1109\/ICIVC.2017.7984606"},{"Title":"A Unified Deep Learning Model for Protein Structure Prediction","Description":"L. Bai,  L. Yang","ShortDetails":"2017 3rd IEEE International Conference on Cybernetics (CYBCONF). 2017","abstract":"Predicting protein tertiary structure from its primary amino acid sequence is one of the most challenging problems in bioinformatics, which makes an important impact in the field of medical science. The mainly difficult is how to learn the most useful and suitable protein features to improve the prediction. In this paper, we propose a novel unified deep learning model for improving protein tertiary structure prediction. The core contribution of this work is the group of deep convolution neural networks (deep CNNs) that can directly learn the high-level relational features from a pair of the query and target protein sequences. The deep CNN can learn high-level relational features from the pairwise protein sequences in a hierarchy by progressively integrating convergent protein property representations from lower levels. Multiple deep CNNs are designed to achieve robust protein structure similarities from different aspects. These relational features are fully connected to the top two Restricted Boltzmann Machines (RBMS) layer to further extract global relational features, which significantly improve the protein structure prediction. Experiments conducted on a well-known benchmark, SCOPe dataset, show that our model significantly outperforms the state-of-the-art methods in various statistical measurements. The results also demonstrate that our deep learning model can improve the learning of protein properties.","email":["bailin@gxu.edu.cn","lnyang@gxu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7985752","source":"ieee","year":2017,"key":"e8174ecc-c267-4fe9-95af-f5c22daff332","use":1,"doi":"10.1109\/CYBConf.2017.7985752"},{"Title":"An Automatic Detection System of Lung Nodule Based on Multi-Group Patch-Based Deep Learning Network","Description":"H. Jiang,  H. Ma,  W. Qian,  M. Gao,  Y. Li","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"High-efficiency lung nodule detection dramatically contributes to the risk assessment of lung cancer. It is a significant and challenging task to quickly locate the exact positions of lung nodules. Extensive work has been done by researchers around this domain for approximately two decades. However, previous computer aided detection (CADe) schemes are mostly intricate and time-consuming since they may require more image processing modules, such as the computed tomography (CT) image transformation, the lung nodule segmentation and the feature extraction, to construct a whole CADe system. It is difficult for those schemes to process and analyze enormous data when the medical images continue to increase. Besides, some state of the art deep learning schemes may be strict in the standard of database. This study proposes an effective lung nodule detection scheme based on multi-group patches cut out from the lung images, which are enhanced by the Frangi filter. Through combining two groups of images, a four-channel convolution neural networks (CNN) model is designed to learn the knowledge of radiologists for detecting nodules of four levels. This CADe scheme can acquire the sensitivity of 80.06% with 4.7 false positives per scan and the sensitivity of 94% with 15.1 false positives per scan. The results demonstrate that the multi-group patch-based learning system is efficient to improve the performance of lung nodule detection and greatly reduce the false positives under a huge amount of image data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7981333","source":"ieee","year":2017,"key":"9c23171d-0419-4052-af1c-7859c9ca04a6","use":1,"doi":"10.1109\/JBHI.2017.2725903"},{"Title":"Domain specific convolutional neural nets for detection of architectural distortion in mammograms","Description":"R. Ben-Ari,  A. Akselrod-Ballin,  L. Karlinsky,  S. Hashoul","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Detection of Architectural distortion (AD) is important for ruling out possible pre-malignant lesions in breast, but due to its subtlety, it is often missed on the screening mammograms. In this work we suggest a novel AD detection method based on region proposal convolution neural nets (R-CNN). When the data is scarce, as typically the case in medical domain, R-CNN yields poor results. In this study, we suggest a new R-CNN method addressing this shortcoming by using a pretrained network on a candidate region guided by clinical observations. We test our method on the publicly available DDSM data set, with comparison to the latest faster R-CNN and previous works. Our detection accuracy allows binary image classification (normal vs. containing AD) with over 80% sensitivity and specificity, and yields 0.46 false-positives per image at 83% true-positive rate, for localization accuracy. These measures significantly improve the best results in the literature.","email":["mahe@bmie.neu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950581","source":"ieee","year":2017,"key":"2d52b59d-9ef1-4979-a8be-24f185a01232","use":1,"doi":"10.1109\/ISBI.2017.7950581"},{"Title":"HEp-2 cell classification based on a Deep Autoencoding-Classification convolutional neural network","Description":"J. Liu,  B. Xu,  L. Shen,  J. Garibaldi,  G. Qiu","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In this paper, we present a novel deep learning model termed Deep Autoencoding-Classification Network (DACN) for HEp-2 cell classification. The DACN consists of an autoencoder and a normal classification convolutional neural network (CNN), while the two architectures shares the same encoding pipeline. The DACN model is jointly optimized for the classification error and the image reconstruction error based on a multi-task learning procedure. We evaluate the proposed model using the publicly available ICPR2012 benchmark dataset. We show that this architecture is particularly effective when the training dataset is small which is often the case in medical imaging applications. We present experimental results to show that the proposed approach outperforms all known state of the art HEp-2 cell classification methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950689","source":"ieee","year":2017,"key":"7ff2f1e1-04ab-4a24-bb3b-0a6c0f120a19","use":1,"doi":"10.1109\/ISBI.2017.7950689"},{"Title":"A convolutional neural network approach for abnormality detection in Wireless Capsule Endoscopy","Description":"A. K. Sekuboyina,  S. T. Devarakonda,  C. S. Seelamantula","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In wireless capsule endoscopy (WCE), a swallowable miniature optical endoscope is used to transmit color images of the gastrointestinal tract. However, the number of images transmitted is large, taking a significant amount of the medical expert's time to review the scan. In this paper, we propose a technique to automate the abnormality detection in WCE images. We split the image into several patches and extract features pertaining to each block using a convolutional neural network (CNN) to increase their generality while overcoming the drawbacks of manually crafted features. We intend to exploit the importance of color information for the task. Experiments are performed to determine the optimal color space components for feature extraction and classifier design. We obtained an area under receiver-operating-characteristic (ROC) curve of approximately 0.8 on a dataset containing multiple abnormalities.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950698","source":"ieee","year":2017,"key":"4062c472-2db8-43ef-ab11-f88f91c48e61","use":1,"doi":"10.1109\/ISBI.2017.7950698"},{"Title":"FUIQA: Fetal Ultrasound Image Quality Assessment With Deep Convolutional Networks","Description":"L. Wu,  J. Z. Cheng,  S. Li,  B. Lei,  T. Wang,  D. Ni","ShortDetails":"IEEE Transactions on Cybernetics. 2017","abstract":"The quality of ultrasound (US) images for the obstetric examination is crucial for accurate biometric measurement. However, manual quality control is a labor intensive process and often impractical in a clinical setting. To improve the efficiency of examination and alleviate the measurement error caused by improper US scanning operation and slice selection, a computerized fetal US image quality assessment (FUIQA) scheme is proposed to assist the implementation of US image quality control in the clinical obstetric examination. The proposed FUIQA is realized with two deep convolutional neural network models, which are denoted as L-CNN and C-CNN, respectively. The L-CNN aims to find the region of interest (ROI) of the fetal abdominal region in the US image. Based on the ROI found by the L-CNN, the C-CNN evaluates the image quality by assessing the goodness of depiction for the key structures of stomach bubble and umbilical vein. To further boost the performance of the L-CNN, we augment the input sources of the neural network with the local phase features along with the original US data. It will be shown that the heterogeneous input sources will help to improve the performance of the L-CNN. The performance of the proposed FUIQA is compared with the subjective image quality evaluation results from three medical doctors. With comprehensive experiments, it will be illustrated that the computerized assessment with our FUIQA scheme can be comparable to the subjective ratings from medical doctors.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7875138","source":"ieee","year":2017,"key":"9eb23897-e5bb-4d28-931e-ff3cceff8a15","use":1,"doi":"10.1109\/TCYB.2017.2671898"},{"Title":"An Ensemble of Fine-Tuned Convolutional Neural Networks for Medical Image Classification","Description":"A. Kumar,  J. Kim,  D. Lyndon,  M. Fulham,  D. Feng","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than - ethods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7769199","source":"ieee","year":2017,"key":"07784d36-153c-49f2-a83f-23c09494bdbd","use":1,"doi":"10.1109\/JBHI.2016.2635663"},{"Title":"Unsupervised Joint Mining of Deep Features and Image Labels for Large-Scale Radiology Image Categorization and Scene Recognition","Description":"X. Wang,  L. Lu,  H. C. Shin,  L. Kim,  M. Bagheri,  I. Nogues,  J. Yao,  R. M. Summers","ShortDetails":"2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017","abstract":"The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of \"Google Search\" and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized \"convergence\" of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters\/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75:3%, compared to the state-of-the-art supervised classification accuracy of 81:0% (when both are based on the VGG-VD model).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7926699","source":"ieee","year":2017,"key":"637c6bb6-95e7-4dd3-b984-6e0143b6f516","use":1,"doi":"10.1109\/WACV.2017.116"},{"Title":"L-CNN: Exploiting labeling latency in a CNN learning framework","Description":"M. J. Afridi,  A. Ross,  E. M. Shapiro","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"A supervised learning system requires labeled data during the training phase. Obtaining labels can be an expensive process, especially in medical imaging applications where a qualified expert may be needed to carefully analyze images and annotate them. This constrains the amount of labeled data available. This study explores the possibility of incorporating labeling behavior (viz., labeling latency) in a supervised convolutional neural network (CNN) framework in order to improve its performance in the presence of limited labeled data. The problem of \u201cspot\u201d detection in MRI scans is considered in this work. In this two-class problem, (a) labeling behavior is available only during the training phase unlike traditional features that are available both during training and testing; and (b) the labeling behavior is associated with only one class (the positive samples) unlike other side information that is available for all classes. To address these issues, a new CNN architecture referred to as L-CNN is designed. The proposed method utilizes the labeling behavior of the expert to cluster the labeled data into multiple categories; a source CNN is then trained to distinguish between these categories. Next, a transfer learning paradigm is used where a target CNN is initialized using this source CNN and its weights updated with the limited labeled data that is available. Experimental results on an existing MRI database show that the proposed L-CNN performs better than a conventional CNN and, further, significantly outperforms the previous state-of-the-art, thereby establishing a new baseline for \u201cspot\u201d detection in MRI.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899955","source":"ieee","year":2016,"key":"c7ad23f1-f9ae-4bd0-96d2-583514f4e2b0","use":1,"doi":"10.1109\/ICPR.2016.7899955"},{"Title":"An efficient radiographic Image Retrieval system using Convolutional Neural Network","Description":"M. Chowdhury,  S. R. Bul\u00f2,  R. Moreno,  M. K. Kundu,  \u00d6. Smedby","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Content-Based Medical Image Retrieval (CBMIR) is an important research field in the context of medical data management. In this paper we propose a novel CBMIR system for the automatic retrieval of radiographic images. Our approach employs a Convolutional Neural Network (CNN) to obtain high-level image representations that enable a coarse retrieval of images that are in correspondence to a query image. The retrieved set of images is refined via a non-parametric estimation of putative classes for the query image, which are used to filter out potential outliers in favour of more relevant images belonging to those classes. The refined set of images is finally re-ranked using Edge Histogram Descriptor, i.e. a low-level edge-based image descriptor that allows to capture finer similarities between the retrieved set of images and the query image. To improve the computational efficiency of the system, we employ dimensionality reduction via Principal Component Analysis (PCA). Experiments were carried out to evaluate the effectiveness of the proposed system on medical data from the \u201cImage Retrieval in Medical Applications\u201d (IRMA) benchmark database. The obtained results show the effectiveness of the proposed CBMIR system in the field of medical image retrieval.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7900116","source":"ieee","year":2016,"key":"54dec2fc-e447-404f-b21f-e690d56c131f","use":1,"doi":"10.1109\/ICPR.2016.7900116"},{"Title":"Cardiac left ventricle segmentation using convolutional neural network regression","Description":"L. K. Tan,  Y. M. Liew,  E. Lim,  R. A. McLaughlin","ShortDetails":"2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES). 2016","abstract":"Cardiac MRI is important for the diagnosis and assessment of various cardiovascular diseases. Automated segmentation of the left ventricular (LV) endocardium at end-diastole (ED) and end-systole (ES) enables automated quantification of various clinical parameters including ejection fraction. Neural networks have been used for general image segmentation, usually via per-pixel categorization e.g. \u201cforeground\u201d and \u201cbackground\u201d. In this paper we propose that the generally circular LV endocardium can be parameterized and the endocardial contour determined via neural network regression. We designed two convolutional neural networks (CNN), one for localization of the LV, and the other for determining the endocardial radius. We trained the networks against 100 datasets from the Medical Image Computing and Computer Assisted Intervention (MICCAI) 2011 challenge, and tested the networks against 45 datasets from the MICCAI 2009 challenge. The networks achieved 0.88 average Dice metric, 2.30 mm average perpendicular distance, and 97.9% good contours, the latter being the highest published result to date. These results demonstrate that CNN regression is a viable and highly promising method for automated LV endocardial segmentation at ED and ES phases, and is capable of generalizing learning between highly distinct training and testing data sets.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7843499","source":"ieee","year":2016,"key":"7f61246e-45e5-4932-96a8-64336e195c7e","use":1,"doi":"10.1109\/IECBES.2016.7843499"},{"Title":"Dependency-based convolutional neural network for drug-drug interaction extraction","Description":"S. Liu,  Kai Chen,  Q. Chen,  B. Tang","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Drug-drug interactions (DDIs) are crucial for healthcare. Besides DDIs reported in medical knowledge bases such as DrugBank, a large number of latest DDI findings are also reported in unstructured biomedical literature. Extracting DDIs from unstructured biomedical literature is a worthy addition to the existing knowledge bases. Currently, convolutional neural network (CNN) is a state-of-the-art method for DDI extraction. One limitation of CNN is that it neglects long distance dependencies between words in candidate DDI instances, which may be helpful for DDI extraction. In order to incorporate the long distance dependencies between words in candidate DDI instances, in this work, we propose a dependency-based convolutional neural network (DCNN) for DDI extraction. Experiments conducted on the DDIExtraction 2013 corpus show that DCNN using a public state-of-the-art dependency parser achieves an F-score of 70.19%, outperforming CNN by 0.44%. By analyzing errors of DCNN, we find that errors from dependency parsers are propagated into DCNN and affect the performance of DCNN. To reduce error propagation, we design a simple rule to combine CNN with DCNN, that is, using DCNN to extract DDIs in short sentences and CNN to extract DDIs in long distances as most dependency parsers work well for short sentences but bad for long sentences. Finally, our system that combines CNN and DCNN achieves an F-score of 70.81%, outperforming CNN by 1.06% and DNN by 0.62% on the DDIExtraction 2013 corpus.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822671","source":"ieee","year":2016,"key":"3df0f1ca-a703-4f91-bc7d-b51e883db093","use":1,"doi":"10.1109\/BIBM.2016.7822671"},{"Title":"Deep learning-based pipeline to recognize Alzheimer's disease using fMRI data","Description":"S. Sarraf,  G. Tofighi","ShortDetails":"2016 Future Technologies Conference (FTC). 2016","abstract":"Over the past decade, machine learning techniques and in particular predictive modeling and pattern recognition in biomedical sciences, from drug delivery systems to medical imaging, have become one of the most important methods of assisting researchers in gaining a deeper understanding of issues in their entirety and solving complex medical problems. Deep learning is a powerful machine learning algorithm in classification that extracts low-to high-level features. In this paper, we employ a convolutional neural network to distinguish an Alzheimers brain from a normal, healthy brain. The importance of classifying this type of medical data lies in its potential to develop a predictive model or system in order to recognize the symptoms of Alzheimers disease when compared with normal subjects and to estimate the stages of the disease. Classification of clinical data for medical conditions such as Alzheimers disease has always been challenging, and the most problematic aspect has always been selecting the strongest discriminative features. Using the Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimers subjects from normal controls, where the accuracy of testing data reached 96.85%. This experiment suggests that the shift and scale invariant features extracted by CNN followed by deep learning classification represents the most powerful method of distinguishing clinical data from healthy data in fMRI. This approach also allows for expansion of the methodology to predict more complicated systems.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7821697","source":"ieee","year":2016,"key":"26f99a21-2cd7-41d6-9f08-b9a645cea617","use":1,"doi":"10.1109\/FTC.2016.7821697"},{"Title":"A deep tongue image features analysis model for medical application","Description":"Dan Meng,  Guitao Cao,  Y. Duan,  Minghua Zhu,  Liping Tu,  Jiatuo Xu,  D. Xu","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"With the improvement of people's living standards, there is no doubt that people are paying more and more attention to their health. However, shortage of medical resources is a critical global problem. As a result, an intelligent prognostics system has a great potential to play important roles in computer aided diagnosis. Numerous papers reported that tongue features have been closely related to a human's state. Among them, the majority of the existing tongue image analyses and classification methods are based on the low-level features, which may not provide a holistic view of the tongue. Inspired by a deep convolutional neural network (CNN), we propose a deep tongue image feature analysis system to extract unbiased features and reduce human labor for tongue diagnosis. With the unbalanced sample distribution, it is hard to form a balanced classification model based on feature representations obtained by existing low-level and high-level methods. Our proposed deep tongue image feature analysis model learns high-level features and provide more classification information during training time, which may result in higher accuracy when predicting testing samples. We tested the proposed system on a set of 267 gastritis patients, and a control group of 48 healthy volunteers (labeled according to Western medical practices). Test results show that the proposed deep tongue image feature analysis model can classify a given tongue image into healthy and diseased state with an average accuracy of 91.49%, which demonstrates the relationship between human body's state and its deep tongue image features.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822815","source":"ieee","year":2016,"key":"8f553f4f-6705-4e43-be8b-8f1b2cf2857e","use":1,"doi":"10.1109\/BIBM.2016.7822815"},{"Title":"EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos","Description":"A. P. Twinanda,  S. Shehata,  D. Mutter,  J. Marescaux,  M. de Mathelin,  N. Padoy","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, surgical phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the used visual features are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.","email":["anjany.sekuboyina@tum.de","ee13b1034@iith.ac.in","chandra.sekhar@ieee.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7519080","source":"ieee","year":2017,"key":"14e3c4ac-bc0b-4647-a720-3acd80a03b60","use":1,"doi":"10.1109\/TMI.2016.2593957"},{"Title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation","Description":"F. Milletari,  N. Navab,  S. A. Ahmadi","ShortDetails":"2016 Fourth International Conference on 3D Vision (3DV). 2016","abstract":"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.","email":["fausto.milletari@tum.de","navab@cs.tum.edu","ahmad.ahmadi@med.uni-muenchen.de"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7785132","source":"ieee","year":2016,"key":"4c992cf9-376f-4048-977a-2eba0a611c86","use":1,"doi":"10.1109\/3DV.2016.79"},{"Title":"Lesion border detection using deep learning","Description":"P. Sabouri,  H. GholamHosseini","ShortDetails":"2016 IEEE Congress on Evolutionary Computation (CEC). 2016","abstract":"Computer aided diagnosis of medical images can result in (better) detection in addition to early diagnosis of many symptoms to assist health physicians and therefore reducing the mortality rate. Realization of an efficient mobile device for automatic diagnosis of melanoma would greatly enhance the applicability of medical image classification scheme and make it useful in clinical contexts. In this paper, a deep learning method using convolutional neural networks (CNN) is proposed for border detection of skin lesions based on clinical images. Prepossessing of clinical and dermoscopy images has been common and necessary in the lesion segmentation realm; however, the result of the study shows that CNN can be used with relatively much less prepossessing algorithm compared with previous methods.","email":["hgholomh@aut.ac.nz"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7743955","source":"ieee","year":2016,"key":"ae61e133-45ad-4b84-9b37-3a06cc1e9f3f","use":1,"doi":"10.1109\/CEC.2016.7743955"},{"Title":"A multiclass classification method based on deep learning for named entity recognition in electronic medical records","Description":"X. Dong,  L. Qian,  Y. Guan,  L. Huang,  Q. Yu,  J. Yang","ShortDetails":"2016 New York Scientific Data Summit (NYSDS). 2016","abstract":"Research of named entity recognition (NER) on electrical medical records (EMRs) focuses on verifying whether methods to NER in traditional texts are effective for that in EMRs, and there is no model proposed for enhancing performance of NER via deep learning from the perspective of multiclass classification. In this paper, we annotate a real EMR corpus to accomplish the model training and evaluation. And, then, we present a Convolutional Neural Network (CNN) based multiclass classification method for mining named entities from EMRs. The method consists of two phases. In the phase 1, EMRs are pre-processed for representing samples with word embedding. In the phase 2, the method is built by segmenting training data into many subsets and training a CNN binary classification model on each of subset. Experimental results showed the effectiveness of our method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7747810","source":"ieee","year":2016,"key":"5b6e4248-4608-441d-85b0-4905581a9e6d","use":1,"doi":"10.1109\/NYSDS.2016.7747810"},{"Title":"Weakly-supervised Convolutional learning for detection of inflammatory gastrointestinal lesions","Description":"S. V. Georgakopoulos,  D. K. Iakovidis,  M. Vasilakakis,  V. P. Plagianakos,  A. Koulaouzidis","ShortDetails":"2016 IEEE International Conference on Imaging Systems and Techniques (IST). 2016","abstract":"Graphic image annotations provide the necessary ground truth information for supervised machine learning in image-based computer-aided medical diagnosis. Performing such annotations is usually a time-consuming and cost-inefficient process requiring knowledge from domain experts. To cope with this problem we propose a novel weakly-supervised learning method based on a Convolutional Neural Network (CNN) architecture. The advantage of the proposed method over conventional supervised approaches is that only image-level semantic annotations are used in the training process, instead of pixel-level graphic annotations. This can drastically reduce the required annotation effort. Its advantage over the few state-of-the-art weakly-supervised CNN architectures is its simplicity. The performance of the proposed method is evaluated in the context of computer-aided detection of inflammatory gastrointestinal lesions in wireless capsule endoscopy videos. This is a broad category of lesions, for which early detection and treatment can be of vital importance. The results show that the proposed weakly-supervised learning method can be more effective than the conventional supervised learning, with an accuracy of 90%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7738279","source":"ieee","year":2016,"key":"874abeb2-84e8-420e-b9af-55bfcf231116","use":1,"doi":"10.1109\/IST.2016.7738279"},{"Title":"Generating binary tags for fast medical image retrieval based on convolutional nets and Radon Transform","Description":"X. Liu,  H. R. Tizhoosh,  J. Kofman","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"Content-based image retrieval (CBIR) in large medical image archives is a challenging and necessary task. Generally, different feature extraction methods are used to assign expressive and invariant features to each image such that the search for similar images comes down to feature classification and\/or matching. The present work introduces a new image retrieval method for medical applications that employs a convolutional neural network (CNN) with recently introduced Radon barcodes. We combine neural codes for global classification with Radon barcodes for the final retrieval. We also examine image search based on regions of interest (ROI) matching after image retrieval. The IRMA dataset with more than 14,000 x-rays images is used to evaluate the performance of our method. Experimental results show that our approach is superior to many published works.","email":["x435liu@uwaterloo.ca","jkofman@uwaterloo.ca","tizhoosh@uwaterloo.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727562","source":"ieee","year":2016,"key":"011e6aab-4221-4bb8-998c-a0f494c125cd","use":1,"doi":"10.1109\/IJCNN.2016.7727562"},{"Title":"Brain MRI segmentation with patch-based CNN approach","Description":"Z. Cui,  J. Yang,  Y. Qiao","ShortDetails":"2016 35th Chinese Control Conference (CCC). 2016","abstract":"Brain Magnetic Resonance Image (MRI) plays a non-substitutive role in clinical diagnosis. The symptom of many diseases corresponds to the structural variants of brain. Automatic structure segmentation in brain MRI is of great importance in modern medical research. Some methods were developed for automatic segmenting of brain MRI but failed to achieve desired accuracy. In this paper, we proposed a new patch-based approach for automatic segmentation of brain MRI using convolutional neural network (CNN). Each brain MRI acquired from a small portion of public dataset is firstly divided into patches. All of these patches are then used for training CNN, which is used for automatic segmentation of brain MRI. Experimental results showed that our approach achieved better segmentation accuracy compared with other deep learning methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7554465","source":"ieee","year":2016,"key":"527e3baa-53c3-42ff-9d2d-0c0b2a354f14","use":1,"doi":"10.1109\/ChiCC.2016.7554465"},{"Title":"On the generality of neural image features","Description":"R. Venkatesan,  V. Gatupalli,  B. Li","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Often the filters learned by Convolutional Neural Networks (CNNs) from different image datasets appear similar. This similarity of filters is often exploited for the purposes of transfer learning. This is also being used as an initialization technique for different tasks in the same dataset or for the same task in similar datasets. Off-the-shelf CNN features have capitalized on this idea to promote their networks as best transferable and most general and are used in a cavalier manner in day-to-day computer vision tasks. While the filters learned by these CNNs are related to the atomic structures of the images from which they are learnt, all datasets learn similar looking low-level filters. With the understanding that a dataset that contains many such atomic structures learn general filters and are therefore useful to initialize other networks with, we propose a way to analyse and quantify generality. We applied this metric on several popular character recognition, natural image and a medical image dataset, and arrive at some interesting conclusions. On further experimentation we also discovered that particular classes in a dataset themselves are more general than others.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532315","source":"ieee","year":2016,"key":"41f05dca-cc21-4132-8b2a-766097b926bb","use":1,"doi":"10.1109\/ICIP.2016.7532315"},{"Title":"Colonic Polyp Classification with Convolutional Neural Networks","Description":"E. Ribeiro,  A. Uhl,  M. H\u00e4fner","ShortDetails":"2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS). 2016","abstract":"Texture patch classification is an important task in many different computer-aided medical systems. Convolutional Neural Networks (CNN's) have become state-of-the-art for many computer vision tasks in recent years. In this paper, we propose the use of CNN's for the automated classification of colonic mucosa for colon polyp staging in the context of colon cancer screening. This deep learning approach has the property of extracting features and classifying images in the same architecture by exploiting directly the input image pixels being successful in handling distortions such as different light conditions, presence of partial occlusions, etc. For this type of deep learning approach it is common to require that the database contains large amounts of data, which is quite rare in the medical field. The method proposed allows the use of small patches (subimages) to increase the size of the database as well to classify different regions in the same image. We show experimentally that this model is more efficient than some of the commonly used features for colonic polyp classification.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7545996","source":"ieee","year":2016,"key":"1d015917-4be3-45b5-b82b-4791f084756c","use":1,"doi":"10.1109\/CBMS.2016.39"},{"Title":"Deep vessel tracking: A generalized probabilistic approach via deep learning","Description":"A. Wu,  Z. Xu,  M. Gao,  M. Buty,  D. J. Mollura","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Analysis of vascular geometry is important in many medical imaging applications, such as retinal, pulmonary, and cardiac investigations. In order to make reliable judgments for clinical usage, accurate and robust segmentation methods are needed. Due to the high complexity of biological vasculature trees, manual identification is often too time-consuming and tedious to be used in practice. To design an automated and computerized method, a major challenge is that the appearance of vasculatures in medical images has great variance across modalities and subjects. Therefore, most existing approaches are specially designed for a particular task, lacking the flexibility to be adapted to other circumstances. In this paper, we present a generic approach for vascular structure identification from medical images, which can be used for multiple purposes robustly. The proposed method uses the state-of-the-art deep convolutional neural network (CNN) to learn the appearance features of the target. A Principal Component Analysis (PCA)-based nearest neighbor search is then utilized to estimate the local structure distribution, which is further incorporated within the generalized probabilistic tracking framework to extract the entire connected tree. Qualitative and quantitative results over retinal fundus data demonstrate that the proposed framework achieves comparable accuracy as compared with state-of-the-art methods, while efficiently producing more information regarding the candidate tree structure.","email":["ziyue.xu@nih.gov.ThisresearchissupportedbytheCenterforInfectiousDiseaseImaging"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493520","source":"ieee","year":2016,"key":"91877bf2-6e60-409c-9f7a-20d7268f08fd","use":1,"doi":"10.1109\/ISBI.2016.7493520"},{"Title":"A hybrid learning approach for semantic labeling of cardiac CT slices and recognition of body position","Description":"M. Moradi,  Y. Gur,  H. Wang,  P. Prasanna,  T. Syeda-Mahmood","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"We work towards efficient methods of categorizing visual content in medical images as a precursor step to segmentation and anatomy recognition. In this paper, we address the problem of automatic detection of level\/position for a given cardiac CT slice. Specifically, we divide the body area depicted in chest CT into nine semantic categories each representing an area most relevant to the study of a disease and\/or key anatomic cardiovascular feature. Using a set of handcrafted image features together with features derived form a deep convolutional neural network (CNN), we build a classification scheme to map a given CT slice to the relevant level. Each feature group is used to train a separate support vector machine classifier. The resulting labels are then combined in a linear model, also learned from training data. We report margin zero and margin one accuracy of 91.7% and 98.8% and show that this hybrid approach is a very effective methodology for assigning a given CT image to a relatively narrow anatomic window.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493533","source":"ieee","year":2016,"key":"369539b4-b6b9-4344-9a8e-7dd8280058d7","use":1,"doi":"10.1109\/ISBI.2016.7493533"},{"Title":"Discriminative feature extraction from X-ray images using deep convolutional neural networks","Description":"M. Srinivas,  D. Roy,  C. K. Mohan","ShortDetails":"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2016","abstract":"Feature extraction is one of the most important phases of medical image classification which requires extensive domain knowledge. Convolutional Neural Networks (CNN) have been successfully used for feature extraction in images from different domains involving a lot of classes. In this paper, CNNs are exploited to extract a hierarchical and discriminative representation of X-ray images. This representation is then used for classification of the X-ray images as various parts of the body. Visualization of the feature maps in the hidden layers show that features learnt by the CNN resemble the essential features which help discern the discrimination among different body parts. A comparison on the standard IRMA X-ray image dataset demonstrates that the CNNs easily outperform classifiers with hand-engineered features.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7471809","source":"ieee","year":2016,"key":"2ae2f0f3-b3bb-4216-97dc-19da78bea993","use":1,"doi":"10.1109\/ICASSP.2016.7471809"},{"Title":"Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images","Description":"M. J. J. P. van Grinsven,  B. van Ginneken,  C. B. Hoyng,  T. Theelen,  C. I. S\u00e1nchez","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.","email":["Mark.vanGrinsven@radboudumc.nl."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7401052","source":"ieee","year":2016,"key":"088ab5f2-9d24-4716-8672-2e2854787b7b","use":1,"doi":"10.1109\/TMI.2016.2526689"},{"Title":"Multi-Instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition","Description":"Z. Yan,  Y. Zhan,  Z. Peng,  S. Liao,  Y. Shinagawa,  S. Zhang,  D. N. Metaxas,  X. S. Zhou","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. \u201cBodypart identity\u201d of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.","email":["yiqiang.zhan@siemens.com","dnm@cs.rutgers.edu","szhang16@uncc.edu","pubs-permissions@ieee.org.","maps10@84x84C3","maps30@36x36S2","maps10@42x42S4","maps30@18x18Convolutions10","7x7Convolutions30@7x7Max-pooling2x2Max-pooling2x2H5"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7398101","source":"ieee","year":2016,"key":"b99f88e3-838e-4cad-bdc2-e9aa63b191d6","use":1,"doi":"10.1109\/TMI.2016.2524985"},{"Title":"Convolutional Neural Networks for Branch Retinal Vein Occlusion recognition?","Description":"R. Zhao,  Z. Chen,  Z. Chi","ShortDetails":"2015 IEEE International Conference on Information and Automation. 2015","abstract":"Branch Retinal Vein Occlusion (BRVO) is one of the most common retinal diseases that could impair people's vision seriously if it is not timely diagnosed and treated. It would save a lot of time and money for both medical institutions and patients if BRVO could be well recognized automatically. In this paper, we propose to exploit Convolutional Neural Networks (CNN) for BRVO recognition. We propose patch-based method and image-based voting method to implement the recognition. As it could learn abstract and useful features, CNN can achieve a high recognition accuracy. The accuracy of CNN is over 97%. Experimental results demonstrate the efficiency of our proposed CNN based methods for BRVO recognition.","email":["chi.zheru@polyu.edu.hk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7279547","source":"ieee","year":2015,"key":"c3266ab4-be2d-4e8c-8cb4-0d9648780257","use":1,"doi":"10.1109\/ICInfA.2015.7279547"},{"Title":"Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation","Description":"Y. Xu,  Z. Jia,  Y. Ai,  F. Zhang,  M. Lai,  E. I. C. Chang","ShortDetails":"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2015","abstract":"We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7178109","source":"ieee","year":2015,"key":"6a3fa06b-cd73-4ee2-94aa-942ee6a5a28a","use":1,"doi":"10.1109\/ICASSP.2015.7178109"},{"Title":"Chest pathology detection using deep learning with non-medical training","Description":"Y. Bar,  I. Diamant,  L. Wolf,  S. Lieberman,  E. Konen,  H. Greenspan","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.","email":["xuyan04@gmail.com","lmd@zju.edu.cnABSTRACTWeproposeasimple"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163871","source":"ieee","year":2015,"key":"abc3762d-3978-400e-b7d5-d6a7863bd669","use":1,"doi":"10.1109\/ISBI.2015.7163871"},{"Title":"Automated anatomical landmark detection ondistal femur surface using convolutional neural network","Description":"D. Yang,  S. Zhang,  Z. Yan,  C. Tan,  K. Li,  D. Metaxas","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Accurate localization of the anatomical landmarks on distal femur bone in the 3D medical images is very important for knee surgery planning and biomechanics analysis. However, the landmark identification process is often conducted manually or by using the inserted auxiliaries, which is time-consuming and lacks of accuracy. In this paper, an automatic localization method is proposed to determine positions of initial geometric landmarks on femur surface in the 3D MR images. Based on the results from the convolutional neural network (CNN) classifiers and shape statistics, we use the narrow-band graph cut optimization to achieve the 3D segmentation of femur surface. Finally, the anatomical landmarks are located on the femur according to the geometric cues of surface mesh. Experiments demonstrate that the proposed method is effective, efficient, and reliable to segment femur and locate the anatomical landmarks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163806","source":"ieee","year":2015,"key":"f7d3f674-5f48-455a-b35f-38bed18eafa3","use":1,"doi":"10.1109\/ISBI.2015.7163806"},{"Title":"Medical image classification with convolutional neural network","Description":"Q. Li,  W. Cai,  X. Wang,  Y. Zhou,  D. D. Feng,  M. Chen","ShortDetails":"2014 13th International Conference on Control Automation Robotics & Vision (ICARCV). 2014","abstract":"Image patch classification is an important task in many different medical imaging applications. In this work, we have designed a customized Convolutional Neural Networks (CNN) with shallow convolution layer to classify lung image patches with interstitial lung disease (ILD). While many feature descriptors have been proposed over the past years, they can be quite complicated and domain-specific. Our customized CNN framework can, on the other hand, automatically and efficiently learn the intrinsic image features from lung image patches that are most suitable for the classification purpose. The same architecture can be generalized to perform other medical image or texture classification tasks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7064414","source":"ieee","year":2014,"key":"32635294-1eb3-4653-80b9-6dfad10183bf","use":1,"doi":"10.1109\/ICARCV.2014.7064414"},{"Title":"Cellular neural networks assisted automatic detection of elements in microscopic medical images. A preliminary study","Description":"C. Botoca","ShortDetails":"2014 11th International Symposium on Electronics and Telecommunications (ISETC). 2014","abstract":"This paper presents a new algorithm for object recognition in medical microscopic images, assisted by a cellular neural network (CNN). The CNN flowchart and its component parts are described based on successions of interconnections templates. The experiments results are shown and they appear to be promising. Our results sustain the usability of CNN as a real time processing tool for assisting the medical act.","email":["corina.botoca@upt.ro"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7010801","source":"ieee","year":2014,"key":"f5936075-ef4d-44ee-be8c-5f5766320042","use":1,"doi":"10.1109\/ISETC.2014.7010801"},{"Title":"cellular neural network based medical image segmentation using artificial bee colony algorithm","Description":"M. Duraisamy,  F. M. M. Jane","ShortDetails":"2014 International Conference on Green Computing Communication and Electrical Engineering (ICGCCEE). 2014","abstract":"Magnetic Resonance Imaging (MRI) has become an efficient instrument for clinical diagnoses and research in recent years. It has become a very useful medical modality for the detection of various diseases through segmentation methods. In this paper, we have presented an effective CNN based segmentation method with lung and brain MRI images. This approach hits the target with the aid of the following major steps, which includes, 1) Preprocessing of the brain and lung images, 2) Segmentation using cellular neural network. Initially, the MRI image is pre-processed to make it fit for segmentation. Here, in the pre-processing step, image de-noising is done using the linear smoothing filters, such as Gaussian Filter. Then, the pre-processed image is segmented according to our proposed technique, CNN-based image segmentation. Finally, the different MRI images (brain and lung) are given to the proposed approach to evaluate the performance of the proposed approach in segmentation process. The Comparative analysis is carried out Fuzzy C-means (FCM) and K-means classification. From the comparative analysis, the accuracy of proposed segmentation approach produces better results (83.7% for lung and 93% for brain images) than that of existing Fuzzy C-means (FCM) and K-means classification.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6922413","source":"ieee","year":2014,"key":"787d8ce8-4119-468e-a338-7d7897e7e6d7","use":1,"doi":"10.1109\/ICGCCEE.2014.6922413"},{"Title":"Intervertebral disc detection in X-ray images using faster R-CNN","Description":"R. Sa,  W. Owens,  R. Wiegand,  M. Studin,  D. Capoferri,  K. Barooha,  A. Greaux,  R. Rattray,  A. Hutton,  J. Cintineo,  V. Chaudhary","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Automatic identification of specific osseous landmarks on the spinal radiograph can be used to automate calculations for correcting ligament instability and injury, which affect 75% of patients injured in motor vehicle accidents. In this work, we propose to use deep learning based object detection method as the first step towards identifying landmark points in lateral lumbar X-ray images. The significant breakthrough of deep learning technology has made it a prevailing choice for perception based applications, however, the lack of large annotated training dataset has brought challenges to utilizing the technology in medical image processing field. In this work, we propose to fine tune a deep network, Faster-RCNN, a state-of-the-art deep detection network in natural image domain, using small annotated clinical datasets. In the experiment we show that, by using only 81 lateral lumbar X-Ray training images, one can achieve much better performance compared to traditional sliding window detection method on hand crafted features. Furthermore, we fine-tuned the network using 974 training images and tested on 108 images, which achieved average precision of 0.905 with average computation time of 3 second per image, which greatly outperformed traditional methods in terms of accuracy and efficiency.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036887","source":"ieee","year":2017,"key":"ebe32e30-5433-478a-b03d-e27282c07757","use":1,"doi":"10.1109\/EMBC.2017.8036887"},{"Title":"Surgical-tools detection based on Convolutional Neural Network in laparoscopic robot-assisted surgery","Description":"B. Choi,  K. Jo,  S. Choi,  J. Choi","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Laparoscopic surgery, a type of minimally invasive surgery, is used in a variety of clinical surgeries because it has a faster recovery rate and causes less pain. However, in general, the robotic system used in laparoscopic surgery can cause damage to the surgical instruments, organs, or tissues during surgery due to a narrow field of view and operating space, and insufficient tactile feedback. This study proposes real-time models for the detection of surgical instruments during laparoscopic surgery by using a CNN(Convolutional Neural Network). A dataset included information of the 7 surgical tools is used for learning CNN. To track surgical instruments in real time, unified architecture of YOLO apply to the models. So as to evaluate performance of the suggested models, degree of recall and precision is calculated and compared. Finally, we achieve 72.26% mean average precision over our dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037183","source":"ieee","year":2017,"key":"567223c4-0bf7-4545-9edd-0fb6df868a85","use":1,"doi":"10.1109\/EMBC.2017.8037183"},{"Title":"Cerebral Micro-Bleed Detection Based on the Convolution Neural Network With Rank Based Average Pooling","Description":"S. Wang,  Y. Jiang,  X. Hou,  H. Cheng,  S. Du","ShortDetails":"IEEE Access. 2017","abstract":"Cerebral micro-bleed (CMB) is small perivascular hemosiderin deposits from leakage through cerebral small vessels. They can result from cerebra-vascular disease, dementia, or simply from normal aging. It can be visualized via the susceptibility weighted imaging (SWI). Based on the SWI, we propose to use different structures of the CNN with rank-based average pooling to detect the CMB, and compare this method used in this paper to the current state-of-the-art methods. We can find that the CNN with five layers obtains the best performance, with a sensitivity of 96.94%, a specificity of 97.18%, and an accuracy of 97.18%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8013653","source":"ieee","year":2017,"key":"a76d39f7-67fa-418b-b953-bdb8a3200bfd","use":1,"doi":"10.1109\/ACCESS.2017.2736558"},{"Title":"Nursing-care text classification using word vector representation and convolutional neural networks","Description":"M. Nii,  Y. Tsuchida,  Y. Kato,  A. Uchinuno,  R. Sakashita","ShortDetails":"2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS). 2017","abstract":"In this paper, we propose a convolutional neural network (CNN) based classification method for nursing-care classification. CNNs have obtained strong performance in computer vision speech recognition areas. Recently, CNNs have been also applied sentence classification. We have studied nursing-care text classification [6]-[18]. In our former works, we proposed several types of feature definitions and examined some classification models. In this paper, each text is represented as a concatenated word vector. Then, every text is classified using CNN-based classification methods. We examined some classification models at the classification layer in CNNs. From our experimental results, the proposed CNN-based method obtained better performance than our former works.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8023240","source":"ieee","year":2017,"key":"d174a1ea-0354-44e9-89e0-e148e2d03a3f","use":1,"doi":"10.1109\/IFSA-SCIS.2017.8023240"},{"Title":"Deep Learning for Categorization of Lung Cancer CT Images","Description":"A. M. Rossetto,  W. Zhou","ShortDetails":"2017 IEEE\/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2017","abstract":"Lung cancer is a serious health problem. In the United States alone, approximately 225,000 people each year are diagnosed with lung cancer. Early detection is a crucial part of giving patients the best chance of recovery. Deep learning gives us an opportunity to increase the accuracy of the automated initial diagnosis. Here we present an ensemble of Convolution Neural Networks(CNN) using multiple preprocessing methods to increase the accuracy of the automated labeling of the scans. We have done this by implementing ensembles of CNNs along with a voting system to get the consensus of the two networks. The initial results of our best method show both a consistently high accuracy (97.5%) and a low percentage of false positives (<;10%).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8010653","source":"ieee","year":2017,"key":"8d98e0d0-7b18-4a1b-9a51-129c6c2f38e3","use":1,"doi":"10.1109\/CHASE.2017.98"},{"Title":"Learning to Read Chest X-Ray Images from 16000+ Examples Using CNN","Description":"Y. Dong,  Y. Pan,  J. Zhang,  W. Xu","ShortDetails":"2017 IEEE\/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2017","abstract":"Chest radiography (chest X-ray) is a low-cost yet effective and widely used medical imaging procedures. The lacking of qualified radiologist seriously limits the applicability of the technique. We explore the possibility of designing a computer-aided diagnosis for chest X-rays using deep convolutional neural networks. Using a real-world dataset of 16,000 chest X-rays with natural language diagnosis reports, we can train a multi-class classification model from images and preform accurate diagnosis, without any prior domain knowledge.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8010614","source":"ieee","year":2017,"key":"dac21ea7-d3c1-41a5-a55a-7d51428972cd","use":1,"doi":"10.1109\/CHASE.2017.59"},{"Title":"Deep learning for tumour classification in homogeneous breast tissue in medical microwave imaging","Description":"B. Gerazov,  R. C. Conceicao","ShortDetails":"IEEE EUROCON 2017 -17th International Conference on Smart Technologies. 2017","abstract":"Deep learning has become the state-of-the-art in the area of biomedical imaging, leading to a large boost in performance that approaches human levels. Medical microwave imaging is an emerging technology that has great potential especially in the area of breast cancer diagnosis. Moreover, the obtained backscatter signals have also been shown to be a good basis for differentiating malignant and benign tumour type. We further analyse these results by applying deep learning methods to a dataset of Finite Difference Time Domain (FDTD) numerical simulations of tumour models embedded in homogeneous breast adipose tissue. Specifically we use Deep and Convolutional Neural Networks and obtain an accuracy of 93.44% which outperforms conventional machine learning previously used on the analysed dataset.","email":["gerazov@feit.ukim.edu.mk","raquelcruzconceicao@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8011175","source":"ieee","year":2017,"key":"a60ce26d-7e7f-4f56-851c-8f6929ed4526","use":1,"doi":"10.1109\/EUROCON.2017.8011175"},{"Title":"A medical image fusion method based on convolutional neural networks","Description":"Y. Liu,  X. Chen,  J. Cheng,  H. Peng","ShortDetails":"2017 20th International Conference on Information Fusion (Fusion). 2017","abstract":"Medical image fusion technique plays an an increasingly critical role in many clinical applications by deriving the complementary information from medical images with different modalities. In this paper, a medical image fusion method based on convolutional neural networks (CNNs) is proposed. In our method, a siamese convolutional network is adopted to generate a weight map which integrates the pixel activity information from two source images. The fusion process is conducted in a multi-scale manner via image pyramids to be more consistent with human visual perception. In addition, a local similarity based strategy is applied to adaptively adjust the fusion mode for the decomposed coefficients. Experimental results demonstrate that the proposed method can achieve promising results in terms of both visual quality and objective assessment.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8009769","source":"ieee","year":2017,"key":"aaa20839-51fc-4026-94cd-04286660dd5d","use":1,"doi":"10.23919\/ICIF.2017.8009769"},{"Title":"EEG-based biometric identification with deep learning","Description":"Z. Mao,  W. X. Yao,  Y. Huang","ShortDetails":"2017 8th International IEEE\/EMBS Conference on Neural Engineering (NER). 2017","abstract":"Despite the recent increasing interest in biometric identification using electroencephalogram (EEG) signals, the state of the art still lacks a simple and robust model that is useful in real applications. This work proposes a new approach based on convolutional neural network CNN. The proposed CNN works directly on raw EEG data, thus alleviating the need for engineering features. We investigate the performance of the CNN on datasets of 100 subjects collected from one driving fatigue experiment. Our results show that the CNN model is fast highly efficient in training (<;0.5h on >100K training epochs) and highly robust, achieving 97% accuracy in identifying ~14K testing epochs from 100 subjects with non-time-locked natural driving fatigue data and much higher than from randomly sampled epochs (90%). Overall, this work demonstrates the potential of deep learning solutions for real-life EEG-based biometric identification.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8008425","source":"ieee","year":2017,"key":"603e3cd8-453a-4b4b-85b1-4aadba9d92f4","use":1,"doi":"10.1109\/NER.2017.8008425"},{"Title":"Super-resolution of Magnetic Resonance Images using deep Convolutional Neural Networks","Description":"K. Srinivasan,  A. Ankur,  A. Sharma","ShortDetails":"2017 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW). 2017","abstract":"This research focuses on developing a Super-resolution magnetic resonance (MR) Image restoration method using Convolutional Neural Networks (CNN). The main aim is to train an end to end mapping that takes low-resolution image as input and returns a high-resolution output. Low overhead and a state of the art reconstruction makes the model perform efficiently.","email":["kathiravan@niu.edu.tw","y13uc058@lnmiit.ac.in","y13uc025@lnmiit.ac.in"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7990985","source":"ieee","year":2017,"key":"c28e9e72-7871-4b05-a87a-b26b4fdfb3e5","use":1,"doi":"10.1109\/ICCE-China.2017.7990985"},{"Title":"Automatic 1D convolutional neural network-based detection of artifacts in MEG acquired without electrooculography or electrocardiography","Description":"P. Garg,  E. Davenport,  G. Murugesan,  B. Wagner,  C. Whitlow,  J. Maldjian,  A. Montillo","ShortDetails":"2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI). 2017","abstract":"Magnetoencephalography (MEG) is a functional neuroimaging tool that records the magnetic fields induced by electrical neuronal activity; however, signal from non-neuronal sources can corrupt the data. Eye-Blinks (EB) and Cardiac Activity (CA) are two of the most common types of non-neuronal artifacts. They can be measured by affixing eye proximal electrodes, as in electrooculography (EOG) and chest electrodes, as in electrocardiography (EKG), however this complicates imaging setup, decreases patient comfort, and often induces further artifacts from facial twitching and postural muscle movement. We propose an EOG- and EKG-free approach to identify eye-blink, cardiac, or neuronal signals for automated artifact suppression. Our contributions are two-fold. First, we combine a data driven, multivariate decomposition approach based on Independent Component Analysis (ICA) and a highly accurate classifier constructed as a deep 1-D Convolutional Neural Network. Second, we visualize the features learned to reveal what features the model uses and to bolster user confidence in our model's training and potential for generalization. We train and test three variants of our method on resting state MEG data from 49 subjects. Our cardiac model achieves a 96% sensitivity and 99% specificity on the set-aside test-set. Our eye-blink model achieves a sensitivity of 85% and specificity of 97%. This work facilitates automated MEG processing for both, clinical and research use, and can obviate the need for EOG or EKG electrodes.","email":["Albert.Montillo@UTSouthwestern.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7981506","source":"ieee","year":2017,"key":"a4dde216-92e3-4159-bacd-208006241be7","use":1,"doi":"10.1109\/PRNI.2017.7981506"},{"Title":"Tongue shape classification integrating image preprocessing and Convolution Neural Network","Description":"C. M. Huo,  H. Zheng,  H. Y. Su,  Z. L. Sun,  Y. J. Cai,  Y. F. Xu","ShortDetails":"2017 2nd Asia-Pacific Conference on Intelligent Robot Systems (ACIRS). 2017","abstract":"Tongue diagnosis is one of the most important parts in \u201cinspection diagnosis\u201d of Traditional Chinese Medicine (TCM). Observing tongue shape can help to understand the changes in human body and thereby to estimate the illness. This paper presents a method of recognizing tongue shapes based on Convolution Neural Network. The proposed method enhances the features of tongue images with preprocessing to ensure the data suitable for tongue shape binary classification. In view of the special texture and outline of tongue, the whole tongue images of dot-sting tongue and fissured tongue is transformed by Gabor filter, and the tooth-marked are processed by boundary detection approach. CNN is adopted because it has achieved remarkable results in computer vision and pattern recognition, and the model training through neural network coincides with the Chinese medicine dialectics through experience. Based on commonly used Alex-net, network is optimized with batch normalization to improve efficiency. The experimental results indicate that the preprocessing methods increase the accuracy and decreases the time of training process of tongue shape classification, which proves that the method is effective for the recognition of different tongue shapes.","email":["1874111766@qq.com","sapphire@bit.edu.cn","hongzheng@bit.edu.cn","henrysu@bit.edu.cn","sunzl8417@163.com","2120160980@bit.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7986062","source":"ieee","year":2017,"key":"ec3f15f0-6c20-4e9d-a864-f8ce2568fd1b","use":1,"doi":"10.1109\/ACIRS.2017.7986062"},{"Title":"Segmentation of Fetal Left Ventricle in Echocardiographic Sequences Based on Dynamic Convolutional Neural Networks","Description":"L. Yu,  Y. Guo,  Y. Wang,  J. Yu,  P. Chen","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2017","abstract":"Segmentation of fetal left ventricle (LV) in echocardiographic sequences is important for further quantitative analysis of fetal cardiac function. However, image gross inhomogeneities and fetal random movements make the segmentation a challenging problem. In this paper, a dynamic convolutional neural networks (CNN) based on multiscale information and fine-tuning is proposed for fetal LV segmentation. The CNN is pretrained by amount of labeled training data. In the segmentation, the first frame of each echocardiographic sequence is delineated manually. The dynamic CNN is fine-tuned by deep tuning with the first frame and shallow tuning with the rest of frames, respectively, to adapt to the individual fetus. Additionally, to separate the connection region between LV and left atrium (LA), a matching approach, which consists of block matching and line matching, is used for mitral valve (MV) base points tracking. Advantages of our proposed method are compared with an active contour model (ACM), a dynamical appearance model (DAM), and a fixed multiscale CNN method. Experimental results in 51 echocardiographic sequences show that the segmentation results agree well with the ground truth, especially in the cases with leakage, blurry boundaries, and subject-to-subject variations. The CNN architecture can be simple, and the dynamic fine-tuning is efficient.","email":["yywang@fudan.edu.cn","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7744576","source":"ieee","year":2017,"key":"98a1ce7d-5ad9-491d-b09b-5056d9badf6f","use":1,"doi":"10.1109\/TBME.2016.2628401"},{"Title":"Computer aided diagnosis in digital pathology application: Review and perspective approach in lung cancer classification","Description":"A. K. AlZubaidi,  F. B. Sideseq,  A. Faeq,  M. Basil","ShortDetails":"2017 Annual Conference on New Trends in Information & Communications Technology Applications (NTICT). 2017","abstract":"This electronic document is a \u201clive\u201d template and already defines the components of your paper [title, text, heads, etc.] in its style sheet This paper provide a broad review for most important algorithms used in the CAD application for lung tissue diagnostics and highlighted the performance of each distinctive algorithm. Moreover, ROC characteristics have been made for each selected algorithms (support vector machine (SVM), Fuzzy C-mean (FCM), Conventional Neural network (CNN) and CAD-FCM). The features for each algorithm discussed and related performance in clinical aided diagnosis (CAD) discussed and explained. Moreover, comparison of different research groups has been made to spotlight each criterion for different algorithms and approach used in CAD platforms in lung cancer. Finally, limitation and constrains for these algorithms has been discussed in order to optimize performance for each of these algorithms.","email":["ahad.sedeeq@al-bahja.com","abbas_khudair@yahoo.com","ahmed.f.h.1976@gmail","Abbas.khudair@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7976109","source":"ieee","year":2017,"key":"dcb1cfc0-d2e3-4698-b7e2-31432694073a","use":1,"doi":"10.1109\/NTICT.2017.7976109"},{"Title":"Semantic segmentation of microscopic images of H&E stained prostatic tissue using CNN","Description":"J. Isaksson,  I. Arvidsson,  K. \u00c5astr\u00f6m,  A. Heyden","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"There is a need for an automatic Gleason scoring system that can be used for prostate cancer diagnosis. Today the diagnoses are determined by pathologists manually, which is both a complex and a time-consuming task. To reduce the pathologists' workload, but also to reduce variations between different pathologists, an automatic classification system would be of great use. Some previous works have aimed for this, but still more work needs to be done. It is probable that such a tool would benefit from having access to individually segmented, pathologically relevant objects from the images. Therefore, we have developed an algorithm for semantic segmentation of the microscopic images of H&E stained prostate tissue into Background, Stroma, Epithelial Cytoplasm and Nuclei. This algorithm is based on deep learning, or more specifically a convolutional neural network. The network design is inspired by architectures that previously have been proved successful in different applications. It consists of a contracting and an expanding part, which are symmetrical. We have reached an accuracy of 80 %, as measured by the mean of the intersection over union, for segmentation into four classes. Previous works have only investigated nuclei segmentation, and our network performed similar but for the more challenging task of four class segmentation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7965996","source":"ieee","year":2017,"key":"296152df-3c64-4446-a885-5f3fa4205adc","use":1,"doi":"10.1109\/IJCNN.2017.7965996"},{"Title":"Deeply-supervised CNN for prostate segmentation","Description":"Q. Zhu,  B. Du,  B. Turkbey,  P. L. Choyke,  P. Yan","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided intervention. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.","email":["ida.arvidsson@math.lth.se"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7965852","source":"ieee","year":2017,"key":"e0bc08d7-962c-4245-b781-f55e1f8aa40d","use":1,"doi":"10.1109\/IJCNN.2017.7965852"},{"Title":"Similarities and differences between stimulus tuning in the inferotemporal visual cortex and convolutional networks","Description":"B. P. Tripp","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"Deep convolutional neural networks (CNNs) trained for object classification have a number of striking similarities with the primate ventral visual stream. In particular, activity in early, intermediate, and late layers is closely related to activity in V1, V4, and the inferotemporal cortex (IT). This study further compares activity in late layers of object-classification CNNs to activity patterns reported in the IT electrophysiology literature. There are a number of close similarities, including the distributions of population response sparseness across stimuli, and the distribution of size tuning bandwidth. Statisics of scale invariance, responses to clutter and occlusion, and orientation tuning are less similar. Statistics of object selectivity are quite different. These results agree with recent studies that highlight strong parallels between object-categorization CNNs and the ventral stream, and also highlight differences that could perhaps be reduced in future CNNs.","email":["bptripp@uwaterloo.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7966303","source":"ieee","year":2017,"key":"642acf00-5a02-42e3-bd17-c0048a1d4261","use":1,"doi":"10.1109\/IJCNN.2017.7966303"},{"Title":"Multi-Task Convolutional Neural Network for Patient Detection and Skin Segmentation in Continuous Non-Contact Vital Sign Monitoring","Description":"S. Chaichulee,  M. Villarroel,  J. Jorge,  C. Arteta,  G. Green,  K. McCormick,  A. Zisserman,  L. Tarassenko","ShortDetails":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). 2017","abstract":"Patient detection and skin segmentation are important steps in non-contact vital sign monitoring as skin regions contain pulsatile information required for the estimation of vital signs such as heart rate, respiratory rate and peripheral oxygen saturation (SpO2). Previous methods based on face detection or colour-based image segmentation are less reliable in a hospital setting. In this paper, we develop a multi-task convolutional neural network (CNN) for detecting the presence of a patient and segmenting the patient's skin regions. The multi-task model has a shared core network with two branches: a segmentation branch which was implemented using a fully convolutional network, and a classification branch which was implemented using global average pooling. The whole network was trained using images from a clinical study conducted in the neonatal intensive care unit (NICU) of the John Radcliffe hospital, Oxford, UK. Our model can produce accurate results and is robust to changes in different skin tones, pose variations, lighting variations, and routine interaction of clinical staff.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961751","source":"ieee","year":2017,"key":"285c1967-e0ca-4ffc-919a-d51a88cf466a","use":1,"doi":"10.1109\/FG.2017.41"},{"Title":"A deep learning based approach for classification of CerbB2 tumor cells in breast cancer","Description":"G. A. Tataro\u011flu,  A. Gen\u00e7,  K. A. Kabak\u00e7\u0131,  A. \u00c7apar,  B. U. T\u00f6reyin,  H. K. Ekenel,  \u0130. T\u00fcrkmen,  A. \u00c7ak\u0131r","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"This study proposes a unique approach to classify CerbB2 tumor cell scores in breast cancer based on deep learning models. Another contribution of the study is the creation of a dataset from original breast cancer tissues. On the purpose of training, validating and testing with deep learning models cell fragments were generated from sample tissue images. CerbB2 tumor scores were generated for the cell fragments were classified with high performance by the aid of convolutional neural networks (CNN).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960587","source":"ieee","year":2017,"key":"2f514f1f-e9aa-488d-bdb0-de3d1bc15367","use":1,"doi":"10.1109\/SIU.2017.7960587"},{"Title":"Segmentation of precursor lesions in cervical cancer using convolutional neural networks","Description":"A. Albayrak,  A. \u00dcnl\u00fc,  N. \u00c7al\u0131k,  G. Bilgin,  \u0130. T\u00fcrkmen,  A. \u00c7ak\u0131r,  A. \u00c7apar,  B. U. T\u00f6reyin,  L. D. Ata","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"Cervical carcinoma is one of the frequently seen cancers in the world and in our country, develops from precursor lesions. These precursor lesions are analyzed by pathologists so that the diagnosis of the disease can be made. In this study, a system that performs automatic detection of pre-cancerous lesions was performed using the convolutional neural networks (CNNs). In the training phase, lesion recognition performance of the proposed system has reached 92%. Thereafter, whole image was segmented by using 60 \u00d7 60 pixel tiles during the training phase. After all, the precursor lesions were segmented with 81.71% Dice coefficient.","email":["albayrak@yildiz.edu.tr","aunlu@medipol.edu.tr","gbilgin@yildiz.edu.tr","iturkmen@medipol.edu.tr","capar@itu.edu.tr","toreyin@itu.edu.tr","ncalik@yildiz.edu.tr","acakir@medipol.edu.tr","durakata@itu.edu.tr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960459","source":"ieee","year":2017,"key":"a975aed5-6091-45ea-912f-9f5989912eab","use":1,"doi":"10.1109\/SIU.2017.7960459"},{"Title":"Cerebral vessel classification with convolutional neural networks","Description":"Y. H. \u015eahin,  G. \u00dcnal","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"Analysing brain magnetic resonance angiography (MRA) images is important for detecting arteriovenous malformations and aneurysms. To detect these diseases, extracting the vessel structure in the image can be seen as a first step. In this paper, it was aimed to classify the cubic image parts obtained from brain MRA images according to whether they belong to vein structure or not. For this purpose, a 9 layers deep convolutional neural network (CNN) architecture is designed. With the model trained using this architecture, 85% accuracy was obtained in the classification performed on the test data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960697","source":"ieee","year":2017,"key":"a907ed93-a51d-4212-8371-9e720f74ef2c","use":1,"doi":"10.1109\/SIU.2017.7960697"},{"Title":"Auto-context Convolutional Neural Network (Auto-Net) for Brain Extraction in Magnetic Resonance Imaging","Description":"S. S. M. Salehi,  D. Erdogmus,  A. Gholipour","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Brain extraction or whole brain segmentation is an important first step in many of the neuroimage analysis pipelines. The accuracy and robustness of brain extraction, therefore, is crucial for the accuracy of the entire brain analysis process. State-of-the-art brain extraction techniques rely heavily on the accuracy of alignment or registration between brain atlases and query brain anatomy, and\/or make assumptions about the image geometry; therefore have limited success when these assumptions do not hold or image registration fails. With the aim of designing an accurate, learning-based, geometry-independent and registration-free brain extraction tool in this study, we present a technique based on an auto-context convolutional neural network (CNN), in which intrinsic local and global image features are learned through 2D patches of different window sizes. We consider two different architectures: 1) a voxelwise approach based on three parallel 2D convolutional pathways for three different directions (axial, coronal, and sagittal) that implicitly learn 3D image information without the need for computationally expensive 3D convolutions, and 2) a fully convolutional network based on the U-net architecture. Posterior probability maps generated by the networks are used iteratively as context information along with the original image patches to learn the local shape and connectedness of the brain to extract it from non-brain tissue. The brain extraction results we have obtained from our CNNs are superior to the recently reported results in the literature on two publicly available benchmark datasets, namely LPBA40 and OASIS, in which we obtained Dice overlap coefficients of 97.73% and 97.62%, respectively. Significant improvement was achieved via our auto-context algorithm. Furthermore, we evaluated the performance of our algorithm in the challenging problem of extracting arbitrarily-oriented fetal brains in reconstructed fe- al brain magnetic resonance imaging (MRI) datasets. In this application our voxelwise auto-context CNN performed much better than the other methods (Dice coefficient: 95.97%), where the other methods performed poorly due to the non-standard orientation and geometry of the fetal brain in MRI. Through training, our method can provide accurate brain extraction in challenging applications. This in-turn may reduce the problems associated with image registration in segmentation tasks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961201","source":"ieee","year":2017,"key":"e32334df-0528-4a34-84b5-584bd2b36606","use":1,"doi":"10.1109\/TMI.2017.2721362"},{"Title":"Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction","Description":"R. Li,  T. Zeng,  H. Peng,  S. Ji","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7874113","source":"ieee","year":2017,"key":"d0332832-1794-4d2f-a59c-2df48daa35f6","use":1,"doi":"10.1109\/TMI.2017.2679713"},{"Title":"Classifying histopathology whole-slides using fusion of decisions from deep convolutional network on a collection of random multi-views at multi-magnification","Description":"K. Das,  S. P. K. Karri,  A. Guha Roy,  J. Chatterjee,  D. Sheet","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Histopathology forms the gold standard for confirmed diagnosis of a suspicious hyperplasia being benign or malignant and for its sub-typing. While techniques like whole-slide imaging have enabled computer assisted analysis for exhaustive reporting of the tissue section, it has also given rise to the big-data deluge and the time complexity associated with processing GBs of image data acquired over multiple magnifications. Since preliminary screening of a slide into benign or malignant carried out on the fly during the digitization process can reduce a Pathologist's work load, to devote more time for detailed analysis, slide screening has to be performed on the fly with high sensitivity. We propose a deep convolutional neural network (CNN) based solution, where we analyse images from random number of regions of the tissue section at multiple magnifications without any necessity of view correspondence across magnifications. Further a majority voting based approach is used for slide level diagnosis, i.e., the class posteriori estimate of each views at a particular magnification is obtained from the magnification specific CNN, and subsequently posteriori estimate across random multi-views at multi-magnification are voting filtered to provide a slide level diagnosis. We have experimentally evaluated performance using a patient level 5-folded cross-validation with 58 malignant and 24 benign cases of breast tumors to obtain average accuracy of 94.67 \u00b1 14.60%, sensitivity of 96.00 \u00b1 8.94%, specificity of 92.00 \u00b1 17.85% and F-score of 96.24 \u00b1 5.29% while processing each view in \u2248 10 ms.","email":["ssalehi@ece.neu.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950690","source":"ieee","year":2017,"key":"ef2ccc21-bf3f-4d78-8871-ad7487279946","use":1,"doi":"10.1109\/ISBI.2017.7950690"},{"Title":"Deep residual learning for compressed sensing MRI","Description":"D. Lee,  J. Yoo,  J. C. Ye","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Compressed sensing (CS) enables significant reduction of MR acquisition time with performance guarantee. However, computational complexity of CS is usually expensive. To address this, here we propose a novel deep residual learning algorithm to reconstruct MR images from sparsely sampled k-space data. In particular, based on the observation that coherent aliasing artifacts from downsampled data has topologically simpler structure than the original image data, we formulate a CS problem as a residual regression problem and propose a deep convolutional neural network (CNN) to learn the aliasing artifacts. Experimental results using single channel and multi channel MR data demonstrate that the proposed deep residual learning outperforms the existing CS and parallel imaging algorithms. Moreover, the computational time is faster in several orders of magnitude.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950457","source":"ieee","year":2017,"key":"6ff96cd5-574a-472c-97bd-84d151ab480e","use":1,"doi":"10.1109\/ISBI.2017.7950457"},{"Title":"Hybrid dermoscopy image classification framework based on deep convolutional neural network and Fisher vector","Description":"Z. Yu,  D. Ni,  S. Chen,  J. Qin,  S. Li,  T. Wang,  B. Lei","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Dermoscopy image is usually used in early diagnosis of malignant melanoma. The diagnosis accuracy by visual inspection is highly relied on the dermatologist's clinical experience. Due to the inaccuracy, subjectivity, and poor reproducibility of human judgement, an automatic recognition algorithm of dermoscopy image is highly desired. In this work, we present a hybrid classification framework for dermoscopy image assessment by combining deep convolutional neural network (CNN), Fisher vector (FV) and support vector machine (SVM). Specifically, the deep representations of subimages at various locations of a rescaled dermoscopy image are first extracted via a natural image dataset pre-trained on CNN. Then we adopt an orderless visual statistics based FV encoding methods to aggregate these features to build more invariant representations. Finally, the FV encoded representations are classified for diagnosis using a linear SVM. Compared with traditional low-level visual features based recognition approaches, our scheme is simpler and requires no complex preprocessing. Furthermore, the orderless representations are less sensitive to geometric deformation. We evaluate our proposed method on the ISBI 2016 Skin lesion challenge dataset and promising results are obtained. Also, we achieve consistent improvement in accuracy even without fine-tuning the CNN.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950524","source":"ieee","year":2017,"key":"9653ed6a-d4e8-4f17-a782-00cba9638c5b","use":1,"doi":"10.1109\/ISBI.2017.7950524"},{"Title":"Automated vesicle fusion detection using Convolutional Neural Networks","Description":"H. Li,  Z. Yin,  Y. Xu","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Quantitative analysis of vesicle-plasma membrane fusion events in the fluorescence microscopy, has been proven to be important in the vesicle exocytosis study. In this paper, we present a framework to automatically detect fusion events. First, an iterative searching algorithm is developed to extract image patch sequences containing potential events. Then, we propose an event image to integrate the critical image patches of a candidate event into a single-image joint representation as the input to Convolutional Neural Networks (CNNs). According to the duration of candidate events, we design three CNN architectures to automatically learn features for the fusion event classification. Compared on 9 challenging datasets, our proposed method showed very competitive performance and outperformed two state-of-the-arts.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950497","source":"ieee","year":2017,"key":"b7b551b8-099b-4687-94ee-c7bb56520a69","use":1,"doi":"10.1109\/ISBI.2017.7950497"},{"Title":"A novel hybrid approach for severity assessment of Diabetic Retinopathy in colour fundus images","Description":"P. Roy,  R. Tennakoon,  K. Cao,  S. Sedai,  D. Mahapatra,  S. Maetschke,  R. Garnavi","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Diabetic Retinopathy (DR) is one of the leading causes of blindness worldwide. Detecting DR and grading its severity is essential for disease treatment. Convolutional neural networks (CNNs) have achieved state-of-the-art performance in many different visual classification tasks. In this paper, we propose to combine CNNs with dictionary based approaches, which incorporates pathology specific image representation into the learning framework, for improved DR severity classification. Specifically, we construct discriminative and generative pathology histograms and combine them with feature representations extracted from fully connected CNN layers. Our experimental results indicate that the proposed method shows improvement in quadratic kappa score (\u03ba<sup>2<\/sup> = 0.86) compared to the state-of-the-art CNN based method (\u03ba<sup>2<\/sup> = 0.81).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950703","source":"ieee","year":2017,"key":"c486d0de-dcce-4f73-9f95-8a39790522c1","use":1,"doi":"10.1109\/ISBI.2017.7950703"},{"Title":"Lung nodule segmentation using deep learned prior based graph cut","Description":"S. Mukherjee,  X. Huang,  R. R. Bhagalia","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"We propose an automated framework for lung nodule segmentation from pulmonary CT scan using graph cut with a deep learned prior. The segmentation problem is formulated as a hybrid cost function minimization task, which combines a domain specific data term with a deep learned probability map. The proposed segmentation framework embodies the robustness of deep learning in object localization, while retaining the hallmark of traditional segmentation models in addressing the morphological intricacies of elaborate objects. The proposed solution offers more than 20% performance improvement over a contemporary data driven model, and also outperforms traditional graph cuts especially in situations where model initialization is slightly inaccurate.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950733","source":"ieee","year":2017,"key":"9ecf8387-5985-46e0-9bc3-51220001b05d","use":1,"doi":"10.1109\/ISBI.2017.7950733"},{"Title":"M-net: A Convolutional Neural Network for deep brain structure segmentation","Description":"R. Mehta,  J. Sivaswamy","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In this paper, we propose an end-to-end trainable Convolutional Neural Network (CNN) architecture called the M-net, for segmenting deep (human) brain structures from Magnetic Resonance Images (MRI). A novel scheme is used to learn to combine and represent 3D context information of a given slice in a 2D slice. Consequently, the M-net utilizes only 2D convolution though it operates on 3D data, which makes M-net memory efficient. The segmentation method is evaluated on two publicly available datasets and is compared against publicly available model based segmentation algorithms as well as other classification based algorithms such as Random Forrest and 2D CNN based approaches. Experiment results show that the M-net outperforms all these methods in terms of dice coefficient and is at least 3 times faster than other methods in segmenting a new volume which is attractive for clinical use.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950555","source":"ieee","year":2017,"key":"bd296016-3805-42e2-b620-b66c215638cd","use":1,"doi":"10.1109\/ISBI.2017.7950555"},{"Title":"Deep learning model based breast cancer histopathological image classification","Description":"Benzheng Wei,  Zhongyi Han,  Xueying He,  Yilong Yin","ShortDetails":"2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA). 2017","abstract":"The automatic and precision classification for breast cancer histopathological image has a great significance in clinical application. However, the existing analysis approaches are difficult to addressing the breast cancer classification problem because the feature subtle differences of inter-class histopathological image and the classification accuracy still hard to meet the clinical application. Recent advancements in data-driven sharing processing and multi-level hierarchical feature learning have made available considerable chance to dope out a solution to this problem. To address the challenging problem, we propose a novel breast cancer histopathological image classification method based on deep convolutional neural networks, named as BiCNN model, to address the two-class breast cancer classification on the pathological image. This deep learning model considers class and sub-class labels of breast cancer as prior knowledge, which can restrain the distance of features of different breast cancer pathological images. In addition, an advanced data augmented method is proposed to fit tolerance whole slide image recognition, which can full reserve image edge feature of cancerization region. The transfer learning and fine-tuning method are adopted as an optimal training strategy to improve breast cancer histopathological image classification accuracy. The experiment results show that the proposed method leads to a higher classification accuracy (up to 97%) and displays good robustness and generalization, which provides efficient tools for breast cancer clinical diagnosis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7951937","source":"ieee","year":2017,"key":"314f7900-6623-465e-b580-1a22aee94612","use":1,"doi":"10.1109\/ICCCBDA.2017.7951937"},{"Title":"Exploring texture Transfer Learning for Colonic Polyp Classification via Convolutional Neural Networks","Description":"E. Ribeiro,  M. H\u00e4fner,  G. Wimmer,  T. Tamaki,  J. J. W. Tischendorf,  S. Yoshida,  S. Tanaka,  A. Uhl","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"This work addresses Transfer Learning via Convolutional Neural Networks (CNN's) for the automated classification of colonic polyps in eight HD-endoscopic image databases acquired using different modalities. For this purpose, we explore if the architecture, the training approach, the number of classes, the number of images as well as the nature of the images in the training phase can influence the results. The experiments show that when the number of classes and the nature of the images are similar to the target database, the results are improved. Also, the better results obtained by the transfer learning compared to the most used features in the literature suggest that features learned by CNN's can be highly relevant for automated classification of colonic polyps.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950695","source":"ieee","year":2017,"key":"f588a3e0-e667-446d-b8e4-76d7c1cfc58b","use":1,"doi":"10.1109\/ISBI.2017.7950695"},{"Title":"Modeling Task fMRI Data via Deep Convolutional Autoencoder","Description":"H. Huang,  X. Hu,  Y. Zhao,  M. Makkie,  Q. Dong,  S. Zhao,  L. Guo,  T. Liu","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Task-based fMRI (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, Independent Component Analysis (ICA) and Sparse Dictionary Learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this study, we developed a new neural network structure based on CNN, called Deep Convolutional Auto-Encoder (DCAE), in order to take the advantages of both data-driven approach and CNN\u2019s hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project (HCP) tfMRI datasets, and promising results are achieved.","email":["huangheng2014@gmail.com","xintao.hu@gmail.com","lguo@nwpu.edu.cn","shijiezhao666@gmail.com","lynnqd2000@gmail.com","miladmakkie@gmail.com","zhaoyu.hust@gmail.com","tianming.liu@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7949140","source":"ieee","year":2017,"key":"39264e57-17d7-40cb-b2bd-fd790cfc28b0","use":1,"doi":"10.1109\/TMI.2017.2715285"},{"Title":"Identifying Carotid Plaque Composition in MRI with Convolutional Neural Networks","Description":"Y. Dong,  Y. Pan,  X. Zhao,  R. Li,  C. Yuan,  W. Xu","ShortDetails":"2017 IEEE International Conference on Smart Computing (SMARTCOMP). 2017","abstract":"Carotid plaques may cause strokes. The composition of the plaque helps assessing the risk. Magnetic resonance imaging (MRI) is a powerful technology for analyzing the composition. It is both tedious and error-prone for a human radiologist to review such images. Traditional computer-aided diagnosis tools use manually crafted features that lack both generality and accuracy. We propose a novel approach using Deep convolutional neural networks (CNN) to classify these plaque tissues. In order to accommodate the multi-contrast MRI images, we modify state-of-the-art CNN models to support different number of input channels, and also adapt the models to do pixel- wise predictions. On a dataset with 1,098 human subjects, we show that we achieve significantly better accuracy than previous models. Our result also indicates interesting relations between contrast weightings and tissue types.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7947015","source":"ieee","year":2017,"key":"9c38e969-8416-49bb-b397-19c0615b4b01","use":1,"doi":"10.1109\/SMARTCOMP.2017.7947015"},{"Title":"Deep Convolutional Neural Networks and Learning ECG Features for Screening Paroxysmal Atrial Fibrillation Patients","Description":"B. Pourbabaee,  M. J. Roshtkhari,  K. Khorasani","ShortDetails":"IEEE Transactions on Systems, Man, and Cybernetics: Systems. 2017","abstract":"In this paper, a novel computationally intelligent-based electrocardiogram (ECG) signal classification methodology using a deep learning (DL) machine is developed. The focus is on patient screening and identifying patients with paroxysmal atrial fibrillation (PAF), which represents a life threatening cardiac arrhythmia. The proposed approach operates with a large volume of raw ECG time-series data as inputs to a deep convolutional neural networks (CNN). It autonomously learns representative and key features of the PAF to be used by a classification module. The features are therefore learned directly from the large time domain ECG signals by using a CNN with one fully connected layer. The learned features can effectively replace the traditional ad hoc and time-consuming user's hand-crafted features. Our experimental results verify and validate the effectiveness and capabilities of the learned features for PAF patient screening. The main advantages of our proposed approach are to simplify the feature extraction process corresponding to different cardiac arrhythmias and to remove the need for using a human expert to define appropriate and critical features working with a large time-series data set. The extensive simulations and case studies conducted indicate that combining the learned features with other classifiers will significantly improve the performance of the patient screening system as compared to an end-to-end CNN classifier. The effectiveness and capabilities of our proposed ECG DL classification machine is demonstrated and quantitative comparisons with several conventional machine learning classifiers are also provided.","email":["b_pourba@ece.concordia.ca","kash@ece.concordia.ca","mehrsan@sportlogiq.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7937819","source":"ieee","year":2017,"key":"668b7996-1e43-40b5-8fb9-d79f349bbc3f","use":1,"doi":"10.1109\/TSMC.2017.2705582"},{"Title":"Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning","Description":"W. Xue,  A. Islam,  M. Bhaduri,  S. Li","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.440.71mm) and areas of cavity and myocardium (204133mm2). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7934404","source":"ieee","year":2017,"key":"d402996b-007f-414f-b190-0658dd9a4e78","use":1,"doi":"10.1109\/TMI.2017.2709251"},{"Title":"Single-Trial Classification of Event-Related Potentials in Rapid Serial Visual Presentation Tasks Using Supervised Spatial Filtering","Description":"H. Cecotti,  M. P. Eckstein,  B. Giesbrecht","ShortDetails":"IEEE Transactions on Neural Networks and Learning Systems. 2014","abstract":"Accurate detection of single-trial event-related potentials (ERPs) in the electroencephalogram (EEG) is a difficult problem that requires efficient signal processing and machine learning techniques. Supervised spatial filtering methods that enhance the discriminative information in EEG data are commonly used to improve single-trial ERP detection. We propose a convolutional neural network (CNN) with a layer dedicated to spatial filtering for the detection of ERPs and with training based on the maximization of the area under the receiver operating characteristic curve (AUC). The CNN is compared with three common classifiers: 1) Bayesian linear discriminant analysis; 2) multilayer perceptron (MLP); and 3) support vector machines. Prior to classification, the data were spatially filtered with xDAWN (for the maximization of the signal-to-signal-plus-noise ratio), common spatial pattern, or not spatially filtered. The 12 analytical techniques were tested on EEG data recorded in three rapid serial visual presentation experiments that required the observer to discriminate rare target stimuli from frequent nontarget stimuli. Classification performance discriminating targets from nontargets depended on both the spatial filtering method and the classifier. In addition, the nonlinear classifier MLP outperformed the linear methods. Finally, training based AUC maximization provided better performance than training based on the minimization of the mean square error. The results support the conclusion that the choice of the systems architecture is critical and both spatial filtering and classification must be considered together.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6737255","source":"ieee","year":2014,"key":"b47301c1-da74-461a-a3bd-b24db9cf1fd8","use":1,"doi":"10.1109\/TNNLS.2014.2302898"},{"Title":"ECG Monitoring System Integrated With IR-UWB Radar Based on CNN","Description":"W. Yin,  X. Yang,  L. Zhang,  E. Oki","ShortDetails":"IEEE Access. 2016","abstract":"In the demand for protecting the increasing aged groups from heart attacks, the improvement of the mobile electrocardiogram (ECG) monitoring systems becomes significant. The limitations of the arrhythmia classification in these systems are the lack of ability to cope with motion state and the low accuracy in new users' data. This paper proposes a system which applies the impulse radio ultra wideband radar data as additional information to assist the arrhythmia classification of ECG recordings in the slight motion state. Besides, this proposed system employs a cascade convolutional neural network to achieve an integrated analysis of ECG recordings and radar data. The experiments are implemented in the Caffe platform and the result reaches an accuracy of 88.89% in the slight motion state. It turns out that this proposed system keeps a stable accuracy of classification for normal and abnormal heartbeats in the slight motion state.","email":["ywf2014@bupt.edu.cn."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7576640","source":"ieee","year":2016,"key":"85236bc2-7b11-4acb-8da5-a554a9f09f59","use":1,"doi":"10.1109\/ACCESS.2016.2608777"},{"Title":"Computer diagnostic tools based on biomedical image analysis","Description":"O. Berezsky,  O. Pitsun,  S. Verbovyy,  T. Datsko,  A. Bodnar","ShortDetails":"2017 14th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM). 2017","abstract":"In this paper, the authors investigated the main types of mammary dysplasia. In order to classify biomedical images, the researchers developed a basic model of convolutional neural network (CNN). Input parameters of the neural network to classify cytological and histological images were thoroughly researched and selected.","email":["ob@tneu.edu.ua","o.pitsun@tneu.edu.ua","vso@tneu.edu.ua","datskotv@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7916157","source":"ieee","year":2017,"key":"99281111-aa76-46e8-9ab9-8b54778f6faf","use":1,"doi":"10.1109\/CADSM.2017.7916157"},{"Title":"Skin disease classification versus skin lesion characterization: Achieving robust diagnosis using multi-label deep neural networks","Description":"Haofu Liao,  Yuncheng Li,  Jiebo Luo","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"In this study, we investigate what a practically useful approach is in order to achieve robust skin disease diagnosis. A direct approach is to target the ground truth diagnosis labels, while an alternative approach instead focuses on determining skin lesion characteristics that are more visually consistent and discernible. We argue that, for computer aided skin disease diagnosis, it is both more realistic and more useful that lesion type tags should be considered as the target of an automated diagnosis system such that the system can first achieve a high accuracy in describing skin lesions, and in turn facilitate disease diagnosis using lesion characteristics in conjunction with other evidences. To further meet such an objective, we employ convolutional neutral networks (CNNs) for both the disease-targeted and lesion-targeted classifications. We have collected a large-scale and diverse dataset of 75,665 skin disease images from six publicly available dermatology atlantes. Then we train and compare both disease-targeted and lesion-targeted classifiers, respectively. For disease-targeted classification, only 27.6% top-1 accuracy and 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of 0.42. In contrast, for lesion-targeted classification, we can achieve a much higher mAP of 0.70.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899659","source":"ieee","year":2016,"key":"1e34fc7d-1d3c-451b-a00e-2ad47e344508","use":1,"doi":"10.1109\/ICPR.2016.7899659"},{"Title":"Severity grading of psoriatic plaques using deep CNN based multi-task learning","Description":"A. Pal,  A. Chaturvedi,  U. Garain,  A. Chandra,  R. Chatterjee","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"This paper addresses the problem of automatic machine analysis based severity scoring of psoriasis skin disease. Three different disease parameters namely, erythema, scaling and induration are considered for such severity grading. Given an image containing a psoriatic plaque the task is to predict severity scores for all the three parameters. This paper presents a novel deep CNN based architecture for achieving the task. Apart from viewing this task as three different single task learning (STL) problems (i.e. three different classification problems), a new multi-task learning (MTL) is also presented where the three classification tasks are treated as interdependent and thereby the neural net is trained accordingly. A new annotated dataset consisting of seven hundred and seven (707) images has been constructed on which the performance of the severity scoring algorithms have been reported. Several competing baselines are considered to compare the performance of STL and MTL approaches. Experimental result shows that the deep CNN based architectures (both the STL and MTL) achieve promising performances, MTL producing slightly superior results to that of STL.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899846","source":"ieee","year":2016,"key":"3770f003-1cd9-4026-b994-c3106edb4601","use":1,"doi":"10.1109\/ICPR.2016.7899846"},{"Title":"Deep learning for magnification independent breast cancer histopathology image classification","Description":"N. Bayramoglu,  J. Kannala,  J. Heikkil\u00e4","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.","email":["nyalcinb@ee.oulu.","juho.kannala@aalto.","jth@ee.oulu."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7900002","source":"ieee","year":2016,"key":"21280ddc-8abe-4d4c-aabd-91880959f6f7","use":1,"doi":"10.1109\/ICPR.2016.7900002"},{"Title":"Skin lesion segmentation in clinical images using deep learning","Description":"M. H. Jafari,  N. Karimi,  E. Nasr-Esfahani,  S. Samavi,  S. M. R. Soroushmehr,  K. Ward,  K. Najarian","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Melanoma is the most aggressive form of skin cancer and is on rise. There exists a research trend for computerized analysis of suspicious skin lesions for malignancy using images captured by digital cameras. Analysis of these images is usually challenging due to existence of disturbing factors such as illumination variations and light reflections from skin surface. One important stage in diagnosis of melanoma is segmentation of lesion region from normal skin. In this paper, a method for accurate extraction of lesion region is proposed that is based on deep learning approaches. The input image, after being preprocessed to reduce noisy artifacts, is applied to a deep convolutional neural network (CNN). The CNN combines local and global contextual information and outputs a label for each pixel, producing a segmentation mask that shows the lesion region. This mask will be further refined by some post processing operations. The experimental results show that our proposed method can outperform the existing state-of-the-art algorithms in terms of segmentation accuracy.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899656","source":"ieee","year":2016,"key":"31b6136f-a2f9-43ad-8968-bf2e621e58d4","use":1,"doi":"10.1109\/ICPR.2016.7899656"},{"Title":"Quantifying radiographic knee osteoarthritis severity using deep convolutional neural networks","Description":"J. Antony,  K. McGuinness,  N. E. O'Connor,  K. Moran","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren & Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.","email":["DublinCityUniversityjoseph.antony2@mail.dcu.ieAbstract"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899799","source":"ieee","year":2016,"key":"f8ce9555-71a2-46c9-afa5-bfe4c7df1272","use":1,"doi":"10.1109\/ICPR.2016.7899799"},{"Title":"Optic Disc Detection Using Fine Tuned Convolutional Neural Networks","Description":"F. Calimeri,  A. Marzullo,  C. Stamile,  G. Terracina","ShortDetails":"2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS). 2016","abstract":"The detection of the Optic Disc (OD) is an significant step in retinal fundus images analysis, it allows to extract relevant information that proved to be useful for the prevention of several pathologies, such as glaucoma, hypertension, diabetes and other cardiovascular diseases, which manifest their effects in the retina. In this work we present a supervised method for automatically detecting the position of the Optic Disc in retinal fundus digital images, the goal has been achieved by means of a proper reuse of previous knowledge from a pre-trained Convolutional Neural Network (CNN), already able to detect faces in an image. Experimental analyses showed high level of accuracy in the detection of the optic disc on the DRIVE, STARE and DRIONS databases.","email":["francesco.calimeri@unical.it","marzullo@mat.unical.it","Claudio.Stamile@esat.kuleuven.be","giorgio.terracina@unical.it"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7907447","source":"ieee","year":2016,"key":"3d3e0aab-e726-46af-8471-d48d406052e1","use":1,"doi":"10.1109\/SITIS.2016.20"},{"Title":"Hybrid deep learning for Reflectance Confocal Microscopy skin images","Description":"P. Kaur,  K. J. Dana,  G. O. Cula,  M. C. Mack","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Reflectance Confocal Microscopy (RCM) is used for evaluation of human skin disorders and the effects of skin treatments by imaging the skin layers at different depths. Traditionally, clinical experts manually categorize the images captured into different skin layers. This time-consuming labeling task impedes the convenient analysis of skin image datasets. In recent automated image recognition tasks, deep learning with convolutional neural nets (CNN) has achieved remarkable results. However in many clinical settings, training data is often limited and insufficient for CNN training. For recognition of RCM skin images, we demonstrate that a CNN trained on a moderate size dataset leads to low accuracy. We introduce a hybrid deep learning approach which uses traditional texton-based feature vectors as input to train a deep neural network. This hybrid method uses fixed filters in the input layer instead of tuned filters, yet superior performance is achieved. Our dataset consists of 1500 images from 15 RCM stacks belonging to six different categories of skin layers. We show that our hybrid deep learning approach performs with a test accuracy of 82% compared with 51% for CNN. We also compare the results with additional proposed methods for RCM image recognition and show improved accuracy.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899844","source":"ieee","year":2016,"key":"4f543628-4b7f-41a3-b518-84cc459929de","use":1,"doi":"10.1109\/ICPR.2016.7899844"},{"Title":"HEp-2 specimen classification via deep CNNs and pattern histogram","Description":"Hongwei Li,  Hao Huang,  W. S. Zheng,  Xiaohua Xie,  J. Zhang","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Automatic classification of Human Epithelial Type-2 (HEp-2) specimen patterns is an important yet challenging problem in medical image analysis. Most prior works have primarily focused on cells images classification problem which is one of the early essential steps in the system pipeline, while less attention has been paid to the classification of whole-specimen ones. In this work, a specimen pattern recognition system combining convolutional neural networks (CNNs) and pattern histogram was proposed. The pattern histograms were obtained based on the prediction of each single cell inside the specimens. Two strategies were designed to predicted the pattern of a whole specimen: 1) the most dominant cell pattern in pattern histogram was represented as the specimen pattern, 2) the pattern histograms were employed as bags of patterns and then were trained and predicted separately by a SVM classifier. Experimental results show that the proposed system is effective and achieves high classification accuracy on public benchmark datasets. We further evaluate the robustness of the proposed framework by testing trained CNNs on another different dataset, demonstrating that the system is robust to inter-lab data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899953","source":"ieee","year":2016,"key":"c34fb693-dcd2-4a7e-8176-972b67f432a0","use":1,"doi":"10.1109\/ICPR.2016.7899953"},{"Title":"Deep convolutional neural network based HEp-2 cell classification","Description":"Xi Jia,  Linlin Shen,  Xiande Zhou,  Shiqi Yu","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"As different staining patterns of HEp-2 cells indicate different diseases, the classification of Indirect Immune Fluorescence (IIF) images on Human Epithelial-2 (HEp-2) cell is important for clinical applications. Different from traditional pattern recognition techniques, we use CNN to extract more high-level features for cell images classification. Compared to the existing CNN based HEp-2 classification methods, we proposed a network with deeper architecture. A class-balanced approach is also proposed to augment the HEp-2 cell dataset for network training. The proposed framework achieves an average class accuracy of 79.29% on ICPR 2012 HEp-2 dataset and a mean class accuracy of 98.26% on ICPR 2016 HEp-2 training set.","email":["wszheng@ieee.org","xiexiaoh6@mail.sysu.edu.cn","j.n.zhang@dundee.ac.ukAbstract","20@56","20@28","50@24","50@12","100@8","100@4","500@1","7@1"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899611","source":"ieee","year":2016,"key":"3694638f-6e13-453b-a15e-46410b970680","use":1,"doi":"10.1109\/ICPR.2016.7899611"},{"Title":"A temporal deep learning approach for MR perfusion parameter estimation in stroke","Description":"K. C. Ho,  F. Scalzo,  K. V. Sarma,  S. El-Saden,  C. W. Arnold","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Perfusion magnetic resonance (MR) images are often used in the assessment of acute ischemic stroke to distinguish between salvageable tissue and infarcted core. Deconvolution methods such as singular value decomposition have been used to approximate model-based perfusion parameters from these images. However, studies have shown that these existing deconvolution algorithms can introduce distortions that may negatively influence the utility of these parameter maps. There is limited previous work on utilizing machine learning algorithms to estimate perfusion parameters. In this work, we present a novel bi-input convolutional neural network (bi-CNN) to approximate four perfusion parameters without using an explicit deconvolution method. These bi-CNNs produced good approximations for all four parameters, with relative average root-mean-square errors (ARMSEs) \u2264 5% of the maximum values. We further demonstrate the utility of the estimated perfusion maps for quantifying the salvageable tissue volume in stroke, with more than 80% agreement with the ground truth. These results show that deep learning techniques are a promising tool for perfusion parameter estimation without requiring a standard deconvolution process.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899819","source":"ieee","year":2016,"key":"60cf8b81-c054-4512-88b1-fbd0a67f0a38","use":1,"doi":"10.1109\/ICPR.2016.7899819"},{"Title":"Low quality dermal image classification using transfer learning","Description":"M. S. Elmahdy,  S. S. Abdeldayem,  I. A. Yassine","ShortDetails":"2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). 2017","abstract":"In this study, we investigate three class skin lesion classification problem of a low quality and small size dataset using transfer learning using AlexNet deep Convolutional Neural Network (CNN). Our approach involves modifying the pre-trained AlexNet model; through replacing the decision layer to be compatible with our three class problem. In addition, we propose adding two dropout layers to overcome the over fitting problem. The fine tuning process of the complete network, based on stochastic gradient descent, is performed using skin lesion dataset. Furthermore, we investigated augmenting the original dataset through three flipping directions and sixteen rotation angles processes using a new methodology. The proposed algorithm has been compared with a hand crafted features, based on Local Binary Pattern (LBP) representation followed by Support Vector Machine (SVM) classifier. Increasing the dataset size has dramatically boosted the performance of classifiers achieving accuracy of 98.67% for the modified AlexNet compared to 96.8% using the LBP based system.","email":["eng.sarasaeed@yahoo.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7897283","source":"ieee","year":2017,"key":"e6570abd-53ed-46c4-8d3c-b5677b125be4","use":1,"doi":"10.1109\/BHI.2017.7897283"},{"Title":"Augmenting data when training a CNN for retinal vessel segmentation: How to warp?","Description":"A. Oliveira,  S. Pereira,  C. A. Silva","ShortDetails":"2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG). 2017","abstract":"The retinal vascular condition is a trustworthy biomarker of several ophthalmologic and cardiovascular diseases, so automatic vessel segmentation is a crucial step to diagnose and monitor these problems. Deep Learning models have recently revolutionized the state-of-the-art in several fields, since they can learn features with multiple levels of abstraction from the data itself. However, these methods can easily fall into overfitting, since a huge number of parameters must be learned. Having bigger datasets may act as regularization and lead to better models. Yet, acquiring and manually annotating images, especially in the medical field, can be a long and costly procedure. Hence, when using regular datasets, people heavily need to apply artificial data augmentation. In this work, we use a fully convolutional neural network capable of reaching the state-of-the-art. Also, we investigate the benefits of augmenting data with new samples created by warping retinal fundus images with nonlinear transformations. Our results hint that may be possible to halve the amount of data, while maintaining the same performance.","email":["a68396@alunos.uminho.pt","csilva@dei.uminho.pt"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7889443","source":"ieee","year":2017,"key":"d1b1b0ef-d947-401d-8733-c038bcac8e2c","use":1,"doi":"10.1109\/ENBENG.2017.7889443"},{"Title":"A Digital Pathology application for whole-slide histopathology image analysis based on genetic algorithm and Convolutional Networks","Description":"M. Puerto,  T. Vargas,  A. Cruz-Roa","ShortDetails":"2016 IEEE Latin American Conference on Computational Intelligence (LA-CCI). 2016","abstract":"The last decade Digital Pathology is coming as a relevant and promising area for cancer research and clinical practice thanks to two main trends, 1) the availability of whole slide scanners for complete pathology slide digitalization, and 2) the development of several computational method for histopathology image analysis. However, there are very few works addressed to analyze the whole-slide digitized images (WSI) because their large resolution (e.g. 80,000 \u00d7 80,000 pixels at 40\u00d7 magnification) resulting in huge computational cost for automatic analysis. This paper presents an application design of a meta-heuristic optimization method based on a genetic algorithm (GA) for exploration and exploitation of regions of interest for diagnosis in a WSI in combination with a Convolutional Neural Network (CNN) trained in previous works [10], [11]. The preliminary results show that presented solution scales in computing time given the initial number of samples (initial population). The developed application in Java including the GA method for WSI analysis could be used for diagnosis support by pathologists thanks of its usability and visual interpretability through a probability map of the invasive tumor regions in the WSI.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7885738","source":"ieee","year":2016,"key":"20631576-9a93-4569-b114-56e5aae962ee","use":1,"doi":"10.1109\/LA-CCI.2016.7885738"},{"Title":"Multimodal learning using convolution neural network and Sparse Autoencoder","Description":"Tien Duong Vu,  Hyung-Jeong Yang,  V. Q. Nguyen,  A-Ran Oh,  Mi-Sun Kim","ShortDetails":"2017 IEEE International Conference on Big Data and Smart Computing (BigComp). 2017","abstract":"In the last decade, pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease (AD) have been the subject of extensive research. Deep learning has recently been a great interest in AD classification. Previous works had done almost on single modality dataset, such as Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET), shown high performances. However, identifying the distinctions between Alzheimer's brain data and healthy brain data in older adults (age > 75) is challenging due to highly similar brain patterns and image intensities. The corporation of multimodalities can solve this issue since it discovers and uses the further complementary of hidden biomarkers from other modalities instead of only one, which itself cannot provide. We therefore propose a deep learning method on fusion multimodalities. In details, our approach includes Sparse Autoencoder (SAE) and convolution neural network (CNN) train and test on combined PET-MRI data to diagnose the disease status of a patient. We focus on advantages of multimodalities to help providing complementary information than only one, lead to improve classification accuracy. We conducted experiments in a dataset of 1272 scans from ADNI study, the proposed method can achieve a classification accuracy of 90% between AD patients and healthy controls, demonstrate the improvement than using only one modality.","email":["51000579@hcmut.edu.vn","hjyang@jnu.ac.kr","quanap5@gmail.com","dhdkfks@chonnam.ac.kr","qorgkq475@naver.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881683","source":"ieee","year":2017,"key":"72acaf4a-47e9-4cb4-96a9-9eb4778b09f5","use":1,"doi":"10.1109\/BIGCOMP.2017.7881683"},{"Title":"Simultaneous reconstruction and restoration of sparsely sampled optical coherence tomography image through learning separable filters for deep architectures","Description":"S. P. K. Karri,  N. Garai,  D. Nawn,  S. Ghosh,  D. Chakraborty,  J. Chatterjee","ShortDetails":"2016 IEEE Students&#8217; Technology Symposium (TechSym). 2016","abstract":"Spectral domain optical coherence tomography (SD-OCT) is widely employed across ophthalmology practices for visual investigation of live tissues. The involuntary movements of subjects frequently infuse motion artifacts to SD-OCT images. Sub-sampling of signals is introduced in imaging protocol to avoid such artifacts which causes fall in spatial resolution and peak signal to noise ratio (PSNR). Sparse coding (SC) is opted for restoration and rectification of complete signals from sparse samples through constructing complete and sparse space dictionaries independently. Convolutional neural networks (CNN) can be casted as SC for jointly learning dictionaries resulting less number of CNN filters (equivalence of SC dictionaries) to be trained. The proposed approach extends the separable filters to CNN through architectural constrain. This results in a parallel architecture and reduced number of parameters without compromising on performance. The approach scaled down trainable parameters by 46% with a trade-off of 0.108 PSNR during training and 0.107 PSNR during testing in comparison to conventional CNN.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7872654","source":"ieee","year":2016,"key":"b48927dd-d160-470d-a3c2-76cda2353367","use":1,"doi":"10.1109\/TechSym.2016.7872654"},{"Title":"HEp-2 Cell Image Classification With Deep Convolutional Neural Networks","Description":"Z. Gao,  L. Wang,  L. Zhou,  J. Zhang","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Efficient Human Epithelial-2 cell image classification can facilitate the diagnosis of many autoimmune diseases. This paper proposes an automatic framework for this classification task, by utilizing the deep convolutional neural networks (CNNs) which have recently attracted intensive attention in visual recognition. In addition to describing the proposed classification framework, this paper elaborates several interesting observations and findings obtained by our investigation. They include the important factors that impact network design and training, the role of rotation-based data augmentation for cell images, the effectiveness of cell image masks for classification, and the adaptability of the CNN-based classification system across different datasets. Extensive experimental study is conducted to verify the above findings and compares the proposed framework with the well-established image classification models in the literature. The results on benchmark datasets demonstrate that 1) the proposed framework can effectively outperform existing models by properly applying data augmentation, 2) our CNN-based framework has excellent adaptability across different datasets, which is highly desirable for cell image classification under varying laboratory settings. Our system is ranked high in the cell image classification competition hosted by ICPR 2014.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7400923","source":"ieee","year":2017,"key":"b206f933-3d80-46e0-89fb-824617cf1256","use":1,"doi":"10.1109\/JBHI.2016.2526603"},{"Title":"Subject-specific detection of ventricular tachycardia using convolutional neural networks","Description":"B. S. Chandra,  C. S. Sastry,  S. Jana","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Onset of ventricular tachycardia (VT) is clinically significant, including as a trigger to defibrillator implants. In this paper, we propose a reliable technique to detect such onset using convolutional neural networks (CNNs). The proposed CNN adds convolution and pooling layers below the input layer and above the hidden and output layers of usual neural network (NN). Such layers would learn suitable linear features from training data, while eliminating the need to extract the traditionally used adhoc features. Employing such subject-specific features, we reported the performance of the proposed classifier using Creighton University ventricular tachyarrhythmia database (CUVT). In particular, we achieved mean (\u00b1 standard deviation) performance of 95.6 (\u00b1 00.6) using subject-specific evaluation scheme over 100 random independent iterations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868677","source":"ieee","year":2016,"key":"1a50f1bd-dd3e-48fb-ad49-4545e30395b0","use":1,"doi":"10.23919\/CIC.2016.7868677"},{"Title":"Classification of heart sound recordings using convolution neural network","Description":"H. Ryu,  J. Park,  H. Shin","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Aims: This study proposes a cardiac diagnostic model using convolution neural network (CNN). This model can predict whether a heart sound recording is normal or not by classifying phonocardiograms (PCGs) from both clinical and non-clinical environments - in accordance with the \u201c2016 Physionet\/CinCChallenge\u201d. Methods: Heart sound recordings in the training data set are filtered by using Windowed-sinc Hamming filter algorithm to remove signals regarded as noise. The filtered recordings are then scaled and segmented. Using the filtered and segmented recordings, CNN is trained to extract features and construct a classification function. The CNN is trained by back propagation algorithm with stochastic gradient descent and mini-batch learning. To classify one sound recording, the signal should be filtered and segmented. Each segment of the signal is then classified by the trained CNN model. The model assigns each segment signal a relative probability between normal and abnormal labels. By accumulating these relative probability values for all the segmented signals, one can reliably and robustly determine whether the target signal is normal or abnormal. Results: The proposed model achieved an overall score of 79.5 with a sensitivity of 70.8 and a specificity of 88.2.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868952","source":"ieee","year":2016,"key":"a46c4e18-9537-4c94-abb8-171c14a1e486","use":1,"doi":"10.23919\/CIC.2016.7868952"},{"Title":"Heart sound classification using deep structured features","Description":"M. Tschannen,  T. Kramer,  G. Marti,  M. Heinzmann,  T. Wiatowski","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"We present a novel machine learning-based method for heart sound classification which we submitted to the PhysioNet\/CinC Challenge 2016. Our method relies on a robust feature representation - generated by a wavelet-based deep convolutional neural network (CNN) - of each cardiac cycle in the test recording, and support vector machine classification. In addition to the CNN-based features, our method incorporates physiological and spectral features to summarize the characteristics of the entire test recording. The proposed method obtained a score, sensitivity, and specificity of 0.812, 0.848, and 0.776, respectively, on the hidden challenge testing set.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868805","source":"ieee","year":2016,"key":"b10f55a1-1c85-4300-8bed-c4ff0af4059a","use":1,"doi":"10.23919\/CIC.2016.7868805"},{"Title":"Ensemble of feature-based and deep learning-based classifiers for detection of abnormal heart sounds","Description":"C. Potes,  S. Parvaneh,  A. Rahman,  B. Conroy","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"The goal of the 2016 PhysioNet\/CinC Challenge is the development of an algorithm to classify normal\/abnormal heart sounds. A total of 124 time-frequency features were extracted from the phonocardiogram (PCG) and input to a variant of the AdaBoost classifier. A second classifier using convolutional neural network (CNN) was trained using PCGs cardiac cycles decomposed into four frequency bands. The final decision rule to classify normal\/abnormal heart sounds was based on an ensemble of classifiers combining the outputs of AdaBoost and the CNN. The algorithm was trained on a training dataset (normal= 2575, abnormal= 665) and evaluated on a blind test dataset. Our classifier ensemble approach obtained the highest score of the competition with a sensitivity, specificity, and overall score of 0.9424, 0.7781, and 0.8602, respectively.","email":["zg126@uowmail.edu.au","lupingz@uow.edu.au","jz163@uowmail.edu.au","6@72","6@72x72P2","6@36x36C3","16@33x33P4","16@11x11C5","32@9x9P6","32@3x3F7"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868819","source":"ieee","year":2016,"key":"e13e55dc-501d-483e-b181-148ef21157f4","use":1,"doi":"10.23919\/CIC.2016.7868819"},{"Title":"Adaptive Estimation of Active Contour Parameters Using Convolutional Neural Networks and Texture Analysis","Description":"A. Hoogi,  A. Subramaniam,  R. Veerapaneni,  D. L. Rubin","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"In this paper, we propose a generalization of the level set segmentation approach by supplying a novel method for adaptive estimation of active contour parameters. The presented segmentation method is fully automatic once the lesion has been detected. First, the location of the level set contour relative to the lesion is estimated using a convolutional neural network (CNN). The CNN has two convolutional layers for feature extraction, which lead into dense layers for classification. Second, the output CNN probabilities are then used to adaptively calculate the parameters of the active contour functional during the segmentation process. Finally, the adaptive window size surrounding each contour point is re-estimated by an iterative process that considers lesion size and spatial texture. We demonstrate the capabilities of our method on a dataset of 164 MRI and 112 CT images of liver lesions that includes low contrast and heterogeneous lesions as well as noisy images. To illustrate the strength of our method, we evaluated it against state of the art CNN-based and active contour techniques. For all cases, our method, as assessed by Dice similarity coefficients, performed significantly better than currently available methods. An average Dice improvement of 0.27 was found across the entire dataset over all comparisons. We also analyzed two challenging subsets of lesions and obtained a significant Dice improvement of 0.24 with our method (p <;0.001, Wilcoxon).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7742396","source":"ieee","year":2017,"key":"9d709e3c-8ff9-49fe-87a1-c7b197e40191","use":1,"doi":"10.1109\/TMI.2016.2628084"},{"Title":"Prognostic Analysis of Polypoidal Choroidal Vasculopathy Using an Image-Based Approach","Description":"Y. M. Chen,  W. Y. Lin,  C. L. Tsai","ShortDetails":"2016 International Computer Symposium (ICS). 2016","abstract":"In this paper, we firstly propose to perform prognostic analysis of polypoidal choroidal vasculopathy (PCV) using indocyanine green angiography (ICGA) sequence. Our goal is to develop a computer-aided diagnostic system which can predict the likely treatment outcome of patients with PCV based on their before-treatment ICGA sequences. In order to create a prognostic model for PCV, we utilize both the before-treatment and the aftertreatment ICGA sequences collected in the EVEREST study. By comparing the before-treatment and the after-treatment PCV region in ICGA sequences, we can generate positive and negative samples for training our prognostic model. Here, we design an 8-layer convolution neural network (CNN) and use it to serve as the prognostic model. We have conducted experiments using 17 patients cases. In particular, we perform leave-one-out cross validation so that each patient can be utilized as testing case once. Our proposed method achieves promising results on the EVEREST dataset.","email":["A@K","A@K","E@A","FHA@E","H@AH","IJK@O","H@E","FHA@E","H@E","ELE@K","IE@AH","E@A","E@A","FHA@E","A@E","FHA@E","IJK@O","A@CA","K@E","H@E","-@CA","H@AH","LE@A","IJK@O","FHA@E","IJHE@A","HA@K","IJHE@A","ANFA@EJA","IJHE@A","IJHE@A","IJHE@A","FHA@E","BAA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","FHA@E","IE@AH","A@E","AA@E","A@E","H@A","E@DK","A@E","A@CA","A@E","IJK@O","AA@E"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7858510","source":"ieee","year":2016,"key":"432a9b59-845d-4f67-855b-56f5fc278b01","use":1,"doi":"10.1109\/ICS.2016.0088"},{"Title":"Brain tumor image segmentation based on convolution neural network","Description":"R. Lang,  L. Zhao,  K. Jia","ShortDetails":"2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI). 2016","abstract":"Automatic segmentation and early diagnosis of brain tumor is a challenging problem in computer vision and it can provide possibility for pre-operative planning, and solve the problem such as low accurateness and time-consuming in traditional manual segmentation. Under the mentioned problems above, this paper put forward a new method: Based on traditional convolutional neural networks (CNNs), a new architecture model is proposed for automatic brain tumor segmentation, which combines multi-modality images. The newly designed CNNs model automatically learns useful features from multi-modality images to combine multi-modality information. Experiment results show that the proposed model is more accurate than traditional methods and can provide reliable information for clinic treatments.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7852936","source":"ieee","year":2016,"key":"9a691824-9f7f-499d-882b-e83be0db3142","use":1,"doi":"10.1109\/CISP-BMEI.2016.7852936"},{"Title":"Mitosis detection using convolutional neural network based features","Description":"A. Albayrak,  G. Bilgin","ShortDetails":"2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI). 2016","abstract":"Breast cancer is the second leading cause of cancer death in women according to World Health Organization (WHO). Development of computer aided diagnostic (CAD) systems has great importance as a secondary reader systems for a correct diagnosis and treatment process. In this paper, a deep learning based feature extraction method by convolutional neural network (CNN) is proposed for automated mitosis detection for cancer diagnosis and grading by histopathological images. The proposed framework is tested on the MITOS data set provided for a contest on mitosis detection in breast cancer histological images released for research purposes in International Conference on Pattern Recognition (ICPR'2014). By using provided histopathological images, cellular structures are initially found by combined clustering based segmentation and blob analysis after preprocessing step. Then, obtained cellular image patches are cropped automatically from the histopathological images for feature extraction stage. CNN, which is a prominent deep learning method on image processing tasks, is utilized for extracting discriminative features. Due to the high dimensional output of the CNN, combination of PCA and LDA dimension reduction methods are performed respectively for regularization and dimension reduction process. Afterwards, a robust kernel based classifier, support vector machine (SVM), is used for final classification of mitotic and non-mitotic cells. The test results on MITOS data set prove that the proposed framework achieved promising results for mitosis detection on histopathological images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7846429","source":"ieee","year":2016,"key":"83c8fb33-ea55-44f0-b4cf-039966c1cdac","use":1,"doi":"10.1109\/CINTI.2016.7846429"},{"Title":"Automatic segmentation of the left atrium from MR images via semantic information","Description":"Chunhua Deng,  Xiaolong Zhang","ShortDetails":"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2016","abstract":"Magnetic resonance imaging (MRI) can aid in assessing post-ablation scar formation. Automatic segmentation of left atrium (LA) offers great benefits for an accurate statistical assessment of LA region. However, how to robustly segment LA is still remaining as a challenging task for its high anatomical variability. In this paper, a robust segmentation method that exploits semantic information from different parts is proposed. The semantic correlation is exploited by the K Nearest Neighbor (KNN) search from corpus images with Convolutional Neural Network (CNN) features, which can be regarded as our main contribution. We propose a graph model to fuse semantic cues and eliminate accidental factors. Meanwhile, to optimize segmentation results, a super pixel voting method is also proposed. Experiments on public datasets of MRI image demonstrate the validity and accuracy of our semantic segmentation.","email":["xiaolong.zhang@wust.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7844745","source":"ieee","year":2016,"key":"68a463d4-3a67-4d36-afc1-76cadfc5ec84","use":1,"doi":"10.1109\/SMC.2016.7844745"},{"Title":"DemNet: A Convolutional Neural Network for the detection of Alzheimer's Disease and Mild Cognitive Impairment","Description":"C. D. Billones,  O. J. L. D. Demetria,  D. E. D. Hostallero,  P. C. Naval","ShortDetails":"2016 IEEE Region 10 Conference (TENCON). 2016","abstract":"The early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the diagnosis of AD and MCI using structural Magnetic Resonance Imaging (MRI) scans. In this paper, we propose the use of a Convolutional Neural Network (CNN) in the detection of AD and MCI. In particular, we modified the 16-layered VGGNet for the 3-way classification of AD, MCI and Healthy Controls (HC) on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset achieving an overall accuracy of 91.85% and outperforming several classifiers from other studies.","email":["pcnaval@dcs.upd.edu.ph"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7848755","source":"ieee","year":2016,"key":"279c6394-f97b-4307-99d8-71d593434033","use":1,"doi":"10.1109\/TENCON.2016.7848755"},{"Title":"DeepCut: Object Segmentation From Bounding Box Annotations Using Convolutional Neural Networks","Description":"M. Rajchl,  M. C. H. Lee,  O. Oktay,  K. Kamnitsas,  J. Passerat-Palmbach,  W. Bai,  M. Damodaram,  M. A. Rutherford,  J. V. Hajnal,  B. Kainz,  D. Rueckert","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled weak annotations, in our case bounding boxes. It extends the approach of the well-known GrabCut[1] method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a nai\u0308ve approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy.","email":["pubs-permissions@ieee.org.","m.rajchl@imperial.ac.uk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7739993","source":"ieee","year":2017,"key":"8210cdbd-fcd0-4262-b0b7-4270332c2061","use":1,"doi":"10.1109\/TMI.2016.2621185"},{"Title":"Convolutional Neural Network for Retinal Blood Vessel Segmentation","Description":"Z. Yao,  Z. Zhang,  L. Q. Xu","ShortDetails":"2016 9th International Symposium on Computational Intelligence and Design (ISCID). 2016","abstract":"This paper proposes a CNN (Convolutional neural network) based blood vessel segmentation algorithm. Each pixel with its neighbors of the fundus image is checked by the CNN. The preliminary segmentation results of fundus images were refined by a two stages binarization and a morphological operation successively. The algorithm was tested on DRIVE dataset. While the specificity is 0.9603, sensitivity is 0.7731, which is very close to that of manual annotation. The sensitivity is 2% better than the ones found in current studies. The CNN based algorithm improves the segmentation of blood vessels performance significantly.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7830374","source":"ieee","year":2016,"key":"7fe99901-c1e9-45f2-a991-57136d5c11dd","use":1,"doi":"10.1109\/ISCID.2016.1100"},{"Title":"Cancer Cells Detection in Phase-Contrast Microscopy Images Based on Faster R-CNN","Description":"J. Zhang,  H. Hu,  S. Chen,  Y. Huang,  Q. Guan","ShortDetails":"2016 9th International Symposium on Computational Intelligence and Design (ISCID). 2016","abstract":"In biology and medicine research, detection and identification of cancer cells plays an essential role to further analysis of cell properties and developing new drugs experiments. However, owing to the adhesion among cells and great changes in morphology, it is a very challenging task to detect and locate the cells accurately, especially for the cells adhesion area. In this work, a deep detector for cells based on the framework of Faster R-CNN is proposed, and based on this, a Circle Scanning Algorithm (CSA) is presented for the redetection of adhesion cells. And then a series of experiments are achieved. The results show that the proposed deep detector can detect and identify all separate individual cells in an image, and that the hybrid method by combining Faster R-CNN with the proposed CSA can effectively detect and identify the adhesion cells under the conditions of the limited samples of adhesion cells.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7830364","source":"ieee","year":2016,"key":"69ec199e-2ee4-4e8e-b21a-695854b1242b","use":1,"doi":"10.1109\/ISCID.2016.1090"},{"Title":"CNN transfer learning for the automated diagnosis of celiac disease","Description":"G. Wimmer,  A. V\u00e9csei,  A. Uhl","ShortDetails":"2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA). 2016","abstract":"In this work, four well known convolutional neural networks (CNNs) that were pretrained on the ImageNet database are applied for the computer assisted diagnosis of celiac disease based on endoscopic images of the duodenum. The images are classified using three different transfer learning strategies and a experimental setup specifically adapted for the classification of endoscopic imagery. The CNNs are either used as fixed feature extractors without any fine-tuning to our endoscopic celiac disease image database or they are fine-tuned by training either all layers of the CNN or by fine-tuning only the fully connected layers. Classification is performed by the CNN SoftMax classifier as well as linear support vector machines. The CNN results are compared with the results of four state-of-the-art image representations. We will show that fine-tuning all the layers of the nets achieves the best results and outperforms the comparison approaches.","email":["gwimmer@cosy.sbg.ac.at","uhl@cosy.sbg.ac.at"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7821020","source":"ieee","year":2016,"key":"b478e0c7-2d88-45c0-89fc-4ea10a025ac1","use":1,"doi":"10.1109\/IPTA.2016.7821020"},{"Title":"Emotion recognition from multi-channel EEG data through Convolutional Recurrent Neural Network","Description":"Xiang Li,  Dawei Song,  Peng Zhang,  Guangliang Yu,  Yuexian Hou,  Bin Hu","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Automatic emotion recognition based on multi-channel neurophysiological signals, as a challenging pattern recognition task, is becoming an important computer-aided method for emotional disorder diagnoses in neurology and psychiatry. Traditional approaches require designing and extracting a range of features from single or multiple channel signals based on extensive domain knowledge. This may be an obstacle for non-domain experts. Moreover, traditional feature fusion method can not fully utilize correlation information between different channels. In this paper, we propose a preprocessing method that encapsulates the multi-channel neurophysiological signals into grid-like frames through wavelet and scalogram transform. We further design a hybrid deep learning model that combines the `Convolutional Neural Network (CNN)' and `Recurrent Neural Network (RNN)', for extracting task-related features, mining inter-channel correlation and incorporating contextual information from those frames. Experiments are carried out, in a trial-level emotion recognition task, on the DEAP benchmarking dataset. Our results demonstrate the effectiveness of the proposed methods, with respect to the emotional dimensions of Valence and Arousal.","email":["yxhoug@tju.edu.cn","bh@lzu.edu.cnAbstract"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822545","source":"ieee","year":2016,"key":"b7bccb13-662b-46ce-813a-d950ca6252c8","use":1,"doi":"10.1109\/BIBM.2016.7822545"},{"Title":"ML-CNN: A novel deep learning based disease named entity recognition architecture","Description":"Zhehuan Zhao,  Zhihao Yang,  Ling Luo,  Yin Zhang,  Lei Wang,  Hongfei Lin,  Jian Wang","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"In this paper, we present a deep learning based disease named entity recognition architecture. First, the word-level embedding, character-level embedding and lexicon feature embedding are concatenated as input. Then multiple convolutional layers are stacked over the input to extract useful features automatically. Finally, multiple label strategy, which is firstly introduced, is applied to the output layer to capture the correlation information between neighboring labels. Experimental results on both NCBI and CDR corpora show that ML-CNN can achieve the state-of-the-art performance.","email":["yangzh@dlut.edu.cn","zhangyinbihami@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822625","source":"ieee","year":2016,"key":"739046f8-a836-4c9f-a1e5-009edd1a3d32","use":1,"doi":"10.1109\/BIBM.2016.7822625"},{"Title":"CNN-based image analysis for malaria diagnosis","Description":"Z. Liang,  A. Powell,  I. Ersoy,  M. Poostchi,  K. Silamut,  K. Palaniappan,  P. Guo,  M. A. Hossain,  A. Sameer,  R. J. Maude,  J. X. Huang,  S. Jaeger,  G. Thoma","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Malaria is a major global health threat. The standard way of diagnosing malaria is by visually examining blood smears for parasite-infected red blood cells under the microscope by qualified technicians. This method is inefficient and the diagnosis depends on the experience and the knowledge of the person doing the examination. Automatic image recognition technologies based on machine learning have been applied to malaria blood smears for diagnosis before. However, the practical performance has not been sufficient so far. This study proposes a new and robust machine learning model based on a convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. In a ten-fold cross-validation based on 27,578 single cell images, the average accuracy of our new 16-layer CNN model is 97.37%. A transfer learning model only achieves 91.99% on the same images. The CNN model shows superiority over the transfer learning model in all performance indicators such as sensitivity (96.99% vs 89.00%), specificity (97.75% vs 94.98%), precision (97.73% vs 95.12%), F1 score (97.36% vs 90.24%), and Matthews correlation coefficient (94.75% vs 85.25%).","email":["stefan.jaeger@nih.gov"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822567","source":"ieee","year":2016,"key":"01e90ec4-65b2-4204-bce8-61d335ea877a","use":1,"doi":"10.1109\/BIBM.2016.7822567"},{"Title":"Cardiac left ventricular volumes prediction method based on atlas location and deep learning","Description":"G. Luo,  S. Dong,  K. Wang,  H. Zhang","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"In this paper, we proposed a novel left ventricular volumes prediction method. This method is a cascade architecture which is based on multi-scale LV atlas location and deep convolutional neural networks (CNN). Firstly, we adopted LV atlas mapping method to achieve accurate location of LV region in cardiac magnetic resonance (CMR) images. And then, the CNN were used to train an end-to-end LV volumes prediction model to achieve the direct prediction. What's more, the large number of CMR images data (1140 subjects, more than 1026000 images) make the proposed deep CNN have relatively better feature representation and robust prediction ability. The experiment results on the large-scale CMR datasets prove that the proposed method has higher accuracy than the state-of-the-art prediction methods in terms of the end-diastole volumes (EDV), the end-systole volumes (ESV), and the ejection fraction (EF). Besides, we make the proposed method open accessible to public for wide application in other biomedical image processing fields.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822759","source":"ieee","year":2016,"key":"b5941ff0-942c-4c52-828a-929090b0e677","use":1,"doi":"10.1109\/BIBM.2016.7822759"},{"Title":"Oriented tooth localization for periapical dental X-ray images via convolutional neural network","Description":"H. Eun,  C. Kim","ShortDetails":"2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). 2016","abstract":"In studies using dental X-ray images, tooth localization is essential to produce accurate results. In this paper, we propose a tooth localization method that tightly localize diverse teeth in periapical dental X-ray images. Oriented tooth proposals are generated by using teeth separation lines (TSLs) and a tooth top line, which are reliable and tight to teeth. To classify each tooth proposal into either a tooth or a non-tooth, we utilize a convolutional neural network (CNN). Our CNN model is trained with three classes, i.e., one negative and two positives, for better classification. In addition, we propose scale based non-maximum suppression by integrating scale confidence with non-maximum suppression to efficiently eliminate multiple tooth localizations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7820720","source":"ieee","year":2016,"key":"9f39197c-d7dd-4724-81e6-c1c680348789","use":1,"doi":"10.1109\/APSIPA.2016.7820720"},{"Title":"A Novel Deep Model for Biopsy Image Grading","Description":"G. Zhang,  Z. H. Liang,  H. D. Lai,  Y. Y. Lin,  D. Lin,  Z. P. Li","ShortDetails":"2016 International Conference on Information System and Artificial Intelligence (ISAI). 2016","abstract":"We propose in this paper a deep learning model based on convolutional neural network (CNN) for biopsy image grading. The model outputs a vector of scores indicating presence or severity of the target histopathological characteristics. Within the model, we first design a 7-layer CNN for feature representation and high level concept extraction. Each biopsy image is expressed as a feature vector through our CNN processor. We then place a sigmoid function into the output layer so as to generate a score for each target characteristic. The proposed model is evaluated on a benchmark dataset and a real biopsy image dataset to show its effectiveness.","email":["lzip_008@163.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7816728","source":"ieee","year":2016,"key":"8b2f0cad-ccd4-4944-986e-f5fe86393351","use":1,"doi":"10.1109\/ISAI.2016.0075"},{"Title":"Recognition of persisting emotional valence from EEG using convolutional neural networks","Description":"M. Yanagimoto,  C. Sugimoto","ShortDetails":"2016 IEEE 9th International Workshop on Computational Intelligence and Applications (IWCIA). 2016","abstract":"Recently there has been considerable interest in EEG-based emotion recognition (EEG-ER), which is one of the utilization of BCI. However, it is not easy to realize the EEG-ER system which can recognize emotions with high accuracy because of the tendency for important information in EEG signals to be concealed by noises. Deep learning is the golden tool to grasp the features concealed in EEG data and enable highly accurate EEG-ER because deep neural networks (DNNs) may have higher recognition capability than humans'. The publicly available dataset named DEAP, which is for emotion analysis using EEG, was used in the experiment. The CNN and a conventional model used for comparison are evaluated by the tests according to 11-fold cross validation scheme. EEG raw data obtained from 16 electrodes without general preprocesses were used as input data. The models classify and recognize EEG signals according to the emotional states \"positive\" or \"negative\" which were caused by watching music videos. The results show that the more training data are, the much higher the accuracies of CNNs are (by over 20%). It also suggests that the increased training data need not to belong to the same person's EEG data as the test data so as to get the CNN recognizing emotions accurately. The results indicate that there are not only the considerable amount of the interpersonal difference but also commonality of EEG properties.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7805744","source":"ieee","year":2016,"key":"2ee6733b-ebd9-4c04-9b70-1a007772448e","use":1,"doi":"10.1109\/IWCIA.2016.7805744"},{"Title":"Deep convolutional encoder-decoder for myelin and axon segmentation","Description":"R. Mesbah,  B. McCane,  S. Mills","ShortDetails":"2016 International Conference on Image and Vision Computing New Zealand (IVCNZ). 2016","abstract":"We propose a fully automatic method for segmenting myelin and axon from microscopy images of excised mouse spinal cord based on Convolutional Neural Networks (CNNs) and Deep Convolutional Encoder-Decoder. We compare a two-class CNN, multi-class CNN, and multi-class deep convolutional encoder-decoder with traditional methods. The CNN method gives a pixel-wise accuracy of 79.7% whereas an Active Contour method gives 59.4%. The encoder-decoder shows better performance with 82.3% and noticeably shorter classification time than CNN methods.","email":["rassoul@cs.otago.ac.nz","mccane@cs.otago.ac.nz","steven@cs.otago.ac.nz"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7804455","source":"ieee","year":2016,"key":"04327d42-c350-4022-901a-6e46d79139bc","use":1,"doi":"10.1109\/IVCNZ.2016.7804455"},{"Title":"Diagnosis of Parkinson's disease from continuous speech using deep convolutional networks without manual selection of features","Description":"A. Frid,  A. Kantor,  D. Svechin,  L. M. Manevitz","ShortDetails":"2016 IEEE International Conference on the Science of Electrical Engineering (ICSEE). 2016","abstract":"Parkinson's Disease (PD) is a relatively common neurodegenerative disabling disease. It affects central nervous system with profound effect on the motor system. The most common symptoms include slowness, rigidity and tremor during motion. It has been suggested that the vocal cords are among the first one to be affected and thus the speech is affected at very early stage of the disease and continues to deteriorate as the disease progress. In this work, we focus on automating the process of diagnosis from continuous native speech by removing the necessity of a trained personal from the diagnosis process. This is done by using an adaptation of Convolutional Neural Network (CNN) architecture for one-dimensional signal processing (i.e. raw speech signal) on a relatively small training set. This is a continuation to previous works where we showed (i) that this task can be achieved by using manually-extracted features of the speech (such as formants and their ratios) and (ii) by using an automatic process of auditory features extraction, where the features were selected by signal processing specialist.","email":["alex.frid@gmail.com","arik.k8@gmail.com","dima.svech@gmail.com","manevitz@cs.haifa.ac.il"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7806118","source":"ieee","year":2016,"key":"c429fe8c-1094-485c-a538-ec76289a7546","use":1,"doi":"10.1109\/ICSEE.2016.7806118"},{"Title":"Mental Disease Feature Extraction with MRI by 3D Convolutional Neural Network with Multi-channel Input","Description":"L. Cao,  Z. Liu,  X. He,  Y. Cao,  K. Li","ShortDetails":"2016 IEEE International Conference on Smart Cloud (SmartCloud). 2016","abstract":"Magnetic resonance imaging (MRI) plays an important role in early diagnosis, which can accurately capture the disease variations of the anatomical brain structure. We propose a novel method for improving feature extraction performance from magnetic resonance images (MRI). This study presents a combination of multi-channel input and 3D convolutional neural network architecture which can reduce the feature dimensionality. Multi-channel input scheme is devised to apply prior knowledge on MRI original inputs in order to overcame possibly uncertainty and unsteadiness on the final features. While, the 3D-CNN model can simultaneously extract features from spatial and temporal dimensions for purpose of capturing the variations of constructive information.","email":["liuzhi@sdu.edu.cn","xiaofuhe2008@yahoo.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7796178","source":"ieee","year":2016,"key":"f2864ed6-a26d-41c4-bf8f-319707bb0bbd","use":1,"doi":"10.1109\/SmartCloud.2016.38"},{"Title":"Image Registration for Placenta Reconstruction","Description":"F. Gaisser,  P. P. Jonker,  T. Chiba","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2016","abstract":"In this paper we introduce a method to handle the challenges posed by image registration for placenta reconstruction from fetoscopic video as used in the treatment of Twinto-Twin Transfusion Syndrome (TTTS). Panorama reconstruction of the placenta greatly supports the surgeon in obtaining a complete view of the placenta to localize vascular anastomoses. The found shunts can subsequently be blocked by coagulation in the correct order. By using similarity learning in training a Convolutional Neural Network we created a novel feature extraction method, allowing robust matching of keypoints for image registration and therefore taking the most critical step in placenta reconstruction from fetoscopic video. The fetoscopic video we used for our experiments was acquired from a training simulator for TTTS surgery. We compared our method with state-of-the-art methods. The matching performance of our method is up to three times better while the mean projection error is reduced with 64% for the registered images. Our image registration method provides the ground work for a complete panorama reconstruction of the placenta.","email":["f.gaisser@tudelft.nl","p.p.jonker@tudelft.nl","chiba-t@sea.plala.or.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7789556","source":"ieee","year":2016,"key":"aaefc655-ead3-4b78-917f-56eacc5d53b2","use":1,"doi":"10.1109\/CVPRW.2016.66"},{"Title":"Neuron Segmentation Based on CNN with Semi-Supervised Regularization","Description":"K. Xu,  H. Su,  J. Zhu,  J. S. Guan,  B. Zhang","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2016","abstract":"Neuron segmentation in two-photon microscopy images is a critical step to investigate neural network activities in vivo. However, it still remains as a challenging problem due to the image qualities, which largely results from the non-linear imaging mechanism and 3D imaging diffusion. To address these issues, we proposed a novel framework by incorporating the convolutional neural network (CNN) with a semi-supervised regularization term, which reduces the human efforts in labeling without sacrificing the performance. Specifically, we generate a putative label for each unlabeled sample regularized with a graph-smooth term, which are used as if they were true labels. A CNN model is therefore trained in a supervised fashion with labeled and unlabeled data simultaneously, which is used to detect neuron regions in 2D images. Afterwards, neuron segmentation in a 3D volume is conducted by associating the corresponding neuron regions in each image. Experiments on real-world datasets demonstrate that our approach outperforms neuron segmentation based on the graph-based semisupervised learning, the supervised CNN and variants of the semi-supervised CNN.","email":["xuk12@mails."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7789657","source":"ieee","year":2016,"key":"22b01918-1eef-40eb-bf46-44c689904960","use":1,"doi":"10.1109\/CVPRW.2016.167"},{"Title":"Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification","Description":"L. Hou,  D. Samaras,  T. M. Kurc,  Y. Gao,  J. E. Davis,  J. H. Saltz","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016","abstract":"Convolutional Neural Networks (CNN) are state-of-theart models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7780635","source":"ieee","year":2016,"key":"99437614-2573-4ced-8ad8-1ab2028f02c8","use":1,"doi":"10.1109\/CVPR.2016.266"},{"Title":"Cellular neural network processing of CEUS examination. A pilot study","Description":"C. Botoca,  M. Botoca","ShortDetails":"2016 12th IEEE International Symposium on Electronics and Telecommunications (ISETC). 2016","abstract":"A new method for a differential diagnosis of focal liver lesions (FLL), using cellular neural networks (CNN), intended to develop a computer assisted diagnosis (CAD) procedure, is presented. Two types of lesions with a known typical behavior were selected for the CNN image processing. The database consisted of 20 cases of video recordings of contrast enhancement ultrasound (CEUS) examination. The images processed by the CNN algorithm were selected by ultrasound experts. The images resulted from the CNN processing were accurate and characteristic enough to make possible an instant differential diagnosis between hemangioma and hepatocellular carcinoma.","email":["corina.botoca@upt.ro","mbotoca@yahoo.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7781119","source":"ieee","year":2016,"key":"b285d78f-0653-4f2e-aab6-74c93e9da0c1","use":1,"doi":"10.1109\/ISETC.2016.7781119"},{"Title":"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation","Description":"H. C. Shin,  K. Roberts,  L. Lu,  D. Demner-Fushman,  J. Yao,  R. M. Summers","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016","abstract":"Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normalvs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN\/RNN on the domain-specific image\/text dataset, to infer the joint image\/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image\/text contexts into account.","email":["ddemner@mail.nih.gov","JYao@cc.nih.gov"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7780643","source":"ieee","year":2016,"key":"28a5607a-959a-4e4c-b9d4-45fb387e6e15","use":1,"doi":"10.1109\/CVPR.2016.274"},{"Title":"Automating Carotid Intima-Media Thickness Video Interpretation with Convolutional Neural Networks","Description":"J. Y. Shin,  N. Tajbakhsh,  R. T. Hurst,  C. B. Kendall,  J. Liang","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016","abstract":"Cardiovascular disease (CVD) is the leading cause of mortality yet largely preventable, but the key to prevention is to identify at-risk individuals before adverse events. For predicting individual CVD risk, carotid intima-media thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable, offering several advantages over CT coronary artery calcium score. However, each CIMT examination includes several ultrasound videos, and interpreting each of these CIMT videos involves three operations: (1) select three end-diastolic ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI) in each selected frame, and (3) trace the lumen-intima interface and the media-adventitia interface in each ROI to measure CIMT. These operations are tedious, laborious, and time consuming, a serious limitation that hinders the widespread utilization of CIMT in clinical practice. To overcome this limitation, this paper presents a new system to automate CIMT video interpretation. Our extensive experiments demonstrate that the suggested system performs reliably. The reliable performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7780646","source":"ieee","year":2016,"key":"88d41e22-36fd-4439-b7cd-226070788421","use":1,"doi":"10.1109\/CVPR.2016.277"},{"Title":"Automatic histopathology image analysis with CNNs","Description":"L. Hou,  K. Singh,  D. Samaras,  T. M. Kurc,  Y. Gao,  R. J. Seidman,  J. H. Saltz","ShortDetails":"2016 New York Scientific Data Summit (NYSDS). 2016","abstract":"We define Pathomics as the process of high throughput generation, interrogation, and mining of quantitative features from high-resolution histopathology tissue images. Analysis and mining of large volumes of imaging features has great potential to enhance our understanding of tumors. The basic Pathomics workflow consists of several steps: segmentation of tissue images to delineate the boundaries of nuclei, cells, and other structures; computation of size, shape, intensity, and texture features for each segmented object; classification of images and patients based on imaging features; and correlation of classification results with genomic signatures and clinical outcome. Executing a Pathomics workflow on a dataset of thousands of very high resolution (gigapixels) and heterogeneous histopathology images is a computationally challenging problem. In this paper, we use Convolutional Neural Networks (CNN) for automatic recognition of nuclear morphological attributes in histopathology images of glioma, the most common malignant brain tumor. We constructed a comprehensive multi-label dataset of glioma nuclei and applied two CNN based methods on this dataset. Both methods perform well recognizing some but not all morphological attributes and are complementary with each other.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7747812","source":"ieee","year":2016,"key":"1907a33d-e648-48f2-985f-fe3005014b5e","use":1,"doi":"10.1109\/NYSDS.2016.7747812"},{"Title":"Customizing CNNs for blood vessel segmentation from fundus images","Description":"S. K. Vengalil,  N. Sinha,  S. S. S. Kruthiventi,  R. V. Babu","ShortDetails":"2016 International Conference on Signal Processing and Communications (SPCOM). 2016","abstract":"For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 \u00d7 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.","email":["sunilkumar.vengalil@iiitb.org","neelam.sinha@iiitb.ac.in","kssaisrinivas@gmail.com","venky@cds.iisc.ac.in"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7746702","source":"ieee","year":2016,"key":"afa5c419-2eb3-41b6-b4d3-8cd23d8c6c45","use":1,"doi":"10.1109\/SPCOM.2016.7746702"},{"Title":"Deep convolutional neural networks for classification of mild cognitive impaired and Alzheimer's disease patients from scalp EEG recordings","Description":"F. C. Morabito,  M. Campolo,  C. Ieracitano,  J. M. Ebadi,  L. Bonanno,  A. Bramanti,  S. Desalvo,  N. Mammone,  P. Bramanti","ShortDetails":"2016 IEEE 2nd International Forum on Research and Technologies for Society and Industry Leveraging a better tomorrow (RTSI). 2016","abstract":"In spite of the worldwide financial and research efforts made, the pathophysiological mechanism at the basis of Alzheimer's disease (AD) is still poorly understood. Previous studies using electroencephalography (EEG) have focused on the slowing of oscillatory brain rhythms, coupled with complexity reduction of the corresponding time-series and their enhanced compressibility. These analyses have been typically carried out on single channels. However, limited investigations have focused on the possibility yielded by computational intelligence methodologies and novel machine learning approaches applied to multichannel schemes. The study at screening level on EEG recordings of subjects at risk could be useful to highlight the emergence of underlying AD progression (or at least support any further clinical investigation). In this work, the representational power of Deep Learning on Convolutional Neural Networks (CNN) is exploited to generate suitable sets of features that are then able to classify EEG patterns of AD from a prodromal version of dementia (Mild Cognitive Impairment, MCI) and from age-matched Healthy Controls (HC). The processing system here used enforces a series of convolutional-subsampling layers in order to derive a multivariate assembly of latent, novel patterns, finally used to categorize sets of EEG from different classes of subjects. The final processor here proposed is able to reach an averaged 80% of correct classification with good performance on both sensitivity and specificity by using a Multilayered Feedforward Perceptron (MLP) of the standard type as a final block of the procedure.","email":["morabito@unirc.it"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7740576","source":"ieee","year":2016,"key":"f34bb6af-7bea-4bb2-b206-2174cc415b2a","use":1,"doi":"10.1109\/RTSI.2016.7740576"},{"Title":"Improving convolutional neural network using accelerated proximal gradient method for epilepsy diagnosis","Description":"D. Li,  G. Wang,  T. Song,  Q. Jin","ShortDetails":"2016 UKACC 11th International Conference on Control (CONTROL). 2016","abstract":"The task of epilepsy diagnosing in medicine by classification of electroencephao-graph (EEG) signals is considered. Since an EEG signal has a large number of dimensions as an input sample vector, many previous classification methods have been proposed as hybrid frameworks, which are structurally complex and computationally expensive. In this paper, convolutional neural network (CNN) is used to realize feature extraction and classification simultaneously. The scheme of CNN is adopted to overcome the curse of dimensionality. Meanwhile, the accelerated proximal gradient method is used to increase the training ratio. Experimental results show that the proposed method achieves ideal accuracy of epilepsy diagnosis and converges faster than CNNs based on traditional gradient back propagation.","email":["lidz@mail.buct.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7737620","source":"ieee","year":2016,"key":"dac5783b-78aa-492b-9e19-dc720ded38ae","use":1,"doi":"10.1109\/CONTROL.2016.7737620"},{"Title":"An Automated ECG Beat Classification System Using Convolutional Neural Networks","Description":"M. Zubair,  J. Kim,  C. Yoon","ShortDetails":"2016 6th International Conference on IT Convergence and Security (ICITCS). 2016","abstract":"Classification of Electrocardiogram (ECG) plays an important role in clinical diagnosis of cardiac diseases. In this paper, we introduce an ECG beat classification system using convolutional neural networks (CNNs). The proposed model integrates two main parts, feature extraction and classification, of ECG pattern recognition system. This model automatically learns a suitable feature representation from raw ECG data and thus negates the need of hand-crafted features. By using a small and patient-specific training data, the proposed classification system efficiently classified ECG beats into five different classes recommended by Association for Advancement of Medical Instrumentation (AAMI). ECG signal from 44 recordings of the MIT-BIH database are used to evaluate the classification performance and the results demonstrate that the proposed approach achieves a significant classification accuracy and superior computational efficiency than most of the state-of-the-art methods for ECG signal classification.","email":["Zubair5608@etri.re.kr","jsworld@chonnam.ac.kr","cwyoon@etri.re.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7740310","source":"ieee","year":2016,"key":"1be61391-52dc-4b7a-971a-e4506e87e089","use":1,"doi":"10.1109\/ICITCS.2016.7740310"},{"Title":"Breast cancer histopathological image classification using Convolutional Neural Networks","Description":"F. A. Spanhol,  L. S. Oliveira,  C. Petitjean,  L. Heutte","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"The performance of most conventional classification systems relies on appropriate data representation and much of the efforts are dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful features. On the other hand, deep learning can extract and organize the discriminative information from the data, not requiring the design of feature extractors by a domain expert. Convolutional Neural Networks (CNNs) are a particular type of deep, feedforward network that have gained attention from research community and industry, achieving empirical successes in tasks such as speech recognition, signal processing, object recognition, natural language processing and transfer learning. In this paper, we conduct some preliminary experiments using the deep learning approach to classify breast cancer histopathological images from BreaKHis, a publicly dataset available at http:\/\/web.inf.ufpr.br\/vri\/breast-cancer-database. We propose a method based on the extraction of image patches for training the CNN and the combination of these patches for final classification. This method aims to allow using the high-resolution histopathological images from BreaKHis as input to existing CNN, avoiding adaptations of the model that can lead to a more complex and computationally costly architecture. The CNN performance is better when compared to previously reported results obtained by other machine learning models trained with hand-crafted textural descriptors. Finally, we also investigate the combination of different CNNs using simple fusion rules, achieving some improvement in recognition rates.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727519","source":"ieee","year":2016,"key":"48b9a14c-8222-4fb7-9573-644aa2867b42","use":1,"doi":"10.1109\/IJCNN.2016.7727519"},{"Title":"Classification of mammogram images by using CNN classifier","Description":"K. Sharma,  B. Preet","ShortDetails":"2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI). 2016","abstract":"Classification of the breast tissues into the benign and malignant classes is a difficult assignment. The experimental results are takes 40 input images from DDSM dataset. We extract the GLCM, GLDM and Geometrical features from the mammogram images. In this paper we apply Convolution Neural Network as a classifier on the mammogram images to enhance the accuracy rate of CAD. Performance of the different classifiers is measured on receiver operating characteristic. In training stage, overall classification accuracy of 73%, with 71.5% sensitivity and 73.5% specificity for dense tissue is achieved by our proposed method along with it, accuracy of 79.23%, 73.25% sensitivity and 74.5% specificity is achieved for fatty tissue. Convolution neural network classifier is used to boost the classification performance. This classifier performs better than previous classifiers in that it shows more accuracy than the other classifiers, the misclassification rate of normal mammograms as abnormal. This approach performs good on overlapping problem. This method is different from all other approaches, which are used to identify normal mammograms by detecting cancers. Overlapped tissues are also detected by this using this classifier.","email":["ktnsharma91@gmail.com","bobbinece@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7732477","source":"ieee","year":2016,"key":"a442f56e-64a9-4a6b-870e-d8eb294d87f8","use":1,"doi":"10.1109\/ICACCI.2016.7732477"},{"Title":"Using Convolutional Neural Network for edge detection in musculoskeletal ultrasound images","Description":"S. I. Jabbar,  C. R. Day,  N. Heinz,  E. K. Chadwick","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"Fast and accurate segmentation of musculoskeletal ultrasound images is an on-going challenge. Two principal factors make this task difficult: firstly, the presence of speckle noise arising from the interference that accompanies all coherent imaging approaches; secondly, the sometimes subtle interaction between musculoskeletal components that leads to non-uniformity of the image intensity. Our work presents an investigation of the potential of Convolutional Neural Networks (CNNs) to address both of these problems. CNNs are an effective tool that has previously been used in image processing of several biomedical imaging modalities. However, there is little published material addressing the processing of musculoskeletal ultrasound images. In our work we explore the effectiveness of CNNs when trained to act as a pre-segmentation pixel classifier that determines whether a pixel is an edge or non-edge pixel. Our CNNs are trained using two different ground truth interpretations. The first one uses an automatic Canny edge detector to provide the ground truth image; the second ground truth was obtained using the same image marked-up by an expert anatomist. In this initial study the CNNs have been trained using half of the prepared data from one image, using the other half for testing; validation was also carried out using three unseen ultrasound images. CNN performance was assessed using Mathew's Correlation Coefficient, Sensitivity, Specificity and Accuracy. The results show that CNN performance when using expert ground truth image is better than using Canny ground truth image. Our technique is promising and has the potential to speed-up the image processing pipeline using appropriately trained CNNs.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727805","source":"ieee","year":2016,"key":"9704e133-c906-4c70-b83b-212bea16bd6a","use":1,"doi":"10.1109\/IJCNN.2016.7727805"},{"Title":"Feature leaning with deep Convolutional Neural Networks for screening patients with paroxysmal atrial fibrillation","Description":"B. Pourbabaee,  M. J. Roshtkhari,  K. Khorasani","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"In this paper, a novel electrocardiogram (ECG) signal classification and patient screening method is developed. The focus is on identifying patients with paroxysmal atrial fibrillation (PAF) which is a life threatening cardiac arrhythmia. The proposed approach uses the raw ECG signal as the input and automatically learns the representative features for PAF to be used by a classification mechanism. The features are learned directly from the time domain ECG signals by using a Convolutional Neural Network (CNN) with one fully connected layer. The learned features can replace the hand-crafted features and our experimental results indicate the effectiveness of the learned features in patient screening. The experimental results indicate that combining the learned features with other classifiers will improve the performance of the patient screening system as compared to an End-to-End convolutional neural network classifier. The major characteristics of the proposed approach are to simplify the process of feature extraction for different cardiac arrhythmias and to remove the need for using a human expert to specify the appropriate features. The effectiveness of the proposed ECG classification method is demonstrated through performing extensive simulation studies.","email":["pourba@ece.concordia.ca","kash@ece.concordia.ca.","mehrsan@sportlogiq.com."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727866","source":"ieee","year":2016,"key":"3749ff57-e7f6-4972-9422-2b631d80f73b","use":1,"doi":"10.1109\/IJCNN.2016.7727866"},{"Title":"Vessel extraction in X-ray angiograms using deep learning","Description":"E. Nasr-Esfahani,  S. Samavi,  N. Karimi,  S. M. R. Soroushmehr,  K. Ward,  M. H. Jafari,  B. Felfeliyan,  B. Nallamothu,  K. Najarian","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Coronary artery disease (CAD) is the most common type of heart disease which is the leading cause of death all over the world. X-ray angiography is currently the gold standard imaging technique for CAD diagnosis. These images usually suffer from low quality and presence of noise. Therefore, vessel enhancement and vessel segmentation play important roles in CAD diagnosis. In this paper a deep learning approach using convolutional neural networks (CNN) is proposed for detecting vessel regions in angiography images. Initially, an input angiogram is preprocessed to enhance its contrast. Afterward, the image is evaluated using patches of pixels and the network determines the vessel and background regions. A set of 1,040,000 patches is used in order to train the deep CNN. Experimental results on angiography images of a dataset show that our proposed method has a superior performance in extraction of vessel regions.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590784","source":"ieee","year":2016,"key":"a48e5ce5-3b56-4e4b-9b3b-5016edaf1bf5","use":1,"doi":"10.1109\/EMBC.2016.7590784"},{"Title":"Thorax disease diagnosis using deep convolutional neural network","Description":"J. Chen,  X. Qi,  O. Tervonen,  O. Silv\u00e9n,  G. Zhao,  M. Pietik\u00e4inen","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Computer aided diagnosis (CAD) is an important issue, which can significantly improve the efficiency of doctors. In this paper, we propose a deep convolutional neural network (CNN) based method for thorax disease diagnosis. We firstly align the images by matching the interest points between the images, and then enlarge the dataset by using Gaussian scale space theory. After that we use the enlarged dataset to train a deep CNN model and apply the obtained model for the diagnosis of new test data. Our experimental results show our method achieves very promising results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7591186","source":"ieee","year":2016,"key":"6fd38017-ee85-4696-8e63-84d4e0a6eea3","use":1,"doi":"10.1109\/EMBC.2016.7591186"},{"Title":"Melanoma detection by analysis of clinical images using convolutional neural network","Description":"E. Nasr-Esfahani,  S. Samavi,  N. Karimi,  S. M. R. Soroushmehr,  M. H. Jafari,  K. Ward,  K. Najarian","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Melanoma, most threatening type of skin cancer, is on the rise. In this paper an implementation of a deep-learning system on a computer server, equipped with graphic processing unit (GPU), is proposed for detection of melanoma lesions. Clinical (non-dermoscopic) images are used in the proposed system, which could assist a dermatologist in early diagnosis of this type of skin cancer. In the proposed system, input clinical images, which could contain illumination and noise effects, are preprocessed in order to reduce such artifacts. Afterward, the enhanced images are fed to a pre-trained convolutional neural network (CNN) which is a member of deep learning models. The CNN classifier, which is trained by large number of training samples, distinguishes between melanoma and benign cases. Experimental results show that the proposed method is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590963","source":"ieee","year":2016,"key":"55a04b03-72ef-41b2-a882-ead353186659","use":1,"doi":"10.1109\/EMBC.2016.7590963"},{"Title":"Automatic Lumbar Vertebrae Detection Based on Feature Fusion Deep Learning for Partial Occluded C-arm X-ray Images","Description":"Y. Li,  W. Liang,  Y. Zhang,  H. An,  J. Tan","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.","email":["liyang@sia.cn","liang-wei2014315@gmail.com","tan@utk.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590785","source":"ieee","year":2016,"key":"9ccddf12-0caa-4122-8bc2-a9f56a370bde","use":1,"doi":"10.1109\/EMBC.2016.7590785"},{"Title":"A deep symmetry convnet for stroke lesion segmentation","Description":"Y. Wang,  A. K. Katsaggelos,  X. Wang,  T. B. Parrish","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Stroke is one of the leading causes of death and disability. Clinically, to establish stroke patient prognosis, an accurate delineation of brain lesion is essential, which is time consuming and prone to subjective errors. In this paper, we propose a novel method call Deep Lesion Symmetry ConvNet to automatically segment chronic stroke lesions using MRI. An 8-layer 3D convolutional neural network is constructed to handle the MRI voxels. An additional CNN stream using the corresponding symmetric MRI voxels is combined, leading to a significant improvement in system performance. The high average dice coefficient achieved on our dataset based on data collected from three research labs demonstrates the effectiveness of our method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532329","source":"ieee","year":2016,"key":"4b8b170f-57a8-4901-a382-c1f9bc559221","use":1,"doi":"10.1109\/ICIP.2016.7532329"},{"Title":"Alzheimer's disease diagnostics by adaptation of 3D convolutional network","Description":"E. Hosseini-Asl,  R. Keynton,  A. El-Baz","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset.","email":["ehsan.hosseiniasl@louisville.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532332","source":"ieee","year":2016,"key":"730c4f11-ff7d-4d4d-924d-ae0c9c17f939","use":1,"doi":"10.1109\/ICIP.2016.7532332"},{"Title":"Improving Tuberculosis Diagnostics Using Deep Learning and Mobile Health Technologies among Resource-Poor and Marginalized Communities","Description":"Y. Cao,  C. Liu,  B. Liu,  M. J. Brunette,  N. Zhang,  T. Sun,  P. Zhang,  J. Peinado,  E. S. Garavito,  L. L. Garcia,  W. H. Curioso","ShortDetails":"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2016","abstract":"Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Peru\u0301, a densely occupied urban community and high-burden TB.","email":["ycao@cs.uml.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7545842","source":"ieee","year":2016,"key":"adf82185-9dc4-4028-8250-0cbf71aae893","use":1,"doi":"10.1109\/CHASE.2016.18"},{"Title":"Human Pulse Recognition Based on Convolutional Neural Networks","Description":"S. R. Zhang,  Q. F. Sun","ShortDetails":"2016 International Symposium on Computer, Consumer and Control (IS3C). 2016","abstract":"Human pulse recognition is an important part of the objective study of Traditional Chinese Medicine (TCM). In the current human pulse recognition methods, there are many feature extraction algorithms but many are complex and redundancy exists in the features selections. This paper focused on the typical convolutional neural network (CNN), and designed a 9-layer CNN which can be used to human wrist pulse signal classification. Feature extraction process is not necessary in the proposed method so the computing and the complexity are reduced. Experimental results show that the recognition rate can be 93.49%, which further verified the feasibility of our method.","email":["zsr0504@qq.com","sun_qingfu@qq.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7545211","source":"ieee","year":2016,"key":"e6c8c862-7ae3-4ed3-852c-def0d48f5446","use":1,"doi":"10.1109\/IS3C.2016.101"},{"Title":"Application of deep learning for recognizing infant cries","Description":"C. Y. Chang,  J. J. Li","ShortDetails":"2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW). 2016","abstract":"Crying is a way which infants express their needs to their parents. In general, parents often feel worried and anxious when infant crying. For realizing the reason of baby crying, this paper presents an automatic infant crying recognition method. Crying is convert to spectrogram. A convolutional neural networks (CNN) based deep learning is then adopted to train and classify the crying into three categories including hungry, pain, and sleepy. Experimental results shows that the proposed method achieves high classification accuracy.","email":["chuanyu@yuntech.edu.tw"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7520947","source":"ieee","year":2016,"key":"bd3122c2-1476-44fe-ad2a-6edfc15b7c68","use":1,"doi":"10.1109\/ICCE-TW.2016.7520947"},{"Title":"Prediction of visual attention with Deep CNN for studies of neurodegenerative diseases","Description":"S. Chaabouni,  F. Tison,  J. Benois-Pineau,  C. Ben Amar","ShortDetails":"2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI). 2016","abstract":"As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video.","email":["souad.chaabouni@labri.fr","benois-p@labri.fr","francois.tison@chu-bordeaux.fr","chokri.benamar@ieee.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7500243","source":"ieee","year":2016,"key":"21ca4519-4415-4c97-8d05-a26b35ebefae","use":1,"doi":"10.1109\/CBMI.2016.7500243"},{"Title":"New Signal Processing Methods for the Development of Seizure Warning Devices in Epilepsy","Description":"V. Senger,  R. Tetzlaff","ShortDetails":"IEEE Transactions on Circuits and Systems I: Regular Papers. 2016","abstract":"The seizure prediction problem has been addressed by many researchers from very different fields for more than three decades. The vision of an implantable seizure prediction device may become reality soon: the first clinical study of such a device has been realized very recently and other realizations are not far behind. Cellular Nonlinear Networks (CNN) were firstly introduced by Chua and Yang in 1988 and later extended to an inherently parallel processing framework called the CNN Universal Machine (CNN-UM). This framework combines high computational power with low power consumption and miniaturized design-making it a very promising basis for the realization of a seizure warning device. In this contribution, we compare the seizure prediction performance of an eigenvalue based PCA-preprocessing followed by a nonlinear CNN signal prediction to the performance of a linear signal prediction approach followed by a level-crossing behavior analysis as well as to the performance of a combination of the two methods.","email":["vanessa.senger@tu-dresden.de","tetzlaff@tu-dresden.de"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7464898","source":"ieee","year":2016,"key":"856a44bb-71e2-4b64-8acf-8b2fcbc117ce","use":1,"doi":"10.1109\/TCSI.2016.2553278"},{"Title":"Feature extraction for histopathological images using Convolutional Neural Network","Description":"N. Hatipoglu,  G. Bilgin","ShortDetails":"2016 24th Signal Processing and Communication Application Conference (SIU). 2016","abstract":"In this study, it is intended to increase the classification accuracy results of histopathalogical images by evaluating spatial relations. As a first step, Convolutional Neural Network (CNN) based features are extracted in the original RGB color space of digital histopathalogical images. Training data sets are formed by selecting equal number of different cellular and extra-cellular structures in spatial domain from the images. Classification models of each training data set are obtained by utilizing CNN (as a supervised classifier), Support Vector Machine (SVM) and Random Forest (RF) methods. Visual classification maps and output tables which are obtained from supervised training methods are presented for comparison purpose in the experimental results section.","email":["nhatipoglu@trakya.edu.tr","gbilgin@yildiz.edu.tr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7495823","source":"ieee","year":2016,"key":"51529103-3bc4-4662-bd32-fb272c7a5455","use":1,"doi":"10.1109\/SIU.2016.7495823"},{"Title":"Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation","Description":"M. Ghafoorian,  N. Karssemeijer,  T. Heskes,  I. W. M. van Uder,  F. E. de Leeuw,  E. Marchiori,  B. van Ginneken,  B. Platel","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Convolutional neural networks (CNN) have been widely used for visual recognition tasks including semantic segmentation of images. While the existing methods consider uniformly sampled single-or multi-scale patches from the neighborhood of each voxel, this approach might be sub-optimal as it captures and processes unnecessary details far away from the center of the patch. We instead propose to train CNNs with non-uniformly sampled patches that allow a wider extent for the sampled patches. This results in more captured contextual information, which is in particular of interest for biomedical image analysis, where the anatomical location of imaging features are often crucial. We evaluate and compare this strategy for white matter hyperintensity segmentation on a test set of 46 MRI scans. We show that the proposed method not only outperforms identical CNNs with uniform patches of the same size (0.780 Dice coefficient compared to 0.736), but also gets very close to the performance of an independent human expert (0.796 Dice coefficient).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493532","source":"ieee","year":2016,"key":"a240d9dc-d43a-4ca1-bbb5-481703653130","use":1,"doi":"10.1109\/ISBI.2016.7493532"},{"Title":"Structure-based assessment of cancerous mitochondria using deep networks","Description":"M. Mishra,  S. Schmitt,  L. Wang,  M. K. Strasser,  C. Marr,  N. Navab,  H. Zischka,  T. Peng","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Mitochondrial functions are essential for cell survival. Pathologic situations, e.g. cancer, can impair mitochondrial function which is frequently reflected by an altered morphology. So far, feature description of mitochondrial structure in cancer remains largely qualitative. In this study, we propose a learning-based approach to quantitatively assess the structure of mitochondria isolated from liver tumor cell lines using convolutional neural network (CNN). Besides achieving a high classification accuracy on isolated mitochondria from healthy tissue and different tumor cell lines which the CNN model was trained on, CNN is also able to classify unseen tumor cell lines, which suggests its superior capability to capture the intrinsic structural transition from healthy to tumor mitochondria.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493327","source":"ieee","year":2016,"key":"b0e2454e-7ac3-46f2-9935-aa4c9c5b305e","use":1,"doi":"10.1109\/ISBI.2016.7493327"},{"Title":"Gland segmentation in colon histology images using hand-crafted features and convolutional neural networks","Description":"W. Li,  S. Manivannan,  S. Akbar,  J. Zhang,  E. Trucco,  S. J. McKenna","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"We investigate glandular structure segmentation in colon histology images as a window-based classification problem. We compare and combine methods based on fine-tuned convolutional neural networks (CNN) and hand-crafted features with support vector machines (HC-SVM). On 85 images of H&E-stained tissue, we find that fine-tuned CNN outperforms HC-SVM in gland segmentation measured by pixel-wise Jaccard and Dice indices. For HC-SVM we further observe that training a second-level window classifier on the posterior probabilities - as an output refinement - can substantially improve the segmentation performance. The final performance of HC-SVM with refinement is comparable to that of CNN. Furthermore, we show that by combining and refining the posterior probability outputs of CNN and HC-SVM together, a further performance boost is obtained.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493530","source":"ieee","year":2016,"key":"3074d1ad-0406-4bdf-aa64-928f7788a8c4","use":1,"doi":"10.1109\/ISBI.2016.7493530"},{"Title":"X-ray image classification using domain transferred convolutional neural networks and local sparse spatial pyramid","Description":"E. Ahn,  A. Kumar,  J. Kim,  C. Li,  D. Feng,  M. Fulham","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"The classification of medical images is a critical step for imaging-based clinical decision support systems. Existing classification methods for X-ray images, however, generally represent the image using only local texture or generic image features (e.g. color or shape) derived from predefined feature spaces. This limits the ability to quantify the image characteristics using general data-derived features learned from image datasets. In this study we present a new algorithm to improve the performance of X-ray image classification, where we propose a late-fusion of domain transferred convolutional neural networks (DT-CNNs) with sparse spatial pyramid (SSP) features derived from a local image dictionary. Our method is robust as it exploits the rich generic information provided by the DT-CNNs and uses the specific local features and characteristics inherent in the X-ray images. Our method was evaluated on a public dataset of X-ray images and was compared to several state-of-the-art approaches. Experimental results show that our method was the most accurate for classification.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493400","source":"ieee","year":2016,"key":"146b1c79-ec56-4232-8bbe-261c9b0fa90e","use":1,"doi":"10.1109\/ISBI.2016.7493400"},{"Title":"Sub-cortical brain structure segmentation using F-CNN'S","Description":"M. Shakeri,  S. Tsogkas,  E. Ferrante,  S. Lippe,  S. Kadoury,  N. Paragios,  I. Kokkinos","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"In this paper we propose a deep learning approach for segmenting sub-cortical structures of the human brain in Magnetic Resonance (MR) image data. We draw inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN) architecture for semantic segmentation of objects in natural images, and adapt it to our task. Unlike previous CNN-based methods that operate on image patches, our model is applied on a full blown 2D image, without any alignment or registration steps at testing time. We further improve segmentation results by interpreting the CNN output as potentials of a Markov Random Field (MRF), whose topology corresponds to a volumetric grid. Alpha-expansion is used to perform approximate inference imposing spatial volumetric homogeneity to the CNN priors. We compare the performance of the proposed pipeline with a similar system using Random Forest-based priors, as well as state-of-art segmentation algorithms, and show promising results on two different brain MRI datasets.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493261","source":"ieee","year":2016,"key":"9b2d19a4-ca25-4c32-8b34-47fce0537133","use":1,"doi":"10.1109\/ISBI.2016.7493261"},{"Title":"Handcrafted features with convolutional neural networks for detection of tumor cells in histology images","Description":"M. N. Kashif,  S. E. A. Raza,  K. Sirinukunwattana,  M. Arif,  N. Rajpoot","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493441","source":"ieee","year":2016,"key":"020b7fe4-7486-4d09-bd25-1cfebf92888c","use":1,"doi":"10.1109\/ISBI.2016.7493441"},{"Title":"An automatic breast cancer grading method in histopathological images based on pixel-, object-, and semantic-level features","Description":"J. Cao,  Z. Qin,  J. Jing,  J. Chen,  T. Wan","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"We present an automatic breast cancer grading method in histopathological images based on the computer extracted pixel-, object-, and semantic-level features derived from convolutional neural networks (CNN). The multiple level features allow not only characterization of nuclear polymorphism, but also extraction of structural and interpretable information within the images. In this study, a hybrid level set based segmentation method was used to segment nuclei from the images. A quantile normalization approach was utilized to improve image color consistency. The semantic level features are extracted by a CNN approach, which describe the proportions of nuclei belonging to the different grades, in conjunction with pixel-level (texture) and object-level (structure) features, to form an integrated set of attributes. A support vector machine classifier was trained to discriminate the breast cancer between low, intermediate, and high grades. The results demonstrated that our method achieved accuracy of 0.90 (low vs. high), and 0.74 (low vs. intermediate), and 0.76 (intermediate vs. high), suggesting that the present method could play a fundamental role in developing a computer-aided breast cancer grading system.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493470","source":"ieee","year":2016,"key":"6cee12ee-edb4-4a50-83e5-d80841608525","use":1,"doi":"10.1109\/ISBI.2016.7493470"},{"Title":"Real-time 2D\/3D registration via CNN regression","Description":"S. Miao,  Z. J. Wang,  Y. Zheng,  R. Liao","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D\/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493536","source":"ieee","year":2016,"key":"9770284b-6194-487f-a284-abb5003670b5","use":1,"doi":"10.1109\/ISBI.2016.7493536"},{"Title":"Epileptiform spike detection via convolutional neural networks","Description":"A. R. Johansen,  J. Jin,  T. Maszczyk,  J. Dauwels,  S. S. Cash,  M. B. Westover","ShortDetails":"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2016","abstract":"The EEG of epileptic patients often contains sharp waveforms called \"spikes\", occurring between seizures. Detecting such spikes is crucial for diagnosing epilepsy. In this paper, we develop a convolutional neural network (CNN) for detecting spikes in EEG of epileptic patients in an automated fashion. The CNN has a convolutional architecture with filters of various sizes applied to the input layer, leaky ReLUs as activation functions, and a sigmoid output layer. Balanced mini-batches were applied to handle the imbalance in the data set. Leave-one-patient-out cross-validation was carried out to test the CNN and benchmark models on EEG data of five epilepsy patients. We achieved 0.947 AUC for the CNN, while the best performing benchmark model, Support Vector Machines with Gaussian kernel, achieved an AUC of 0.912.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7471776","source":"ieee","year":2016,"key":"d517201c-6993-4254-8b47-3d6221d8117f","use":1,"doi":"10.1109\/ICASSP.2016.7471776"},{"Title":"AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images","Description":"S. Albarqouni,  C. Baur,  F. Achilles,  V. Belagiannis,  S. Demirci,  N. Navab","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7405343","source":"ieee","year":2016,"key":"819df952-c3e8-4e43-aafc-8267889d9cee","use":1,"doi":"10.1109\/TMI.2016.2528120"},{"Title":"Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images","Description":"S. Pereira,  A. Pinto,  V. Alves,  C. A. Silva","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 \u00d73 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.","email":["id5692@alunos.uminho.pt","valves@di.uminho.pt","csilva@dei.uminho.pt"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7426413","source":"ieee","year":2016,"key":"ecdbe0a6-54b4-44b5-be32-7cc1075d0077","use":1,"doi":"10.1109\/TMI.2016.2538465"},{"Title":"Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images","Description":"K. Sirinukunwattana,  S. E. A. Raza,  Y. W. Tsang,  D. R. J. Snead,  I. A. Cree,  N. M. Rajpoot","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7399414","source":"ieee","year":2016,"key":"cd645817-5324-4c7d-9ccf-c9af41607c87","use":1,"doi":"10.1109\/TMI.2016.2525803"},{"Title":"Deep Learning for Imbalanced Multimedia Data Classification","Description":"Y. Yan,  M. Chen,  M. L. Shyu,  S. C. Chen","ShortDetails":"2015 IEEE International Symposium on Multimedia (ISM). 2015","abstract":"Classification of imbalanced data is an important research problem as lots of real-world data sets have skewed class distributions in which the majority of data instances (examples) belong to one class and far fewer instances belong to others. While in many applications, the minority instances actually represent the concept of interest (e.g., fraud in banking operations, abnormal cell in medical data, etc.), a classifier induced from an imbalanced data set is more likely to be biased towards the majority class and show very poor classification accuracy on the minority class. Despite extensive research efforts, imbalanced data classification remains one of the most challenging problems in data mining and machine learning, especially for multimedia data. To tackle this challenge, in this paper, we propose an extended deep learning approach to achieve promising performance in classifying skewed multimedia data sets. Specifically, we investigate the integration of bootstrapping methods and a state-of-the-art deep learning approach, Convolutional Neural Networks (CNNs), with extensive empirical studies. Considering the fact that deep learning approaches such as CNNs are usually computationally expensive, we propose to feed low-level features to CNNs and prove its feasibility in achieving promising performance while saving a lot of training time. The experimental results show the effectiveness of our framework in classifying severely imbalanced data in the TRECVID data set.","email":["y.yan4@umiami.edu","minchen2@uw.edu","shyu@miami.edu","chens@cs.fiu.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7442383","source":"ieee","year":2015,"key":"f87f3dc9-dde7-4d51-bb1a-68d7f48ad208","use":1,"doi":"10.1109\/ISM.2015.126"},{"Title":"ABM and CNN application in ventral stream of visual system","Description":"B. Yousefi,  P. Yousefi","ShortDetails":"2015 IEEE Student Symposium in Biomedical Engineering & Sciences (ISSBES). 2015","abstract":"This paper addresses an investigation regarding the suitability of two different techniques, Active Basis Model (ABM) and Gabor based Convolutional Neural Network (CNN or G-ConvNets) in the mechanism for recognition of biological movement (mammalian visual system model). This method inspired by ventral streams which provide the form information. Both of these approaches contain information of the shape of human object obtained by the Gabor features. The comparison of these methods concludes advantages and drawbacks of both methods that shown CNN have advantage of recognition of the action (for CNN only walking, running and waving) however ABM basically used for object recognition task (not particularly for action recognition).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7435920","source":"ieee","year":2015,"key":"1323a727-e7c2-4023-a479-5d7519e58e0d","use":1,"doi":"10.1109\/ISSBES.2015.7435920"},{"Title":"A hybrid convolutional neural networks with extreme learning machine for WCE image classification","Description":"J. s. Yu,  J. Chen,  Z. Q. Xiang,  Y. X. Zou","ShortDetails":"2015 IEEE International Conference on Robotics and Biomimetics (ROBIO). 2015","abstract":"Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25% classification accuracy can be achieved in average.","email":["1301213735@pku.edu.cn","1401213843@pku.edu.cn","x-lion@pku.edu.cn","zouyx@pkusz.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7419037","source":"ieee","year":2015,"key":"bf34011f-9ade-441c-ae6c-14081c9c797f","use":1,"doi":"10.1109\/ROBIO.2015.7419037"},{"Title":"Deep Feature Learning with Discrimination Mechanism for Brain Tumor Segmentation and Diagnosis","Description":"L. Zhao,  K. Jia","ShortDetails":"2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP). 2015","abstract":"Brain tumor segmentation is one of the main challenging problems in computer vision and its early diagnosis is critical to clinics. Segmentation needs to be accurate, efficient and robust to avoid influences caused by various large and complex biases added to images. This paper proposes a multiple convolutional neural network (CNNs) framework with discrimination mechanism which is effective to achieve these goals. First of all, this paper proposes to construct different triplanar 2D CNNs architecture for 3D voxel classification, greatly reducing segmentation time. Experiment is conducted on images provided by Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized by MICCAI 2013 for both training and testing. As T1, T1-enhanced, T2 and FLAIR MRI images are utilized, multimodal features are combined. As a result, accuracy, sensitivity and specificity are comparable in comparison with manual gold standard images and better than state-of-the-art segmentation methods.","email":["zhaoliya@emails.bjut.edu.cn","kebinj@bjut.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7415818","source":"ieee","year":2015,"key":"a19f6389-e5e4-41d2-9c41-3dae43535d01","use":1,"doi":"10.1109\/IIH-MSP.2015.41"},{"Title":"Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural Networks","Description":"S. Kiranyaz,  T. Ince,  M. Gabbouj","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2016","abstract":"Goal: This paper presents a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system. Methods: An adaptive implementation of 1-D convolutional neural networks (CNNs) is inherently used to fuse the two major blocks of the ECG classification into a single learning body: feature extraction and classification. Therefore, for each patient, an individual and simple CNN will be trained by using relatively small common and patient-specific training data, and thus, such patient-specific feature extraction ability can further improve the classification performance. Since this also negates the necessity to extract hand-crafted manual features, once a dedicated CNN is trained for a particular patient, it can solely be used to classify possibly long ECG data stream in a fast and accurate manner or alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. Results: The results over the MIT-BIH arrhythmia benchmark database demonstrate that the proposed solution achieves a superior classification performance than most of the state-of-the-art methods for the detection of ventricular ectopic beats and supraventricular ectopic beats. Conclusion: Besides the speed and computational efficiency achieved, once a dedicated CNN is trained for an individual patient, it can solely be used to classify his\/her long ECG records such as Holter registers in a fast and accurate manner. Significance: Due to its simple and parameter invariant nature, the proposed system is highly generic, and, thus, applicable to any ECG dataset.","email":["mkiranyaz@qu.edu.qa","turker.ince@izmirekonomi.edu.tr","Moncef.gabbouj@tut.fi."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7202837","source":"ieee","year":2016,"key":"bd8b8fd6-e59f-49cd-8cd1-80152984d851","use":1,"doi":"10.1109\/TBME.2015.2468589"},{"Title":"Patient prognosis from vital sign time series: Combining convolutional neural networks with a dynamical systems approach","Description":"L. w. Lehman,  M. Ghassemi,  J. Snoek,  S. Nemati","ShortDetails":"2015 Computing in Cardiology Conference (CinC). 2015","abstract":"In this work, we propose a stacked switching vector-autoregressive (SVAR)-CNN architecture to model the changing dynamics in physiological time series for patient prognosis. The SVAR-layer extracts dynamical features (or modes) from the time-series, which are then fed into the CNN-layer to extract higher-level features representative of transition patterns among the dynamical modes. We evaluate our approach using 8-hours of minute-by-minute mean arterial blood pressure (BP) from over 450 patients in the MIMIC-II database. We modeled the time-series using a third-order SVAR process with 20 modes, resulting in first-level dynamical features of size 20\u00d7480 per patient. A fully connected CNN is then used to learn hierarchical features from these inputs, and to predict hospital mortality. The combined CNN\/SVAR approach using BP time-series achieved a median and interquartile-range AUC of 0.74 [0.69, 0.75], significantly outperforming CNN-alone (0.54 [0.46, 0.59]), and SVAR-alone with logistic regression (0.69 [0.65, 0.72]). Our results indicate that including an SVAR layer improves the ability of CNNs to classify nonlinear and nonstationary time-series.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7411099","source":"ieee","year":2015,"key":"65f877fd-9f4f-4ca9-8912-351658aa5436","use":1,"doi":"10.1109\/CIC.2015.7411099"},{"Title":"Lesion detection of endoscopy images based on convolutional neural network features","Description":"R. Zhu,  R. Zhang,  D. Xue","ShortDetails":"2015 8th International Congress on Image and Signal Processing (CISP). 2015","abstract":"Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.","email":["zrs1991@mail.ustc.edu.cn","zrong@ustc.edu.cn","xuedixiu@mail.ustc.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7407907","source":"ieee","year":2015,"key":"edf6b323-ae15-496e-995b-2dc0e98e107e","use":1,"doi":"10.1109\/CISP.2015.7407907"},{"Title":"An Automatic Learning-Based Framework for Robust Nucleus Segmentation","Description":"F. Xing,  Y. Xie,  L. Yang","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7274740","source":"ieee","year":2016,"key":"c065a832-a9d5-4091-88ca-a783f59bd2a7","use":1,"doi":"10.1109\/TMI.2015.2481436"},{"Title":"Prediction of driver's drowsy and alert states from EEG signals with deep learning","Description":"M. Hajinoroozi,  Z. Mao,  Y. Huang","ShortDetails":"2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP). 2015","abstract":"We investigate in this paper deep learning (DL) solutions for prediction of driver's cognitive states (drowsy or alert) using EEG data. We discussed the novel channel-wise convolutional neural network (CCNN) and CCNN-R which is a CCNN variation that uses Restricted Boltzmann Machine in order to replace the convolutional filter. We also consider bagging classifiers based on DL hidden units as an alternative to the conventional DL solutions. To test the performance of the proposed methods, a large EEG dataset from 3 studies of driver's fatigue that includes 70 sessions from 37 subjects is assembled. All proposed methods are tested on both raw EEG and Independent Component Analysis (ICA)-transformed data for cross-session predictions. The results show that CCNN and CCNN-R outperform deep neural networks (DNN) and convolutional neural networks (CNN) as well as other non-DL algorithms and DL with raw EEG inputs achieves better performance than ICA features.","email":["zyq860@my.utsa.edu.edu","mzj168@hotmail.com","Yufei.huang@utsa.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7383844","source":"ieee","year":2015,"key":"6a593034-7ed0-4310-a388-9cb4083dd4be","use":1,"doi":"10.1109\/CAMSAP.2015.7383844"},{"Title":"Maximum Margin Learning of t-SPNs for Cell Classification With Filtered Input","Description":"H. Kang,  C. D. Yoo,  Y. Na","ShortDetails":"IEEE Journal of Selected Topics in Signal Processing. 2016","abstract":"An algorithm based on a deep probabilistic architecture referred to as tree-structured sum-product network (t-SPN) is considered for cells classification. The t-SPN is a rooted acyclic graph constructed as a tree of several sum-product networks where each network is constructed over a subset of most confusing class features. The constructed t-SPN architecture is learned by maximizing the margin which is defined to be the difference in the conditional probability between the true and the most competitive false labels. To enhance generalization, l<sub>2<\/sub>-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate compared to other state-of-the-art algorithms that include convolutional neural network (CNN) based algorithms. Ideal high-pass filter was more effective on the HEp-2 dataset which is based on immunofluorescence staining while the LOG was more effective on Feulgen dataset which is based on Feulgen staining.","email":["ihaeyong@gmail.com.","cdyoo@ee.kaist.ac.kr.","ycna@kaist.ac.kr."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7332738","source":"ieee","year":2016,"key":"3fd32f38-477b-4ca0-9e2b-3e257fd57d6c","use":1,"doi":"10.1109\/JSTSP.2015.2502542"},{"Title":"Rich feature hierarchies for cell detecting under phase contrast microscopy images","Description":"F. Deng,  H. Hu,  S. Chen,  Q. Guan,  Y. Zou","ShortDetails":"2015 Sixth International Conference on Intelligent Control and Information Processing (ICICIP). 2015","abstract":"R-CNN (region-convolutional neural network) has recently achieved very outstanding results in variety of visual detecting fields, and its function of object-proposal-generation can achieve effective training models by using as small samples as possible in the field of machine learning. In this paper, a modified R-CNN is proposed and applied to detect cells under phase contrast microscopy images by adopting multiple object-proposal-generations instead of a single one to extract candidate regions. The results show that the proposed method can obtain better performance than the traditional method by using a single object-proposal-generation.","email":["csy@zjut.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7388195","source":"ieee","year":2015,"key":"2a2d0c01-4d6f-4ee5-9ecd-7652866e82ac","use":1,"doi":"10.1109\/ICICIP.2015.7388195"},{"Title":"Human Epithelial Type 2 cell classification with convolutional neural networks","Description":"N. Bayramoglu,  J. Kannala,  J. Heikkil\u00e4","ShortDetails":"2015 IEEE 15th International Conference on Bioinformatics and Bioengineering (BIBE). 2015","abstract":"Automated cell classification in Indirect Immunofluorescence (IIF) images has potential to be an important tool in clinical practice and research. This paper presents a framework for classification of Human Epithelial Type 2 cell IIF images using convolutional neural networks (CNNs). Previuos state-of-the-art methods show classification accuracy of 75.6% on a benchmark dataset. We conduct an exploration of different strategies for enhancing, augmenting and processing training data in a CNN framework for image classification. Our proposed strategy for training data and pre-training and fine-tuning the CNN network led to a significant increase in the performance over other approaches that have been used until now. Specifically, our method achieves a 80.25% classification accuracy. Source code and models to reproduce the experiments in the paper is made publicly available.","email":["jkannala@ee.oulu.fi","jth@ee.oulu.fi","nyalcinb@ee.oulu.fi"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7367705","source":"ieee","year":2015,"key":"962c98be-05c9-487b-a75b-ce20c74f5a6c","use":1,"doi":"10.1109\/BIBE.2015.7367705"},{"Title":"Microvascular morphological type recognition using trainable feature extractor","Description":"D. X. Xue,  R. Zhang,  R. S. Zhu","ShortDetails":"2015 International Symposium on Bioelectronics and Bioinformatics (ISBB). 2015","abstract":"This paper focuses on the problem of feature extraction and the classification task of microvascular morphological type to aid esophageal cancer detection. A specialized convolutional neural network (CNN) is designed to extract hierarchical features and Support Vector Machines (SVMs) are introduced to enhance the generalization ability of classifiers. Experiments are conducted on the NBI-ME dataset, achieving a recognition rate of 88.19% on patch level. The results show that the CNN-SVM model beats models of traditional features with SVM as well as the original CNN with softmax. The synthesis results indicate this system is able to assist clinical diagnose to a certain extent.","email":["xuedixiu@mail.ustc.edu.cn","zrong@ustc.edu.cn","zrs1991@mail.ustc.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7344925","source":"ieee","year":2015,"key":"a0361dee-a14c-4d09-a259-295243474746","use":1,"doi":"10.1109\/ISBB.2015.7344925"},{"Title":"Convolutional Neural Networks for patient-specific ECG classification","Description":"S. Kiranyaz,  T. Ince,  R. Hamila,  M. Gabbouj","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"We propose a fast and accurate patient-specific electrocardiogram (ECG) classification and monitoring system using an adaptive implementation of 1D Convolutional Neural Networks (CNNs) that can fuse feature extraction and classification into a unified learner. In this way, a dedicated CNN will be trained for each patient by using relatively small common and patient-specific training data and thus it can also be used to classify long ECG records such as Holter registers in a fast and accurate manner. Alternatively, such a solution can conveniently be used for real-time ECG monitoring and early alert system on a light-weight wearable device. The experimental results demonstrate that the proposed system achieves a superior classification performance for the detection of ventricular ectopic beats (VEB) and supraventricular ectopic beats (SVEB).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318926","source":"ieee","year":2015,"key":"bec11f5f-e6be-4151-abd6-18d56dfdab34","use":1,"doi":"10.1109\/EMBC.2015.7318926"},{"Title":"A comparative study for chest radiograph image retrieval using binary texture and deep learning classification","Description":"Y. Anavi,  I. Kogan,  E. Gelbart,  O. Geva,  H. Greenspan","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the \u201cdescriptor\u201d-based approach); the second approach (\u201cclassification\u201d-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.","email":["Serkan.Kiranyaz@gmail.com","turker.ince@izmirekonomi.edu.tr","hamila@qu.edu.qa","Moncef.gabbouj@tut.fi."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7319008","source":"ieee","year":2015,"key":"7f38fda9-436b-48cd-bc53-178eea28d83b","use":1,"doi":"10.1109\/EMBC.2015.7319008"},{"Title":"Automatic localization of the left ventricle in cardiac MRI images using deep learning","Description":"O. Emad,  I. A. Yassine,  A. S. Fahmy","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Automatic localization of the left ventricle (LV) in cardiac MRI images is an essential step for automatic segmentation, functional analysis, and content based retrieval of cardiac images. In this paper, we introduce a new approach based on deep Convolutional Neural Network (CNN) to localize the LV in cardiac MRI in short axis views. A six-layer CNN with different kernel sizes was employed for feature extraction, followed by Softmax fully connected layer for classification. The pyramids of scales analysis was introduced in order to take account of the different sizes of the heart. A publically-available database of 33 patients was used for learning and testing. The proposed method was able it localize the LV with 98.66%, 83.91% and 99.07% for accuracy, sensitivity and specificity respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318454","source":"ieee","year":2015,"key":"c829ce2f-2d03-4521-9830-909cd8bbd055","use":1,"doi":"10.1109\/EMBC.2015.7318454"},{"Title":"Resting State EEG-based biometrics for individual identification using convolutional neural networks","Description":"L. Ma,  J. W. Minett,  T. Blu,  W. S. Y. Wang","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Biometrics is a growing field, which permits identification of individuals by means of unique physical features. Electroencephalography (EEG)-based biometrics utilizes the small intra-personal differences and large inter-personal differences between individuals' brainwave patterns. In the past, such methods have used features derived from manually-designed procedures for this purpose. Another possibility is to use convolutional neural networks (CNN) to automatically extract an individual's best and most unique neural features and conduct classification, using EEG data derived from both Resting State with Open Eyes (REO) and Resting State with Closed Eyes (REC). Results indicate that this CNN-based joint-optimized EEG-based Biometric System yields a high degree of accuracy of identification (88%) for 10-class classification. Furthermore, rich inter-personal difference can be found using a very low frequency band (0-2Hz). Additionally, results suggest that the temporal portions over which subjects can be individualized is less than 200 ms.","email":["malantju@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318985","source":"ieee","year":2015,"key":"c3dccbda-9980-4ca3-b676-16524bde4f60","use":1,"doi":"10.1109\/EMBC.2015.7318985"},{"Title":"Automatic detection of cell divisions (mitosis) in live-imaging microscopy images using Convolutional Neural Networks","Description":"A. Shkolyar,  A. Gefen,  D. Benayahu,  H. Greenspan","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"We propose a semi-automated pipeline for the detection of possible cell divisions in live-imaging microscopy and the classification of these mitosis candidates using a Convolutional Neural Network (CNN). We use time-lapse images of NIH3T3 scratch assay cultures, extract patches around bright candidate regions that then undergo segmentation and binarization, followed by a classification of the binary patches into either containing or not containing cell division. The classification is performed by training a Convolutional Neural Network on a specially constructed database. We show strong results of AUC = 0.91 and F-score = 0.89, competitive with state-of-the-art methods in this field.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318469","source":"ieee","year":2015,"key":"7f65d711-f35a-4f1e-b06d-0d2fb34ac909","use":1,"doi":"10.1109\/EMBC.2015.7318469"},{"Title":"Bacterial colony counting by Convolutional Neural Networks","Description":"A. Ferrari,  S. Lombardi,  A. Signoroni","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Counting bacterial colonies on microbiological culture plates is a time-consuming, error-prone, nevertheless fundamental task in microbiology. Computer vision based approaches can increase the efficiency and the reliability of the process, but accurate counting is challenging, due to the high degree of variability of agglomerated colonies. In this paper, we propose a solution which adopts Convolutional Neural Networks (CNN) for counting the number of colonies contained in confluent agglomerates, that scored an overall accuracy of the 92.8% on a large challenging dataset. The proposed CNN-based technique for estimating the cardinality of colony aggregates outperforms traditional image processing approaches, becoming a promising approach to many related applications.","email":["hayit@eng.tau.ac.il1MedicalImageProcessingLab"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7320116","source":"ieee","year":2015,"key":"0bb3b668-1abd-47aa-b1d1-eb5a05a96a6c","use":1,"doi":"10.1109\/EMBC.2015.7320116"},{"Title":"Interleaved text\/image Deep Mining on a large-scale radiology database","Description":"H. C. Shin,  Le Lu,  L. Kim,  A. Seff,  J. Yao,  R. M. Summers","ShortDetails":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015","abstract":"Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text\/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images\/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning \u201cdata-hungry\u201d obstacle in the medical domain.","email":["jyao@cc.nih.gov","ZN@E","G@1","R@1","R@1","R@1","R@20","R@1","R@1"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7298712","source":"ieee","year":2015,"key":"8d4f319c-c2e0-4db3-b7df-43656d444712","use":1,"doi":"10.1109\/CVPR.2015.7298712"},{"Title":"Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network","Description":"Y. Zou,  L. Li,  Y. Wang,  J. Yu,  Y. Li,  W. J. Deng","ShortDetails":"2015 IEEE International Conference on Digital Signal Processing (DSP). 2015","abstract":"This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.","email":["zouyx@pkusz.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7252086","source":"ieee","year":2015,"key":"8dfaf685-33a2-4bb3-9a89-2d1d94119027","use":1,"doi":"10.1109\/ICDSP.2015.7252086"},{"Title":"Cellular nonlinear network-based signal prediction in epilepsy: Method comparison","Description":"V. Senger,  R. Tetzlaff","ShortDetails":"2015 IEEE International Symposium on Circuits and Systems (ISCAS). 2015","abstract":"The seizure prediction problem has been addressed by many researchers from very different fields for more than three decades. The vision of an implantable seizure prediction device may become reality now: the first clinical study of such a device has been realized very recently and other realizations are not far behind. Cellular Nonlinear Networks (CNN) were firstly introduced by Chua and Yang in 1988 and later extended to an inherently parallel processing framework called the CNN Universal Machine (CNN-UM). This framework combines high computational power with low power consumption and miniaturized design - making it a very promising basis for the realization of a seizure warning device. In this contribution, we compare the seizure prediction performance of an eigenvalue based PCA-preprocessing followed by a nonlinear CNN signal prediction to the performance of a linear signal prediction approach followed by a level-crossing behavior analysis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7168654","source":"ieee","year":2015,"key":"5d2a85d1-39d1-4a46-aa4d-89f484836a62","use":1,"doi":"10.1109\/ISCAS.2015.7168654"},{"Title":"CNN in drug design \u2014 Recent developments","Description":"J. D. Wichard,  M. J. Ogorza\u0142ek,  C. Merkwirth","ShortDetails":"2015 IEEE International Symposium on Circuits and Systems (ISCAS). 2015","abstract":"We describe a method for construction of specific types of Neural Networks composed of structures directly linked to the structure of the molecule under consideration. Each molecule can be represented by a unique neural connectivity problem (graph) which can be programmed onto a Cellular Neural Network. The idea was to translate chemical structures like small organic molecules or peptides into a self learning environment which is CNN based. In the case of small molecules, each cell of the CNN stands for one atom of the molecule under consideration. But in contrast to the standard CNN architecture where each cell is connected to the neighboring cells, only those cells of the feature net are connected for which there also exists a chemical bond in the molecule under consideration. This implies that the feature net topology varies from molecule to molecule. In the case of peptides, the amino acids that form the building blocks of the peptide are reflected by the CNN cells wherein the amino acid sequence defines the network topology. Unlike the standard CNN used for image processing, there are no input values like the input image that are fed into the feature net. Instead, all information about the input molecule is supplied to the feature net by means of the topology. The output of several feature nets is fed into a supervisor neural network which computes the final output value. The combination of several feature nets and a supervisor networks constitutes the Molecular Graph Network (MGN). The designed networks are used for selection of molecules representing wanted properties such as activity against specific diseases, interactions with other compounds, toxicity etc. and possibly being candidates to be tested further as new drugs.","email":["vanessa.senger@tu-dresden.de"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7168656","source":"ieee","year":2015,"key":"57ab81bb-ded1-4e67-aceb-f072699b34f4","use":1,"doi":"10.1109\/ISCAS.2015.7168656"},{"Title":"Iteratively training classifiers for circulating tumor cell detection","Description":"Y. Mao,  Z. Yin,  J. M. Schober","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"The number of Circulating Tumor Cells (CTCs) in blood provides an indication of disease progression and tumor response to chemotherapeutic agents. Hence, routine detection and enumeration of CTCs in clinical blood samples have significant applications in early cancer diagnosis and treatment monitoring. In this paper, we investigate two classifiers for image-based CTC detection: (1) Support Vector Machine (SVM) with hard-coded Histograms of Oriented Gradients (HoG) features; and (2) Convolutional Neural Network (CNN) with automatically learned features. For both classifiers, we present an effective and efficient training algorithm, by which the most representative negative samples are iteratively collected to accurately define the classification boundary between positive and negative samples. The two iteratively trained classifiers are validated on a challenging dataset with high performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163847","source":"ieee","year":2015,"key":"254e2c17-30ad-4fee-8c37-f42a25f7aaf0","use":1,"doi":"10.1109\/ISBI.2015.7163847"},{"Title":"Region segmentation in histopathological breast cancer images using deep convolutional neural network","Description":"H. Su,  F. Liu,  Y. Xie,  F. Xing,  S. Meyyappan,  L. Yang","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Computer aided diagnosis of breast cancers often relies on automatic image analysis of histopathology images. The automatic region segmentation in breast cancer is challenging due to: i) large regional variations, and ii) high computational costs of pixel-wise segmentation. Deep convolutional neural network (CNN) is proven to be an effective method for image recognition and classification. However, it is often computationally expensive. In this paper, we propose to apply a fast scanning deep convolutional neural network (fCNN) to pixel-wise region segmentation. The fCNN removes the redundant computations in the original CNN without sacrificing its performance. In our experiment it takes only 2.3 seconds to segment an image with size 1000 \u00d7 1000. The comparison experiments show that the proposed system outperforms both the LBP feature-based and texton-based pixel-wise methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163815","source":"ieee","year":2015,"key":"06ce600f-0bf3-49c9-9b2e-2eb193e2de94","use":1,"doi":"10.1109\/ISBI.2015.7163815"},{"Title":"Automatic muscle perimysium annotation using deep convolutional neural network","Description":"M. Sapkota,  F. Xing,  H. Su,  L. Yang","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Diseased skeletal muscle expresses mononuclear cell infiltration in the regions of perimysium. Accurate annotation or segmentation of perimysium can help biologists and clinicians to determine individualized patient treatment and allow for reasonable prognostication. However, manual perimysium annotation is time consuming and prone to inter-observer variations. Meanwhile, the presence of ambiguous patterns in muscle images significantly challenge many traditional automatic annotation algorithms. In this paper, we propose an automatic perimysium annotation algorithm based on deep convolutional neural network (CNN). We formulate the automatic annotation of perimysium in muscle images as a pixel-wise classification problem, and the CNN is trained to label each image pixel with raw RGB values of the patch centered at the pixel. The algorithm is applied to 82 diseased skeletal muscle images. We have achieved an average precision of 94% on the test dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163850","source":"ieee","year":2015,"key":"6f01a643-a3e3-47f6-8f48-5d41a5e050d7","use":1,"doi":"10.1109\/ISBI.2015.7163850"},{"Title":"Deep learning for automatic cell detection in wide-field microscopy zebrafish images","Description":"B. Dong,  L. Shao,  M. Da Costa,  O. Bandmann,  A. F. Frangi","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"The zebrafish has become a popular experimental model organism for biomedical research. In this paper, a unique framework is proposed for automatically detecting Tyrosine Hydroxylase-containing (TH-labeled) cells in larval zebrafish brain z-stack images recorded through the wide-field microscope. In this framework, a supervised max-pooling Convolutional Neural Network (CNN) is trained to detect cell pixels in regions that are preselected by a Support Vector Machine (SVM) classifier. The results show that the proposed deep-learned method outperforms hand-crafted techniques and demonstrate its potential for automatic cell detection in wide-field microscopy z-stack zebrafish images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163986","source":"ieee","year":2015,"key":"53677715-d9f4-42b6-9bc2-a5c0e7b9c8f9","use":1,"doi":"10.1109\/ISBI.2015.7163986"},{"Title":"A Reliable Distributed Convolutional Neural Network for Biology Image Segmentation","Description":"X. Zhang,  G. Tan,  M. Chen","ShortDetails":"2015 15th IEEE\/ACM International Symposium on Cluster, Cloud and Grid Computing. 2015","abstract":"Many modern advanced biology experiments are carried on by Electron Microscope(EM) image analysis. Segmentation is one of the most important and complex steps in the process of image analysis. Previous ISBI contest results and related research show that Convolution Neural Network(CNN)has high classification accuracy in EM image segmentation. Besides it eliminates the pain of extracting complex features which's indispensable for traditional classification algorithms. However CNN's extremely time-consuming and fault vulnerability due to long time execution prevent it from being widely used in practice. In this paper, we try to address these problems by providing reliable high performance CNN framework for medial image segmentation. Our CNN has light weighted user level checkpoint, which costs seconds when doing one checkpoint and restart. On the fact of lacking in platform diversity in current parallel CNN framework, our CNN system tries to make it general by providing distributed cross-platform parallelism implementation. Currently we have integrated Theano's GPU implementation in our CNNsystem, and we explore parallelism potential on multi-core CPUs and many-core Intel Phi by testing performance of main kernel functions of CNN. In the future, we will integrate implementation son other two platforms into our CNN framework.","email":["zhangxiuxia@ict.ac.cn","tgm@ict.ac.cn","cmy@ict.ac.cn","16@30X30","16@15X15","16@12X12","16@6X6"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7152555","source":"ieee","year":2015,"key":"f6db039a-47d4-4ad7-9b2a-2bcd70d94be0","use":1,"doi":"10.1109\/CCGrid.2015.108"},{"Title":"Morphological process based segmentation for the detection of exudates from the retinal images of diabetic patients","Description":"Mahendran G.,  Dhanasekaran R.,  Narmadha Devi K. N.","ShortDetails":"2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies. 2014","abstract":"Diabetic Retinopathy is an ocular systemic disease caused by complication of diabetes. It is a major cause of blindness in both middle and advanced age group. Earlier recognition of diabetic retinopathy shields understanding from visual impairment. The heading side effect of this difficulty seeing is the exudates. Exudates are the melted watery grasping solutes, proteins, cells, or cell garbage spilled from the harmed veins into near by tissues or on tissue surfaces in the retina. The spillage of these proteins or lipids causes vision misfortune to the patients. Distinguishing the exudates ahead of time can protect the diabetic patients from difficulty seeing. Ophthalmologists use widening system to identify the exudates. But it causes the irritation to the patients' eyes. This paper focuses on an automated method which detects the diabetic retinopathy through identifying exudates by Morphological process in colour fundus retinal images and then segregates the severity of the lesions. The severity level of the disease was achieved by Cascade Neural Network (CNN) classifier.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7019345","source":"ieee","year":2014,"key":"8db5ad43-c5ee-4c16-9e9b-763b7854f9d3","use":1,"doi":"10.1109\/ICACCCT.2014.7019345"},{"Title":"An approach for chest tube detection in chest radiographs","Description":"C. A. Mercan,  M. S. Celebi","ShortDetails":"IET Image Processing. 2014","abstract":"It is known that overlapping tissues cause highly complex projections in chest radiographs. In addition, artificial objects, such as catheters, chest tubes and pacemakers can appear on these radiographs. It is important that the anomaly detection algorithms are not confused by these objects. To achieve this goal, the authors propose an approach to train a convolutional neural network (CNN) to detect chest tubes present on radiographs. To detect the chest tube skeleton as the final output in a better manner, non-uniform rational B-spline curves are used to automatically fit with the CNN output. This is the first study conducted to automatically detect artificial objects in the lung region of chest radiographs. Other automatic detection schemes work on the mediastinum. The authors evaluated the performance of the model using a pixel-based receiver operating characteristic (ROC) analysis. Each true positive, true negative, false positive and false negative pixel is counted and used for calculating average accuracy, sensitivity and specificity percentages. The results were 99.99% accuracy, 59% sensitivity and 99.99% specificity. Therefore they obtained promising results on the detection of artificial objects.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6733843","source":"ieee","year":2014,"key":"3e55761c-2ce0-47dc-a1b3-ba33cd61e236","use":1,"doi":"10.1049\/iet-ipr.2013.0239"},{"Title":"Probabilistic visual search for masses within mammography images using deep learning","Description":"M. G. Ertosun,  D. L. Rubin","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85% accuracy for the task of identifying images that contain a mass, and we were able to localize 85% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359868","source":"ieee","year":2015,"key":"3dc52778-d22a-4c55-8d06-b61597b2bc2c","use":1,"doi":"10.1109\/BIBM.2015.7359868"},{"Title":"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","Description":"H. R. Roth,  L. Lu,  J. Liu,  J. Yao,  A. Seff,  K. Cherry,  L. Kim,  R. M. Summers","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7279156","source":"ieee","year":2016,"key":"d9a82fd0-a769-40c6-b39b-c9adb06ff686","use":1,"doi":"10.1109\/TMI.2015.2482920"},{"Title":"DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks","Description":"N. Pezzotti,  T. H\u00f6llt,  J. v. Gemert,  B. P. F. Lelieveldt,  E. Eisemann,  A. Vilanova","ShortDetails":"IEEE Transactions on Visualization and Computer Graphics. 2017","abstract":"Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.","email":["c.ahmet.mercan@be.itu.edu.tr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8019872","source":"ieee","year":2017,"key":"eb621b24-9742-416b-be24-7654a1493310","use":1,"doi":"10.1109\/TVCG.2017.2744358"},{"Title":"Efficient Training of Convolutional Deep Belief Networks in the Frequency Domain for Application to High-Resolution 2D and 3D Images","Description":"T. Brosch,  R. Tam","ShortDetails":"Neural Computation. 2015","abstract":"<para>Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8\u00a0times on 2D images and up to 200\u00a0times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128 \u00d7 128 \u00d7 128 voxels practical, which opens new directions for using deep learning for medical image analysis.<\/para>","email":["brosch.tom@gmail.com","roger.tam@ubc.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6998135","source":"ieee","year":2015,"key":"a50178f7-5efa-4e4c-b6ce-0d5663361aeb","use":1,"doi":"10.1162\/NECO_a_00682"},{"Title":"Biopsy-guided learning with deep convolutional neural networks for Prostate Cancer detection on multiparametric MRI","Description":"Y. Tsehay,  N. Lay,  X. Wang,  J. T. Kwak,  B. Turkbey,  P. Choyke,  P. Pinto,  B. Wood,  R. M. Summers","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Prostate Cancer (PCa) is highly prevalent and is the second most common cause of cancer-related deaths in men. Multiparametric MRI (mpMRI) is robust in detecting PCa. We developed a weakly supervised computer-aided detection (CAD) system that uses biopsy points to learn to identify PCa on mpMRI. Our CAD system, which is based on a deep convolutional neural network architecture, yielded an area under the curve (AUC) of 0.903\u00b10.009 on a receiver operation characteristic (ROC) curve computed on 10 different models in a 10 fold cross-validation. 9 of the 10 ROCs were statistically significantly different from a competing support vector machine based CAD, which yielded a 0.86 AUC when tested on the same dataset (\u03b1 = 0.05). Furthermore, our CAD system proved to be more robust in detecting high-grade transition zone lesions.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950602","source":"ieee","year":2017,"key":"a8034352-89c6-4215-8588-d97499a2305b","use":1,"doi":"10.1109\/ISBI.2017.7950602"},{"Title":"Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse Patch Matching","Description":"Y. Guo,  Y. Gao,  D. Shen","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods.","email":["yrguo@email.unc.edu","yzgao@cs.unc.edu","dgshen@med.unc.edu","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7353170","source":"ieee","year":2016,"key":"daf3d641-56c6-407e-81f6-b0d8cde40da7","use":1,"doi":"10.1109\/TMI.2015.2508280"},{"Title":"Combining fully convolutional networks and graph-based approach for automated segmentation of cervical cell nuclei","Description":"L. Zhang,  M. Sonka,  L. Lu,  R. M. Summers,  J. Yao","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Cervical nuclei carry substantial diagnostic information for cervical cancer. Therefore, in automation-assisted reading of cervical cytology, automated and accurate segmentation of nuclei is essential. This paper proposes a novel approach for segmentation of cervical nuclei that combines fully convolutional networks (FCN) and graph-based approach (FCNG). FCN is trained to learn the nucleus high-level features to generate a nucleus label mask and a nucleus probabilistic map. The mask is used to construct a graph by image transforming. The map is formulated into the graph cost function in addition to the properties of the nucleus border and nucleus region. The prior constraints regarding the context of nucleus-cytoplasm position are also utilized to modify the local cost functions. The globally optimal path in the constructed graph is identified by dynamic programming. Validation of our method was performed on cell nuclei from Herlev Pap smear dataset. Our method shows a Zijdenbos similarity index (ZSI) of 0.92 \u00b1 0.09, compared to the best state-of-the-art approach of 0.89 \u00b1 0.15. The nucleus areas measured by our method correlated strongly with the independent standard (r<sup>2<\/sup> = 0.91).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950548","source":"ieee","year":2017,"key":"44e8ac42-129c-4d69-a2a7-9bbfcc3814c0","use":1,"doi":"10.1109\/ISBI.2017.7950548"},{"Title":"Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease","Description":"S. Liu,  S. Liu,  W. Cai,  H. Che,  S. Pujol,  R. Kikinis,  D. Feng,  M. J. Fulham,  ADNI","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2015","abstract":"The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.","email":["sliu4512@uni.sydney.edu.au"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6963480","source":"ieee","year":2015,"key":"0feb5828-830c-4c1a-86f8-4f93d4b3d390","use":1,"doi":"10.1109\/TBME.2014.2372011"},{"Title":"Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique","Description":"H. Greenspan,  B. van Ginneken,  R. M. Summers","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7463094","source":"ieee","year":2016,"key":"6ce1a7a9-1b8c-4cf6-983f-d1808a575eec","use":1,"doi":"10.1109\/TMI.2016.2553401"},{"Title":"Exploring deep features from brain tumor magnetic resonance images via transfer learning","Description":"Renhao Liu,  L. O. Hall,  D. B. Goldgof,  Mu Zhou,  R. A. Gatenby,  K. B. Ahmed","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"Finding appropriate feature representations from radiological images is a vital task for prediction and diagnosis. Deep convolutional neural networks have recently achieved state-of-the-art performance in classification problems from several different domains. Research has also shown the feasibility of using a pre-trained deep neural network as a feature extractor when only a small dataset is available. This paper proposes a novel image feature extraction method for predicting survival time from brain tumor magnetic resonance images using pretrained deep neural networks. Since all tumors are different sizes, we also explore different image resizing methods in the paper. We demonstrate that deep features can result in better survival time prediction with the highest accuracy of 95.45% versus conventional feature extraction methods from magnetic resonance images of the brain.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727204","source":"ieee","year":2016,"key":"9eb3aa48-7205-4a46-901a-d7a01936074e","use":1,"doi":"10.1109\/IJCNN.2016.7727204"},{"Title":"A comparison of deep learning and hand crafted features in medical image modality classification","Description":"S. Khan,  S. P. Yong","ShortDetails":"2016 3rd International Conference on Computer and Information Sciences (ICCOINS). 2016","abstract":"Modality corresponding to medical images is a vital filter in medical image retrieval systems, as radiologists or physicians are interested in only one of radiology images e.g CT scan, MRI, X-ray. Various handcrafted feature schemes have been proposed for medical image modality classification. On the other hand not enough attempts have been made for deep learned feature extraction. A comparative evaluation of both handcrafted and deep learned features for medical image modality classification is presented in this paper. The experiments are performed on IMAGECLEF 2012 data. After carrying out the experiments it is shown that the handcrafted features outperforms the deep learned features and shows the potential of handcrafted feature extraction models in the medical image field.","email":["sameer15khan@gmail.com","yongsuetpeng@petronas.com.my","sameer15khan@gmail.com","yongsuetpeng@petronas.com.my"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7783289","source":"ieee","year":2016,"key":"38a39bee-fbb6-4fd6-ae1e-6bc2684c8402","use":1,"doi":"10.1109\/ICCOINS.2016.7783289"},{"Title":"Cloud-based deep learning of big EEG data for epileptic seizure prediction","Description":"M. P. Hosseini,  H. Soltanian-Zadeh,  K. Elisevich,  D. Pompili","ShortDetails":"2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP). 2016","abstract":"Developing a Brain-Computer Interface (BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.","email":["hsoltan1@hfhs.org","konstantin.Elisevich@spectrumhealth.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7906022","source":"ieee","year":2016,"key":"225bf96e-2bc4-4636-bea5-8e2067f60d92","use":1,"doi":"10.1109\/GlobalSIP.2016.7906022"},{"Title":"Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning","Description":"G. Wu,  M. Kim,  Q. Wang,  B. C. Munsell,  D. Shen","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2016","abstract":"Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.","email":["wangqian@sjtu.edu.cn","munsellb@cofc.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7314894","source":"ieee","year":2016,"key":"47a5f58f-be64-4a1a-bf9d-c0d687c0377e","use":1,"doi":"10.1109\/TBME.2015.2496253"},{"Title":"The importance of stain normalization in colorectal tissue classification with convolutional networks","Description":"F. Ciompi,  O. Geessink,  B. E. Bejnordi,  G. S. de Souza,  A. Baidoshvili,  G. Litjens,  B. van Ginneken,  I. Nagtegaal,  J. van der Laak","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950492","source":"ieee","year":2017,"key":"e86e8f6a-075d-4f70-930c-96b2695a015c","use":1,"doi":"10.1109\/ISBI.2017.7950492"},{"Title":"Investigation of transfer learning on pulmonary nodule characteristics","Description":"A. Kaya,  A. S. Ke\u00e7eli,  A. B. Can","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"Studies on the classification of small pulmonary nodules generally focus on the prediction of malignancy of the nodule. In the recent years, publicly available databases provided different types of data to researchers, such as nodule characteristics, apart from the lung image and malignancy degree. In this paper, a study on the classification of pulmonary nodule characteristics using conventional features and deep features obtained from transfer learning method has been proposed. The results were assessed by sensitivity, specificity, and classification accuracy. The results of the study can be used to form multi-level classifiers in predicting malignancy by combining different types of features.","email":["aydinkaya@cs.hacettepe.edu.tr","aliseydi@cs.hacettepe.edu.tr","abc@cs.hacettepe.edu.tr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960357","source":"ieee","year":2017,"key":"bd4831df-779e-4dc1-963f-02b58c61d02c","use":1,"doi":"10.1109\/SIU.2017.7960357"},{"Title":"Segmentation label propagation using deep convolutional neural networks and dense conditional random field","Description":"M. Gao,  Z. Xu,  L. Lu,  A. Wu,  I. Nogues,  R. M. Summers,  D. J. Mollura","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Availability and accessibility of large-scale annotated medical image datasets play an essential role in robust supervised learning of medical image analysis. Missed labeling of regions of interest is a common issue on existing medical image datasets due to the labor intensive nature of the annotation task which requires high levels of clinical proficiency. In this paper, we present a segmentation based label propagation method to a publicly available dataset on interstitial lung disease [3], to address the missing annotation challenge. Upon validation from an expert radiologist, the amount of available annotated training data is largely increased. Such a dataset expansion can can potentially increase the accuracy of Computer-aided Detection (CAD) systems. The proposed constrained segmentation propagation algorithm combines the cues from the initial annotations, deep convolutional neural networks and a dense fully-connected Conditional Random Field (CRF) that achieves high quantitative accuracy levels.","email":["ziyue.xu@nih.gov.ThisresearchissupportedbytheCenterforInfectiousDiseaseImaging"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493497","source":"ieee","year":2016,"key":"14e29f84-8adb-4795-a261-506c0e416a75","use":1,"doi":"10.1109\/ISBI.2016.7493497"},{"Title":"Deep random forest-based learning transfer to SVM for brain tumor segmentation","Description":"S. Amiri,  I. Rekik,  M. A. Mahjoub","ShortDetails":"2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP). 2016","abstract":"Using neuroimaging techniques to diagnose brain tumors and detect both visible and invisible cancer cells infiltration boundaries motivated the emergence of diverse tumor segmentation algorithms. Noting the large variability in both tumor appearance and shape, the task of automatic segmentation becomes more difficult. In this paper, we propose a random-forest (RF) based learning transfer to SVM classifier method for segmenting tumor lesions while capturing their complex characteristics. Our framework is composed of two cascaded stages. In the first stage, we train a random forest to learn the mapping from the image space to the tumor label space. In the testing stage, we use the predicted label output from the random forest and feed it along with the testing intensity image to an SVM classifier to get the refined segmentation. Then we make our RF-SVM cascaded classification steps deep through an iterative process. We tested our method on 20 patients with high-grade gliomas from the Brain Tumor Image Segmentation Challenge (BRATS) dataset. Our proposed framework significantly outperformed SVM-based segmentation and RF-based segmentation-when used solely.","email":["islem.rekik@gmail.com","medali.mahjoub@ipeim.rnu.tn","amiri.sam6@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7523095","source":"ieee","year":2016,"key":"62078a39-4fea-4866-9801-387e179f9f9a","use":1,"doi":"10.1109\/ATSIP.2016.7523095"},{"Title":"Deep feature learning for pulmonary nodule classification in a lung CT","Description":"B. C. Kim,  Y. S. Sung,  H. I. Suk","ShortDetails":"2016 4th International Winter Conference on Brain-Computer Interface (BCI). 2016","abstract":"In this paper, we propose a novel method of identifying pulmonary nodules in a lung CT. Specifically, we devise a deep neural network by which we extract abstract information inherent in raw hand-crafted imaging features. We then combine the deep learned representations with the original raw imaging features into a long feature vector. By taking the combined feature vectors, we train a classifier, preceded by a feature selection via t-test. To validate the effectiveness of the proposed method, we performed experiments on our in-house dataset of 20 subjects; 3,598 pulmonary nodules (malignant: 178, benign: 3,420), which were manually segmented by a radiologist. In our experiments, we achieved the maximal accuracy of 95.5%, sensitivity of 94.4%, and AUC of 0.987, outperforming the competing method.","email":["hisuk@korea.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7457462","source":"ieee","year":2016,"key":"8747b054-61d5-401e-a76b-bf23c74e30c4","use":1,"doi":"10.1109\/IWW-BCI.2016.7457462"},{"Title":"Brain tumor grading based on Neural Networks and Convolutional Neural Networks","Description":"Y. Pan,  W. Huang,  Z. Lin,  W. Zhu,  J. Zhou,  J. Wong,  Z. Ding","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"This paper studies brain tumor grading using multiphase MRI images and compares the results with various configurations of deep learning structure and baseline Neural Networks. The MRI images are used directly into the learning machine, with some combination operations between multiphase MRIs. Compared to other researches, which involve additional effort to design and choose feature sets, the approach used in this paper leverages the learning capability of deep learning machine. We present the grading performance on the testing data measured by the sensitivity and specificity. The results show a maximum improvement of 18% on grading performance of Convolutional Neural Networks based on sensitivity and specificity compared to Neural Networks. We also visualize the kernels trained in different layers and display some self-learned features obtained from Convolutional Neural Networks.","email":["jocelyn_yl_wong@nuhs.edu.sg","hangzhoudzx73@126.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318458","source":"ieee","year":2015,"key":"473c47e9-6d00-4ac5-9ceb-7e13262d237f","use":1,"doi":"10.1109\/EMBC.2015.7318458"},{"Title":"A new NMF-autoencoder based CAD system for early diagnosis of prostate cancer","Description":"I. Reda,  A. Shalaby,  M. A. El-Ghar,  F. Khalifa,  M. Elmogy,  A. Aboulfotouh,  E. Hosseini-Asl,  A. El-Baz,  R. Keynton","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"In this paper, we propose a novel non-invasive framework for the early diagnosis of prostate cancer from diffusion-weighted magnetic reasoning imaging (DW-MRI). The proposed approach consists of three main steps. In the first step, the prostate is localized and segmented based on a new level-set model. This model is guided by a stochastic speed function that is derived using nonnegative matrix factorization (NMF). The NMF attributes are calculated using information from the MRI intensity, a probabilistic shape model, and the spatial interactions between prostate voxels. In the second step, the apparent diffusion coefficient (ADC) of the segmented prostate volume is mathematically calculated for different b-values. To preserve continuity, the calculated ADC values are normalized and refined using a Generalized Gauss-Markov Random Field (GGMRF) image model. The cumulative distribution function (CDF) of refined ADC for the prostate tissues at different b-values are then constructed. These CDFs are considered as global features which can be used to distinguish between benign and malignant tumors. Finally, a deep learning auto-encoder network, trained by a non-negativity constraint algorithm (NCAE), is used to classify the prostate tumor as benign or malignant based on the CDFs extracted from the previous step. Preliminary experiments on 42 clinical DW-MRI data sets resulted in 97.6% correct classification (sensitivity = 100% and specificity = 95.24%), indicating the high accuracy of the proposed framework.","email":["aselba01@exchange.louisville.eduBecauseofthesideeffectsofthepreviouslymentionedtech-niques"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493490","source":"ieee","year":2016,"key":"29235c08-dd06-43e2-b7e2-985c7fcb7004","use":1,"doi":"10.1109\/ISBI.2016.7493490"},{"Title":"DeepPap: Deep Convolutional Networks for Cervical Cell Classification","Description":"L. Zhang,  L. Lu,  I. Nogues,  R. Summers,  S. Liu,  J. Yao","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into \u201dabnormal\u201d and \u201dnormal\u201d categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells \u2013 without prior segmentation \u2013 based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.","email":["ling.zhang3@nih.gov","jyao@nih.gov","3@227","96@27","256@13","256@6"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7932065","source":"ieee","year":2017,"key":"4b5193c9-e8bf-4b0a-9ce4-5cae475dd1b8","use":1,"doi":"10.1109\/JBHI.2017.2705583"},{"Title":"A Robust Deep Model for Improved Classification of AD\/MCI Patients","Description":"F. Li,  L. Tran,  K. H. Thung,  S. Ji,  D. Shen,  J. Li","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2015","abstract":"Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.","email":["JLi@odu.edu","dgshen@med.unc.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7101222","source":"ieee","year":2015,"key":"0c60c5db-3612-4988-a3ad-21a0f81e208e","use":1,"doi":"10.1109\/JBHI.2015.2429556"},{"Title":"Residual and plain convolutional neural networks for 3D brain MRI classification","Description":"S. Korolev,  A. Safiullin,  M. Belyaev,  Y. Dodonova","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In the recent years there have been a number of studies that applied deep learning algorithms to neuroimaging data. Pipelines used in those studies mostly require multiple processing steps for feature extraction, although modern advancements in deep learning for image classification can provide a powerful framework for automatic feature generation and more straightforward analysis. In this paper, we show how similar performance can be achieved skipping these feature extraction steps with the residual and plain 3D convolutional neural network architectures. We demonstrate the performance of the proposed approach for classification of Alzheimer's disease versus mild cognitive impairment and normal controls on the Alzheimers Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans.","email":["InstituteforInformationTransmissionProblemss.korolev@skoltech.ru","aesafiullin@edu.hse.ru","m.belyaev@skoltech.ru","dodonova@iitp.ruABSTRACTIntherecentyearstherehavebeenanumberofstudiesthatapplieddeeplearningalgorithmstoneuroimagingdata.Pipelinesusedinthosestudiesmostlyrequiremultiplepro-cessingstepsforfeatureextraction"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950647","source":"ieee","year":2017,"key":"27407651-a90f-4ac7-9c58-b1b7eb0ff430","use":1,"doi":"10.1109\/ISBI.2017.7950647"},{"Title":"Brain MRI super-resolution using deep 3D convolutional networks","Description":"C. H. Pham,  A. Ducournau,  R. Fablet,  F. Rousseau","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Example-based single image super-resolution (SR) has recently shown outcomes with high reconstruction performance. Several methods based on neural networks have successfully introduced techniques into SR problem. In this paper, we propose a three-dimensional (3D) convolutional neural network to generate high-resolution (HR) brain image from its input low-resolution (LR) with the help of patches of other HR brain images. Our work demonstrates the need of fitting data and network parameters for 3D brain MRI.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950500","source":"ieee","year":2017,"key":"5ea47aa9-3808-4588-b1b2-26492f0bb0df","use":1,"doi":"10.1109\/ISBI.2017.7950500"},{"Title":"On hierarchical brain tumor segmentation in MRI using fully convolutional neural networks: A preliminary study","Description":"S. Pereira,  A. Oliveira,  V. Alves,  C. A. Silva","ShortDetails":"2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG). 2017","abstract":"Magnetic Resonance Imaging is the preferred imaging modality for assessing brain tumors, and segmentation is necessary for diagnosis and treatment planning. Thus, robust automatic segmentation methods are required. Machine learning proposals where the model is learned from data are quite successful. Hierarchical segmentation approaches firstly segment the whole tumor, followed by intra-tumor tissue identification. However, results comparing it with single stages approaches are needed, as state of the art results are also achieved by all-at-once strategies. Currently, fully convolutional networks approaches for segmentation are very efficient. In this paper, a hierarchical approach for brain tumor segmentation using a fully convolutional network is studied. The evaluation is performed on the Brain Tumor Segmentation Challenge 2013 dataset, and we report the metrics Dice Score Coefficient, Positive Predictive Value, and Sensitivity. Results show benefits from segmenting the complete tumor first, over all tissues in one stage. Moreover, the tumor core also benefits from such approach. This behavior may be justified by the high data imbalance observed between tumor and normal tissues, which is mitigated by considering the tumor as a whole.","email":["id5692@alunos.uminho.pt","csilva@dei.uminho.pt","valves@di.uminho.pt"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7889452","source":"ieee","year":2017,"key":"a75c6ea4-1f6d-4711-bf2a-e45dfc2734f0","use":1,"doi":"10.1109\/ENBENG.2017.7889452"},{"Title":"MRI image segmentation by fully convolutional networks","Description":"Y. Wang,  Z. Sun,  C. Liu,  W. Peng,  J. Zhang","ShortDetails":"2016 IEEE International Conference on Mechatronics and Automation. 2016","abstract":"With the development of various imaging technologies, medical imaging has been playing more important roles on providing scientific proof for doctors to make decisions on clinical diagnosis. At the same time, it is very important to excavate valuable information hidden in those images and take over some auxiliary medical works from doctors by the computer. Therefore, a large number of image segmentation methods, including some classic algorithms have been proposed and some of them perform well. In this paper, we built a deep convolutional neural network to segment the MRI brain images. Results show that the network has a good performance on segmentation of the gray and white matter of brains, it also had a good generalization ability.","email":["jhzhang@bit.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7558819","source":"ieee","year":2016,"key":"2d681b43-b383-48fc-9833-1b45c9eaf8e9","use":1,"doi":"10.1109\/ICMA.2016.7558819"},{"Title":"Compressed sensing reconstruction of dynamic contrast enhanced MRI using GPU-accelerated convolutional sparse coding","Description":"T. M. Quan,  W. K. Jeong","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"In this paper, we propose a data-driven image reconstruction algorithm that specifically aims to reconstruct undersampled dynamic contrast enhanced (DCE) MRI data. The proposed method is based on the convolutional sparse coding algorithm, which leverages the Fourier convolution theorem to accelerate the process of learning a collections of filters and iteratively refines the reconstruction result using the sparse codes found during the reconstruction process. We introduce a novel energy formation based on the learning over time-varing DCE-MRI images, and propose an extension of Alternating Direction Method of Multiplier (ADMM) method to solve the constrained optimization problem efficiently using the GPU. We assess the performance of the proposed method by comparing with the state-of-the-art dictionary-based compressed sensing (CS) MRI method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493321","source":"ieee","year":2016,"key":"024109a2-d5e6-4bb5-9ea1-abb43e86e892","use":1,"doi":"10.1109\/ISBI.2016.7493321"},{"Title":"Deep Convolutional Neural Networks for left ventricle segmentation","Description":"S. Molaei,  M. Shiri,  K. Horan,  D. Kahrobaei,  B. Nallamothu,  K. Najarian","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Left ventricle (LV) segmentation is crucial for quantitative cardiac function analysis. Manual segmentation of the endocardium and epicardium is highly cumbersome; physicians limit delineation to the end-diastolic and end-systolic phases. A fully automated system could provide an analysis of cardiac morphology for all phases in a much shorter time. Most of the current LV segmentation methods are semi-automated and require error prone manual initialization. A fully-automated LV segmentation method would expedite the functional analysis of the LV, reduce subjectivity and improve patient experience. We automatically segment the LV wall in cardiac MRI images with a Deep Convolutional Neural Network (DCNN). This algorithm first calculates the probability of a pixel belonging to the LV wall or background and then generates a label based on those probabilities without manual initialization. We then compare these results to the results obtained with another DCNN initialization method using Gabor filters. With Gabor DCNN we obtain an accuracy of 0.97, specificity of 0.984, sensitivity of 0.841 and mean accuracy of 0.902. This shows that Gabor filters perform better than random filters in the DCNN for LV segmentation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036913","source":"ieee","year":2017,"key":"5010b5e2-a583-407a-9c85-18f3893ee9e3","use":1,"doi":"10.1109\/EMBC.2017.8036913"},{"Title":"End-to-end learning of brain tissue segmentation from imperfect labeling","Description":"A. Fedorov,  J. Johnson,  E. Damaraju,  A. Ozerin,  V. Calhoun,  S. Plis","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"Segmenting a structural magnetic resonance imaging (MRI) scan is an important pre-processing step for analytic procedures and subsequent inferences about longitudinal tissue changes. Manual segmentation defines the current gold standard in quality but is prohibitively expensive. Automatic approaches are computationally intensive, incredibly slow at scale, and error prone due to usually involving many potentially faulty intermediate steps. In order to streamline the segmentation, we introduce a deep learning model that is based on volumetric dilated convolutions, subsequently reducing both processing time and errors. Compared to its competitors, the model has a reduced set of parameters and thus is easier to train and much faster to execute. The contrast in performance between the dilated network and its competitors becomes obvious when both are tested on a large dataset of unprocessed human brain volumes. The dilated network consistently outperforms not only another state-of-the-art deep learning approach, the up convolutional network, but also the ground truth on which it was trained. Not only can the incredible speed of our model make large scale analyses much easier but we also believe it has great potential in a clinical setting where, with little to no substantial delay, a patient and provider can go over test results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7966333","source":"ieee","year":2017,"key":"d270dffc-bfb4-4ef5-98dd-a8eecb48225f","use":1,"doi":"10.1109\/IJCNN.2017.7966333"},{"Title":"Hippocampus segmentation through multi-view ensemble ConvNets","Description":"Y. Chen,  B. Shi,  Z. Wang,  P. Zhang,  C. D. Smith,  J. Liu","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automated segmentation of brain structures from MR images is an important practice in many neuroimage studies. In this paper, we explore the utilization of a multi-view ensemble approach that relies on neural networks (NN) to combine multiple decision maps in achieving accurate hippocampus segmentation. Constructed under a general convolutional NN structure, our Ensemble-Net networks explore different convolution configurations to capture the complementary information residing in the multiple label probabilities produced by our U-Seg-Net (a modified U-Net) segmentation neural network. T1-weighted MRI scans and the associated Hippocampal masks of 110 healthy subjects from the ADNI project were used as the training and testing data. The combined U-Seg-Net + Ensemble-Net framework achieves over 89% Dice ratio on the test dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950499","source":"ieee","year":2017,"key":"33161ce7-9afe-444d-895a-36ef77032968","use":1,"doi":"10.1109\/ISBI.2017.7950499"},{"Title":"A deep learning network for right ventricle segmentation in short-axis MRI","Description":"G. Luo,  R. An,  K. Wang,  S. Dong,  H. Zhang","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"The segmentation of the right ventricle (RV) myocardium on MRI is a prerequisite step for the evaluation of RV structure and function, which is of great importance in the diagnose of most cardiac diseases, such as pulmonary hypertension, congenital heart disease, coronary heart disease, and dysplasia. However, RV segmentation is considered challenging, mainly because of the complex crescent shape of the RV across slices and phases. Hence this study aims to propose a new approach to segment RV endocardium and epicardium based on deep learning. The proposed method contains two subtasks: (1) localizing the region of interest (ROI), the biventricular region which contains more meaningful features and can facilitate the RV segmentation, and (2) segmenting the RV myocardium based on the localization. The two subtasks are integrated into a joint task learning framework, in which each task is solved via two multilayer convolutional neural networks. The experiments results show that the proposed method has big potential to be further researched and applied in clinical diagnosis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868785","source":"ieee","year":2016,"key":"b9dc2adb-9e74-4c85-aaf8-39166680f3b8","use":1,"doi":"10.23919\/CIC.2016.7868785"},{"Title":"Accelerating magnetic resonance imaging via deep learning","Description":"S. Wang,  Z. Su,  L. Ying,  X. Peng,  S. Zhu,  F. Liang,  D. Feng,  D. Liang","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"This paper proposes a deep learning approach for accelerating magnetic resonance imaging (MRI) using a large number of existing high quality MR images as the training datasets. An off-line convolutional neural network is designed and trained to identify the mapping relationship between the MR images obtained from zero-filled and fully-sampled k-space data. The network is not only capable of restoring fine structures and details but is also compatible with online constrained reconstruction methods. Experimental results on real MR data have shown encouraging performance of the proposed method for efficient and accurate imaging.","email":["CPUE5-2680V3@2.5GHz"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493320","source":"ieee","year":2016,"key":"b077ee63-a3f3-4dce-8e9b-be3c8d1a276a","use":1,"doi":"10.1109\/ISBI.2016.7493320"},{"Title":"Deep Learning with Edge Computing for Localization of Epileptogenicity Using Multimodal rs-fMRI and EEG Big Data","Description":"M. P. Hosseini,  T. X. Tran,  D. Pompili,  K. Elisevich,  H. Soltanian-Zadeh","ShortDetails":"2017 IEEE International Conference on Autonomic Computing (ICAC). 2017","abstract":"Epilepsy is a chronic brain disorder characterized by the occurrence of spontaneous seizures of which about 30 percent of patients remain medically intractable and may undergo surgical intervention; despite the latter, some may still fail to attain a seizure-free outcome. Functional changes may precede structural ones in the epileptic brain and may be detectable using existing noninvasive modalities. Functional connectivity analysis through electroencephalography (EEG) and resting state-functional magnetic resonance imaging (rs-fMRI), complemented by diffusion tensor imaging (DTI), has provided such meaningful input in cases of temporal lobe epilepsy (TLE). Recently, the emergence of edge computing has provided competent solutions enabling context-aware and real-time response services for users. By leveraging the potential of autonomic edge computing in epilepsy, we develop and deploy both noninvasive and invasive methods for the monitoring, evaluation and regulation of the epileptic brain, with responsive neurostimulation (RNS; Neuropace). First, an autonomic edge computing framework is proposed for processing of big data as part of a decision support system for surgical candidacy. Second, an optimized model for estimation of the epileptogenic network using independently acquired EEG and rs-fMRI is presented. Third, an unsupervised feature extraction model is developed based on a convolutional deep learning structure for distinguishing interictal epileptic discharge (IED) periods from nonIED periods using electrographic signals from electrocorticography (ECoG). Experimental and simulation results from actual patient data validate the effectiveness of the proposed methods.","email":["3kost.elisevich@spectrumhealth.org","4hszadeh@ut.ac.ir","5hsoltan1@hfhs.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8005336","source":"ieee","year":2017,"key":"848e9caf-2eb4-400d-83fa-bd0e36b62619","use":1,"doi":"10.1109\/ICAC.2017.41"},{"Title":"Computer-aided classification of multi-types of dementia via convolutional neural networks","Description":"E. M. Alkabawi,  A. R. Hilal,  O. A. Basir","ShortDetails":"2017 IEEE International Symposium on Medical Measurements and Applications (MeMeA). 2017","abstract":"With millions of people suffering from dementia worldwide, the global prevalence of dementia has a significant impact on the patients' lives, their caregivers' physical and emotional states, and the global economy. Early diagnosis of dementia helps in finding suitable therapies that reduce or even prevent further deterioration of patients' cognitive abilities. In recent years, state-of-the-art literature has proposed various computer-aided diagnosis systems based on 3-dimensional brain imagery analysis to identify early symptoms of dementia. These systems aim to assist radiologists in increasing the accuracy of diagnoses and reducing false positives. However, the early diagnosis of dementia is a challenging task due to the image quality, noise, and human brain irregularities. The state-of-the-art has focused on differentiating multi-stages of Alzheimer's disease, however, the diagnosis of various types of dementia is still a gap. This paper proposes a deep learning-based computer-aided diagnosis approach for the early detection of multi-type of dementia. To show the performance of the proposed CAD algorithm, three conventional CAD methods are implemented for comparison. The proposed algorithm yields a 74.93% accuracy in early diagnosis of multi-type of dementia and outperforms the state of the art CAD methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7985847","source":"ieee","year":2017,"key":"de868e41-0a78-4768-96af-5e587d28a0d4","use":1,"doi":"10.1109\/MeMeA.2017.7985847"},{"Title":"Classification of brain tissues as lesion or healthy by 3D convolutional neural networks","Description":"C. Y. Aydo\u011fdu,  E. Albay,  G. \u00dcnal","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"In this paper, a three dimensional convolutional neural network based solution is proposed for classification of brain tissues as lesion or healthy in terms of ischemic stroke disease. Three dimensional data used in this work are obtained by magnetic resonance imaging technique. Proposed method is compared with traditional methods that are in the same category, via K-fold cross validation technique in terms of sensitivity, specificity and accuracy measures. In conclusion, it is obtained nearly 89% accuracy using our proposed method. Comparing this method with others, our proposed method is the best method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960524","source":"ieee","year":2017,"key":"44a2af95-f3d6-4828-b0bb-e3e4eaa4215e","use":1,"doi":"10.1109\/SIU.2017.7960524"},{"Title":"SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D Convolutional Neural Networks","Description":"P. Ferdinand Christ,  F. Ettlinger,  G. Kaissis,  S. Schlecht,  F. Ahmaddy,  F. Gr\u00fcn,  A. Valentinitsch,  S. A. Ahmadi,  R. Braren,  B. Menze","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automatic non-invasive assessment of hepatocellular carcinoma (HCC) malignancy has the potential to substantially enhance tumor treatment strategies for HCC patients. In this work we present a novel framework to automatically characterize the malignancy of HCC lesions from DWI images. We predict HCC malignancy in two steps: As a first step we automatically segment HCC tumor lesions using cascaded fully convolutional neural networks (CFCN). A 3D neural network (SurvivalNet) then predicts the HCC lesions' malignancy from the HCC tumor segmentation. We formulate this task as a classification problem with classes being \u201clow risk\u201d and \u201chigh risk\u201d represented by longer or shorter survival times than the median survival. We evaluated our method on DWI of 31 HCC patients. Our proposed framework achieves an end-to-end accuracy of 65% with a Dice score for the automatic lesion segmentation of 69% and an accuracy of 68% for tumor malignancy classification based on expert annotations. We compared the SurvivalNet to classical handcrafted features such as Histogram and Haralick and show experimentally that SurvivalNet outperforms the handcrafted features in HCC malignancy classification. End-to-end assessment of tumor malignancy based on our proposed fully automatic framework corresponds to assessment based on expert annotations with high significance (p > 0.95).","email":["rbraren@tum.deandbjoern.menze"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950648","source":"ieee","year":2017,"key":"e3fc6e1f-24d7-43e3-9bdf-e9acbcd43412","use":1,"doi":"10.1109\/ISBI.2017.7950648"},{"Title":"Aneurysm detection in 3D cerebral angiograms based on intra-vascular distance mapping and convolutional neural networks","Description":"T. Jerman,  F. Pernus,  B. Likar,  \u017d. \u0160piclin","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Early and more sensitive detection of small aneurysms in 3D cerebral angiograms is required to prevent potentially fatal rupture events. Herein, we propose a novel method that entails structure enhancement filtering to highlight potential aneurysm locations, intra-vascular distance mapping for regional vascular shape encoding and dimensionality reduction and a convolutional neural network to automatically determine optimal features and classification rules for aneurysm detection. Evaluation on 15 3D digital subtraction angiograms showed better performance of the proposed method compared to enhancement filtering and random forest based methods, as it achieved a 100% detection sensitivity at a low number of false positives (2.4 per dataset). The proposed method is also applicable to other angiographic modalities.","email":["tim.jerman@fe.uni-lj.si"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950595","source":"ieee","year":2017,"key":"1b7d7256-e282-43f1-8b4c-794894a5676c","use":1,"doi":"10.1109\/ISBI.2017.7950595"},{"Title":"Classification of MRI data using deep learning and Gaussian process-based model selection","Description":"H. Bertrand,  M. Perrot,  R. Ardon,  I. Bloch","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20% for the most difficult classes).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950626","source":"ieee","year":2017,"key":"44caa9cd-7a25-40c5-8fa2-58d6f8896d6d","use":1,"doi":"10.1109\/ISBI.2017.7950626"},{"Title":"Automatic detection of motion artifacts in MR images using CNNS","Description":"K. Meding,  A. Loktyushin,  M. Hirsch","ShortDetails":"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017","abstract":"Considerable practical interest exists in being able to automatically determine whether a recorded magnetic resonance image is affected by motion artifacts caused by patient movements during scanning. Existing approaches usually rely on the use of navigators or external sensors to detect and track patient motion during image acquisition. In this work, we present an algorithm based on convolutional neural networks that enables fully automated detection of motion artifacts in MR scans without special hardware requirements. The approach is data driven and uses the magnitude of MR images in the spatial domain as input. We evaluate the performance of our algorithm on both synthetic and real data and observe adequate performance in terms of accuracy and generalization to different types of data. Our proposed approach could potentially be used in clinical practice to tag an MR image as motion-free or motion-corrupted immediately after a scan is finished. This process would facilitate the acquisition of high-quality MR images that are often indispensable for accurate medical diagnosis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7952268","source":"ieee","year":2017,"key":"d93b13b9-b5c3-4c7c-8334-397c791aca87","use":1,"doi":"10.1109\/ICASSP.2017.7952268"},{"Title":"Fast predictive multimodal image registration","Description":"X. Yang,  R. Kwitt,  M. Styner,  M. Niethammer","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950652","source":"ieee","year":2017,"key":"41d6ff95-8a31-4e2b-8283-a28852271e59","use":1,"doi":"10.1109\/ISBI.2017.7950652"},{"Title":"3-D functional brain network classification using Convolutional Neural Networks","Description":"D. Ren,  Y. Zhao,  H. Chen,  Q. Dong,  J. Lv,  T. Liu","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Several recent studies have shown that dictionary learning and sparse representation can effectively reconstruct hundreds of interacting functional brain networks simultaneously from whole-brain fMRI data. However, accurate classification and recognition of those hundreds of functional networks from an individual or a population of many subjects is still a challenging and open problem due to the intrinsic variability of functional networks and other noise sources. To tackle this problem, this paper presents an effective deep learning framework to train convolutional neural networks from a large dataset of hundreds of thousands of available brain network volume maps, which was then applied on testing samples for network classification and recognition. We effectively applied computer-labeled data as training set so the whole process can be automated. Experimental results showed that the proposed method is quite robust in handling noisy patterns in the dataset, which suggests that our work offers a new computational framework for modeling functional connectomes from fMRI big data in the future.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950736","source":"ieee","year":2017,"key":"089cd429-9d0f-4410-9c1c-acb8b0872106","use":1,"doi":"10.1109\/ISBI.2017.7950736"},{"Title":"Prostate segmentation in MR images using ensemble deep convolutional neural networks","Description":"H. Jia,  Y. Xia,  W. Cai,  M. Fulham,  D. D. Feng","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"The automated segmentation of the prostate gland from MR images is increasingly used for clinical diagnosis. Since deep learning demonstrates superior performance in computer vision applications, we propose a coarse-to-fine segmentation strategy using ensemble deep convolutional neural networks (DCNNs) to address prostate segmentation in MR images. First, we use registration-based coarse segmentation on pre-processed prostate MR images to define the potential boundary region. We then train four DCNNs as voxel-based classifiers and classify the voxel in the potential region is a prostate voxel when at least three DCNNs made that decision. Finally, we use boundary refinement to eliminate the outliers and smooth the boundary. We evaluated our approach on the MICCAI PROMIS12 challenge dataset and our experimental results verify the effectiveness of the proposed algorithms.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950630","source":"ieee","year":2017,"key":"bd73c033-dd72-43c6-8166-180051b3b47c","use":1,"doi":"10.1109\/ISBI.2017.7950630"},{"Title":"Fast Fully Automatic Segmentation of the Severely Abnormal Human Right Ventricle from Cardiovascular Magnetic Resonance Images Using a Multi-Scale 3D Convolutional Neural Network","Description":"A. Giannakidis,  K. Kamnitsas,  V. Spadotto,  J. Keegan,  G. Smith,  B. Glocker,  D. Rueckert,  S. Ernst,  M. A. Gatzoulis,  D. J. Pennell,  S. Babu-Narayan,  D. N. Firmin","ShortDetails":"2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS). 2016","abstract":"Cardiac magnetic resonance (CMR) is regarded as the reference examination for cardiac morphology in tetralogy of Fallot (ToF) patients allowing images of high spatial resolution and high contrast. The detailed knowledge of the right ventricular anatomy is critical in ToF management. The segmentation of the right ventricle (RV) in CMR images from ToF patients is a challenging task due to the high shape and image quality variability. In this paper we propose a fully automatic deep learning-based framework to segment the RV from CMR anatomical images of the whole heart. We adopt a 3D multi-scale deep convolutional neural network to identify pixels that belong to the RV. Our robust segmentation framework was tested on 26 ToF patients achieving a Dice similarity coefficient of 0.8281\u00b10.1010 with reference to manual annotations performed by expert cardiologists. The proposed technique is also computationally efficient, which may further facilitate its adoption in the clinical routine.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7907443","source":"ieee","year":2016,"key":"753db23b-8fe3-4472-a016-c5f6b34589dd","use":1,"doi":"10.1109\/SITIS.2016.16"},{"Title":"Automatic segmentation of left ventricular myocardium by deep convolutional and de-convolutional neural networks","Description":"X. L. Yang,  L. Gobeawan,  S. Y. Yeo,  W. T. Tang,  Z. Z. Wu,  Y. Su","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Deep learning has been integrated into several existing left ventricle (LV) endocardium segmentation methods to yield impressive accuracy improvements. However, challenges remain for segmentation of LV epicardium due to its fuzzier appearance and complications from the right ventricular insertion points. Segmenting the myocardium collectively (i.e., endocardium and epicardium together) confers the potential for better segmentation results. In this work, we develop a computational platform based on deep learning to segment the whole LV myocardium simultaneously from a cardiac magnetic resonance (CMR) image. The deep convolutional network is constructed using Caffe platform, which consists of 6 convolutional layers, 2 pooling layers, and 1 de-convolutional layer. A preliminary result with Dice metric of 0.75\u00b10.04 is reported on York MR dataset. While in its current form, our proposed one-step deep learning method cannot compete with state-of-art myocardium segmentation methods, it delivers promising first pass segmentation results.","email":["A.Giannakidis@rbht.nhs.uk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868684","source":"ieee","year":2016,"key":"6992e7d5-e048-4f57-85da-f5ad3e064954","use":1,"doi":"10.23919\/CIC.2016.7868684"},{"Title":"Moving from detection to pre-detection of Alzheimer's Disease from MRI data","Description":"K. A. N. N. P. Gunawardena,  R. N. Rajapakse,  N. D. Kodikara,  I. U. K. Mudalige","ShortDetails":"2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer). 2016","abstract":"Alzheimer's Disease (AD) is the most common form of dementia, affecting approximately 10% of individuals under 65 years of age, with the prevalence doubling every 5 years up to age 80, above which the prevalence exceeds 40%. Currently diagnosis of AD is largely based on the examination of clinical history and tests such as MMSE (Mini-mental state examination) and PAL (Paired Associates Learning). However many present studies have highlighted the inaccuracies and limitations of such tests. Thus medical officers are now moving to the more accurate neuroimaging data (Magnetic Resonance Imaging- MRI) based diagnosis for these types of diseases where brain atrophy transpires. However it is a considerable challenge to analyse large numbers of images manually to get the most accurate diagnosis at present.","email":["1nishan.nilanka@gmail.com","2r.n.r@ieee.org","3ndk@ucsc.cmb.ac.lk","4indikamudalige2015@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7829940","source":"ieee","year":2016,"key":"01dce7b7-0227-41aa-b932-2f13884f81f7","use":1,"doi":"10.1109\/ICTER.2016.7829940"},{"Title":"Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation","Description":"T. Brosch,  L. Y. W. Tang,  Y. Yoo,  D. K. B. Li,  A. Traboulsee,  R. Tam","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.","email":["pubs-permissions@ieee.org.","brosch.tom@gmail.com","youngjin@msmri.medicine.ubc.ca","t.traboulsee@ubc.ca","lisat@sfu.cu","david.li@ubc.ca","roger.tam@ubc.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7404285","source":"ieee","year":2016,"key":"1fc8db4a-da01-4e86-9ca0-9848ec5f4737","use":1,"doi":"10.1109\/TMI.2016.2528821"},{"Title":"Automatic Segmentation of MR Brain Images With a Convolutional Neural Network","Description":"P. Moeskops,  M. A. Viergever,  A. M. Mendrik,  L. S. de Vries,  M. J. N. L. Benders,  I. I\u0161gum","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T<sub>2<\/sub>-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T<sub>2<\/sub>-weighted images of preterm infants acquired at 40 weeks PMA, axial T<sub>1<\/sub>-weighted images of ageing adults acquired at an average age of 70 years, and T<sub>1<\/sub>-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.","email":["p.moeskops@umcutrecht.nl"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7444155","source":"ieee","year":2016,"key":"b60a2f62-5d99-4fd4-a46a-37c8d4df68a9","use":1,"doi":"10.1109\/TMI.2016.2548501"},{"Title":"Anatomical Landmark Detection in Medical Applications Driven by Synthetic Data","Description":"G. Riegler,  M. Urschler,  M. R\u00fcther,  H. Bischof,  D. Stern","ShortDetails":"2015 IEEE International Conference on Computer Vision Workshop (ICCVW). 2015","abstract":"An important initial step in many medical image analysis applications is the accurate detection of anatomical landmarks. Most successful methods for this task rely on data-driven machine learning algorithms. However, modern machine learning techniques, e.g. convolutional neural networks, need a large corpus of training data, which is often an unrealistic setting for medical datasets. In this work, we investigate how to adapt synthetic image datasets from other computer vision tasks to overcome the under-representation of the anatomical pose and shape variations in medical image datasets. We transform both data domains to a common one in such a way that a convolutional neural network can be trained on the larger synthetic image dataset and fine-tuned on the smaller medical image dataset. Our evaluations on data of MR hand and whole body CT images demonstrate that this approach improves the detection results compared to training a convolutional neural network only on the medical data. The proposed approach may also be usable in other medical applications, where training data is scarce.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7406370","source":"ieee","year":2015,"key":"9d407fe1-9231-4d51-825c-a6c77e6277ce","use":1,"doi":"10.1109\/ICCVW.2015.21"},{"Title":"Automatic cerebral microbleeds detection from MR images via Independent Subspace Analysis based hierarchical features","Description":"Q. Dou,  H. Chen,  L. Yu,  L. Shi,  D. Wang,  V. C. Mok,  P. A. Heng","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"With the development of susceptibility weighted imaging (SWI) technology, cerebral microbleed (CMB) detection is increasingly essential in cerebrovascular diseases diagnosis and cognitive impairment assessment. Clinical CMB detection is based on manual rating which is subjective and time-consuming with limited reproducibility. In this paper, we propose a computer-aided system for automatic detection of CMBs from brain SWI images. Our approach detects the CMBs within three stages: (i) candidates screening based on intensity values (ii) compact 3D hierarchical features extraction via a stacked convolutional Independent Subspace Analysis (ISA) network (iii) false positive candidates removal with a support vector machine (SVM) classifier based on the learned representation features from ISA. Experimental results on 19 subjects (161 CMBs) achieve a high sensitivity of 89.44% with an average of 7.7 and 0.9 false positives per subject and per CMB, respectively, which validate the efficacy of our approach.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7320232","source":"ieee","year":2015,"key":"8ffc6c17-096c-4322-9b00-11e9ee55014a","use":1,"doi":"10.1109\/EMBC.2015.7320232"},{"Title":"MR Image Reconstruction with Convolutional Characteristic Constraint (CoCCo)","Description":"X. Peng,  D. Liang","ShortDetails":"IEEE Signal Processing Letters. 2015","abstract":"The problem of recovering an image from limited or sparsely sampled Fourier measurements occurs in the application of magnetic resonance imaging. To address this problem, we propose a novel MR image reconstruction method with convolutional characteristic constraints. We first estimate the convolutional characteristics using standard compressed sensing method in a parallel fashion. Then we use the recovered image characteristics to constrain the target image function. The image characteristics should either be sparser or of higher SNR than the original image to enable superior performance. In this work, we studied using thirteen kernels and experiments based on a brain data set were conducted. It is demonstrated that the proposed method outperforms the existing methods in terms of high quality imaging due to multiple characteristic constraints and the robustness to measurement noise.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6971076","source":"ieee","year":2015,"key":"633c5cb1-994a-4617-b664-85c5071824c4","use":1,"doi":"10.1109\/LSP.2014.2376699"},{"Title":"Iterative deep convolutional encoder-decoder network for medical image segmentation","Description":"J. U. Kim,  H. G. Kim,  Y. M. Ro","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"In this paper, we propose a novel medical image segmentation using iterative deep learning framework. We have combined an iterative learning approach and an encoder-decoder network to improve segmentation results, which enables to precisely localize the regions of interest (ROIs) including complex shapes or detailed textures of medical images in an iterative manner. The proposed iterative deep convolutional encoder-decoder network consists of two main paths: convolutional encoder path and convolutional decoder path with iterative learning. Experimental results show that the proposed iterative deep learning framework is able to yield excellent medical image segmentation performances for various medical images. The effectiveness of the proposed method has been proved by comparing with other state-of-the-art medical image segmentation methods.","email":["martin.urschler@cfi.lbg.ac.at"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036917","source":"ieee","year":2017,"key":"e26aa6e0-1745-4860-814d-c5d918ca5195","use":1,"doi":"10.1109\/EMBC.2017.8036917"},{"Title":"A regularized deep learning approach for clinical risk prediction of acute coronary syndrome using electronic health records","Description":"Z. Huang,  W. Dong,  H. Duan,  J. Liu","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2017","abstract":"Objective: Acute coronary syndrome (ACS), as a common and severe cardiovascular disease, is a leading cause of death and the principal cause of serious long-term disability globally. Clinical risk prediction of ACS is important for early intervention and treatment. Existing ACS risk scoring models are based mainly on a small set of hand-picked risk factors and often dichotomize predictive variables to simplify the score calculation [1-3]. Methods: This study develops a regularized stacked denoising auto-encoder (SDAE) model to stratify clinical risks of ACS patients from a large volume of electronic health records (EHR). To capture characteristics of patients at similar risk levels, and preserve the discriminating information across different risk levels, two constraints are added on SDAE to make the reconstructed feature representations contain more risk information of patients, which contribute to a better clinical risk prediction result. Results: We validate our approach on a real clinical dataset consisting of 3,464 ACS patient samples. The performance of our approach for predicting ACS risk remains robust and reaches 0.868 and 0.73 in terms of both AUC and Accuracy, respectively. Conclusions: The obtained results show that the proposed approach achieves a competitive performance compared to state-of-the-art models in dealing with the clinical risk prediction problem. In addition, our approach can extract informative risk factors of ACS via a reconstructive learning strategy. Some of these extracted risk factors are not only consistent with existing medical domain knowledge, but also contain suggestive hypotheses that could be validated by further investigations in the medical domain.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7990180","source":"ieee","year":2017,"key":"e8270ef2-d420-4beb-ad72-7c7d6ec41cfb","use":1,"doi":"10.1109\/TBME.2017.2731158"},{"Title":"Deep learning of feature representation with multiple instance learning for medical image analysis","Description":"Y. Xu,  T. Mo,  Q. Feng,  P. Zhong,  M. Lai,  E. I. C. Chang","ShortDetails":"2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2014","abstract":"This paper studies the effectiveness of accomplishing high-level tasks with a minimum of manual annotation and good feature representations for medical images. In medical image analysis, objects like cells are characterized by significant clinical features. Previously developed features like SIFT and HARR are unable to comprehensively represent such objects. Therefore, feature representation is especially important. In this paper, we study automatic extraction of feature representation through deep learning (DNN). Furthermore, detailed annotation of objects is often an ambiguous and challenging task. We use multiple instance learning (MIL) framework in classification training with deep learning features. Several interesting conclusions can be drawn from our work: (1) automatic feature learning outperforms manual feature; (2) the unsupervised approach can achieve performance that's close to fully supervised approach (93.56%) vs. (94.52%); and (3) the MIL performance of coarse label (96.30%) outweighs the supervised performance of fine label (95.40%) in supervised deep learning features.","email":["xuyan04@gmail.com","lmd@zju.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6853873","source":"ieee","year":2014,"key":"a5f663d1-80c9-47cb-b839-bddd463f93e2","use":1,"doi":"10.1109\/ICASSP.2014.6853873"},{"Title":"Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks With Jaccard Distance","Description":"Y. Yuan,  M. Chao,  Y. C. Lo","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 <italic>skin lesion analysis towards melanoma detection<\/italic> challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7903636","source":"ieee","year":2017,"key":"c7d9068c-30f0-41d7-bfb0-5e197b0221e1","use":1,"doi":"10.1109\/TMI.2017.2695227"},{"Title":"Looking Under the Hood: Deep Neural Network Visualization to Interpret Whole-Slide Image Analysis Outcomes for Colorectal Polyps","Description":"B. Korbar,  A. M. Olofson,  A. P. Miraflor,  C. M. Nicka,  M. A. Suriawinata,  L. Torresani,  A. A. Suriawinata,  S. Hassanpour","ShortDetails":"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017","abstract":"Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. The process of characterization is time-intensive and requires years of specialized medical training. In this work, we propose a deep-learning-based image analysis approach that not only can accurately classify different types of polyps in whole-slide images, but also generates major regions and features on the slide through a model visualization approach. We argue that this visualization approach will make sense of the underlying reasons for the classification outcomes, significantly reduce the cognitive burden on clinicians, and improve the diagnostic accuracy for whole-slide image characterization tasks. Our results show the efficacy of this network visualization approach in recovering decisive regions and features for different types of polyps on whole-slide images according to the domain expert pathologists.","email":["yading.yuan@mssm.edu","ming.chao@mountsinai.org","chi.lo@mountsinai.org.","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8014848","source":"ieee","year":2017,"key":"99102227-e37d-4fc6-99aa-a531d2db1053","use":1,"doi":"10.1109\/CVPRW.2017.114"},{"Title":"Retrieval From and Understanding of Large-Scale Multi-modal Medical Datasets: A Review","Description":"H. M\u00fcller,  D. Unay","ShortDetails":"IEEE Transactions on Multimedia. 2017","abstract":"Content-based multimedia retrieval (CBMR) has been an active research domain since the mid 1990s. In medicine visual retrieval started later and has mostly remained a research instrument and less a clinical tool. The limited size of data sets due to privacy constraints is often mentioned as reason for these limitations. Nevertheless, much work has been done in CBMR, including the availability of increasingly large data sets and scientific challenges. Annotated data sets and clinical data for images have now become available and can be combined for multi-modal retrieval. Much has been learned on user behavior and application scenarios. This text is motivated by the advances in medical image analysis and the availability of public large data sets that often include clinical data. It is a systematic review of recent work (concentrating on the period 2011-2017) on multi-modal CBMR and image understanding in the medical domain, where image understanding includes techniques such as detection, localization, and classification for leveraging visual content. With the objective of summarizing the current state of research for multimedia researchers outside the medical field, the text provides ways to get data sets and identifies current limitations and promising research directions. The text highlights advances in the past six years and a trend to use larger scale training data and deep learning approaches that can replace\/complement hand-crafted features. Using images alone will likely only work in limited domains but combining multiple sources of data for multi-modal retrieval has the biggest chances of success, particularly for clinical impact.","email":["henning.mueller@hevs.ch.","devrim.unay@ieu.edu.tr."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7984864","source":"ieee","year":2017,"key":"7df60877-9224-4fe8-9a8d-5c7e3d8491f3","use":1,"doi":"10.1109\/TMM.2017.2729400"},{"Title":"Application of neural network based on SIFT local feature extraction in medical image classification","Description":"Shuqi Cui,  Hong Jiang,  Zheng Wang,  Chaomin Shen","ShortDetails":"2017 2nd International Conference on Image, Vision and Computing (ICIVC). 2017","abstract":"In the medical image analysis, ROI (Region of Interest) is one of the key features of clinical diagnostic analysis. The applying of local features of ROI to the deep learning of image classification has the advantage of noise eliminating and information reducing. Based on existing research results, using Scale Invariant Feature Transformation (SIFT) algorithm combined with SVM classifier and sliding window to extract the local features and describe ROI precisely in the image. Finally, the extracted feature is used as the input layer of BP neural network in mammary gland X - ray image classification. The experimental results show that the accuracy of neural network classifier based on SIFT is 96.57%, which is 3.44% higher than that of traditional SVM classification accuracy. It is verified that our classifier is important to support clinical diagnosis and diagnosis.","email":["18817380037@163.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7984525","source":"ieee","year":2017,"key":"de5c0c75-c6e1-4a78-bb2f-50bb5ca01552","use":1,"doi":"10.1109\/ICIVC.2017.7984525"},{"Title":"Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection","Description":"D. Sarikaya,  J. J. Corso,  K. A. Guru","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.","email":["gusar@buffalo.edu","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7847313","source":"ieee","year":2017,"key":"e911aadd-df2d-46d4-8a06-9b0c35ff2e5b","use":1,"doi":"10.1109\/TMI.2017.2665671"},{"Title":"A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology","Description":"N. Kumar,  R. Verma,  S. Sharma,  S. Bhargava,  A. Vahadane,  A. Sethi","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7872382","source":"ieee","year":2017,"key":"6011dff4-4995-40e3-9827-068f13a3b4e1","use":1,"doi":"10.1109\/TMI.2017.2677499"},{"Title":"Knowledge transfer for melanoma screening with deep learning","Description":"A. Menegola,  M. Fornaciali,  R. Pires,  F. V. Bittencourt,  S. Avila,  E. Valle","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Knowledge transfer impacts the performance of deep learning - the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pretrained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.","email":["dovalle@dca.fee.unicamp.br.1AmericanCancerSociety"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950523","source":"ieee","year":2017,"key":"29806987-f5b0-4aa6-8c97-12c2d91fa3e0","use":1,"doi":"10.1109\/ISBI.2017.7950523"},{"Title":"A study on automated segmentation of blood regions in Wireless Capsule Endoscopy images using fully convolutional networks","Description":"X. Jia,  M. Q. H. Meng","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Wireless Capsule Endoscopy (WCE) is a novel diagnostic modality of endoscopic imaging which facilitates direct visualization of the gastrointestinal (GI) tract. Many computational methods that can automatically detect and\/or characterize the abnormalities from WCE sequences are developed to support medical decision-making. This paper presents a new approach for automated segmentation of blood regions in WCE images via a deep learning strategy. The proposed method first classify the bleeding samples into active and inactive subgroups based on the statistical features derived from the histogram probability of the color space. Then for each subgroup, we highlight the blood regions via fully convolutional networks (FCNs). Experimental results on the clinical WCE dataset demonstrate the efficacy of our approach, which achieves comparable or better performance than the state-of-the-art methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950496","source":"ieee","year":2017,"key":"4c2dc308-4875-46d4-8ae6-cafa8783da2d","use":1,"doi":"10.1109\/ISBI.2017.7950496"},{"Title":"Deep Learning for Health Informatics","Description":"D. Rav\u00ec,  C. Wong,  F. Deligianni,  M. Berthelot,  J. Andreu-Perez,  B. Lo,  G. Z. Yang","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.","email":["d.ravi@imperial.ac.uk","fani.deligianni@imperial.ac.uk","m.berthelot14@imperial.","javier.andreu@imperial.ac.uk","benny.lo@imperial.ac.uk","yang@imperial.ac.uk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7801947","source":"ieee","year":2017,"key":"f41ed1e1-963f-4755-8837-370e9f1e18be","use":1,"doi":"10.1109\/JBHI.2016.2636665"},{"Title":"Evaluation of feature descriptors for cancerous tissue recognition","Description":"P. Stanitsas,  A. Cherian,  Xinyan Li,  A. Truskinovsky,  V. Morellas,  N. Papanikolopoulos","ShortDetails":"2016 23rd International Conference on Pattern Recognition (ICPR). 2016","abstract":"Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7899848","source":"ieee","year":2016,"key":"ec322014-095c-497c-8199-a06698616b61","use":1,"doi":"10.1109\/ICPR.2016.7899848"},{"Title":"Skin lesion classification from dermoscopic images using deep learning techniques","Description":"A. Romero Lopez,  X. Giro-i-Nieto,  J. Burdick,  O. Marques","ShortDetails":"2017 13th IASTED International Conference on Biomedical Engineering (BioMed). 2017","abstract":"The recent emergence of deep learning methods for medical image analysis has enabled the development of intelligent medical imaging-based diagnosis systems that can assist the human expert in making better decisions about a patients health. In this paper we focus on the problem of skin lesion classification, particularly early melanoma detection, and present a deep-learning based approach to solve the problem of classifying a dermoscopic image containing a skin lesion as malignant or benign. The proposed solution is built around the VGGNet convolutional neural network architecture and uses the transfer learning paradigm. Experimental results are encouraging: on the ISIC Archive dataset, the proposed method achieves a sensitivity value of 78.66%, which is significantly higher than the current state of the art on that dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7893267","source":"ieee","year":2017,"key":"dc072b46-1163-4666-a7a1-8a63afb73f14","use":1,"doi":"10.2316\/P.2017.852-053"},{"Title":"Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks","Description":"L. Yu,  H. Chen,  Q. Dou,  J. Qin,  P. A. Heng","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.","email":["lqyu@cse.cuhk.edu.hk","hchen@cse.cuhk.edu.hk","qdou@cse.cuhk.edu.hk","pheng@cse.cuhk.edu.hk","harry.qin@polyu.edu.hk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7792699","source":"ieee","year":2017,"key":"0104bacd-b252-4612-b689-cc1c856707b8","use":1,"doi":"10.1109\/TMI.2016.2642839"},{"Title":"Predicting high-risk prognosis from diagnostic histories of adult disease patients via deep recurrent neural networks","Description":"Jung-Woo Ha,  Adrian Kim,  Dongwon Kim,  J. Kim,  Jeong-Whun Kim,  Jin Joo Park,  Borim Ryu","ShortDetails":"2017 IEEE International Conference on Big Data and Smart Computing (BigComp). 2017","abstract":"It is a critical issue to predict the prognosis of adult disease patients due to the possibility of spreading to high-risk symptoms in medical fields. Most studies for predicting prognosis have used complex data from patients such as biomedical images, biomarkers, and pathological measurements. We demonstrate a language model-like method for predicting high-risk prognosis from diagnosis histories of patients using deep recurrent neural networks (RNNs), i.e., prognosis prediction using RNN (PP-RNN). The proposed PP-RNN uses multiple RNNs for learning from diagnosis code sequences of patients in order to predict occurrences of high-risk diseases. The use of RNNs allows the model to learn the status changes of patients considering time, thus enhancing prediction accuracy. We evaluate our method on real-world diagnosis data of over 67,000 adult disease patients recorded for 14 years. Experimental results show the proposed PP-RNN outperforms other standard classification models. In particular, our method provides competitive performance with respect to recall and F1-score on high-risk diseases compared to other models. Furthermore, we investigate the effects of the parameters on the performances.","email":["borim@snu.ac.kr","jeonghee.kim@naverlabs.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881742","source":"ieee","year":2017,"key":"456a2da1-8410-41ba-a4e4-f5e01b8ee656","use":1,"doi":"10.1109\/BIGCOMP.2017.7881742"},{"Title":"Advanced deep learning for blood vessel segmentation in retinal fundus images","Description":"Lua Ngo,  Jae-Ho Han","ShortDetails":"2017 5th International Winter Conference on Brain-Computer Interface (BCI). 2017","abstract":"Rising of deep learning methodologies draws huge attention to their application in image processing and classification. Catching up the trends, this study briefly presents state-of-the-art of deep learning applications in medical imaging interfered with achievements of blood vessel segmentation methods in neurosensory retinal fundus images. Successful segmentation based on deep learning offers advantage in diagnosing ophthalmological disease or pathology.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7858169","source":"ieee","year":2017,"key":"f0a07a25-cdb3-4db6-83d0-3cdb4a7392ed","use":1,"doi":"10.1109\/IWW-BCI.2017.7858169"},{"Title":"Medical Image Denoising Using Convolutional Denoising Autoencoders","Description":"L. Gondara","ShortDetails":"2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW). 2016","abstract":"Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.","email":["lgondara@sfu.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7836672","source":"ieee","year":2016,"key":"e09f0ed5-d190-436f-acb1-0e3d8852edb9","use":1,"doi":"10.1109\/ICDMW.2016.0041"},{"Title":"A Perspective on Deep Imaging","Description":"G. Wang","ShortDetails":"IEEE Access. 2016","abstract":"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.","email":["ge-wang@ieee.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7733110","source":"ieee","year":2016,"key":"67753e02-b1bb-467a-9fc7-aeba59de81f3","use":1,"doi":"10.1109\/ACCESS.2016.2624938"},{"Title":"Super-resolution of medical image using representation learning","Description":"X. Yang,  S. Zhant,  C. Hu,  Z. Liang,  D. Xie","ShortDetails":"2016 8th International Conference on Wireless Communications & Signal Processing (WCSP). 2016","abstract":"Super-resolution (SR) of single image is a meaningful challenge in medical images based diagnosis, while the image resolution is limited. Also, numerous deep neural networks based models were proposed and achieve excellent performance which is superior to the previous handcrafted methods. In this paper, we employ a deep convolutional neural networks for the super-resolution (SR) of single medical image, which learns the nonlinear mapping from the low-resolution space to high-resolution space directly. In addition, we use three sets imaging data (Mammary gland, Prostate tissue and Human brain) training deep network respectively. Firstly, we use Randomized Rectified Linear Unit (RReLU), which incorporates a nonzero slope for negative part to solve the problem of over compression. Secondly, for the purpose of enhancing the quality of reconstructed result and reducing the noise of over-fitting, Nesterov's Accelerated Gradient (NAG) method on the SRCNN is used to accelerate the convergence of loss function and avoid the large oscillations. A comparative performance evaluation is carried out over a set of experiments using real imaging data to verify the validity of proposed algorithm.","email":["zhan@hfut.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7752617","source":"ieee","year":2016,"key":"749fd3f7-ddb7-4bd2-b802-00b81963eef1","use":1,"doi":"10.1109\/WCSP.2016.7752617"},{"Title":"Mass detection using deep convolutional neural network for mammographic computer-aided diagnosis","Description":"S. Suzuki,  X. Zhang,  N. Homma,  K. Ichiji,  N. Sugita,  Y. Kawasumi,  T. Ishibashi,  M. Yoshizawa","ShortDetails":"2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE). 2016","abstract":"In recent years, a deep convolutional neural network (DCNN) has attracted great attention due to its outstanding performance in recognition of natural images. However, the DCNN performance for medical image recognition is still uncertain because collecting a large amount of training data is difficult. To solve the problem of the DCNN, we adopt a transfer learning strategy, and demonstrate feasibilities of the DCNN and of the transfer learning strategy for mass detection in mammographic images. We adopt a DCNN architecture that consists of 8 layers with weight, including 5 convolutional layers, and 3 fully-connected layers in this study. We first train the DCNN using about 1.2 million natural images for classification of 1,000 classes. Then, we modify the last fully-connected layer of the DCNN and subsequently train the DCNN using 1,656 regions of interest in mammographic image for two classes classification: mass and normal. The detection test is conducted on 198 mammographic images including 99 mass images and 99 normal images. The experimental results showed that the sensitivity of the mass detection was 89.9 % and the false positive was 19.2 %. These results demonstrated that the DCNN trained by transfer learning strategy has a potential to be a key system for mammographic mass detection computer-aided diagnosis (CAD). In addition, to the best of our knowledge, our study is the first demonstration of the DCNN for mammographic CAD application.","email":["shintaro.suzuki.r4@dc.tohoku.ac.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7749265","source":"ieee","year":2016,"key":"51579d66-f008-4798-85e9-ad96b9f7b26a","use":1,"doi":"10.1109\/SICE.2016.7749265"},{"Title":"Binary codes for tagging x-ray images via deep de-noising autoencoders","Description":"A. Sze-To,  H. R. Tizhoosh,  A. K. C. Wong","ShortDetails":"2016 International Joint Conference on Neural Networks (IJCNN). 2016","abstract":"A Content-Based Image Retrieval (CBIR) system which identifies similar medical images based on a query image can assist clinicians for more accurate diagnosis. The recent CBIR research trend favors the construction and use of binary codes to represent images. Deep architectures could learn the non-linear relationship among image pixels adaptively, allowing the automatic learning of high-level features from raw pixels. However, most of them require class labels, which are expensive to obtain, particularly for medical images. The methods which do not need class labels utilize a deep autoencoder for binary hashing, but the code construction involves a specific training algorithm and an ad-hoc regularization technique. In this study, we explored using a deep de-noising autoencoder (DDA), with a new unsupervised training scheme using only backpropagation and dropout, to hash images into binary codes. We conducted experiments on more than 14,000 x-ray images. By using class labels only for evaluating the retrieval results, we constructed a 16-bit DDA and a 512-bit DDA independently. Comparing to other unsupervised methods, we succeeded to obtain the lowest total error by using the 512-bit codes for retrieval via exhaustive search, and speed up 9.27 times with the use of the 16-bit codes while keeping a comparable total error. We found that our new training scheme could reduce the total retrieval error significantly by 21.9%. To further boost the image retrieval performance, we developed Radon Autoencoder Barcode (RABC) which are learned from the Radon projections of images using a de-noising autoencoder. Experimental results demonstrated its superior performance in retrieval when it was combined with DDA binary codes.","email":["hy2szeto@uwaterloo.ca","tizhoosh@uwaterloo.ca","akcwong@uwaterloo.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7727561","source":"ieee","year":2016,"key":"d48ba61f-3b02-4379-ad8c-f4a4de21a626","use":1,"doi":"10.1109\/IJCNN.2016.7727561"},{"Title":"A deep bag-of-features model for the classification of melanomas in dermoscopy images","Description":"S. Sabbaghi,  M. Aldeen,  R. Garnavi","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Deep learning and unsupervised feature learning have received great attention in past years for their ability to transform input data into high level representations using machine learning techniques. Such interest has been growing steadily in the field of medical image diagnosis, particularly in melanoma classification. In this paper, a novel application of deep learning (stacked sparse auto-encoders) is presented for skin lesion classification task. The stacked sparse auto-encoder discovers latent information features in input images (pixel intensities). These high-level features are subsequently fed into a classifier for classifying dermoscopy images. In addition, we proposed a new deep neural network architecture based on bag-of-features (BoF) model, which learns high-level image representation and maps images into BoF space. Then, we examine how using this deep representation of BoF, compared with pixel intensities of images, can improve the classification accuracy. The proposed method is evaluated on a test set of 244 skin images. To test the performance of the proposed method, the area under the receiver operating characteristics curve (AUC) is utilized. The proposed method is found to achieve 95% accuracy.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590962","source":"ieee","year":2016,"key":"1f4ec751-3b88-49a7-9977-32acef80ae39","use":1,"doi":"10.1109\/EMBC.2016.7590962"},{"Title":"Membrane segmentation via active learning with deep networks","Description":"U. Gaur,  M. Kourakis,  E. Newman-Smith,  W. Smith,  B. S. Manjunath","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Segmentation is a key component of several bio-medical image processing systems. Recently, segmentation methods based on supervised learning such as deep convolutional networks have enjoyed immense success for natural image datasets and biological datasets alike. These methods require large volumes of data to avoid overfitting which limits their applicability. In this work, we present a transfer learning mechanism based on active learning which allows us to utilize pre-trained deep networks for segmenting new domains with limited labelled data. We introduce a novel optimization criterion to allow feedback on the most uncertain, yet abundant image patterns thus provisioning for an expert in the loop albeit with minimum amount of guidance. Our experiments demonstrate the effectiveness of the proposed method in improving segmentation performance with very limited labelled data.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532697","source":"ieee","year":2016,"key":"7e59ba07-0cf3-4c13-9afa-7a6a2692f3c3","use":1,"doi":"10.1109\/ICIP.2016.7532697"},{"Title":"Retinal vessel segmentation via deep learning network and fully-connected conditional random fields","Description":"H. Fu,  Y. Xu,  D. W. K. Wong,  J. Liu","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493362","source":"ieee","year":2016,"key":"9ac767ad-078a-4749-ad9b-a424df266751","use":1,"doi":"10.1109\/ISBI.2016.7493362"},{"Title":"Privacy-preserving deep learning","Description":"R. Shokri,  V. Shmatikov","ShortDetails":"2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton). 2015","abstract":"Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extrajudicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we present a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility\/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our pr- vacy-preserving deep learning on benchmark datasets.","email":["UTAustin.shokri@cs.utexas.edu2VitalyShmatikov","CornellTech.shmat@cs.cornell.eduandvideofrommillionsofindividualsisripewithprivacyrisks.First"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7447103","source":"ieee","year":2015,"key":"260d7230-ac85-41da-a6c3-a04664ecb7de","use":1,"doi":"10.1109\/ALLERTON.2015.7447103"},{"Title":"The 3-dimensional medical image recognition of right and left kidneys by deep GMDH-type neural network","Description":"T. Kondo,  S. Takao,  J. Ueno","ShortDetails":"2015 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS). 2015","abstract":"In this study, the deep multi-layered Group Method of Data Handling (GMDH)-type neural network algorithm using principal component-regression analysis is applied to recognition problems of the right and left kidney regions. The deep multi-layered GMDH-type neural network algorithm can automatically organize the deep neural network architectures which have many hidden layers and these deep neural networks can identify the characteristics of very complex nonlinear systems. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In this deep GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the deep GMDH-type neural network, and multi-colinearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the right and left kidney regions. The optimum neural network architectures, which fit the complexity of the right and left kidney regions, are automatically organized and the right and left kidney regions are automatically recognized and extracted by the organized deep GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this deep GMDH-type neural networks are useful for the medical image recognition problems of the right and left kidney regions.","email":["kondomedsci@gmail.com","stakao@medsci.tokushima-u.ac.jp","ueno@medsci.tokushima-u.ac.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7439548","source":"ieee","year":2015,"key":"98aaa5f1-7168-4a88-ac81-b37c1552ae9f","use":1,"doi":"10.1109\/ICIIBMS.2015.7439548"},{"Title":"A Histopathological Image Feature Representation Method Based on Deep Learning","Description":"G. Zhang,  L. Zhong,  Y. Huang,  Y. Zhang","ShortDetails":"2015 7th International Conference on Information Technology in Medicine and Education (ITME). 2015","abstract":"Automated annotation and grading for histopathological image plays an important role in CAD systems. It provides valuable information and support for medical diagnosis. Currently, computer-aid analysis of histopathological images mainly relies on some well-designed digital features, which requires abundant human efforts and experiences in problem domain. Learning a good feature representation from data can have positive effects on constructing the target model. We propose a novel method for histopathological image feature representation based on deep learning. The method extracts high level representation of raw pixels of a local region through a network model with several hidden layers, which can learn potential features automatically. The proposed method is evaluated on a real data set from a large local hospital with comparison to two current state-of-the-art methods. The result is promising indicating that it achieves significant improvement of the model performance. Moreover, our study suggests that features learned through deep models can achieve better performance than human designed features.","email":["doczy2006@126.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7429087","source":"ieee","year":2015,"key":"a1cbab22-7dc3-4d39-99e0-b1e4716fb851","use":1,"doi":"10.1109\/ITME.2015.34"},{"Title":"Transformation-Invariant Convolutional Jungles","Description":"D. Laptev,  J. M. Buhmann","ShortDetails":"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015","abstract":"Many Computer Vision problems arise from information processing of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or - in medical imaging - staining and local warping. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised feature learning approach, which efficiently extracts information from these constraints to produce interpretable, transformation-invariant features. The proposed method can incorporate a large class of transformations, e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel image classification and segmentation method, which we call Transformation-Invariant Convolutional Jungles (TICJ). We test the algorithm on two benchmarks in face recognition and medical imaging, where it achieves state of the art results, while being computationally significantly more efficient than Deep Neural Networks.","email":["dlaptev@inf.ethz.ch","jbuhmann@inf.ethz.ch"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7298923","source":"ieee","year":2015,"key":"d296a5ad-18e6-4483-84ee-46a04ca6e7eb","use":1,"doi":"10.1109\/CVPR.2015.7298923"},{"Title":"Medical Image Recognition of Abdominal Multi-organs by Hybrid Multi-layered GMDH-type Neural Network Using Principal Component-Regression Analysis","Description":"T. Kondo,  J. Ueno,  S. Takao","ShortDetails":"2014 Second International Symposium on Computing and Networking. 2014","abstract":"In this study, hybrid multi-layered Group Method of Data Handling (GMDH) type neural network algorithm using principal component-regression analysis is applied to recognition problems of the abdominal multi-organs such as the liver and spleen. In this GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the GMDH-type neural network which is a type of the deep neural network with many hidden layers. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In the GMDH-type neural network, the multi-co linearity occurs and the prediction values become unstable because the architecture of the neural network has many hidden layers whose characteristics are very complex. In the GMDH-type neural networks in this study, multi-co linearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the liver and spleen. The optimum neural network architectures, which fit the complexity of the liver and spleen images, are automatically organized from the multi-detector row CT (MDCT) image of the abdominal regions and the liver and spleen regions are automatically recognized and extracted by the organized GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this GMDH-type neural networks are useful for the medical image recognition problems of the abdominal multi-organs.","email":["kondo@medsci.tokushima-u.ac.jp","ueno@medsci.tokushima-u.ac.jp","stakao@medsci.tokushima-u.ac.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7052176","source":"ieee","year":2014,"key":"7d89ed75-5b67-4779-aed9-a283482f524c","use":1,"doi":"10.1109\/CANDAR.2014.62"},{"Title":"Hybrid feedback GMDH-type neural network using principal component-regression analysis and its application to medical image recognition of heart regions","Description":"T. Kondo,  J. Ueno,  S. Takao","ShortDetails":"2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS). 2014","abstract":"Hybrid feedback Group Method of Data Handling (GMDH)-type neural network using principal component-regression analysis is applied to the medical image recognition of the heart regions. In the GMDH-type neural network, the multi-layered deep neural networks are automatically organized so as to fit the complexity of the nonlinear system and, in general, the architectures of the GMDH-type neural network have many hidden layers and become complex for the nonlinear systems. In the multi-layered deep GMDH-type neural network with many hidden layers, the multi-colinearity occurs and the perdition accuracy become worse and the prediction values become unstable. In the GMDH-type neural network used in this study, the principal component-regression analysis is used as the learning algorithm of the neural network and the multi-colinearity do not occur and accurate and stable GMDH-type neural network architectures are automatically organized so as to fit the complexity of the nonlinear system. Furthermore, in this algorithm, three types of neural networks, such as sigmoid function neural network, radial basis function (RBF) neural network and polynomial neural network, can be generated using three types of neuron architectures, and the neural network architecture which fits the complexity of medical images, is selected from these three neural network architectures. This GMDH-type neural network is applied to the medical image recognition of the heart regions and it is shown that this GMDH-type neural network is useful for the medical image recognition of the heart regions.","email":["kondo@medsci.tokushima-u.ac.jp","ueno@medsci.tokushima-u.ac.jp","stakao@medsci.tokushima-u.ac.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7044800","source":"ieee","year":2014,"key":"57ef6b12-8ce9-43f1-848d-732e92e41bc4","use":1,"doi":"10.1109\/SCIS-ISIS.2014.7044800"},{"Title":"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data","Description":"Y. Liu,  B. Logan,  N. Liu,  Z. Xu,  J. Tang,  Y. Wang","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031178","source":"ieee","year":2017,"key":"a9c1a40f-8b8a-4b23-b6cb-10557448210b","use":1,"doi":"10.1109\/ICHI.2017.45"},{"Title":"Single Sensor Techniques for Sleep Apnea Diagnosis Using Deep Learning","Description":"R. K. Pathinarupothi,  D. P. J,  E. S. Rangan,  G. E. A,  V. R,  K. P. Soman","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"A large number of obstructive sleep apnea (OSA) cases are under-diagnosed due unavailability, inconvenience or expense of sleep labs. Hence, an automated detection by applying computational techniques to multivariate signals has already become a well-researched subject. However, the best-known techniques that use various features have not achieved the gold standard of polysomnography (PSG) tests. In this paper, we substantiate the medical conjecture that OSA directly impacts body parameters such as Instantaneous Heart Rate (IHR) and blood oxygen saturation (SpO2). We then use a deep learning technique called LSTM-RNN (long short-term memory recurrent neural networks) to experimentally prove that OSA severity detection can be solely based on either IHR or SpO2 signals, which can be easily, obtained using off-the-shelf non-intrusive wearable single sensors. The results obtained from LSTM-RNN model shows an area under curve (AUC) of 0.98 associated with very high accuracy on a dataset of more than 16,000 apnea non-apnea minutes. These results have encouraged our collaborating doctors to further come up with a diagnostic protocol that is based on LSTM-RNN, SpO2, and IHR, thereby increasing the chances of larger adoption among medical community.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031206","source":"ieee","year":2017,"key":"88ed290e-84ce-4c9a-ad9c-6b454d9de54e","use":1,"doi":"10.1109\/ICHI.2017.37"},{"Title":"Deep tessellated retinal image detection using Convolutional Neural Networks","Description":"X. Lyu,  H. Li,  Y. Zhen,  X. Ji,  S. Zhang","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Tessellation in fundus is not only a visible feature for aged-related and myopic maculopathy but also confuse retinal vessel segmentation. The detection of tessellated images is an inevitable processing in retinal image analysis. In this work, we propose a model using convolutional neural network for detecting tessellated images. The input to the model is pre-processed fundus image, and the output indicate whether this photograph has tessellation or not. A database with 12,000 colour retinal images is collected to evaluate the classification performance. The best tessellation classifier achieves accuracy of 97.73% and AUC value of 0.9659 using pretrained GoogLeNet and transfer learning technique.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036915","source":"ieee","year":2017,"key":"c11e923f-816d-4305-90ab-5a8f2051e703","use":1,"doi":"10.1109\/EMBC.2017.8036915"},{"Title":"Automated Analysis of Unregistered Multi-view Mammograms with Deep Learning","Description":"G. Carneiro,  J. Nascimento,  A. P. Bradley","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient\u2019s risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calcifications). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calcification detection systems, and still producing accurate results. The semi-automated approach (using manually defined mass and micro-calcification segmentation maps) is tested on two publicly available datasets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class \u201dbenign vs malignant\u201d problem is over 0.9, and for the 2- class breast screening problem (malignancy vs normal\/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class \u201dbenign vs malignant\u201d problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8032490","source":"ieee","year":2017,"key":"e56f6cc1-1f21-4a2d-88a5-8103d0ce52e7","use":1,"doi":"10.1109\/TMI.2017.2751523"},{"Title":"Structure Prediction for Gland Segmentation with Hand-Crafted and Deep Convolutional Features","Description":"S. Manivannan,  W. Li,  J. Zhang,  E. Trucco,  S. McKenna","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"We present a novel method to segment instances of glandular structures from colon histopathology images. We use a structure learning approach which represents local spatial configurations of class labels, capturing structural information normally ignored by sliding-window methods. This allows us to reveal different spatial structures of pixel labels (e.g., locations between adjacent glands, or far from glands), and to identify correctly neighbouring glandular structures as separate instances. Exemplars of label structures are obtained via clustering and used to train support vector machine classifiers. The label structures predicted are then combined and post-processed to obtain segmentation maps. We combine hand-crafted, multi-scale image features with features computed by a deep convolutional network trained to map images to segmentation maps. We evaluate the proposed method on the public domain GlaS dataset, which allows extensive comparisons with recent, alternative methods. Using the GlaS contest protocol, our method achieves the overall best performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8030141","source":"ieee","year":2017,"key":"71f8908b-6dc3-4f2e-ade7-b89904700ff6","use":1,"doi":"10.1109\/TMI.2017.2750210"},{"Title":"HCNN: Heterogeneous Convolutional Neural Networks for Comorbid Risk Prediction with Electronic Health Records","Description":"J. Zhang,  J. Gong,  L. Barnes","ShortDetails":"2017 IEEE\/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2017","abstract":"The increasing adoption of electronic health record (EHR) systems has brought tremendous opportunities in medicine enabling more personalized prognostic models. However, most work to date has investigated the binary classification problem for predicting the onset of one chronic disease, but little attention has been given to assessing risk of developing comorbidities that are major causes of morbidity and mortality. For example, type 2 diabetes and chronic kidney disease frequently accompany congestive heart failure. This paper is motivated by the problem of predicting comorbid diseases and aims to answer the following question: can we predict the comorbid risk using a patient's medical history? We propose a new predictive learning framework, Heterogeneous Convolutional Neural Network (HCNN), that represents EHRs as graphs with heterogeneous attributes (e.g. diagnoses, procedures, and medication), and then develop a novel deep learning methodology for risk prediction of multiple comorbid diseases. The main innovation of the framework is that it defines the distance between the heterogeneous attributes of the graph representation extracted from the EHR and develops an appropriate learning infrastructure that is a composition of sparse convolutional layers and local pooling steps that match with the local structure of the space of the heterogeneous attributes. As a result, the new method is capable of capturing features about the relationships between heterogeneous attributes of the graphs. Through a comparative study on patient EHR data, HCNN achieves better performance than traditional convolutional neural networks on the risk prediction of comorbid diseases.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8010635","source":"ieee","year":2017,"key":"871fb2c0-30ec-4724-b02d-70f91a415fd0","use":1,"doi":"10.1109\/CHASE.2017.80"},{"Title":"Leveraging deep preference learning for indexing and retrieval of biomedical images","Description":"S. Pang,  M. A. Orgun,  A. Du,  Z. Yu","ShortDetails":"2017 8th International IEEE\/EMBS Conference on Neural Engineering (NER). 2017","abstract":"This paper presents an original framework based on deep learning and preference learning to retrieve and characterize biomedical images for assisting physicians in diagnosing complex diseases with potentially only small differences between them. In particular, we use deep learning to extract the high-level and compact features for biomedical images. In contrast to the traditional biomedical algorithms or general image retrieval systems that only consider the use of pixel and\/or hand-crafted features to represent images, we utilize deep neural networks for feature discovery of biomedical images. Moreover, in order to be able to index the similarly referenced images, we introduce preference learning in a novel way to learn what kinds of images we need so that we can obtain the similarity ranking list of biomedical images. We evaluate the performance of our system in detailed experiments over the well-known available OASIS-MRI database for whole brain neuroimaging as a benchmark and compare it with those of the traditional biomedical and general image retrieval approaches. Our proposed system exhibits an outstanding retrieval ability and efficiency for biomedical image applications.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8008308","source":"ieee","year":2017,"key":"42660d64-b0ac-471f-b622-7d62b35836d5","use":1,"doi":"10.1109\/NER.2017.8008308"},{"Title":"Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition From Human Brain EEG Sensors","Description":"R. Majid Mehmood,  R. Du,  H. J. Lee","ShortDetails":"IEEE Access. 2017","abstract":"Recent advancements in human\u2013computer interaction research have led to the possibility of emotional communication via brain\u2013computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12\u201314 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.","email":["rmeex07@gmail.com","dury@njupt.edu.cn","hlee@chonbuk.ac.kr","hlee@chonbuk.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7997991","source":"ieee","year":2017,"key":"800a2bce-0a71-4e97-91dd-8dba427aec16","use":1,"doi":"10.1109\/ACCESS.2017.2724555"},{"Title":"Adaptive smoothing in fMRI data processing neural networks","Description":"A. Vilamala,  K. H. Madsen,  L. K. Hansen","ShortDetails":"2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI). 2017","abstract":"Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data processing pipelines to accurately determine brain activity; among them, the crucial step of spatial smoothing. These pipelines are commonly suboptimal, given the local optimisation strategy they use, treating each step in isolation. With the advent of new tools for deep learning, recent work has proposed to turn these pipelines into end-to-end learning networks. This change of paradigm offers new avenues to improvement as it allows for a global optimisation. The current work aims at benefitting from this paradigm shift by defining a smoothing step as a layer in these networks able to adaptively modulate the degree of smoothing required by each brain volume to better accomplish a given data analysis task. The viability is evaluated on real fMRI data where subjects did alternate between left and right finger tapping tasks.","email":["alvmu@dtu.dk","kristofferm@drcmr.dk","lkai@dtu.dk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7981499","source":"ieee","year":2017,"key":"1a1d141e-72f8-47a4-b965-6d529f44468a","use":1,"doi":"10.1109\/PRNI.2017.7981499"},{"Title":"Deep learning approach for EEG compression in mHealth system","Description":"A. Ben Said,  A. Mohamed,  T. Elfouly","ShortDetails":"2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC). 2017","abstract":"The emergence of mobile health (mHealth) systems has risen the challenges and concerns due to the sensitivity of the data involved in such systems. It is essential to ensure that these data are well delivered to the health monitoring center for accurate and perfect diagnosis and follow-up. Due to the wireless network constraints, these requirements become more challenging. In this paper, we propose a deep learning approach for EEG data compression in mHealth system. We show that the stacked autoencoder neural network architecture is efficient for EEG data compression. We conduct a comprehensive comparative study that demonstrates the effectiveness of our system for EEG compression in addition to preserving the total energy consumption.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7986507","source":"ieee","year":2017,"key":"379c62d9-0457-4206-b9a2-934b378e6b02","use":1,"doi":"10.1109\/IWCMC.2017.7986507"},{"Title":"Automating Papanicolaou Test Using Deep Convolutional Activation Feature","Description":"J. Hyeon,  H. J. Choi,  K. N. Lee,  B. D. Lee","ShortDetails":"2017 18th IEEE International Conference on Mobile Data Management (MDM). 2017","abstract":"Cervical cancer is the women's fourth most common cancer worldwide, with 266,000 deaths in a year. Cervical cancer can be diagnosed by the Papanicolaou test. In this test, a cytopathologist observes a microscopic image of the cervix cells and decides whether the patient is abnormal or not. According to research, the accuracy of the cervical cytology is reported as 89.7%. Because it is associated with the patient's life, it is important to improve the accuracy of this test. Many systems have been proposed to help judge experts to improve the accuracy of tests in the medical field, but development has been limited to areas where there are cleanly quantified test data. In this paper, we design and train a model to automatically classify the normal\/abnormal state of cervical cells from microscopic images by using a convolutional neural network and several machine learning classifiers. As a result, the support vector machine achieves the highest performance with 78% F1 score.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7962484","source":"ieee","year":2017,"key":"6fcdd210-af37-41bc-896d-abd1e97ee7c0","use":1,"doi":"10.1109\/MDM.2017.66"},{"Title":"Deep learning of texture and structural features for multiclass Alzheimer's disease classification","Description":"C. V. Dolph,  M. Alam,  Z. Shboul,  M. D. Samad,  K. M. Iftekharuddin","ShortDetails":"2017 International Joint Conference on Neural Networks (IJCNN). 2017","abstract":"This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6% and 58.0% tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4% and 56.8%. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2% in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7966129","source":"ieee","year":2017,"key":"49d873f2-2d91-4426-93e8-c13312cd05e9","use":1,"doi":"10.1109\/IJCNN.2017.7966129"},{"Title":"Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation","Description":"J. Egede,  M. Valstar,  B. Martinez","ShortDetails":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). 2017","abstract":"Automatic continuous time, continuous value assessment of a patient's pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961808","source":"ieee","year":2017,"key":"d97a1378-2a7f-45b4-9252-6c3f64ced20b","use":1,"doi":"10.1109\/FG.2017.87"},{"Title":"Cells classification with deep learning","Description":"A. Sezer,  U. \u00c7ekmez","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"Proteomic analysis is a rapidly developing research field that has recently been used in the diagnosis and treatment of various diseases by analyzing the structure and functions of protein patterns in the cell. Numerous computer based decision support mechanisms implemented in this context have mostly used special image processing techniques until now. Recently, high performance self-learning deep learning methods have taken place in the classification studies over the conventional methods examining the structural features of the patterns, shapes and the texture properties in the images. In this study, different intracellular patterns of HeLa cells taken by the microscope used in the testing of pattern analysis and the output is compared by classifying these patterns by using both deep learning methods and bag-of-features method. As a result of the experiments, it is seen that the success of the proposed deep learning model has a very high performance in classifying compared to the existing models and bag-of-features technique.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960647","source":"ieee","year":2017,"key":"1f71e53e-4ce6-4fb9-b798-68106d38e200","use":1,"doi":"10.1109\/SIU.2017.7960647"},{"Title":"Deep neural network based diagnosis system for melanoma skin cancer","Description":"A. Ba\u015ft\u00fcrk,  M. E. Y\u00fcksei,  H. Badem,  A. \u00c7al\u0131\u015fkan","ShortDetails":"2017 25th Signal Processing and Communications Applications Conference (SIU). 2017","abstract":"Melanoma is a serious cancer that causes many people to lose their lives. This disease can be diagnosed by a dermatologist as a result of interpretation of the dermoscopy images by the ABCD rule. In this study, a deep neural network (DNN) is used as a new method for diagnosis of melanoma skin cancer. This method is compared with the-state-art-methods in literature. According to the obtained results, DNN was more successful than the comparative methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7960563","source":"ieee","year":2017,"key":"024ca5b1-a477-446a-a1e2-3deba56e33fa","use":1,"doi":"10.1109\/SIU.2017.7960563"},{"Title":"Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods","Description":"G. Carneiro,  T. Peng,  C. Bayer,  N. Navab","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitativ- results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7869416","source":"ieee","year":2017,"key":"388c19b7-abcb-4c64-aa2f-e31e5d6335ad","use":1,"doi":"10.1109\/TMI.2017.2677479"},{"Title":"Nuclei segmentation in histopathology images using deep neural networks","Description":"P. Naylor,  M. La\u00e9,  F. Reyal,  T. Walter","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950669","source":"ieee","year":2017,"key":"36755a8e-af64-41e9-be72-40bee074eda7","use":1,"doi":"10.1109\/ISBI.2017.7950669"},{"Title":"Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images","Description":"B. Ehteshami Bejnordi,  J. Lin,  B. Glass,  M. Mullooly,  G. L. Gierach,  M. E. Sherman,  N. Karssemeijer,  J. van der Laak,  A. H. Beck","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950668","source":"ieee","year":2017,"key":"f487ce6e-d10f-4af7-9af7-2293291d08ca","use":1,"doi":"10.1109\/ISBI.2017.7950668"},{"Title":"Multi-stage segmentation of the fovea in retinal fundus images using fully Convolutional Neural Networks","Description":"S. Sedai,  R. Tennakoon,  P. Roy,  K. Cao,  R. Garnavi","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"The fovea is one of the most important anatomical landmarks in the eye and its localization is required in automated analysis of retinal diseases due to its role in sharp central vision. In this paper, we propose a two-stage deep learning framework for accurate segmentation of the fovea in retinal colour fundus images. In the first stage, coarse segmentation is performed to localize the fovea in the fundus image. The location information from the first stage is then used to perform fine-grained segmentation of the fovea region in the second stage. The proposed method performs end-to-end pixelwise segmentation by creating a deep learning model based on fully convolutional neural networks, which does not require the prior knowledge of the location of other retinal structures such as optic disc (OD) and vasculature geometry. We demonstrate the effectiveness of our method on a dataset with 400 retinal images with average localization error of 14 \u00b1 7 pixels.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950704","source":"ieee","year":2017,"key":"d44f8a37-a799-4fe4-bbea-ebbd3f87a0b8","use":1,"doi":"10.1109\/ISBI.2017.7950704"},{"Title":"Wide residual networks for mitosis detection","Description":"E. Zerhouni,  D. L\u00e1nyi,  M. Viana,  M. Gabrani","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"One of the most important prognostic markers to assess proliferation activity of breast tumors is estimating the number of mitotic figures in H&E stained tissue. We propose the use of a recently published convolutional neural network architecture, Wide Residual Networks, for mitosis detection in breast histology images. The model is trained to classify each pixel of on an image using as context a patch centered on the pixel. We apply post-processing on the network output in order to filter out noise and select true mitosis. Finally, we combine the output of several networks using majority vote. Our approach ranked 2nd in the MICCAI TUPAC 2016 competition for mitosis detection, outperforming most other contestants by a significant margin.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950667","source":"ieee","year":2017,"key":"dbfb292a-d3e1-4e94-8909-c69b7cc3efcf","use":1,"doi":"10.1109\/ISBI.2017.7950667"},{"Title":"Epithelium-stroma classification in histopathological images via convolutional neural networks and self-taught learning","Description":"Y. Huang,  H. Zheng,  C. Liu,  G. Rohde,  D. Zeng,  J. Wang,  X. Ding","ShortDetails":"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017","abstract":"Epithelium-stroma classification is always considered as an important preprocessing step for morphological quantitative analysis in image-based histological researches of oncologic diseases. However, large-scale accurate ground-truth labeling is expensive in histopathological image analysis, thus the classification performances will still be limited with the insufficient labeled training samples. Considering that acquisition of public unlabeled histopathological images is much cheaper, an epithelium-stroma classification framework is developed, based on the deep convolutional neural network framework and the strategies of self-taught learning. The method has the ability of taking advantage of large-scale unlabeled public histopathological data as auxiliary data, and then transferring the knowledge to enhance the performances in epithelium-stroma classification with limited labeled training data. The experiments demonstrate that the proposed method outperforms traditional CNNs when the labeled training data size is decreasing dramatically.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7952321","source":"ieee","year":2017,"key":"fb849d80-312f-4b12-b8e2-74aef906e81a","use":1,"doi":"10.1109\/ICASSP.2017.7952321"},{"Title":"Disease grading of heterogeneous tissue using convolutional autoencoder","Description":"E. Zerhouni,  B. Prisacari,  Q. Zhong,  P. Wild,  M. Gabrani","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"One of the main challenges of histological image analysis is the high dimensionality of the images. This can be addressed via summarizing techniques or feature engineering. However, such approaches can limit the performance of subsequent machine learning models, particularly when dealing with highly heterogeneous tissue samples. One possible alternative is to employ unsupervised learning to determine the most relevant features automatically. In this paper, we propose a method of generating representative image signatures that are robust to tissue heterogeneity. At the core of our approach lies a novel deep-learning based mechanism to simultaneously produce representative image features as well as perform dictionary learning to further reduce dimensionality. By integrating this mechanism in a broader framework for disease grading, we show significant improvement in terms of grading accuracy compared to alternative local feature extraction methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950591","source":"ieee","year":2017,"key":"be7fb88f-51f5-416e-ad88-a7fae04b0443","use":1,"doi":"10.1109\/ISBI.2017.7950591"},{"Title":"Deep learning based multi-label classification for surgical tool presence detection in laparoscopic videos","Description":"S. Wang,  A. Raju,  J. Huang","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automatic recognition of surgical workflow is an unresolved problem among the community of computer-assisted interventions. Among all the features used for surgical workflow recognition, one important feature is the presence of the surgical tools. Extracting this feature leads to the surgical tool presence detection problem to detect what tools are used at each time in surgery. This paper proposes a deep learning based multi-label classification method for surgical tool presence detection in laparoscopic videos. The proposed method combines two state-of-the-art deep neural networks and uses ensemble learning to solve the tool presence detection problem as a multi-label classification problem. The performance of the proposed method has been evaluated in the surgical tool presence detection challenge held by Modeling and Monitoring of Computer Assisted Interventions workshop. The proposed method shows superior performance compared to other methods and has won the first place of the challenge.","email":["jzhuang@uta.edu.Thisworkwaspartiallysup-portedbyU.S.NSFIIS-1423056"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950597","source":"ieee","year":2017,"key":"1c7b4f7f-6c70-4b5a-ae2d-a92d2f249798","use":1,"doi":"10.1109\/ISBI.2017.7950597"},{"Title":"A fully automatic deep learning method for atrial scarring segmentation from late gadolinium-enhanced MRI images","Description":"G. Yang,  X. Zhuang,  H. Khan,  S. Haldar,  E. Nyktari,  X. Ye,  G. Slabaugh,  T. Wong,  R. Mohiaddin,  J. Keegan,  D. Firmin","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Precise and objective segmentation of atrial scarring (SAS) is a prerequisite for quantitative assessment of atrial fibrillation using non-invasive late gadolinium-enhanced (LGE) MRI. This also requires accurate delineation of the left atrium (LA) and pulmonary veins (PVs) geometry. Most previous studies have relied on manual segmentation of LA wall and PVs, which is a tedious and error-prone procedure with limited reproducibility. There are many attempts on automatic SAS using simple thresholding, histogram analysis, clustering and graph-cut based approaches; however, in general, these methods are considered as unsupervised learning thus subject to limited segmentation accuracy. In this study, we present a fully-automated multi-atlas based whole heart segmentation method to derive the LA and PVs geometry objectively that is followed by a fully automatic deep learning method for SAS. Our deep learning method consists of a feature extraction step via super-pixel over-segmentation and a supervised classification step via stacked sparse auto-encoders. We demonstrate the efficacy of our method on 20 clinical LGE MRI scans acquired from a longstanding persistent atrial fibrillation cohort. Both quantitative and qualitative results show that our fully automatic method obtained accurate segmentation results compared to the manual segmentation based ground truths.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950649","source":"ieee","year":2017,"key":"c90d3a08-f033-4e06-b8af-f7f45af7edb8","use":1,"doi":"10.1109\/ISBI.2017.7950649"},{"Title":"Parcellation of visual cortex on high-resolution histological brain sections using convolutional neural networks","Description":"H. Spitzer,  K. Amunts,  S. Harmeling,  T. Dickscheid","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Microscopic analysis of histological sections is considered the \u201cgold standard\u201d to verify structural parcellations in the human brain. Its high resolution allows the study of laminar and columnar patterns of cell distributions, which build an important basis for the simulation of cortical areas and networks. However, such cytoarchitectonic mapping is a semiautomatic, time consuming process that does not scale with high throughput imaging. We present an automatic approach for parcellating histological sections at 2\u03bcm resolution. It is based on a convolutional neural network that combines topological information from probabilistic atlases with the texture features learned from high-resolution cell-body stained images. The model is applied to visual areas and trained on a sparse set of partial annotations. We show how predictions are transferable to new brains and spatially consistent across sections.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950666","source":"ieee","year":2017,"key":"c30242d1-45ca-4526-afdc-c1182618e055","use":1,"doi":"10.1109\/ISBI.2017.7950666"},{"Title":"A generalized MRI-based CAD system for functional assessment of renal transplant","Description":"F. Khalifa,  M. Shehata,  A. Soliman,  M. Abou El-Ghar,  T. El-Diasty,  A. C. Dwyer,  M. El-Melegy,  G. Gimel'farb,  R. Keynton,  A. El-Baz","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In recent years, magnetic resonance imaging (MRI) has been explored for non-invasive assessment of renal transplant function. This paper proposes a computer-aided diagnostic (CAD) system for the assessment of renal transplant status, which integrates both clinical and MRI-derived biomarkers. The latter are derived from either 3D (2D + time) dynamic contrast-enhanced MRI or 4D (3D + b-value) diffusion-weighted (DW) MRI. In order to extract the MRI-based biomarkers, our framework performs multiple image processing steps, including MRI data alignment to handle the motion effects, kidney segmentation using a geometric deformable model, local motion correction, and estimation of image-based biomarkers. These biomarkers are fused with clinical biomarkers (creatinine clearance and serum plasma creatinine) for the classification of transplant status using a machine learning classifier. Our CAD system has been tested on a cohort of 100 subjects (50 DCE-MRI and 50 DW-MRI) using a \u201cleave-one-subject-out\u201d approach and distinguished rejection from non-rejection transplants with an overall accuracy of 98% for both DCE-MRI and DW-MRI data sets. These preliminary results demonstrate the promise of the proposed CAD system as a reliable non-invasive diagnostic tool for renal transplant assessment.","email":["aselba01@louisville.edu.Inrecentyears"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950629","source":"ieee","year":2017,"key":"a2adf437-391e-498b-91e0-12c801f2f7a9","use":1,"doi":"10.1109\/ISBI.2017.7950629"},{"Title":"Automated 5-year mortality prediction using deep learning and radiomics features from chest computed tomography","Description":"G. Carneiro,  L. Oakden-Rayner,  A. P. Bradley,  J. Nascimento,  L. Palmer","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"In this paper, we propose new prognostic methods that predict 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on two state-of-the-art deep learning models extended to 3-D inputs, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection and extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning models produces a mean 5-year mortality prediction AUC in [68.8%,69.8%] and accuracy in [64.5%,66.5%], while radiomics produces a mean AUC of 64.6% and accuracy of 64.6%. The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950485","source":"ieee","year":2017,"key":"f85cf6cf-ae26-4a54-8116-a6b077110f77","use":1,"doi":"10.1109\/ISBI.2017.7950485"},{"Title":"Skin melanoma segmentation using recurrent and convolutional neural networks","Description":"M. Attia,  M. Hossny,  S. Nahavandi,  A. Yazdabadi","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Skin melanoma is one of the highly addressed health problems in many countries. Dermatologists diagnose melanoma by visual inspections of mole using clinical assessment tools such as ABCD. However, computer vision tools have been introduced to assist in quantitative analysis of skin lesions. Deep learning is one of the trending machine learning techniques that have been successfully utilized to solve many difficult computer vision tasks. We proposed using a hybrid method that utilizes two popular deep learning methods: convolutional and recurrent neural networks. The proposed method was trained using 900 images and tested on 375 images. Images were obtained from \u201cSkin Lesion Analysis Toward Melanoma Detection\u201d challenge which was hosted by ISBI 2016 conference. We achieved segmentation average accuracy of 0.98 and Jaccard index of 0.93. Results were compared with other state-of-the-art methods, including winner of ISBI 2016 challenge for skin melanoma segmentation, along with the same evaluation criteria.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950522","source":"ieee","year":2017,"key":"e72af7f5-55c1-456a-89b9-34d78b504796","use":1,"doi":"10.1109\/ISBI.2017.7950522"},{"Title":"Age estimation from brain MRI images using deep learning","Description":"T. W. Huang,  H. T. Chen,  R. Fujimoto,  K. Ito,  K. Wu,  K. Sato,  Y. Taki,  H. Fukuda,  T. Aoki","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Estimating human age from brain MR images is useful for early detection of Alzheimer's disease. In this paper we propose a fast and accurate method based on deep learning to predict subject's age. Compared with previous methods, our algorithm achieves comparable accuracy using fewer input images. With our GPU version program, the time needed to make a prediction is 20 ms. We evaluate our methods using mean absolute error (MAE) and our method is able to predict subject's age with MAE of 4.0 years.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950650","source":"ieee","year":2017,"key":"91ae70cc-2501-4b68-b269-24bbb0d3fec8","use":1,"doi":"10.1109\/ISBI.2017.7950650"},{"Title":"MIMO-Net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images","Description":"S. E. A. Raza,  L. Cheung,  D. Epstein,  S. Pelengaris,  M. Khan,  N. M. Rajpoot","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"We propose a novel multiple-input multiple-output convolution neural network (MIMO-Net) for cell segmentation in fluorescence microscopy images. The proposed network trains the network parameters using multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The MIMO-Net allows us to deal with variable intensity cell boundaries and highly variable cell size in the mouse pancreatic tissue by adding extra convolutional layers which bypass the max-pooling operation. The results show that our method outperforms state-of-the-art deep learning based approaches for segmentation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950532","source":"ieee","year":2017,"key":"3f7f9557-aed7-4795-a2cb-ec0feba77e80","use":1,"doi":"10.1109\/ISBI.2017.7950532"},{"Title":"Deep residual Hough voting for mitotic cell detection in histopathology images","Description":"T. Wollmann,  K. Rohr","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Cell detection in microscopy images is a common and challenging task. We propose a new approach for mitotic cell detection in histopathology images, which is based on a Deep Residual Network architecture combined with Hough voting. We propose a voting layer for neural networks and introduce a novel loss function. Our approach is learned from scratch using cell centroids and the original images. We benchmarked our approach on the challenging AMIDA13 dataset containing histology images of invasive breast carcinoma. It turned out that our approach achieved competitive results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950533","source":"ieee","year":2017,"key":"9bcc1f51-5cb0-400c-b0b2-7410b6772d3c","use":1,"doi":"10.1109\/ISBI.2017.7950533"},{"Title":"Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images","Description":"T. H. Song,  V. Sanchez,  H. EIDaly,  N. M. Rajpoot","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Automated cell detection is a critical step for a number of computer-assisted pathology related image analysis algorithm. However, automated cell detection is complicated due to the variable cytomorphological and histological factors associated with each cell. In order to efficiently resolve the challenge of automated cell detection, deep learning strategies are widely applied and have recently been shown to be successful in histopathological images. In this paper, we concentrate on bone marrow trephine biopsy images and propose a hybrid deep autoencoder (HDA) network with Curvature Gaussian model for efficient and precise bone marrow hematopoietic stem cell detection via related high-level feature correspondence. The accuracy of our proposed method is up to 94%, outperforming other supervised and unsupervised detection approaches.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950694","source":"ieee","year":2017,"key":"7c379502-5261-4825-8487-2a4804e75941","use":1,"doi":"10.1109\/ISBI.2017.7950694"},{"Title":"Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View","Description":"A. H. Abdi,  C. Luong,  T. Tsang,  G. Allan,  S. Nouranian,  J. Jue,  D. Hawley,  S. Fleming,  K. Gin,  J. Swift,  R. Rohling,  P. Abolmaesumi","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 \u00b1 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7892028","source":"ieee","year":2017,"key":"987cb21e-b53a-4593-b50c-29e1e74564a4","use":1,"doi":"10.1109\/TMI.2017.2690836"},{"Title":"Integrating Online and Offline Three-Dimensional Deep Learning for Automated Polyp Detection in Colonoscopy Videos","Description":"L. Yu,  H. Chen,  Q. Dou,  J. Qin,  P. A. Heng","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7776845","source":"ieee","year":2017,"key":"b3dcae16-f114-4a52-ba96-ce24b7e4a340","use":1,"doi":"10.1109\/JBHI.2016.2637004"},{"Title":"$mathtt {Deepr}$: A Convolutional Net for Medical Records","Description":"P. Nguyen,  T. Tran,  N. Wickramasinghe,  S. Venkatesh","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space.","email":["nilmini.work@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7762861","source":"ieee","year":2017,"key":"3c9bf04d-4976-4591-8897-76614368da1d","use":1,"doi":"10.1109\/JBHI.2016.2633963"},{"Title":"Diabetic retinopathy detection using deep convolutional neural networks","Description":"D. Doshi,  A. Shenoy,  D. Sidhpura,  P. Gharpure","ShortDetails":"2016 International Conference on Computing, Analytics and Security Trends (CAST). 2016","abstract":"Diabetic retinopathy is when damage occurs to the retina due to diabetes, which affects up to 80 percent of all patients who have had diabetes for 10 years or more. The expertise and equipment required are often lacking in areas where diabetic retinopathy detection is most needed. Most of the work in the field of diabetic retinopathy has been based on disease detection or manual extraction of features, but this paper aims at automatic diagnosis of the disease into its different stages using deep learning. This paper presents the design and implementation of GPU accelerated deep convolutional neural networks to automatically diagnose and thereby classify high-resolution retinal images into 5 stages of the disease based on severity. The single model accuracy of the convolutional neural networks presented in this paper is 0.386 on a quadratic weighted kappa metric and ensembling of three such similar models resulted in a score of 0.3996.","email":["darshit.doshi@gmail.com","prachigharpure@spit.ac.in","shenoyaniket95@gmail.com","deepsidhpura777@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7914977","source":"ieee","year":2016,"key":"c68bdeda-193a-41e5-8dfd-272ff6d717ce","use":1,"doi":"10.1109\/CAST.2016.7914977"},{"Title":"Detecting Cardiovascular Disease from Mammograms With Deep Learning","Description":"J. Wang,  H. Ding,  F. A. Bidgoli,  B. Zhou,  C. Iribarren,  S. Molloi,  P. Baldi","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected in mammograms, can be useful risk markers associated with the disease. We investigate the feasibility of automated and accurate detection of BACs in mammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-response receiver operating characteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection in mammograms to help identify and assess patients with cardiovascular risks.","email":["juanw3@uci.edu","pfbaldi@ics.uci.edu","pubs-permissions@ieee.org."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7827150","source":"ieee","year":2017,"key":"47499223-1b2b-4ccb-b6cb-4e3eb8ef5623","use":1,"doi":"10.1109\/TMI.2017.2655486"},{"Title":"Predicting heart rejection using histopathological whole-slide imaging and deep neural network with dropout","Description":"L. Tong,  R. Hoffman,  S. R. Deshpande,  M. D. Wang","ShortDetails":"2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). 2017","abstract":"Cardiac allograft rejection is one major limitation for long-term survival for patients with heart transplants. The endomyocardial biopsy is one gold standard to screen heart rejection for patients that have heart transplantation. However, manual identification of heart rejection is expensive and time-consuming. With the development of imaging processing techniques and machine learning tools, automatic prediction of heart rejection using whole-slide images is one promising approach to improve the care of patients with heart transplants. In this paper, we first develop a histopathological whole-slide image processing pipeline to extract features automatically. Then, we construct deep neural networks with and without regularization and dropout to classify the patients into non-rejection and rejection respectively. Our results show that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracies.","email":["ltong9@gatech.edu","rhoffman12@gatech.edu","DeshpandeS@kidsheart.com","maywang@bme.gatech.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7897190","source":"ieee","year":2017,"key":"b9da1df5-34c0-4f4a-a0f1-f5ab3d3b628b","use":1,"doi":"10.1109\/BHI.2017.7897190"},{"Title":"Automatic polyp detection in endoscopy videos: A survey","Description":"B. Taha,  N. Werghi,  J. Dias","ShortDetails":"2017 13th IASTED International Conference on Biomedical Engineering (BioMed). 2017","abstract":"Early detection of polyps play an essential role for the prevention of colorectal cancer. Manual clinical inspection have many limitations and could result to either false or missed polyps. Computer aided diagnosis system has been used to help the medical expert and to provide more accurate diagnosis. Since their introduction, many types of algorithms have been proposed in the literature using different types of features and classifiers. This paper provides a state-of-the-art for the automatic detection of polyps using endoscopic videos. Given the increasing evolution of medical imaging technologies and algorithms, it is important to have a recent review in order to know the current state of the art, and the opportunities for improving existing algorithms, or developing innovative ones. The paper divides the work done on this research area according to the type of features and classification methods implemented. The features have been divided into shape, texture or fusion features. Future directions and challenges for more accurate polyp detection in endoscopy videos are also discussed.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7893296","source":"ieee","year":2017,"key":"4836915d-ed73-4eef-886b-fc11cabf34a9","use":1,"doi":"10.2316\/P.2017.852-031"},{"Title":"Automated blood vessel segmentation based on de-noising auto-encoder and neural network","Description":"Z. Fan,  J. J. Mo","ShortDetails":"2016 International Conference on Machine Learning and Cybernetics (ICMLC). 2016","abstract":"Retinal vessel segmentation has been widely used for screening, diagnosis and treatment of cardiovascular and ophthalmologic diseases. In this paper, we propose an automated approach for vessel segmentation in digital retinal images based on de-noising auto-encoders layer-wise initialized neural networks. The proposed method utilized a deep neural network, which is layer-wise initialized by de-noising auto-encoders and fine-tuned by BP algorithm, to segment vessel structures in retinal images. The proposed method is very competitive with the state-of-the-art methods. It achieves an average accuracy of 0.9612, 0.9614, 0.6761, sensitivity of 0.7814, 0.7234, 0.9702, and specificity of 0.9788, 0.9799, 0.9702 on 3 public databases DRIVE, STARE, and CHASE_DB1 respectively. The proposed method is promising for automated blood vessel segmentation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7872998","source":"ieee","year":2016,"key":"260f7869-0fbb-4829-a17c-70d7cd754ea0","use":1,"doi":"10.1109\/ICMLC.2016.7872998"},{"Title":"A combined multi-scale deep learning and random forests approach for direct left ventricular volumes estimation in 3D echocardiography","Description":"S. Dong,  G. Luo,  G. Sun,  K. Wang,  H. Zhang","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Estimation of left ventricular (LV) volumes from 3D echocardiography (3DE) is a popular clinical approach in accurate assessment of left ventricular function for the diagnosis of cardiac disease. The segmentation of 3DE volumes is a crucial step in traditional methods. Nevertheless, segmentation itself is an extremely challenging problem due to the presence of speckle noise and discontinuous edges. Therefore, direct left ventricular volumes estimation methods without the segmentation become attractive in cardiac function analysis. The aim of this paper is to present a fully learning framework to estimate the left ventricular volume in 3DE. The proposed method combined unsupervised multi-scale convolutional deep network and random forests. The multi-scale convolution deep network adopted multi-scale convolutional filters to represent features of unlabeled end-diastolic and end-systolic 3DE volumes (EDV and ESV). And then we formulated left ventricular volume estimation as a regression problem and used random forests for efficient volume estimation. The experiments results suggested that our proposed method is feasible and can achieve higher accuracy, even in case of echocardiography images with irregular geometry.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868886","source":"ieee","year":2016,"key":"bab4c321-ef00-4719-81e5-1f0beefa04be","use":1,"doi":"10.23919\/CIC.2016.7868886"},{"Title":"A left ventricular segmentation method on 3D echocardiography using deep learning and snake","Description":"S. Dong,  G. Luo,  G. Sun,  K. Wang,  H. Zhang","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Segmentation of left ventricular (LV) endocardium from 3D echocardiography is important for clinical diagnosis because it not only can provide some clinical indices (e.g. ventricular volume and ejection fraction) but also can be used for the analysis of anatomic structure of ventricle. In this work, we proposed a new full-automatic method, combining the deep learning and deformable model, for the segmentation of LV endocardium. We trained convolutional neural networks to generate a binary cuboid to locate the region of interest (ROI). And then, using ROI as the input, we trained stacked autoencoder to infer the LV initial shape. At last, we adopted snake model initiated by inferred shape to segment the LV endocardium. In the experiments, we used 3DE data, from CETUS challenge 2014 for training and testing by segmentation accuracy and clinical indices. The results demonstrated the proposed method is accuracy and efficiency respect to expert's measurements.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868782","source":"ieee","year":2016,"key":"f6e8214b-bc59-4ada-bca8-d9b741780fe4","use":1,"doi":"10.23919\/CIC.2016.7868782"},{"Title":"A novel left ventricular volumes prediction method based on deep learning network in cardiac MRI","Description":"G. Luo,  G. Sun,  K. Wang,  S. Dong,  H. Zhang","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Accurate estimation of left ventricle (LV) volumes plays an essential role in clinical diagnosis of cardiac diseases using MRI. Conventional methods of estimating ventricular volumes depend on the results of manual or automatic segmentation of MRI. However, manual segmentation of MRI sequences is extremely time-consuming and subjective, and automatic segmentation is still a challenging task. Therefore, this study aims to develop a new LV volumes prediction method without segmentation, motivated by deep learning technology and the large scale cardiac MRI (CMR) datasets from the second Annual Data Science Bowl (ADSB) in 2016. The experiments results shows that the predicted LV volumes have high correlation with the ground truth. These results prove that the proposed method has big potential to be researched and applied in clinical diagnosis and screening of cardiac diseases.","email":["zfan@stu.edu.cn","12jjmo@stu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868686","source":"ieee","year":2016,"key":"dd03307d-462f-491e-ae45-f4482f3bb7dc","use":1,"doi":"10.23919\/CIC.2016.7868686"},{"Title":"Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification","Description":"P. Rodriguez,  G. Cucurull,  J. Gonz\u00e0lez,  J. M. Gonfaus,  K. Nasrollahi,  T. B. Moeslund,  F. X. Roca","ShortDetails":"IEEE Transactions on Cybernetics. 2017","abstract":"Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively, there is the need for automatic systems to measure it. In this paper, contrary to current state-of-the-art techniques in pain assessment, which are based on facial features only, we suggest that the performance can be enhanced by feeding the raw frames to deep learning models, outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline, our approach first uses convolutional neural networks (CNNs) to learn facial features from VGG_Faces, which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result, we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition, to evaluate the generalization properties of our proposed methodology on facial motion recognition, we also report competitive results in the Cohn Kanade+ facial expression database.","email":["pau.rodriguez@uab.cat"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7849133","source":"ieee","year":2017,"key":"2c045075-843c-433f-b4ab-25c69973687e","use":1,"doi":"10.1109\/TCYB.2017.2662199"},{"Title":"Feature Fusion for Denoising and Sparse Autoencoders: Application to Neuroimaging Data","Description":"A. Moussavi-Khalkhali,  M. Jamshidi,  S. Wijemanne","ShortDetails":"2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA). 2016","abstract":"Although there is no cure to date, Alzheimer's disease detection in early stages has a significant impact on the patient's life in terms of cost, the progress, and helping to plan in advance for an appropriate healthcare in the life ahead as well as providing clinical etiologies for further research. This paper discusses implementing a feature fusion method utilizing sparse and denoising autoencoders to reveal the stage of Alzheimer's disease. Four cohorts consisted of individuals with Alzheimer's disease, late mild cognitive impairment, early mild cognitive impairment, and normal control groups are classified using multinomial logistic regression fueled by the fusion of high-level and low-level features. The high-level features are extracted from the stacked autoencoders. The results show that feature fusion enhance the performance of typical autoencoders. However, the performance of feature fusion using denoising autoencoders is superior to that of the sparse training of autoencoders in terms of overall accuracy, precision, and recall.","email":["arezou.moussavikhalkhali@utsa.edu","mo.jamshidi@utsa.edu","wijemannesar@uthscsa.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7838210","source":"ieee","year":2016,"key":"1d477403-e2aa-445e-bc89-4161e0cf9b53","use":1,"doi":"10.1109\/ICMLA.2016.0106"},{"Title":"Automated atrial fibrillation detection based on deep learning network","Description":"C. Yuan,  Y. Yan,  L. Zhou,  J. Bai,  L. Wang","ShortDetails":"2016 IEEE International Conference on Information and Automation (ICIA). 2016","abstract":"Aiming at the shorting of the existing atrial fibrillation (AF) detection algorithms and improve the ability of intelligent recognition and extraction of AF signals. Recently, deep learning theory with massive data has been used on image, voice and other filed widely. In this paper, a method based on the stack sparse autoencoder neural network, a instance of deep learning strategy, was proposed for AF detection. Greedy layer-wise training algorithms and massive unlabeled hotter data from a hospital were used to train the deep learning system, and Back Propagation algorithm and half of the MIT-BIH standard databases were applied to optimized the whole system. Another half of the standard data were used to evaluated the performance of this method. The autoencoder learns the high level features which can describe the necessary information better from the raw data The experimental results show that the accuracy of the algorithm based on stack sparse autoencoder is 98.309%, so this approach is of great significance on the real-time monitoring of atrial fibrillation signal in electrocardiogram.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7831994","source":"ieee","year":2016,"key":"2bb942c5-dcb8-4e5b-95d0-a5c90a2d1901","use":1,"doi":"10.1109\/ICInfA.2016.7831994"},{"Title":"Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding","Description":"Z. Zhu,  C. Yin,  B. Qian,  Y. Cheng,  J. Wei,  F. Wang","ShortDetails":"2016 IEEE 16th International Conference on Data Mining (ICDM). 2016","abstract":"Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. Aproper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is the Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector based representation for patients, which typically requires aggregation of medical events over a certain time period. As aconsequence, the temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture, and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines.","email":["chanyuan@whut.edu.cn","yan.yan@siat.ac.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7837899","source":"ieee","year":2016,"key":"f0d460ce-320d-4072-a9da-2158868144d2","use":1,"doi":"10.1109\/ICDM.2016.0086"},{"Title":"Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation","Description":"A. Fakhry,  T. Zeng,  S. Ji","ShortDetails":"IEEE Transactions on Medical Imaging. 2017","abstract":"Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7575638","source":"ieee","year":2017,"key":"78d046bc-0d81-412d-947f-c29f4d31af74","use":1,"doi":"10.1109\/TMI.2016.2613019"},{"Title":"Extraction of GGO candidate regions from the LIDC database using deep learning","Description":"K. Hirayama,  J. K. Tan,  H. Kim","ShortDetails":"2016 16th International Conference on Control, Automation and Systems (ICCAS). 2016","abstract":"In recent years, development of the computer-aided diagnosis (CAD) systems for the purpose of reducing the false positive on visual screening and improving accuracy of lesion detection has been advanced. Lung cancer is the leading cause of cancer death in the world. Among them, GGO (Ground Glass Opacity) that exhibited early in the before cancer lesion and carcinoma in situ shows a pale concentration, have been concerned about the possibility of undetected on the screening. In this paper, we propose an automatic extraction method of GGO candidate regions from the chest CT image. Our proposed image processing algorithms is consist of four main steps; (1) segmentation of volume of interest from the chest CT image and removing the blood vessel regions, bronchus regions based on 3D line filter, (2) first detection of GGO regions based on density and gradient which is selected the initial GGO candidate regions, (3) identification of the final GGO candidate regions based on DCNN (Deep Convolutional Neural Network) algorithms. Finally, we calculates the statistical features for reducing the false-positive (FP) shadow by the rule-based method, performs identification of the final GGO candidate regions by SVM (Support Vector Machine). Our proposed method performed on to the 31 cases of the LIDC (Lung Image Database Consortium) database, and final identification performance of TP: 93.02[%], FP: 128.52[\/case] are obtained respectively.","email":["afakhry@cs.odu.edu","pubs-permissions@ieee"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7832398","source":"ieee","year":2016,"key":"cb24efbb-e494-4b79-85e3-e347a48af900","use":1,"doi":"10.1109\/ICCAS.2016.7832398"},{"Title":"Size-Invariant Fully Convolutional Neural Network for vessel segmentation of digital retinal images","Description":"Y. Luo,  H. Cheng,  L. Yang","ShortDetails":"2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). 2016","abstract":"Vessel segmentation of digital retinal images plays an important role in diagnosis of diseases such as diabetics, hypertension and retinopathy of prematurity due to these diseases impact the retina. In this paper, a novel Size-Invariant Fully Convolutional Neural Network (SIFCN) is proposed to address the automatic retinal vessel segmentation problems. The input data of the network is the patches of images and the corresponding pixel-wise labels. A consecutive convolution layers and pooling layers follow the input data, so that the network can learn the abstract features to segment retinal vessel. Our network is designed to hold the height and width of data of each layer with padding and assign pooling stride so that the spatial information maintain and up-sample is not required. Compared with the pixel-wise retinal vessel segmentation approaches, our patch-wise segmentation is much more efficient since in each cycle it can predict all the pixels of the patch. Our overlapped SIFCN approach achieves accuracy of 0.9471, with the AUC of 0.9682. And our non-overlap SIFCN is the most efficient approach among the deep learning approaches, costing only 3.68 seconds per image, and the overlapped SIFCN costs 31.17 seconds per image.","email":["1099361041@qq.com","hcheng@uestc.edu.cn","yanglu@uestc.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7820677","source":"ieee","year":2016,"key":"4c314e2b-7f3b-437e-bf24-b341ea4dbe84","use":1,"doi":"10.1109\/APSIPA.2016.7820677"},{"Title":"Deep convolutional neural network for survival analysis with pathological images","Description":"X. Zhu,  J. Yao,  J. Huang","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.","email":["jzhuang@uta.eduAbstract","DeepConvSurvDeepMultiSurv1@1024x102432","339x33932@169x16932","82x8232@40x4032","risk11@1024x102432","339x33932@169x16932","82x8232@40x4032"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822579","source":"ieee","year":2016,"key":"4ab83b5f-d290-49fb-9c19-c721a3d134af","use":1,"doi":"10.1109\/BIBM.2016.7822579"},{"Title":"Combining deep learning and hand-crafted features for skin lesion classification","Description":"T. Majtner,  S. Yildirim-Yayilgan,  J. Y. Hardeberg","ShortDetails":"2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA). 2016","abstract":"Melanoma is one of the most lethal forms of skin cancer. It occurs on the skin surface and develops from cells known as melanocytes. The same cells are also responsible for benign lesions commonly known as moles, which are visually similar to melanoma in its early stage. If melanoma is treated correctly, it is very often curable. Currently, much research is concentrated on the automated recognition of melanomas. In this paper, we propose an automated melanoma recognition system, which is based on deep learning method combined with so called hand-crafted RSurf features and Local Binary Patterns. The experimental evaluation on a large publicly available dataset demonstrates high classification accuracy, sensitivity, and specificity of our proposed approach when it is compared with other classifiers on the same dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7821017","source":"ieee","year":2016,"key":"6e9b910c-c49c-4f96-b82d-b0001c40c052","use":1,"doi":"10.1109\/IPTA.2016.7821017"},{"Title":"New Deep Neural Nets for Fine-Grained Diabetic Retinopathy Recognition on Hybrid Color Space","Description":"H. H. Vo,  A. Verma","ShortDetails":"2016 IEEE International Symposium on Multimedia (ISM). 2016","abstract":"Automatic diabetes retinopathy (DR) recognition can help DR carriers to receive treatment in early stages and avoid the risk of vision loss. In this paper, we emphasize the role of multiple filter sizes in learning fine-grained discriminant features and propose: (i) two deep convolutional neural networks - Combined Kernels with Multiple Losses Network (CKML Net) and VGGNet with Extra Kernel (VNXK), which are an improvement upon GoogLeNet and VGGNet in context of DR tasks. Learning from existing research, (ii) we propose a hybrid color space, LGI, for DR recognition via proposed nets. (iii) Transfer learning is applied to solve the challenge of imbalanced dataset. The effectiveness of proposed new nets and color space is evaluated using two grand challenge retina datasets: EyePACS and Messidor. Our experimental results show: (iv) CKML Net improves upon GoogLeNet and VNXK improves upon VGGNet on both datasets using the LGI color space. Additionally, proposed methodology improves upon other state of the art results on Messidor dataset for referable\/non-referable screening.","email":["hhvo@csu.fullerton.edu","averma@fullerton.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823616","source":"ieee","year":2016,"key":"61e88b6c-394f-41be-888d-82cd96c0c4b1","use":1,"doi":"10.1109\/ISM.2016.0049"},{"Title":"Facial expression recognition based on LLENet","Description":"Dan Meng,  Guitao Cao,  Zhihai He,  Wenming Cao","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Facial expression recognition plays an important role in lie detection, and computer-aided diagnosis. Many deep learning facial expression feature extraction methods have a great improvement in recognition accuracy and robutness than traditional feature extraction methods. However, most of current deep learning methods need special parameter tuning and ad hoc fine-tuning tricks. This paper proposes a novel feature extraction model called Locally Linear Embedding Network (LLENet) for facial expression recognition. The proposed LLENet first reconstructs image sets for the cropped images. Unlike previous deep convolutional neural networks that initialized convolutional kernels randomly, we learn multi-stage kernels from reconstructed image sets directly in a supervised way. Also, we create an improved LLE to select kernels, from which we can obtain the most representative feature maps. Furthermore, to better measure the contribution of these kernels, a new distance based on kernel Euclidean is proposed. After the procedure of multi-scale feature analysis, feature representations are finally sent into a linear classifier. Experimental results on facial expression datasets (CK+) show that the proposed model can capture most representative features and thus improves previous results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822814","source":"ieee","year":2016,"key":"9998b8e3-cf8d-45b8-b12c-81a2f6a0aafe","use":1,"doi":"10.1109\/BIBM.2016.7822814"},{"Title":"Sparse Autoencoder Based Deep Neural Network for Voxelwise Detection of Cerebral Microbleed","Description":"Y. D. Zhang,  X. X. Hou,  Y. D. Lv,  H. Chen,  Y. Zhang,  S. H. Wang","ShortDetails":"2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS). 2016","abstract":"In order to detect cerebral microbleed more efficiently, we developed a novel computer-aided detection method based on susceptibility-weighted imaging. We enrolled five CADASIL patients and five healthy controls. We used a 20x20 neighboring window to generate samples on each slice of the volumetric brain images. The sparse autoencoder (SAE) was used to unsupervised feature learning. Then, a deep neural network was established using the learned features. The results over 10x10-fold cross validation showed our method yielded a sensitivity of 93.20\u00b11.37%, a specificity of 93.25\u00b11.38%, and an accuracy of 93.22\u00b11.37%. Our result is better than Roy's method, which was proposed in 2015.","email":["wangshuihua@njnu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7823881","source":"ieee","year":2016,"key":"ca08a3bf-a9ec-4fe1-95e2-b66a8dd2322a","use":1,"doi":"10.1109\/ICPADS.2016.0166"},{"Title":"Early diagnosis of Alzheimer's disease: A multi-class deep learning framework with modified k-sparse autoencoder classification","Description":"P. Bhatkoti,  M. Paul","ShortDetails":"2016 International Conference on Image and Vision Computing New Zealand (IVCNZ). 2016","abstract":"Successful, timely diagnosis of neuropsychiatry diseases is key to management. Research efforts in the area of diagnosis of Alzheimer's disease have used various aspects of computer-aided multi-class diagnosis approaches with varied degrees of success. However, there is still need for more efficient and reliable approaches to successful diagnosis of the disease. This research used deep learning framework with modified k-sparse autoencoder (oKSA) classification to locate neutrally degenerated areas of the brain magnetic resonance imaging (MRI), low amyloid beta 1-42 imaging in cerebrospinal fluid (CSF) and positron emission tomography (PET) imaging of amyloid; each with a sample of 150 images. Results show a correlation between computational demarcation of infected regions and the images. Degeneration in the studied areas was evidenced by high phosphorylated t-\/p-tau levels in CSF, regional fluorodeoxyglucose PET, and the presence of atrophy patterns. The use of \u03c3KSA algorithm in boosting classification helped to improve the classifier performance. The KSA method with deep learning framework is used for the first time to produce accurate results in diagnosis of Alzheimer's disease.","email":["p@bhatkoti.com","mpaul@csu.edu.au"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7804459","source":"ieee","year":2016,"key":"766086dc-7cc8-4bdb-b219-f51df8aa23e5","use":1,"doi":"10.1109\/IVCNZ.2016.7804459"},{"Title":"A deep learning-based segmentation method for brain tumor in MR images","Description":"Zhe Xiao,  Ruohan Huang,  Yi Ding,  Tian Lan,  RongFeng Dong,  Zhiguang Qin,  Xinjie Zhang,  Wei Wang","ShortDetails":"2016 IEEE 6th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS). 2016","abstract":"Accurate tumor segmentation is an essential and crucial step for computer-aided brain tumor diagnosis and surgical planning. Subjective segmentations are widely adopted in clinical diagnosis and treating, but they are neither accurate nor reliable. An automatical and objective system for brain tumor segmentation is strongly expected. But they are still facing some challenges such as lower segmentation accuracy, demanding a priori knowledge or requiring the human intervention. In this paper, a novel and new coarse-to-fine method is proposed to segment the brain tumor. This hierarchical framework consists of preprocessing, deep learning network based classification and post-processing. The preprocessing is used to extract image patches for each MR image and obtains the gray level sequences of image patches as the input of the deep learning network. The deep learning network based classification is implemented by a stacked auto-encoder network to extract the high level abstract feature from the input, and utilizes the extracted feature to classify image patches. After mapping the classification result to a binary image, the post-processing is implemented by a morphological filter to get the final segmentation result. In order to evaluate the proposed method, the experiment was applied to segment the brain tumor for the real patient dataset. The final performance shows that the proposed brain tumor segmentation method is more accurate and efficient.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7802771","source":"ieee","year":2016,"key":"4ec9f005-787c-4dde-8cf7-e1ed3f9e3c39","use":1,"doi":"10.1109\/ICCABS.2016.7802771"},{"Title":"Deep learning application trial to lung cancer diagnosis for medical sensor systems","Description":"R. Shimizu,  S. Yanagawa,  Y. Monde,  H. Yamagishi,  M. Hamada,  T. Shimizu,  T. Kuroda","ShortDetails":"2016 International SoC Design Conference (ISOCC). 2016","abstract":"Personal and easy-to-use health checking system is an attractive application of sensor systems. Sensing data analysis for diagnosis is important as well as preparing small and mobile sensor nodes because sensing data include variations and noises reflecting individual difference of people and sensing conditions. Deep Neural Network, or Deep Learning, is a well-known method of machine learning and it is effective for feature extraction from pictures. Then, we thought Deep Learning also can extract features from sensing data. In this paper, we tried to build a diagnosis system of lung cancer based on Deep Learning. Input data of the system was generated from human urine by Gas Chromatography Mass Spectrometer (GC-MS) and our system achieved 90% accuracy in judging whether the patient had lung cancer or not. This system will be useful for pre- and personal diagnosis because collecting urine is very easy and not harmful to human body. We are targeting installation of this system not only to gas chromatography systems but also to some combination of multiple sensors for detecting gases of low concentration.","email":["r_shimizu@kuroda.elec.keio.ac.jp"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7799852","source":"ieee","year":2016,"key":"106961f0-2aa0-45e9-a673-f127f49be3c7","use":1,"doi":"10.1109\/ISOCC.2016.7799852"},{"Title":"Segmentation of the Left Ventricle in Echocardiography Using Contextual Shape Model","Description":"G. Belous,  A. Busch,  D. Rowlands,  Y. Gao","ShortDetails":"2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA). 2016","abstract":"Accurate localization of the left ventricle (LV) boundary from echocardiogram images is of vital importance for the diagnosis and treatment of heart disease. Statistical shape models such as active shape models (ASM) have been commonly used to perform automatic detection of this boundary. Such models perform well when there is low variability in the underlying shape subspace and an accurate initialization can be provided, however in the absence of these conditions results are often much poorer. In the case of LV echocardiogram images, such variability is often encountered in patients with abnormal LV function. In this paper we propose a fully automatic segmentation technique using deep learning in a Bayesian nonparametric framework. Our model uses a dynamic statistical shape model comprised of training shapes from select weighted subsets of the feature subspace. Subsets are chosen during the iterative segmentation process according to a latent temporal component allocation variable, determined from joint deep features and LV landmark information using a Dirichlet process mixture model with Chinese restaurant process prior. Testing is performed on a data set comprising images of the LV acquired from patients exhibiting both normal and abnormal LV function, and the results using our technique compared to both the ASM and other state of the art techniques. Results from this testing show an improvement in the LV localization accuracy, particularly when LV function is abnormal.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7797080","source":"ieee","year":2016,"key":"a3890cf1-61d8-4e34-a19c-23edecbaa2af","use":1,"doi":"10.1109\/DICTA.2016.7797080"},{"Title":"Classification of Exacerbation Frequency in the COPDGene Cohort Using Deep Learning with Deep Belief Networks","Description":"J. Ying,  J. Dutta,  N. Guo,  C. Hu,  D. Zhou,  A. Sitek,  Q. Li","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"This study aims to develop an automatic classifier based on deep learning for exacerbation frequency in patients with chronic obstructive pulmonary disease (COPD). A threelayer deep belief network (DBN) with two hidden layers and one visible layer was employed to develop classification models and the models\u2019 robustness to exacerbation was analyzed. Subjects from the COPDGene cohort were labeled with exacerbation frequency, defined as the number of exacerbation events per year. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 91.99%, using a 10-fold cross validation experiment. The analysis of DBN weights showed that there was a good visual spatial relationship between the underlying critical features of different layers. Our findings show that the most sensitive features obtained from the DBN weights are consistent with the consensus showed by clinical rules and standards for COPD diagnostics. We thus demonstrate that DBN is a competitive tool for exacerbation risk assessment for patients suffering from COPD.","email":["yingjun301@sina.com","dutta.joyita@mgh.harvard.edu","guo.ning@mgh.harvard.edu","chenhui.hu@gmail.com","sarkadiu@gmail.com","li.quanzheng@mgh.harvard.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7792616","source":"ieee","year":2017,"key":"ea8046e7-7367-4a7e-b170-66c30d6a9a1a","use":1,"doi":"10.1109\/JBHI.2016.2642944"},{"Title":"DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation","Description":"H. Chen,  X. Qi,  L. Yu,  P. A. Heng","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016","abstract":"The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7780642","source":"ieee","year":2016,"key":"717f3d26-d55a-44da-8e6b-27621e1a3266","use":1,"doi":"10.1109\/CVPR.2016.273"},{"Title":"Convolutional neural networks for deep feature learning in retinal vessel segmentation","Description":"A. F. Khalaf,  I. A. Yassine,  A. S. Fahmy","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Analysis of retinal vessels in fundus images provides a valuable tool for characterizing many retinal and systemic diseases. Accurate automatic segmentation of these vessels is usually required as an essential analysis step. In this work, we propose a new formulation of deep Convolutional Neural Networks that allows simple and accurate segmentation of the retinal vessels. A major modification in this work is to reduce the intra-class variance by formulating the problem as a Three-class problem that differentiates: large vessels, small vessels, and background areas. In addition, different sizes of the convolutional kernels have been studied and it was found that a combination of kernels with different sizes achieve the best sensitivity and specificity. The proposed method was tested using DRIVE dataset and it showed superior performance compared to several other state of the art methods. The segmentation sensitivity, specificity and accuracy were found to be 83.97%, 95.62% and 94.56% respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532384","source":"ieee","year":2016,"key":"eb4e5286-3499-4475-8969-f087eb9ea545","use":1,"doi":"10.1109\/ICIP.2016.7532384"},{"Title":"Deepmole: Deep neural networks for skin mole lesion classification","Description":"V. Pomponiu,  H. Nejati,  N. M. Cheung","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Nowadays, the occurrence of skin cancer cases has grown worldwide due to the extended exposure to the harmful radiation from the Sun. Most common approach to detect the malignancy of skin moles is by visual inspection performed by an expert dermatologist, using a set of specific clinical rules. Computer-aided diagnosis, based on skin mole imaging, is another concurrent method which has experienced major advancements due to improvement of imaging sensors and processing power. However, these schemes use hand-crafted features which are difficult to tune and perform poorly on new cases due to lack of generalization power. In this study we present a method that use a pretrained deep neural network (DNN) to automatically extract a set of representative features that can be later used to diagnose a sample of skin lesion for malignancy. The experimental tests carried out on a clinical dataset show that the classification performance using DNN-based features performs better than the state-of-the-art techniques.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532834","source":"ieee","year":2016,"key":"09320159-ffe5-4e0a-840b-80dbd9ac4613","use":1,"doi":"10.1109\/ICIP.2016.7532834"},{"Title":"Segmenting Retinal Blood Vessels With Deep Neural Networks","Description":"P. Liskowski,  K. Krawiec","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \\thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7440871","source":"ieee","year":2016,"key":"37582fc2-ce9f-4799-9d76-3e318e8a8b5d","use":1,"doi":"10.1109\/TMI.2016.2546227"},{"Title":"Automatic tissue characterization of air trapping in chest radiographs using deep neural networks","Description":"A. Mansoor,  G. Perez,  G. Nino,  M. G. Linguraru","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Significant progress has been made in recent years for computer-aided diagnosis of abnormal pulmonary textures from computed tomography (CT) images. Similar initiatives in chest radiographs (CXR), the common modality for pulmonary diagnosis, are much less developed. CXR are fast, cost effective and low-radiation solution to diagnosis over CT. However, the subtlety of textures in CXR makes them hard to discern even by trained eye. We explore the performance of deep learning abnormal tissue characterization from CXR. Prior studies have used CT imaging to characterize air trapping in subjects with pulmonary disease; however, the use of CT in children is not recommended mainly due to concerns pertaining to radiation dosage. In this work, we present a stacked autoencoder (SAE) deep learning architecture for automated tissue characterization of air-trapping from CXR. To our best knowledge this is the first study applying deep learning framework for the specific problem on 51 CXRs, an F-score of \u2248 76.5% and a strong correlation with the expert visual scoring (R=0.93, p =<; 0.01) demonstrate the potential of the proposed method to characterization of air trapping.","email":["perlungquadrantusingthetrainingdataandbasedontheareaofdetectedairtrappingasshowninFig.3.3.RESULTSOurmethodwasimplementedusingMATLAB-basedTool-boxforDeepLearning1.Routinesnotinvolvingdeeplearn-ingwerealsoimplementedusingMATLAB.Allexperimentswereperformedon64-bitWindows7operatingsystemusingamachinewithIntelE5-1607CPU@3.00GHzand16.0GBofRAM.Totestourframework"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590649","source":"ieee","year":2016,"key":"24eae243-9f17-4f75-ba36-9e6975c82959","use":1,"doi":"10.1109\/EMBC.2016.7590649"},{"Title":"Recent machine learning advancements in sensor-based mobility analysis: Deep learning for Parkinson's disease assessment","Description":"B. M. Eskofier,  S. I. Lee,  J. F. Daneault,  F. N. Golabchi,  G. Ferreira-Carvalho,  G. Vergara-Diaz,  S. Sapienza,  G. Costante,  J. Klucken,  T. Kautz,  P. Bonato","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"The development of wearable sensors has opened the door for long-term assessment of movement disorders. However, there is still a need for developing methods suitable to monitor motor symptoms in and outside the clinic. The purpose of this paper was to investigate deep learning as a method for this monitoring. Deep learning recently broke records in speech and image classification, but it has not been fully investigated as a potential approach to analyze wearable sensor data. We collected data from ten patients with idiopathic Parkinson's disease using inertial measurement units. Several motor tasks were expert-labeled and used for classification. We specifically focused on the detection of bradykinesia. For this, we compared standard machine learning pipelines with deep learning based on convolutional neural networks. Our results showed that deep learning outperformed other state-of-the-art machine learning algorithms by at least 4.6 % in terms of classification rate. We contribute a discussion of the advantages and disadvantages of deep learning for sensor-based movement assessment and conclude that deep learning is a promising method for this field.","email":["bjoern.eskofier@fau.de"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590787","source":"ieee","year":2016,"key":"8140a699-b01f-411d-b629-c05045712696","use":1,"doi":"10.1109\/EMBC.2016.7590787"},{"Title":"Automatic prostate segmentation on MR images with deep network and graph model","Description":"K. Yan,  C. Li,  X. Wang,  A. Li,  Y. Yuan,  D. Feng,  M. Khadra,  J. Kim","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Automated prostate diagnoses and treatments have gained much attention due to the high mortality rate of prostate cancer. In particular, unsupervised (automatic) prostate segmentation is an active and challenging research. Most conventional works usually utilize handcrafted (low-level) features for prostate segmentation; however they often fail to extract the intrinsic structure of the prostate, especially on images with blurred boundaries. In this paper, we propose a novel automated prostate segmentation model with learned features from deep network. Specifically, we first generate a set of prostate proposals in transverse plane via recognizing the position and coarse estimate of the shape of the prostate on the global prostate image and using the deep network to extract highly effective features for the boundary refinement in a finer scale. With consideration of the correlations among different sequential images, we then construct a graph to select the best prostate proposals from proposal set for its use in 3D prostate segmentation. Experimental evaluation demonstrates that our proposed deep network and graph based method is superior to state-of-the-art couterparts, in terms of both dice similarity coefficient and Hausdorff distance, on public dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590782","source":"ieee","year":2016,"key":"f047fc56-3171-4d7b-a564-7720d2cf1b40","use":1,"doi":"10.1109\/EMBC.2016.7590782"},{"Title":"Pain detection from facial images using unsupervised feature learning approach","Description":"R. Kharghanian,  A. Peiravi,  F. Moradi","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"In this paper a new method for continuous pain detection is proposed. One approach to detect the presence of pain is by processing images taken from the face. It has been reported that expression of pain from the face can be detected utilizing Action Units (AUs). In this manner, each action units must be detected separately and then combined together through a linear expression. Also, pain detection can be directly done from a painful face. There are different methods to extract features of both shape and appearance. Shape and appearance features must be extracted separately, and then used to train a classifier. Here, a hierarchical unsupervised feature learning approach is proposed in order to extract the features needed for pain detection from facial images. In this work, features are extracted using convolutional deep belief network (CDBN). The extracted features include different properties of painful images such as head movements, shape and appearance information. The proposed model was tested on the publicly available UNBC MacMaster Shoulder Pain Archive Database and we achieved near 95% for the area under ROC curve metric that is prominent with respect to the other reported results.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590729","source":"ieee","year":2016,"key":"e4cd0326-663a-415f-bc16-fcc6403592f3","use":1,"doi":"10.1109\/EMBC.2016.7590729"},{"Title":"Integrating holistic and local deep features for glaucoma classification","Description":"A. Li,  J. Cheng,  D. W. K. Wong,  J. Liu","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Automated glaucoma detection is an important application of retinal image analysis. Compared with segmentation based approaches, image classification based approaches have a potential of better performance. However, it still remains a challenging problem for two reasons. Firstly, due to insufficient sample size, learning effective features is difficult. Secondly, the shape variations of optic disc introduce misalignment. To address these problem, a new classification based approach for glaucoma detection is proposed, in which deep convolutional networks derived from large-scale generic dataset is used to representing the visual appearance and holistic and local features are combined to mitigate the influence of misalignment. The proposed method achieves an area under the receiver operating characteristic curve of 0.8384 on the Origa dataset, which clearly demonstrates its effectiveness.","email":["jimmyliu@nimte.ac.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590952","source":"ieee","year":2016,"key":"ba385f1f-cc07-40ac-beb4-78a68f735602","use":1,"doi":"10.1109\/EMBC.2016.7590952"},{"Title":"Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography","Description":"A. Lahiri,  A. G. Roy,  D. Sheet,  P. K. Biswas","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.","email":["avisek@ece.iitkgp.ernet.in2AuthorarewithDept.ofElectricalEngineering"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7590955","source":"ieee","year":2016,"key":"b7eab1fd-2339-4828-815e-85676d805ea0","use":1,"doi":"10.1109\/EMBC.2016.7590955"},{"Title":"Text-mining the neurosynth corpus using deep boltzmann machines","Description":"R. Monti,  R. Lorenz,  R. Leech,  C. Anagnostopoulos,  G. Montana","ShortDetails":"2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI). 2016","abstract":"Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7552329","source":"ieee","year":2016,"key":"5c4f67d6-c642-4059-855f-e28d937b6095","use":1,"doi":"10.1109\/PRNI.2016.7552329"},{"Title":"Learning a multiscale patch-based representation for image denoising in X-RAY fluoroscopy","Description":"Y. Matviychuk,  B. Mailh\u00e9,  X. Chen,  Q. Wang,  A. Kiraly,  N. Strobel,  M. Nadar","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Denoising is an indispensable step in processing low-dose X-ray fluoroscopic images that requires development of specialized high-quality algorithms able to operate in near real-time. We address this problem with an efficient deep learning approach based on the process-centric view of traditional iterative thresholding methods. We develop a novel trainable patch-based multiscale framework for sparse image representation. In a computationally efficient way, it allows us to accurately reconstruct important image features on multiple levels of decomposition with patch dictionaries of reduced size and complexity. The flexibility of the chosen machine learning approach allows us to tailor the learned basis for preserving important structural information in the image and noticeably minimize the amount of artifacts. Our denoising results obtained with real clinical data demonstrate significant quality improvement and are computed much faster in comparison with the BM3D algorithm.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532775","source":"ieee","year":2016,"key":"9cf60961-a122-4645-8a53-97251de65eaa","use":1,"doi":"10.1109\/ICIP.2016.7532775"},{"Title":"A Deep Learning Method for Microaneurysm Detection in Fundus Images","Description":"J. Shan,  L. Li","ShortDetails":"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE). 2016","abstract":"Diabetic Retinopathy (DR) is the leading cause of blindness in the working-age population. Microaneurysms (MAs), due to leakage from retina blood vessels, are the early signs of DR. However, automated MA detection is complicated because of the small size of MA lesions and the low contrast between the lesion and its retinal background. Recently deep learning (DL) strategies have been used for automatic feature extraction and classification problems, especially for image analysis. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a DL strategy, is presented for MA detection in fundus images. Small image patches are generated from the original fundus images. The SSAE learns high-level features from pixel intensities alone in order to identify distinguishing features of MA. The high-level features learned by SSAE are fed into a classifier to categorize each image patch as MA or non-MA. The public benchmark DIARETDB is utilized to provide the training\/testing data and ground truth. Among the 89 images, totally 2182 image patches with MA lesions, serve as positive data, and another 6230 image patches without MA lesions are generated by a randomly sliding window operation, to serve as negative data. Without any blood vessel removal or complicated preprocessing operations, SSAE learned directly from the raw image patches, and automatically extracted the distinguishing features to classify the patches using Softmax Classifier. By employing the fine-tuning operation, an improved F-measure 91.3% and an average area under the ROC curve (AUC) 96.2% were achieved using 10-fold cross-validation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7545864","source":"ieee","year":2016,"key":"6bc4345a-5c42-43f5-8590-cbc7add3f9b5","use":1,"doi":"10.1109\/CHASE.2016.12"},{"Title":"Automatic burn area identification in color images","Description":"M. S. Badea,  C. Vertan,  C. Florea,  L. Florea,  S. B\u0103doiu","ShortDetails":"2016 International Conference on Communications (COMM). 2016","abstract":"This papers presents the use of color imaging as a starting point of burn wound evaluation, by the discrimination between healthy skin and burn wound. The skin\/burn area identification is performed pixel-wise, according to the properties of an entire encompassing patch. The classification is learned under a supervised scenario, according to a ground truth defined by specialist surgeons from a large pediatric case database, by a deep learned convolutional neural network. The database is extensive and was recorded over several months in real, hospital conditions The proposed approach achieves an overall performance comparable to the literature-reported average performance of a specialist surgeon.","email":["badea.mihai92@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7528325","source":"ieee","year":2016,"key":"f13d65a2-2625-4f27-b738-6a12c1376225","use":1,"doi":"10.1109\/ICComm.2016.7528325"},{"Title":"Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation","Description":"A. Mansoor,  J. J. Cerrolaza,  R. Idrees,  E. Biggs,  M. A. Alsharid,  R. A. Avery,  M. G. Linguraru","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.","email":["awais.mansoor@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7420737","source":"ieee","year":2016,"key":"a9d2d037-ec56-4961-ace6-73309b866c85","use":1,"doi":"10.1109\/TMI.2016.2535222"},{"Title":"Clinical decision support for Alzheimer's disease based on deep learning and brain network","Description":"C. Hu,  R. Ju,  Y. Shen,  P. Zhou,  Q. Li","ShortDetails":"2016 IEEE International Conference on Communications (ICC). 2016","abstract":"Modern e-health systems have undergone rapid development thanks to the advances in communications, computing and machine learning technology. Especially, deep learning has great superiority in image analysis and disease prediction. In this paper, we use Alzheimer's Disease (AD) as an example to show advantages of deep learning in diagnosing brain diseases and providing clinical decision support. Firstly, we convert raw functional magnetic resonance imaging (fMRI) to a matrix to represent activity of 90 brain regions. Secondly, to represent the functional connectivity between different brain regions, a correlation matrix is obtained by calculating the correlation between each pair of brain regions. In the next, a targeted autoencoder network is built to classify the correlation matrix, which is sensitive to AD. Finally, the experiment results show that our proposed method for AD prediction achieves much better effects than traditional means. It finds the correlations between different brain regions efficiently, provides strong reference for AD prediction. Compared to Support Vector Machine (SVM), about 25% improvement is gained in prediction accuracy. The e-health field becomes more complete and effective owing to that. Our work helps predict AD at an early stage and take measures to slow down or even prevent the onset of it.","email":["hu4@seas.harvard.edu1","Yusong.Shen@rice.edu3","li.quanzheng@mgh.harvard.edu5"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7510831","source":"ieee","year":2016,"key":"31faaaa9-3031-4180-b0c8-106392cc0f97","use":1,"doi":"10.1109\/ICC.2016.7510831"},{"Title":"HEp-2 cell classification using a deep neural network trained for natural image classification","Description":"B. Benligiray,  H. \u00c7. Akak\u0131n","ShortDetails":"2016 24th Signal Processing and Communication Application Conference (SIU). 2016","abstract":"Deep convolutional neural networks is a recently developed method that yields very successful results in image classification. Deep neural networks, which have a high number of parameters, require a large amount of data to avoid overfitting during training. For applications in which the available data is not adequate to train a deep neural network from the scratch, deep neural networks trained for similar objectives can be used as a starting point. In this study, cell images are classified using a deep neural network trained to classify objects in natural images. Even though classification of natural images and cell images are very different objectives, cell images are able to be classified with 74.1% mean class accuracy. The results show that features used for visual classification by deep convolutional neural networks may be more universal than assumed.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7496001","source":"ieee","year":2016,"key":"ec872f14-7e19-4eb6-8d99-4dc40c2e8c3f","use":1,"doi":"10.1109\/SIU.2016.7496001"},{"Title":"Detection of articulated instruments in retinal microsurgery","Description":"M. Alsheakhali,  A. Eslami,  N. Navab","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Instrument detection in retinal microsurgery is still one of the most challenging operations due to illumination changes, fast motion, cluttered background and deformable shape of the instrument. In this work, a new technique is proposed to detect an articulated forceps instrument by modeling it using Conditional Random Field (CRF). The unary potentials of the CRF, which represent the instrument parts, are detected using the deep convolutional neural network, where two probability distribution maps for both the forceps center and its shaft are estimated. The pairwise potentials are modeled using a regression random forest to learn the relation between the instrument parts based on their joint structural features. Sampled combinations from both unary distributions are selected, and each is tested using the regression forest to compute its similarity to the medical instrument structure. The best combination candidate chosen by the CRF predicts the forceps center point (instrument joint point) and the orientation of its shaft. The approach shows high detection accuracy on public datasets and real videos for retinal microsurgery operations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493222","source":"ieee","year":2016,"key":"f3b146a9-23d2-4828-9c98-fcb35ef61f07","use":1,"doi":"10.1109\/ISBI.2016.7493222"},{"Title":"Comprehensive autoencoder for prostate recognition on MR images","Description":"K. Yan,  C. Li,  X. Wang,  Y. Yuan,  A. Li,  J. Kim,  B. Li,  D. Feng","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Automatic recognition of anatomical structures is an essential prerequisite in computer aided diagnoses (CAD) such as tissue segmentation, physiological signal measurement and disease classification. However, insufficient color and speckle information in medical images pose challenges to the recognition of anatomical structures. Such challenges are evident with prostate recognition on magnetic resonance (MR) images and thus remain an open problem, although prostate cancer is an important problem that are attracting increasing interests in medical imaging. In this study, we propose an automatic approach for prostate recognition on MR images. Firstly, compared to existing works which integrate autoencoder with a specific type of classifier, we let autoencoder itself serve as a classifier and therefore lessening the impact from irregular and complex background found in prostate recognition. Secondly, an image energy minimization scheme with consideration of the coherence information from neighboring pixels is proposed to improve the recognition results with clear boundary appearance. We evaluate our method in comparison with three widely applied classifiers and the phase of atlas-based seeds-selection in prostate segmentation on a public prostate database. Our experiment results demonstrate significant superiority of our method in terms of both precision and F-measure.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493479","source":"ieee","year":2016,"key":"4b9b1994-2cfa-4b50-8828-635bcdf00fb3","use":1,"doi":"10.1109\/ISBI.2016.7493479"},{"Title":"Automated mitosis detection with deep regression networks","Description":"H. Chen,  X. Wang,  P. A. Heng","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Mitosis counting is one of the strongest prognostic markers for invasive breast cancer diagnosis. Clinical visual examination on histology slides by pathologists is tedious, error-prone and time-consuming. Furthermore, with the advent of whole slide imaging for high-throughput digitization, a large quantity of histology images need to be analyzed. Therefore, automated mitosis detection methods are highly demanded in clinical practice. In this paper, we proposed a deep regression network (DRN) to meet these challenges. It consisted of a downsampling path for extracting the high level information and an upsampling path for outputting the score map with original input size, thus it can be trained in an end-to-end way. In addition, we transferred knowledge learned from cross domains to mitigate the issue of insufficient medical training data. Experimental results on the benchmark dataset 2012ICPR Mitosis Detection Challenge demonstrated the efficacy of our approach, which achieved comparable or better performance than the state-of-the-art methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493482","source":"ieee","year":2016,"key":"3dc40d3c-454d-4105-8e1a-c957c4820f24","use":1,"doi":"10.1109\/ISBI.2016.7493482"},{"Title":"Multi-loss convolutional networks for gland analysis in microscopy","Description":"A. BenTaieb,  J. Kawahara,  G. Hamarneh","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Manual tissue diagnosis is the most prevalent approach to cancer diagnosis. However, it mainly relies on a subjective visual quantification of specific morphometric features, which often leads to a relatively limited reproducibility among experts. In most computational techniques proposed to automate the diagnostic procedure, accurate segmentation is paramount as a precursor to the extraction of relevant morphometric features. Since the ultimate goal of segmentation is generally classification, yet a given class imparts an expected tissue appearance beneficial to segmentation, we pose the problem of automatic tissue analysis as the joint task of segmentation and classification. We propose a novel multi-objective learning method that optimizes a single unified deep fully convolutional neural network with two distinct loss functions. We illustrate our reasoning on the task of colon adenocarcinomas diagnosis and show how glands' classification can facilitate their segmentation by adding class-specific spatial priors. The final classification also benefits from this joint learning framework yielding an improvement of 6% over classification-only models.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493349","source":"ieee","year":2016,"key":"b26debb9-7e08-4b8f-a77a-67365234f998","use":1,"doi":"10.1109\/ISBI.2016.7493349"},{"Title":"Detection of age-related macular degeneration via deep learning","Description":"P. Burlina,  D. E. Freund,  N. Joshi,  Y. Wolfson,  N. M. Bressler","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"Age-related macular generation (AMD) - when left untreated - is the main cause of blindness for individuals over the age of 50. With the US population now counting over 100 million individuals over 50, it is imperative to develop methods that can effectively determine which individuals with an earlier, often asymptomatic stage, are at risk of developing the advanced stage that can cause severe vision loss. This paper studies the appropriateness of the transfer of image features computed from pre-trained deep neural networks to the problem in AMD detection. Tests using over 5600 images from the NIH AREDS dataset (the largest dataset used thus far for AMD image analysis studies) show good preliminary results (between nearly 92% and 95% accuracy).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493240","source":"ieee","year":2016,"key":"4012a537-f1a0-43b4-8212-d4de48e2b48e","use":1,"doi":"10.1109\/ISBI.2016.7493240"},{"Title":"Towards grading gleason score using generically trained deep convolutional neural networks","Description":"H. K\u00e4ll\u00e9n,  J. Molin,  A. Heyden,  C. Lundstr\u00f6m,  K. \u00c5str\u00f6m","ShortDetails":"2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI). 2016","abstract":"We developed an automatic algorithm with the purpose to assist pathologists to report Gleason score on malignant prostatic adenocarcinoma specimen. In order to detect and classify the cancerous tissue, a deep convolutional neural network that had been pre-trained on a large set of photographic images was used. A specific aim was to support intuitive interaction with the result, to let pathologists adjust and correct the output. Therefore, we have designed an algorithm that makes a spatial classification of the whole slide into the same growth patterns as pathologists do. The 22-layer network was cut at an earlier layer and the output from that layer was used to train both a random forest classifier and a support vector machines classifier. At a specific layer a small patch of the image was used to calculate a feature vector and an image is represented by a number of those vectors. We have classified both the individual patches and the entire images. The classification results were compared for different scales of the images and feature vectors from two different layers from the network. Testing was made on a dataset consisting of 213 images, all containing a single class, benign tissue or Gleason score 35. Using 10-fold cross validation the accuracy per patch was 81 %. For whole images, the accuracy was increased to 89 %.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7493473","source":"ieee","year":2016,"key":"74738325-be0e-45b8-a2df-512d90f4caf0","use":1,"doi":"10.1109\/ISBI.2016.7493473"},{"Title":"TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning","Description":"A. Murali,  A. Garg,  S. Krishnan,  F. T. Pokorny,  P. Abbeel,  T. Darrell,  K. Goldberg","ShortDetails":"2016 IEEE International Conference on Robotics and Automation (ICRA). 2016","abstract":"The growth of robot-assisted minimally invasive surgery has led to sizable datasets of fixed-camera video and kinematic recordings of surgical subtasks. Segmentation of these trajectories into locally-similar contiguous sections can facilitate learning from demonstrations, skill assessment, and salvaging good segments from otherwise inconsistent demonstrations. Manual, or supervised, segmentation can be prone to error and impractical for large datasets. We present Transition State Clustering with Deep Learning (TSC-DL), a new unsupervised algorithm that leverages video and kinematic data for task-level segmentation, and finds regions of the visual feature space that correlate with transition events using features constructed from layers of pre-trained image classification Deep Convolutional Neural Networks (CNNs). We report results on three datasets comparing Deep Learning architectures (AlexNet and VGG), choice of convolutional layer, dimensionality reduction techniques, visual encoding, and the use of Scale Invariant Feature Transforms (SIFT). We find that the deep architectures extract features that result in up-to a 30.4% improvement in Silhouette Score (a measure of cluster tightness) over the traditional \u201cshallow\u201d features from SIFT. We also present cases where TSC-DL discovers human annotator omissions. Supplementary material, data and code is available at: http:\/\/berkeleyautomation.github.io\/tsc-dl\/.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7487607","source":"ieee","year":2016,"key":"574276db-f03d-4aaf-a013-dbd6b6c822e5","use":1,"doi":"10.1109\/ICRA.2016.7487607"},{"Title":"A deep convolutional neural network trained on representative samples for circulating tumor cell detection","Description":"Y. Mao,  Z. Yin,  J. Schober","ShortDetails":"2016 IEEE Winter Conference on Applications of Computer Vision (WACV). 2016","abstract":"The number of Circulating Tumor Cells (CTCs) in blood indicates the tumor response to chemotherapeutic agents and disease progression. In early cancer diagnosis and treatment monitoring routine, detection and enumeration of CTCs in clinical blood samples have significant applications. In this paper, we design a Deep Convolutional Neural Network (DCNN) with automatically learned features for image-based CTC detection. We also present an effective training methodology which finds the most representative training samples to define the classification boundary between positive and negative samples. In the experiment, we compare the performance of auto-learned feature from DCNN and hand-crafted features, in which the DCNN outperforms hand-crafted feature. We also prove that the proposed training methodology is effective in improving the performance of DCNN classifiers.","email":["ym8r8@mst.edu","yinz@mst.edu","joschob@siue.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7477603","source":"ieee","year":2016,"key":"8b5e20f4-18cf-4272-8260-b6841d007d1a","use":1,"doi":"10.1109\/WACV.2016.7477603"},{"Title":"Deep multi-view representation learning for multi-modal features of the schizophrenia and schizo-affective disorder","Description":"J. Qi,  J. Tejedor","ShortDetails":"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2016","abstract":"This work is originated from the MLSP 2014 Classification Challenge which tries to automatically detect subjects with schizophrenia and schizo-affective disorder by analyzing multi-modal features derived from magnetic resonance imaging (MRI) data. We employ Deep Neural Network (DNN)-based multi-view representation learning for combining multimodal features. The DNN-based multi-view models include deep canonical correlation analysis (DCCA) and deep canonically correlated auto-encoders (DCCAE). In addition, support vector machine with Gaussian kernel is used to conduct classification with the compact bottleneck features learned by the deep multi-view models. Our experiments on the dataset provided by the MLSP Classification Challenge show that bottleneck features learned via deep multi-view models obtain better results than the trimming features used in the baseline system in terms of the receiver operating characteristic (ROC) area under the curve (AUC).","email":["qij13@u.washington.edu","javier.tejedor@depeca.uah.es"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7471816","source":"ieee","year":2016,"key":"d9ab8014-1046-4f69-9dc6-2b83d4ffa570","use":1,"doi":"10.1109\/ICASSP.2016.7471816"},{"Title":"Latent feature representation with 3-D multi-view deep convolutional neural network for bilateral analysis in digital breast tomosynthesis","Description":"D. H. Kim,  S. T. Kim,  Y. M. Ro","ShortDetails":"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2016","abstract":"In clinical studies of breast cancer, masses appear as asymmetric densities between the left and the right breasts, which show different breast tissue structures. For classifying breast masses, most researchers have developed hand-crafted bilateral features by extracting the asymmetric information in 2-D mammograms. In digital breast tomosynthesis (DBT), which has 3D volume data, effective bilateral features are needed to detect masses. In this paper, we propose latent bilateral feature representation with 3-D multi-view deep convolutional neural network (DCNN) in the DBT reconstructed volume. The proposed DCNN is designed to discover hidden or latent bilateral feature representation of masses in self-taught learning. Experimental results show that the proposed latent bilateral feature representation outperforms conventional hand-crafted features by achieving a high area under the receiver operating characteristic curve.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7471811","source":"ieee","year":2016,"key":"fa9c390f-d345-4021-a1bd-05c19acf5152","use":1,"doi":"10.1109\/ICASSP.2016.7471811"},{"Title":"Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring","Description":"M. Kallenberg,  K. Petersen,  M. Nielsen,  A. Y. Ng,  P. Diao,  C. Igel,  C. M. Vachon,  K. Holland,  R. R. Winkel,  N. Karssemeijer,  M. Lillholm","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.","email":["ymro@ee.kaist.ac.kr","32@5x5x5","32@5x5x5","32@28x28x21","32@14x14x11","32@10x10x7","32@5x5x4"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7412749","source":"ieee","year":2016,"key":"d288abce-deae-4014-a2cf-8cee4c39ad73","use":1,"doi":"10.1109\/TMI.2016.2532122"},{"Title":"q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI Scans","Description":"V. Golkov,  A. Dosovitskiy,  J. I. Sperl,  M. I. Menzel,  M. Czisch,  P. S\u00e4mann,  T. Brox,  D. Cremers","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.","email":["golkov@cs.tum.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7448418","source":"ieee","year":2016,"key":"db0507fe-b4a0-4eee-b780-ec0bbcca5b7c","use":1,"doi":"10.1109\/TMI.2016.2551324"},{"Title":"Multimodal fusion of brain structural and functional imaging with a deep neural machine translation approach","Description":"M. F. Amin,  S. M. Plis,  E. Damaraju,  D. Hjelm,  K. Cho,  V. D. Calhoun","ShortDetails":"2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI). 2016","abstract":"In this work, we study a novel approach of deep neural machine translation to find linkage between multimodal brain imaging data, such as structural MRI (sMRI) and functional MRI (fMRI). The idea is to consider two different imaging views of the same brain like two different languages conveying some common concepts or facts. An important aspect of the translation model is an attention network module that learns alignment between features from fMRI and sMRI. We use independent component analysis (ICA) based features for the translation model. Our study shows significant group differences between healthy controls and patients with schizophrenia in the learned alignments. Furthermore, this novel approach reveals a group differential relation between a cognitive score (attention and vigilance) and alignments that could not be found when individual modality of data were considered.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7459160","source":"ieee","year":2016,"key":"ade9cc0d-0d00-430d-bda7-c327b6afb6f5","use":1,"doi":"10.1109\/SSIAI.2016.7459160"},{"Title":"Multi-modal learning-based pre-operative targeting in deep brain stimulation procedures","Description":"Y. Liu,  B. M. Dawant","ShortDetails":"2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI). 2016","abstract":"Deep brain stimulation, as a primary surgical treatment for various neurological disorders, involves implanting electrodes to stimulate target nuclei within millimeter accuracy. Accurate pre-operative target selection is challenging due to the poor contrast in its surrounding region in MR images. In this paper, we present a learning-based method to automatically and rapidly localize the target using multi-modal images. A learning-based technique is applied first to spatially normalize the images in a common coordinate space. Given a point in this space, we extract a heterogeneous set of features that capture spatial and intensity contextual patterns at different scales in each image modality. Regression forests are used to learn a displacement vector of this point to the target. The target is predicted as a weighted aggregation of votes from various test samples, leading to a robust and accurate solution. We conduct five-fold cross validation using 100 subjects and compare our method to three indirect targeting methods, a state-of-the-art statistical atlas-based approach, and two variations of our method that use only a single modality image. With an overall error of 2.63\u00b11.37mm, our method improves upon the single modality-based variations and statistically significantly outperforms the indirect targeting ones. Our technique matches state-of-the-art registration methods but operates on completely different principles. Both techniques can be used in tandem in processing pipelines operating on large databases or in the clinical flow for automated error detection.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7455824","source":"ieee","year":2016,"key":"244fce5a-008e-4401-87d9-25a90fbe2b0a","use":1,"doi":"10.1109\/BHI.2016.7455824"},{"Title":"Combining deep convolutional networks and SVMs for mass detection on digital mammograms","Description":"I. Wichakam,  P. Vateekul","ShortDetails":"2016 8th International Conference on Knowledge and Smart Technology (KST). 2016","abstract":"It is important to detect breast cancers as early as possible, which are commonly diagnosed as a mass region on mammograms. Deep Convolutional networks (ConvNets) have been specially designed for various computer vision tasks. In image classification, it contains many layers to automatically extract image features and employs the softmax function at the last layer to predict a probability. Although it excels in feature extraction, the classification is still limited. In this paper, we propose to apply SVMs into ConvNets to detect a mass on mammograms. To overcome the scarcity of training images, a data augmentation technique is employed to increase the sample data. To further enhance the accuracy, two recent techniques in ConvNets are applied including (i) rectified linear units and (ii) dropout. The experiment was conducted on the INbreast data set. The result showed that the proposed method achieved an accuracy at 98.44%, which is superior to the baseline (ConvNets) for 8%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7440527","source":"ieee","year":2016,"key":"bb316a3d-f4a9-47fd-8e9d-5bad4d50c273","use":1,"doi":"10.1109\/KST.2016.7440527"},{"Title":"Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy","Description":"B. Jiang,  X. Wang,  J. Luo,  X. Zhang,  Y. Xiong,  H. Pang","ShortDetails":"2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC). 2015","abstract":"The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7405812","source":"ieee","year":2015,"key":"96681c0a-c5bf-4611-9c91-fe8762770567","use":1,"doi":"10.1109\/IMCCC.2015.33"},{"Title":"Integrated Optic Disc and Cup Segmentation with Deep Learning","Description":"G. Lim,  Y. Cheng,  W. Hsu,  M. L. Lee","ShortDetails":"2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI). 2015","abstract":"Glaucoma is a widespread ocular disorder leading to irreversible loss of vision. Therefore, there is a pressing need for cost-effective screening, such that preventive measures can be taken. This can be achieved with an accurate segmentation of the optic disc and cup from retinal images to obtain the cup-to-disc ratio. We describe a comprehensive solution based on applying convolutional neural networks to feature exaggerated inputs emphasizing disc pallor without blood vessel obstruction, as well as the degree of vessel kinking. The produced raw probability maps then undergo a robust refinement procedure that takes into account prior knowledge about retinal structures. Analysis of these probability maps further allows us to obtain a confidence estimate on the correctness of the segmentation, which can be used to direct the most challenging cases for manual inspection. Tests on two large real-world databases, including the publicly-available MESSIDOR collection, demonstrate the effectiveness of our proposed system.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7372132","source":"ieee","year":2015,"key":"72154e58-10e1-4094-b5f1-9da86333af45","use":1,"doi":"10.1109\/ICTAI.2015.36"},{"Title":"Automated Mass Detection in Mammograms Using Cascaded Deep Learning and Random Forests","Description":"N. Dhungel,  G. Carneiro,  A. P. Bradley","ShortDetails":"2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA). 2015","abstract":"Mass detection from mammograms plays a crucial role as a pre- processing stage for mass segmentation and classification. The detection of masses from mammograms is considered to be a challenging problem due to their large variation in shape, size, boundary and texture and also because of their low signal to noise ratio compared to the surrounding breast tissue. In this paper, we present a novel approach for detecting masses in mammograms using a cascade of deep learning and random forest classifiers. The first stage classifier consists of a multi-scale deep belief network that selects suspicious regions to be further processed by a two-level cascade of deep convolutional neural networks. The regions that survive this deep learning analysis are then processed by a two-level cascade of random forest classifiers that use morphological and texture features extracted from regions selected along the cascade. Finally, regions that survive the cascade of random forest classifiers are combined using connected component analysis to produce state-of-the-art results. We also show that the proposed cascade of deep learning and random forest classifiers are effective in the reduction of false positive regions, while maintaining a high true positive detection rate. We tested our mass detection system on two publicly available datasets: DDSM-BCRP and INbreast. The final mass detection produced by our approach achieves the best results on these publicly available datasets with a true positive rate of 0.96 \u00b1 0.03 at 1.2 false positive per image on INbreast and true positive rate of 0.75 at 4.8 false positive per image on DDSM-BCRP.","email":["TPR@FPI","0.03@1.2","0.14@0.8","0.87@3.67","0.75@4.8","0.70@4","0.70@8","0.80@1.1","0.92@5.4","0.88@2.4","0.81@0.6","0.88@2.7","0.85@1.5","0.8@1","0.70@0.79","0.8@1.2","0.9@2","0.96@4.5","0.96@3.0","0.81@2.3","0.70@0.10","0.80@4.23","0.95@3"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7371234","source":"ieee","year":2015,"key":"117a2e0e-bcb5-4835-9a73-39bc572aedd8","use":1,"doi":"10.1109\/DICTA.2015.7371234"},{"Title":"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images","Description":"J. Xu,  L. Xiang,  Q. Liu,  H. Gilmore,  J. Wu,  J. Tang,  A. Madabhushi","ShortDetails":"IEEE Transactions on Medical Imaging. 2016","abstract":"Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of \u201cDeep Learning\u201d strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 \u00d7 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163353","source":"ieee","year":2016,"key":"77c59385-02aa-4329-a9d1-9b80483efae0","use":1,"doi":"10.1109\/TMI.2015.2458702"},{"Title":"Improving EEG feature learning via synchronized facial video","Description":"X. Li,  X. Jia,  G. Xun,  A. Zhang","ShortDetails":"2015 IEEE International Conference on Big Data (Big Data). 2015","abstract":"Morden physiological analysis begins to involve more and more types of information. Electroencephalogram (EEG) signals as a typical example is starting to be analyzed with facial expressions videos to detect emotions. Emotions play an important role in the daily life of human beings, the need and importance of automatic emotion recognition has grown with increasing role of human computer interface applications. In this paper, we concentrate on recognition of the emotions jointly from \"inner\" and \"outer\" reactions, which are electroencephalogram (EEG) signals and facial expression video. Due to the streaming nature of this problem, the data volume and velocity is very challenging. We address these challenges from the theoretic perspective and propose a real time algorithm based on EEG signals and synchronized facial video to learn feature vector jointly. Our algorithm consists of an unsupervisedly EEG dictionary component based on deep learning theorem, and a probability pooling component transforms a continuous sequential signal into an EEG \"sentence\" which consists of a sequence of EEG words. The EEG sentence is then jointly learned with video features into a new fixed length feature representation for emotion classification. We overcome several computational challenges on the data based on the idea of convolution and pooling, and we conduct extensive evaluation for each component of our model. We also demonstrate the state-of-the-art classification result on real-world dataset. The superior performances on the emotion recognition task indicates that 1) the natural language scenario can be applied in EEG sequences and 2) borrowing video modality can increase the overall performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7363831","source":"ieee","year":2015,"key":"7cb98576-955c-4a44-9dd6-9052cf1fef22","use":1,"doi":"10.1109\/BigData.2015.7363831"},{"Title":"Deep learning of tissue fate features in acute ischemic stroke","Description":"N. Stier,  N. Vincent,  D. Liebeskind,  F. Scalzo","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"In acute ischemic stroke treatment, prediction of tissue survival outcome plays a fundamental role in the clinical decision-making process, as it can be used to assess the balance of risk vs. possible benefit when considering endovascular clot-retrieval intervention. For the first time, we construct a deep learning model of tissue fate based on randomly sampled local patches from the hypoperfusion (Tmax) feature observed in MRI immediately after symptom onset. We evaluate the model with respect to the ground truth established by an expert neurologist four days after intervention. Experiments on 19 acute stroke patients evaluated the accuracy of the model in predicting tissue fate. Results show the superiority of the proposed regional learning framework versus a single-voxel-based regression model.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359869","source":"ieee","year":2015,"key":"eb85fde7-4927-492a-8768-8afe459c9c09","use":1,"doi":"10.1109\/BIBM.2015.7359869"},{"Title":"Temporal Pattern and Association Discovery of Diagnosis Codes Using Deep Learning","Description":"S. Mehrabi,  S. Sohn,  D. Li,  J. J. Pankratz,  T. Therneau,  J. L. S. Sauver,  H. Liu,  M. Palakal","ShortDetails":"2015 International Conference on Healthcare Informatics. 2015","abstract":"Longitudinal health records contain data on patients' visits, condition, treatment, and test results representing progression of their health status over time. In poorly understood patient populations, such data are particularly helpful in characterizing disease progression and early detection. In this work we developed a deep learning algorithm for temporal pattern discovery over Rochester Epidemiology Project data. We modeled each patient's records as a matrix of temporal clinical events with ICD9 and HCUP CSS diagnosis codes as rows and years of diagnosis as columns. Patients aged 18 or younger at the time of diagnosis were selected. A deep Boltzmann machine network with three hidden layers was constructed with each patient's diagnosis matrix values as visible nodes. The final weights of the network model were analyzed as the common features among patients' records.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7349719","source":"ieee","year":2015,"key":"f305ac0a-8276-42ce-9468-a4b66acefa5c","use":1,"doi":"10.1109\/ICHI.2015.58"},{"Title":"Clinical deep brain stimulation region prediction using regression forests from high-field MRI","Description":"J. Kim,  Y. Duchin,  G. Sapiro,  J. Vitek,  N. Harel","ShortDetails":"2015 IEEE International Conference on Image Processing (ICIP). 2015","abstract":"This paper presents a prediction framework of brain subcortical structures which are invisible on clinical low-field MRI, learning detailed information from ultrahigh-field MR training data. Volumetric segmentation of Deep Brain Stimulation (DBS) structures within the Basal ganglia is a prerequisite process for reliable DBS surgery. While ultrahigh-field MR imaging (7 Tesla) allows direct visualization of DBS targeting structures, such ultrahigh-fields are not always clinically available, and therefore the relevant structures need to be predicted from the clinical data. We address the shape prediction problem with a regression forest, non-linearly mapping predictors to target structures with high confidence, exploiting ultrahigh-field MR training data. We consider an application for the subthalamic nucleus (STN) prediction as a crucial DBS target. Experimental results on Parkinson's patients validate that the proposed approach enables reliable estimation of the STN from clinical 1.5T MRI.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7351248","source":"ieee","year":2015,"key":"ec00acf0-825d-4dad-9b95-7d776aa8e379","use":1,"doi":"10.1109\/ICIP.2015.7351248"},{"Title":"Lung segmentation in chest radiographs using distance regularized level set and deep-structured learning and inference","Description":"T. A. Ngo,  G. Carneiro","ShortDetails":"2015 IEEE International Conference on Image Processing (ICIP). 2015","abstract":"Computer-aided diagnosis of digital chest X-ray (CXR) images critically depends on the automated segmentation of the lungs, which is a challenging problem due to the presence of strong edges at the rib cage and clavicle, the lack of a consistent lung shape among different individuals, and the appearance of the lung apex. From recently published results in this area, hybrid methodologies based on a combination of different techniques (e.g., pixel classification and deformable models) are producing the most accurate lung segmentation results. In this paper, we propose a new methodology for lung segmentation in CXR using a hybrid method based on a combination of distance regularized level set and deep structured inference. This combination brings together the advantages of deep learning methods (robust training with few annotated samples and top-down segmentation with structured inference and learning) and level set methods (use of shape and appearance priors and efficient optimization techniques). Using the publicly available Japanese Society of Radiological Technology (JSRT) dataset, we show that our approach produces the most accurate lung segmentation results in the field. In particular, depending on the initialization used, our methodology produces an average accuracy on JSTR that varies from 94.8% to 98.5%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7351179","source":"ieee","year":2015,"key":"02439400-df1b-45e3-9943-221d7c134f30","use":1,"doi":"10.1109\/ICIP.2015.7351179"},{"Title":"Deep structured learning for mass segmentation from mammograms","Description":"N. Dhungel,  G. Carneiro,  A. P. Bradley","ShortDetails":"2015 IEEE International Conference on Image Processing (ICIP). 2015","abstract":"In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to assess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computation of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7351343","source":"ieee","year":2015,"key":"960f70de-2f22-4c9b-833a-2bc16932166b","use":1,"doi":"10.1109\/ICIP.2015.7351343"},{"Title":"FingerNet: Deep learning-based robust finger joint detection from radiographs","Description":"S. Lee,  M. Choi,  H. s. Choi,  M. S. Park,  S. Yoon","ShortDetails":"2015 IEEE Biomedical Circuits and Systems Conference (BioCAS). 2015","abstract":"Radiographic image assessment is the most common method used to measure physical maturity and diagnose growth disorders, hereditary diseases and rheumatoid arthritis, with hand radiography being one of the most frequently used techniques due to its simplicity and minimal exposure to radiation. Finger joints are considered as especially important factors in hand skeleton examination. Although several automation methods for finger joint detection have been proposed, low accuracy and reliability are hindering full-scale adoption into clinical fields. In this paper, we propose FingerNet, a novel approach for the detection of all finger joints from hand radiograph images based on convolutional neural networks, which requires little user intervention. The system achieved 98.02% average detection accuracy for 130 test data sets containing over 1,950 joints. Further analysis was performed to verify the system robustness against factors such as epiphysis and metaphysis in different age groups.","email":["pmsmed@gmail.com","sryoon@snu.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7348440","source":"ieee","year":2015,"key":"e53140cb-3b5e-4498-97b3-356c3a1f0241","use":1,"doi":"10.1109\/BioCAS.2015.7348440"},{"Title":"A deep architecture for visually analyze Pap cells","Description":"O. Chang,  P. Constante,  A. Gordon,  M. Singania,  F. Acuna","ShortDetails":"2015 IEEE 2nd Colombian Conference on Automatic Control (CCAC). 2015","abstract":"This work proposes a deep ANN architecture which accomplishes the reliable visual classification of abnormal Pap smear cell. The system is driven by independent agents where the first agent consists of a three layer ANN pretrained to closely track a reticle pattern. This net participates in a local close loop that oscillates and produces unique time-space versions of the visual data. This information is stabilized and sparsed in order to obtain compact data representations, with implicit space time content. The obtained representations are delivered to second level agents, formed by independent three layers ANNs dedicated to learning and recognition activities. To train the system a noise-balanced algorithm is employed, where the training set is composed by pap cells and white noise. This combination operating on finite databases and in a self controlled learning loop, auto develops enough cell recognition knowledge as to classify whole classes of Pap smear cells. The system has been tested in real time utilizing documented data bases.","email":["ogchang@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7345210","source":"ieee","year":2015,"key":"921c3913-389b-4ad1-895a-764c5e55264e","use":1,"doi":"10.1109\/CCAC.2015.7345210"},{"Title":"Deep independence network analysis of structural brain imaging: A simulation study","Description":"E. Castro,  D. Hjelm,  S. Plis,  L. Dinh,  J. Turner,  V. Calhoun","ShortDetails":"2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). 2015","abstract":"The objective of this paper is to further validate theoretically and empirically a nonlinear independent component analysis (ICA) algorithm implemented with a deep learning architecture. We first revisited its formulation to verify its consistency with the criterion of minimization of mutual information. Then, we applied the nonlinear independent component estimation algorithm (NICE) to synthetic 2D images that resemble structural magnetic resonance imaging (sMRI) data. This data was generated by mixing spatial components that represent axial slices of sMRI tissue concentration images. Next, we generated the images under linear and mildly nonlinear mixtures, being able to show that NICE matches ICA when the data is generated by using the conventional linear mixture and outperforms ICA for the nonlinear mixture of components. The obtained results are promising and suggest that NICE has potential to find richer brain networks if applied to real sMRI data, provided that small conditioning adjustments are performed along with this approach.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7324318","source":"ieee","year":2015,"key":"93f12098-91f2-4d40-a5af-de82574be48e","use":1,"doi":"10.1109\/MLSP.2015.7324318"},{"Title":"Synthetic structural magnetic resonance image generator improves deep learning prediction of schizophrenia","Description":"A. Ulloa,  S. Plis,  E. Erhardt,  V. Calhoun","ShortDetails":"2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). 2015","abstract":"Despite the rapidly growing interest, progress in the study of relations between physiological abnormalities and mental disorders is hampered by complexity of the human brain and high costs of data collection. The complexity can be captured by deep learning approaches, but they still may require significant amounts of data. In this paper, we seek to mitigate the latter challenge by developing a generator for synthetic realistic training data. Our method greatly improves generalization in classification of schizophrenia patients and healthy controls from their structural magnetic resonance images. A feed forward neural network trained exclusively on continuously generated synthetic data produces the best area under the curve compared to classifiers trained on real data alone.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7324379","source":"ieee","year":2015,"key":"ede79da0-c69b-4a3e-a654-48935012c3e1","use":1,"doi":"10.1109\/MLSP.2015.7324379"},{"Title":"Investigating deep learning for fNIRS based BCI","Description":"J. Hennrich,  C. Herff,  D. Heger,  T. Schultz","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318984","source":"ieee","year":2015,"key":"7535b4a2-f0e0-4571-97a6-18c0fbe2b848","use":1,"doi":"10.1109\/EMBC.2015.7318984"},{"Title":"Glaucoma detection based on deep convolutional neural network","Description":"X. Chen,  Y. Xu,  D. W. Kee Wong,  T. Y. Wong,  J. Liu","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318462","source":"ieee","year":2015,"key":"39ca8a5f-c234-423c-a099-3ed9a4c24d5e","use":1,"doi":"10.1109\/EMBC.2015.7318462"},{"Title":"A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks","Description":"C. Wang,  X. Yan,  M. Smith,  K. Kochhar,  M. Rubin,  S. M. Warren,  J. Wrobel,  H. Lee","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system.","email":["mxsmithg@umich.edu","honglak@eecs.umich.edu2UniversityofMichiganMedicalSchool","USAkanikak@umich.edu","jswrobel@med.umich.edu3DepartmentofPlasticSurgery","Stephen.Warreng@nyumc.org"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7318881","source":"ieee","year":2015,"key":"6ea9a041-5059-48af-9f1d-92f432ede800","use":1,"doi":"10.1109\/EMBC.2015.7318881"},{"Title":"Deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images","Description":"D. Maji,  A. Santara,  S. Ghosh,  D. Sheet,  P. Mitra","ShortDetails":"2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2015","abstract":"Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195.","email":["India.debdoot@ee.iitkgp.ernet.in2S.GhoshiswiththeDepartmentofOphthalmology"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7319030","source":"ieee","year":2015,"key":"69dc1069-18f6-4268-8780-88a330ded0a2","use":1,"doi":"10.1109\/EMBC.2015.7319030"},{"Title":"Detection of exudates in fundus photographs using convolutional neural networks","Description":"P. Prenta\u0161i\u0107,  S. Lon\u010dari\u0107","ShortDetails":"2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA). 2015","abstract":"Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7306056","source":"ieee","year":2015,"key":"90661477-0951-4063-bcc1-7822c99ad9ee","use":1,"doi":"10.1109\/ISPA.2015.7306056"},{"Title":"Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning","Description":"X. Gao,  S. Lin,  T. Y. Wong","ShortDetails":"IEEE Transactions on Biomedical Engineering. 2015","abstract":"Goal: Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. Methods: Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade. Results: The proposed system is validated on a large population-based dataset of 5378 images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error (\u03b5) of 0.304, a 70.7% exact integral agreement ratio (R<sub>0<\/sub>), an 88.4% decimal grading error \u22640.5 (R<sub>e0.5<\/sub>), and a 99.0% decimal grading error \u22641.0 (R<sub>e1.0<\/sub>). Significance: The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7122265","source":"ieee","year":2015,"key":"b9264e05-395d-44e6-9a73-97696cd38c34","use":1,"doi":"10.1109\/TBME.2015.2444389"},{"Title":"A Configurable Deep Network for high-dimensional clinical trial data","Description":"J. O' Donoghue,  M. Roantree,  M. Van Boxtel","ShortDetails":"2015 International Joint Conference on Neural Networks (IJCNN). 2015","abstract":"Clinical studies provide interesting case studies for data mining researchers, given the often high degree of dimensionality and long term nature of these studies. In areas such as dementia, accurate predictions from data scientists provide vital input into the understanding of how certain features (representing lifestyle) can predict outcomes such as dementia. Most research involved has used traditional or shallow data mining approaches which have been shown to offer varying degrees of accuracy in datasets with high dimensionality. In this research, we explore the use of deep learning architectures, as they have been shown to have high predictive capabilities in image and audio datasets. The purpose of our research is to build a framework which allows easy reconfiguration for the performance of experiments across a number of deep learning approaches. In this paper, we present our framework for a configurable deep learning machine and our evaluation and analysis of two shallow approaches: regression and multi-layer perceptron, as a platform to a deep belief network, and using a dataset created over the course of 12 years by researchers in the area of dementia.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7280841","source":"ieee","year":2015,"key":"9cc4fd91-c2a8-4e48-ad0d-77224555fe02","use":1,"doi":"10.1109\/IJCNN.2015.7280841"},{"Title":"Multi-Layer and Recursive Neural Networks for Metagenomic Classification","Description":"G. Ditzler,  R. Polikar,  G. Rosen","ShortDetails":"IEEE Transactions on NanoBioscience. 2015","abstract":"Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.","email":["zler@gmail.com","gailr@ece.drexel.edu","zler@gmail.com","gailr@ece.drexel.edu","polikar@rowan.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7219432","source":"ieee","year":2015,"key":"454721f9-3a79-4bdf-9e83-db9f2996e93c","use":1,"doi":"10.1109\/TNB.2015.2461219"},{"Title":"Deep learninig of EEG signals for emotion recognition","Description":"Y. Gao,  H. J. Lee,  R. M. Mehmood","ShortDetails":"2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW). 2015","abstract":"Emotion recognition is an important task for computer to understand the human status in brain computer interface (BCI) systems. It is difficult to perceive the emotion of some disabled people through their facial expression, such as functional autism patient. EEG signal provides us a non-invasive way to recognize the emotion of these disable people through EEG headset electrodes placed on their scalp. In this paper, we propose a deep learning algorithm to simultaneously learn the features and classify the emotions of EEG signals. It differs from the conventional methods as we apply deep learning on the raw signal without explicit hand-crafted feature extraction. Because the EEG signal has subject dependency, it is better to train the emotion model subject-wise, while there is not much epochs available for each subject. Deep learning algorithm provides a solution with a pre-training way using three layers of restricted Boltzmann machines (RBMs). Thus, we can use epochs of all subjects to pre-training the deep network, and use back-propagation to fine tuning the network subject by subject. Experiment results show that our proposed framework achieves better recognition accuracy than conventional algorithms.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7169796","source":"ieee","year":2015,"key":"1272064e-217e-400f-a39b-3d7242540f5c","use":1,"doi":"10.1109\/ICMEW.2015.7169796"},{"Title":"Clinical subthalamic nucleus prediction from high-field brain MRI","Description":"J. Kim,  Y. Duchin,  G. Sapiro,  J. Vitek,  N. Harel","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"The subthalamic nucleus (STN) within the sub-cortical region of the Basal ganglia is a crucial targeting structure for Parkinson's Deep brain stimulation (DBS) surgery. Volumetric segmentation of such small and complex structure, which is elusive in clinical MRI protocols, is thereby a pre-requisite process for reliable DBS direct targeting. While direct visualization of the STN is facilitated with advanced ultrahigh-field MR imaging (7 Tesla), such high fields are not always clinically available. In this paper, we aim at the automatic prediction of the STN region on clinical low-field MRI, exploiting dependencies between the STN and its adjacent structures, learned from ultrahigh-field MRI. We present a framework based on a statistical shape model to learn such shape relationship on high quality MR data sets. This allows for an accurate prediction and visualization of the STN structure, given detectable predictors on the low-field MRI. Experimental results on Parkinson's patients demonstrate that the proposed approach enables accurate estimation of the STN on clinical 1.5T MRI.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7164104","source":"ieee","year":2015,"key":"ab3949df-06b5-495f-94ef-d5741828521c","use":1,"doi":"10.1109\/ISBI.2015.7164104"},{"Title":"Tree RE-weighted belief propagation using deep learning potentials for mass segmentation from mammograms","Description":"N. Dhungel,  G. Carneiro,  A. P. Bradley","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"In this paper, we propose a new method for the segmentation of breast masses from mammograms using a conditional random field (CRF) model that combines several types of potential functions, including one that classifies image regions using deep learning. The inference method used in this model is the tree re-weighted (TRW) belief propagation, which allows a learning mechanism that directly minimizes the mass segmentation error and an inference approach that produces an optimal result under the approximations of the TRW formulation. We show that the use of these inference and learning mechanisms and the deep learning potential functions provides gains in terms of accuracy and efficiency in comparison with the current state of the art using the publicly available datasets INbreast and DDSM-BCRP.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163983","source":"ieee","year":2015,"key":"e104267b-ca6c-4a5b-be3d-a0d1940d91e3","use":1,"doi":"10.1109\/ISBI.2015.7163983"},{"Title":"Generation of synthetic structural magnetic resonance images for deep learning pre-training","Description":"E. Castro,  A. Ulloa,  S. M. Plis,  J. A. Turner,  V. D. Calhoun","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Deep learning methods have significantly improved classification accuracy in different areas such as speech, object and text recognition. However, this field has only began to be explored in the brain imaging field, which differs from other fields in terms of the amount of data available, its data dimensionality and other factors. This paper proposes a methodology to generate an extensive synthetic structural magnetic resonance imaging (sMRI) dataset to be used at the pre-training stage of a shallow network model to address the issue of having a limited amount of data available. Our results show that by extending our dataset using 5,000 synthetic sMRI volumes for pretraining, which accounts to approximately 10 times the size of the original dataset, we can obtain a 5% average improvement on classification results compared to the regular approach on a schizophrenia dataset. While the use of synthetic sMRI data for pre-training has only been tested on a shallow network, this can be readily applied to deeper networks.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7164053","source":"ieee","year":2015,"key":"cd18f929-9c4f-4194-bec0-46b6f4e789ab","use":1,"doi":"10.1109\/ISBI.2015.7164053"},{"Title":"Automatic detection of cerebral microbleeds via deep learning based 3D feature representation","Description":"H. Chen,  L. Yu,  Q. Dou,  L. Shi,  V. C. T. Mok,  P. A. Heng","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Clinical identification and rating of the cerebral microbleeds (CMBs) are important in vascular diseases and dementia diagnosis. However, manual labeling is time-consuming with low reproducibility. In this paper, we present an automatic method via deep learning based 3D feature representation, which solves this detection problem with three steps: candidates localization with high sensitivity, feature representation, and precise classification for reducing false positives. Different from previous methods by exploiting low-level features, e.g., shape features and intensity values, we utilize the deep learning based high-level feature representation. Experimental results validate the efficacy of our approach, which outperforms other methods by a large margin with a high sensitivity while significantly reducing false positives per subject.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163984","source":"ieee","year":2015,"key":"138e7af3-71a2-470b-ae9e-c9548bbe142d","use":1,"doi":"10.1109\/ISBI.2015.7163984"},{"Title":"Using deep learning for robustness to parapapillary atrophy in optic disc segmentation","Description":"R. Srivastava,  J. Cheng,  D. W. K. Wong,  J. Liu","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Optic Disc (OD) segmentation from retinal fundus images is important for many applications such as detecting other optic structures and early detection of glaucoma. One of the major problems in segmenting OD is the presence of Para-papillary Atrophy (PPA) which in many cases looks similar to the OD. Researchers have used many different features to distinguish between PPA and OD, however each of these features has some limitation or the other. In this paper, we propose to use a deep neural network for OD segmentation which can learn features to distinguish PPA from OD. Using simple image intensity based features, the proposed method has the least mean overlapping error (9.7%) among the state-of-the-art works for OD segmentation in fundus images with PPA.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163985","source":"ieee","year":2015,"key":"1020704c-16ee-4d6c-ba5f-c65975871913","use":1,"doi":"10.1109\/ISBI.2015.7163985"},{"Title":"Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology","Description":"D. Sheet,  S. P. K. Karri,  A. Katouzian,  N. Navab,  A. K. Ray,  J. Chatterjee","ShortDetails":"2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI). 2015","abstract":"Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7163987","source":"ieee","year":2015,"key":"6d86c910-d4c6-4d57-9b33-7bf3742b7baa","use":1,"doi":"10.1109\/ISBI.2015.7163987"},{"Title":"Disease Inference from Health-Related Questions via Sparse Deep Learning","Description":"L. Nie,  M. Wang,  L. Zhang,  S. Yan,  B. Zhang,  T. S. Chua","ShortDetails":"IEEE Transactions on Knowledge and Data Engineering. 2015","abstract":"Automatic disease inference is of importance to bridge the gap between what online health seekers with unusual symptoms need and what busy human doctors with biased expertise can offer. However, accurately and efficiently inferring diseases is non-trivial, especially for community-based health services due to the vocabulary gap, incomplete information, correlated medical concepts, and limited high quality training samples. In this paper, we first report a user study on the information needs of health seekers in terms of questions and then select those that ask for possible diseases of their manifested symptoms for further analytic. We next propose a novel deep learning scheme to infer the possible diseases given the questions of health seekers. The proposed scheme is comprised of two key components. The first globally mines the discriminant medical signatures from raw features. The second deems the raw features and their signatures as input nodes in one layer and hidden nodes in the subsequent layer, respectively. Meanwhile, it learns the inter-relations between these two layers via pre-training with pseudo-labeled data. Following that, the hidden nodes serve as raw features for the more abstract signature mining. With incremental and alternative repeating of these two components, our scheme builds a sparsely connected deep architecture with three hidden layers. Overall, it well fits specific tasks with fine-tuning. Extensive experiments on a real-world dataset labeled by online doctors show the significant performance gains of our scheme.","email":["nieliqiang@gmail.com","zglumg@gmail.com","chuats@comp.nus.edu.sg","eleyans@nus.edu.sg","eric.mengwang@gmail.com","dcszb@tsinghua.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7029673","source":"ieee","year":2015,"key":"5bd5d5ef-e479-4738-8c69-c3fce03fce94","use":1,"doi":"10.1109\/TKDE.2015.2399298"},{"Title":"Self-supervised learning model for skin cancer diagnosis","Description":"A. Masood,  A. Al- Jumaily,  K. Anam","ShortDetails":"2015 7th International IEEE\/EMBS Conference on Neural Engineering (NER). 2015","abstract":"Automated diagnosis of skin cancer is an active area of research with different classification methods proposed so far. However, classification models based on insufficient labeled training data can badly influence the diagnosis process if there is no self-advising and semi supervising capability in the model. This paper presents a semi supervised, self-advised learning model for automated recognition of melanoma using dermoscopic images. Deep belief architecture is constructed using labeled data together with unlabeled data, and fine tuning done by an exponential loss function in order to maximize separation of labeled data. In parallel a self-advised SVM algorithm is used to enhance classification results by counteracting the effect of misclassified data. To increase generalization capability and redundancy of the model, polynomial and radial basis function based SA-SVMs and Deep network are trained using training samples randomly chosen via a bootstrap technique. Then the results are aggregated using least square estimation weighting. The proposed model is tested on a collection of 100 dermoscopic images. The variation in classification error is analyzed with respect to the ratio of labeled and unlabeled data used in the training phase. The classification performance is compared with some popular classification methods and the proposed model using the deep neural processing outperforms most of the popular techniques including KNN, ANN, SVM and semi supervised algorithms like Expectation maximization and transductive SVM.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7146798","source":"ieee","year":2015,"key":"e15a8cd3-949c-4c47-b22a-4012f2ecac05","use":1,"doi":"10.1109\/NER.2015.7146798"},{"Title":"Classification on ADHD with Deep Learning","Description":"D. Kuang,  L. He","ShortDetails":"2014 International Conference on Cloud Computing and Big Data. 2014","abstract":"Effective discrimination of attention deficit hyperactivity disorder (ADHD) using imaging and functional biomarkers would have fundamental influence on public health. In usual, the discrimination is based on the standards of American Psychiatric Association. In this paper, we modified one of the deep learning method on structure and parameters according to the properties of ADHD data, to discriminate ADHD on the unique public dataset of ADHD-200. We predicted the subjects as control, combined, inattentive or hyperactive through their frequency features. The results achieved improvement greatly compared to the performance released by the competition. Besides, the imbalance in datasets of deep learning model influenced the results of classification. As far as we know, it is the first time that the deep learning method has been used for the discrimination of ADHD with fMRI data.","email":["helianghua@tongji.edu.cn","Kuangdp1990@163.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7062868","source":"ieee","year":2014,"key":"a0e16d04-b323-47a5-b2ec-f6b8d6f20df4","use":1,"doi":"10.1109\/CCBD.2014.42"},{"Title":"Deep learning for brain decoding","Description":"O. Firat,  L. Oztekin,  F. T. Y. Vural","ShortDetails":"2014 IEEE International Conference on Image Processing (ICIP). 2014","abstract":"Learning low dimensional embedding spaces (manifolds) for efficient feature representation is crucial for complex and high dimensional input spaces. Functional magnetic resonance imaging (fMRI) produces high dimensional input data and with a less then ideal number of labeled samples for a classification task. In this study, we explore deep learning methods for fMRI classification tasks in order to reduce dimensions of feature space, along with improving classification performance for brain decoding. We employ sparse autoencoders for unsupervised feature learning, leveraging unlabeled fMRI data to learn efficient, non-linear representations as the building blocks of a deep learning architecture by stacking them. Proposed method is tested on a memory encoding\/retrieval experiment with ten classes. The results support the efficiency compared to the baseline multi-voxel pattern analysis techniques.","email":["ioztekin@ku.edu.tr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7025563","source":"ieee","year":2014,"key":"35d58426-4a62-49fd-8906-7c84213f62e8","use":1,"doi":"10.1109\/ICIP.2014.7025563"},{"Title":"Experimental Study of Unsupervised Feature Learning for HEp-2 Cell Images Clustering","Description":"Y. Zhao,  Z. Gao,  L. Wang,  L. Zhou","ShortDetails":"2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA). 2014","abstract":"Automatic identification of HEp-2 cell images has received an increasing research attention. Feature representations play a critical role in achieving good identification performance. Much recent work has focused on supervised feature learning. Typical methods consist of BoW model (based on hand-crafted features) and deep learning model (learning hierarchical features). However, these labels used in supervised feature learning are very labour-intensive and time-consuming. They are commonly manually annotated by specialists and very expensive to obtain. In this paper, we follow this fact and focus on unsupervised feature learning. We have verified and compared the features of these two typical models by clustering. Experimental results show the BoW model generally perform better than deep learning models. Also, we illustrate BoW model and deep learning models have complementarity properties.","email":["zg126@uowmail.edu.au","lupingz@uow.edu.au"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7008108","source":"ieee","year":2014,"key":"1ef1a4f0-3786-4131-b942-3d7594e60f02","use":1,"doi":"10.1109\/DICTA.2014.7008108"},{"Title":"Deep learning for healthcare decision making with EMRs","Description":"Z. Liang,  G. Zhang,  J. X. Huang,  Q. V. Hu","ShortDetails":"2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2014","abstract":"Computer aid technology is widely applied in decision-making and outcome assessment of healthcare delivery, in which modeling knowledge and expert experience is technically important. However, the conventional rule-based models are incapable of capturing the underlying knowledge because they are incapable of simulating the complexity of human brains and highly rely on feature representation of problem domains. Thus we attempt to apply a deep model to overcome this weakness. The deep model can simulate the thinking procedure of human and combine feature representation and learning in a unified model. A modified version of convolutional deep belief networks is used as an effective training method for large-scale data sets. Then it is tested by two instances: a dataset on hypertension retrieved from a HIS system, and a dataset on Chinese medical diagnosis and treatment prescription from a manual converted electronic medical record (EMR) database. The experimental results indicate that the proposed deep model is able to reveal previously unknown concepts and performs much better than the conventional shallow models.","email":["jhuang@yorku.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6999219","source":"ieee","year":2014,"key":"57d286cc-2da8-4d82-830d-8d7292ca7f07","use":1,"doi":"10.1109\/BIBM.2014.6999219"},{"Title":"EEG-based emotion classification using deep belief networks","Description":"W. L. Zheng,  J. Y. Zhu,  Y. Peng,  B. L. Lu","ShortDetails":"2014 IEEE International Conference on Multimedia and Expo (ICME). 2014","abstract":"In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.","email":["bllu@sjtu.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6890166","source":"ieee","year":2014,"key":"882933b4-bb4e-4c35-9c9c-9816553f82b2","use":1,"doi":"10.1109\/ICME.2014.6890166"},{"Title":"Convolutional deep belief networks for feature extraction of EEG signal","Description":"Y. Ren,  Y. Wu","ShortDetails":"2014 International Joint Conference on Neural Networks (IJCNN). 2014","abstract":"In recent years, deep learning approaches have been successfully used to learn hierarchical representations of image data, audio data etc. However, to our knowledge, these deep learning approaches have not been extensively studied for electroencephalographic (EEG) data. Considering the properties of EEG data, high-dimensional and multichannel, we applied convolutional deep belief networks to the feature learning of EEG data and evaluated it on the datasets from previous BCI competitions. Compared with other state-of-the-art feature extraction methods, the learned features using convolutional deep belief network have better performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6889383","source":"ieee","year":2014,"key":"dd22b8b8-d8b3-407d-9b44-02a4ed28ad4b","use":1,"doi":"10.1109\/IJCNN.2014.6889383"},{"Title":"Early diagnosis of Alzheimer's disease with deep learning","Description":"S. Liu,  S. Liu,  W. Cai,  S. Pujol,  R. Kikinis,  D. Feng","ShortDetails":"2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI). 2014","abstract":"The accurate diagnosis of Alzheimer's disease (AD) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped. Although many studies have applied machine learning methods for computer-aided-diagnosis (CAD) of AD recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models. In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of AD and its prodromal stage, Mild Cognitive Impairment (MCI). Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge. A significant performance gain on classification of all diagnosis groups was achieved in our experiments.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6868045","source":"ieee","year":2014,"key":"015955e8-3fc3-442b-86e9-ed4b5646a253","use":1,"doi":"10.1109\/ISBI.2014.6868045"},{"Title":"Stacked Sparse Autoencoder (SSAE) based framework for nuclei patch classification on breast cancer histopathology","Description":"J. Xu,  L. Xiang,  R. Hang,  J. Wu","ShortDetails":"2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI). 2014","abstract":"In this paper, a Stacked Sparse Autoencoder (SSAE) based framework is presented for nuclei classification on breast cancer histopathology. SSAE works very well in learning useful high-level feature for better representation of input raw data. To show the effectiveness of proposed framework, SSAE+Softmax is compared with conventional Softmax classifier, PCA+Softmax, and single layer Sparse Autoencoder (SAE)+Softmax in classifying the nuclei and non-nuclei patches extracted from breast cancer histopathology. The SSAE+Softmax for nuclei patch classification yields an accuracy of 83.7%, F1 score of 82%, and AUC of 0.8992, which outperform Softmax classifier, PCA+Softmax, and SAE+Softmax.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6868041","source":"ieee","year":2014,"key":"1ed855fc-d9a6-4fa6-859c-a6ef958f90f0","use":1,"doi":"10.1109\/ISBI.2014.6868041"},{"Title":"Generative Adversarial Learning for Reducing Manual Annotation in Semantic Segmentation on Large Scale Miscroscopy Images: Automated Vessel Segmentation in Retinal Fundus Image as Test Case","Description":"A. Lahiri,  K. Ayush,  P. K. Biswas,  P. Mitra","ShortDetails":"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017","abstract":"Convolutional Neural Network(CNN) based semantic segmentation require extensive pixel level manual annotation which is daunting for large microscopic images. The paper is aimed towards mitigating this labeling effort by leveraging the recent concept of generative adversarial network(GAN) wherein a generator maps latent noise space to realistic images while a discriminator differentiates between samples drawn from database and generator. We extend this concept to a multi task learning wherein a discriminator-classifier network differentiates between fake\/real examples and also assigns correct class labels. Though our concept is generic, we applied it for the challenging task of vessel segmentation in fundus images. We show that proposed method is more data efficient than a CNN. Specifically, with 150K, 30K and 15K training examples, proposed method achieves mean AUC of 0.962, 0.945 and 0.931 respectively, whereas the simple CNN achieves AUC of 0.960, 0.921 and 0.916 respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8014844","source":"ieee","year":2017,"key":"ceb856e2-018e-4d83-96ce-3cd5a099cea6","use":1,"doi":"10.1109\/CVPRW.2017.110"},{"Title":"Recurrent neural networks with missing information imputation for medical examination data prediction","Description":"Han-Gyu Kim,  Gil-Jin Jang,  Ho-Jin Choi,  Minho Kim,  Young-Won Kim,  Jaehun Choi","ShortDetails":"2017 IEEE International Conference on Big Data and Smart Computing (BigComp). 2017","abstract":"In this work, we use recurrent neural network (RNN) to predict the medical examination data with missing parts. There often exist missing parts in medical examination data due to various human factors, for instance, because human subjects occasionally miss their annual examinations. Such missing parts make it hard to predict the future examination data by machines. Thus, imputation of the missing information is needed for accurate prediction of medical examination data. Among various types of RNNs, we choose simple recurrent network (SRN) and long short-term memory (LSTM) to predict the missing information as well as the future medical examination data, as they show good performance in many relevant applications. In our proposed method, the temporal trajectories of the medical examination measurements are modeled by RNNs with the missed measurements compensated, which is then used to predict the future measurements to be used as diagnosing the diseases of the subjects in advance. We have carried out experiments using a medical examination database of Korean people for 12 consecutive years with 13 medical fields. In this database, 11500 people took the medical check-up every year, and 7400 people missed their examination occasionally. We use complete data to train RNNs, and the data with missing parts are used to evaluate the imputation and future measurement prediction performance. In terms of root mean squared error (RMSE) and source to noise ratio (SNR) between the prediction and the actual measurements, the experimental results show that the proposed RNNs predicts medical examination data much better than the conventional linear regression in most of the examination items.","email":["gjang@knu.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881685","source":"ieee","year":2017,"key":"5df2b9ba-b15f-4c02-85c4-b3b85e4066f9","use":1,"doi":"10.1109\/BIGCOMP.2017.7881685"},{"Title":"Recurrent Neural Network Based Classification of ECG Signal Features for Obstruction of Sleep Apnea Detection","Description":"M. Cheng,  W. J. Sori,  F. Jiang,  A. Khan,  S. Liu","ShortDetails":"2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC). 2017","abstract":"This paper introduces an OSA detection method based on Recurrent Neural network. At the first step, RR interval (time interval from one R wave to the next R wave) is employed to extract the signals from Apnea- Electrocardiogram (ECG) where all extracted features are then used as an input for the designed deep model. Then an architecture having four recurrent layers and batch normalization layers are designed and trained with the extracted features for OSA detection. Apnea-ECG datasets from physionet.org are used for training and testing our model. Experimental results reveal that our automatic OSA detection model provides better classification accuracy.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8005997","source":"ieee","year":2017,"key":"5a787fc5-bb37-40e8-8216-71cae2234444","use":1,"doi":"10.1109\/CSE-EUC.2017.220"},{"Title":"A review on emotion recognition using speech","Description":"S. Basu,  J. Chakraborty,  A. Bag,  M. Aftabuddin","ShortDetails":"2017 International Conference on Inventive Communication and Computational Technologies (ICICCT). 2017","abstract":"Emotion recognition or affect detection from speech is an old and challenging problem in the field of artificial intelligence. Many significant research works have been done on emotion recognition. In this paper, the recent works on affect detection using speech and different issues related to affect detection has been presented. The primary challenges of emotion recognition are choosing the emotion recognition corpora (speech database), identification of different features related to speech and an appropriate choice of a classification model. Different types of methods to collect emotional speech data and issues related to them are covered by this presentation along with the previous works review. Literature survey on different features used for recognizing emotion from human speech has been discussed. The significance of various classification models has been presented along with some recent research works review. A detailed description of a prime feature extraction technique named Mel Frequency Cepstral Coefficient (MFCC) and brief description of the working principle of some classification models are also discussed here. In this paper terms like affect detection and emotion recognition are used interchangeably.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7975169","source":"ieee","year":2017,"key":"fb159185-021c-44c0-b912-a1068bf18998","use":1,"doi":"10.1109\/ICICCT.2017.7975169"},{"Title":"Recurrent neural network based retinal nerve fiber layer defect detection in early glaucoma","Description":"R. Panda,  N. B. Puhan,  A. Rao,  D. Padhy,  G. Panda","ShortDetails":"2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017). 2017","abstract":"Retinal nerve fiber layer defect (RNFLD) is the earliest objective evidence of glaucoma in fundus images. Glaucoma is an optic neuropathy which causes irreversible vision impairment. Early glaucoma detection and its prevention are the only way to prevent further damage to human vision. In this paper, we propose a new automated method for RNFLD detection in fundus images through patch features driven recurrent neural network (RNN). A new dataset of fundus images is created for evaluation purpose which contains several challenging RNFLD boundaries. The true boundary pixels are classified using the RNN trained by novel cumulative zero count local binary pattern (CZC-LBP), directional differential energy (DDE) patch features. The experimental results demonstrate high RNFLD detection rate along with accurate boundary localization.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7950614","source":"ieee","year":2017,"key":"b5995c32-30f4-4a20-8b85-13ada10cf919","use":1,"doi":"10.1109\/ISBI.2017.7950614"},{"Title":"ECG-based biometrics using recurrent neural networks","Description":"R. Salloum,  C. C. J. Kuo","ShortDetails":"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017","abstract":"In this paper, we propose the use of recurrent neural networks (RNNs) to develop an effective solution to two problems in electrocardiogram (ECG)-based biometrics: identification\/classification and authentication. Different RNN architectures with various parameter settings were evaluated, including traditional, long short-term memory (LSTM), gated recurrent unit (GRU), unidirectional, and bidirectional networks. Unlike many existing methods, the RNN-based method does not require any feature extraction. The method was evaluated using two publicly available datasets: ECG-ID and MIT-BIH Arrhythmia (MITDB). For the identification problem, nearly 100% classification accuracy on the ECG-ID dataset was achieved, and similar results were observed for the MITDB dataset. For the authentication problem, an RNN was trained and the hidden state at the final time step was extracted to make a decision. We evaluated the effect of the training size on the equal error rate (EER), and showed that the EER drops from approximately 3.5% to nearly 0% as we increased the percentage of subjects used for training from approximately 15% to 80%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7952519","source":"ieee","year":2017,"key":"ea55d0bd-3675-4fd6-9a4e-69566068a27f","use":1,"doi":"10.1109\/ICASSP.2017.7952519"},{"Title":"Instantaneous heart rate as a robust feature for sleep apnea severity detection using deep learning","Description":"R. K. Pathinarupothi,  R. Vinaykumar,  E. Rangan,  E. Gopalakrishnan,  K. P. Soman","ShortDetails":"2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). 2017","abstract":"Automated sleep apnea detection and severity identification has largely focused on multivariate sensor data in the past two decades. Clinically too, sleep apnea is identified using a combination of markers including blood oxygen saturation, respiration rate etc. More recently, scientists have begun to investigate the use of instantaneous heart rates for detection and severity measurement of sleep apnea. However, the best-known techniques that use heart rate and its derivatives have been able to achieve less than 85% accuracy in classifying minute-to-minute apnea data. In our research reported in this paper, we apply a deep learning technique called LSTM-RNN (long short-term memory recurrent neural network) for identification of sleep apnea and its severity based only on instantaneous heart rates. We have tested this model on multiple sleep apnea datasets and obtained perfect accuracy. Furthermore, we have also tested its robustness on an arrhythmia dataset (that is highly probable in mimicking sleep apnea heart rate variability) and found that the model is highly accurate in distinguishing between the two.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7897263","source":"ieee","year":2017,"key":"b89928a5-6ea0-4273-9bf8-fc1a44b1578b","use":1,"doi":"10.1109\/BHI.2017.7897263"},{"Title":"Using deep gated RNN with a convolutional front end for end-to-end classification of heart sound","Description":"C. Thomae,  A. Dominik","ShortDetails":"2016 Computing in Cardiology Conference (CinC). 2016","abstract":"Classification of heart sounds of a diverse set of phono-cardiograms (PCGs) from different recording settings is the challenging objective of the 2016 PhysioNet Challenge. We suggest an end-to-end deep neural network, which is fed with raw PCGs and which learns to autonomously extract features and to classify the recordings. Our architecture combines convolutional and recurrent layers, followed by an attention mechanism, which weights time steps by importance and a dense multilayer perceptron as classifier. Whereas currently trending deep neural networks in speech recognition or computer vision use up to a million of training samples, a restricted set of only 3,153 heart sound recordings is available as training data. We workaround this limitation by artificially increasing the training set by means of augmentation of the raw PCGs using various audio effects. Using this moderately sized neural network, we attain high validation scores of 0.89 on validation data; however the resulting scores on the hidden test data ofthe challenge diverge in range (0.82).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7868820","source":"ieee","year":2016,"key":"9f11b0ac-7c43-4415-9020-c517604c11bd","use":1,"doi":"10.23919\/CIC.2016.7868820"},{"Title":"Predicting Clinical Events by Combining Static and Dynamic Information Using Recurrent Neural Networks","Description":"C. Esteban,  O. Staeck,  S. Baier,  Y. Yang,  V. Tresp","ShortDetails":"2016 IEEE International Conference on Healthcare Informatics (ICHI). 2016","abstract":"In clinical data sets we often find static information (e.g. patient gender, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach based on RNNs, specifically designed for the clinical domain, that combines static and dynamic information in order to predict future events. We work with a database collected in the Charite\u0301 Hospital in Berlin that contains complete information concerning patients that underwent a kidney transplantation. After the transplantation three main endpoints can occur: rejection of the kidney, loss of the kidney and death of the patient. Our goal is to predict, based on information recorded in the Electronic Health Record of each patient, whether any of those endpoints will occur within the next six or twelve months after each visit to the clinic. We compared different types of RNNs that we developed for this work, with a model based on a Feedforward Neural Network and a Logistic Regression model. We found that the RNN that we developed based on Gated Recurrent Units provides the best performance for this task. We also used the same models for a second task, i.e., next event prediction, and found that here the model based on a Feedforward Neural Network outperformed the other models. Our hypothesis is that long-term dependencies are not as relevant in this task.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7776332","source":"ieee","year":2016,"key":"5a2099d9-dcb2-4ae7-979f-bce610a752d3","use":1,"doi":"10.1109\/ICHI.2016.16"},{"Title":"Combined long short-term memory based network employing wavelet coefficients for MI-EEG recognition","Description":"M. Li,  M. Zhang,  X. Luo,  J. Yang","ShortDetails":"2016 IEEE International Conference on Mechatronics and Automation. 2016","abstract":"Motor Imagery Electroencephalography (MI-EEG) plays an important role in brain computer interface (BCI) based rehabilitation robot, and its recognition is the key problem. The Discrete Wavelet Transform (DWT) has been applied to extract the time-frequency features of MI-EEG. However, the existing EEG classifiers, such as support vector machine (SVM), linear discriminant analysis (LDA) and BP network, did not make full use of the time sequence information in time-frequency features, the resulting recognition performance were not very ideal. In this paper, a Long Short-Term Memory (LSTM) based recurrent Neural Network (RNN) is integrated with Discrete Wavelet Transform (DWT) to yield a novel recognition method, denoted as DWT-LSTM. DWT is applied to analyze the each channel of MI-EEG and extract its effective wavelet coefficients, representing the time-frequency features. Then a LSTM based RNN is used as a classifier for the patten recognition of observed MI-EEG data. Experiments are conducted on a publicly available dataset, and the 5-fold cross validation experimental results show that DWT-LSTM yields relatively higher classification accuracies compared to the existing approaches. This is helpful for the further research and application of RNN in processing of MI-EEG.","email":["limingai@bjut.edu.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7558868","source":"ieee","year":2016,"key":"3f208bb7-07ae-4705-b837-344b29743e31","use":1,"doi":"10.1109\/ICMA.2016.7558868"},{"Title":"Modelling the indentation force response of non-uniform soft tissue using a recurrent neural network","Description":"R. Nowell,  B. Shirinzadeh,  J. Smith,  Y. Zhong","ShortDetails":"2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob). 2016","abstract":"A scaled recurrent neural network (RNN) model is developed which accurately predicts the force response from the indentation of a non-uniform soft tissue sample. The model consists of two components. The RNN is used to predict the force response of indentation using data from a reference tissue sample. A two-parameter component then scales the neural networks predictions relative to previously determined properties of the test sample. This component is based on a strain inverse model of force, which is used to account for the non-uniformity of the tissue between the test and reference data. Experimental force measurements were performed on a highly non-uniform soft tissue analogue to develop and validate the model. Using the visco-elastic Hunt-Crossley model as a benchmark, the developed model provides significantly better prediction. Future research will investigate applying this model to surgical simulations and verifying its application to different biological tissues.","email":["Australia.rohan.nowell2@monash.edu2BijanShirinzadehiswiththeDepartmentofMechanicalandAerospaceEngineering"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7523655","source":"ieee","year":2016,"key":"e2194be6-e3f0-44dd-9de3-b265e5ff26b9","use":1,"doi":"10.1109\/BIOROB.2016.7523655"},{"Title":"EEG signal analysis based on fixed-value shift compression algorithm","Description":"Xueyan Guo,  Huanyu Zhao,  Xiaoyun Li,  Tongliang Li,  Mingfang Dai","ShortDetails":"2015 11th International Conference on Natural Computation (ICNC). 2015","abstract":"The analysis of Electroencephalogram (EEG) signals plays a very important role in the biomedical domain and has many applications. It is extensively used in the Brain-Computer Interface (BCI) system and can be used for disease diagnosis, disease treatment, etc. The two main technologies of EEG signal analysis is feature extraction and pattern recognition. The key features of EEG signals can be obtained through time-domain and frequency-domain analysis. The wavelet analysis is one kind of time-frequency analysis and has been considered very promising for data compression. The conventional method find wavelet synopsis to minimize the total mean squared error (L2). It cannot control the approximation error of each single data element in the data vector. Usually, the nonlinear classification algorithms perform better than the linears, also more time-consuming in the meantime. In this paper, one method is provided to realize the feature extraction and pattern recognition of EEG signals. The data compression algorithm Fixed-value Shift (F-Shift) proposed by Pang et al. takes a novel method to construct unrestricted Haar wavelet synopsis under uniform norm (L\u221e) error bound. In their algorithm, the maximum approximation error of each individual element can be bounded by an given error bound. We apply this method to EEG signal compression, thus the key features are obtained. Then a fast nonlinear classification algorithm, one Randomize Neural Network (RNN), is provided to identify different patterns of EEG signals. The experiments indicate that (1) the F-Shift algorithm can compress EEG signals effectively and obtain the key features at the same time and (2) the RNN can discriminate different patterns of EEG signals based on the extracted features.","email":["guoxueyan1989@126.comHuanyuZhaoandXiaoyunLiSJZJKSSTechnologyCo.Ltd","g.litongliang@gmail.comMingfangDaiInformationCenterofNingboCity","g.litongliang@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7378121","source":"ieee","year":2015,"key":"1868261b-e186-4831-8760-8acd99595fe6","use":1,"doi":"10.1109\/ICNC.2015.7378121"},{"Title":"Biomedical named entity recognition based on extended Recurrent Neural Networks","Description":"Lishuang Li,  Liuke Jin,  Zhenchao Jiang,  Dingxin Song,  Degen Huang","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"Biomedical named entity recognition (bio-NER), which extracts important entities such as genes and proteins, has become one of the most fundamental tasks in biomedical knowledge acquisition. However, the performance of traditional NER systems is always limited to the construction of complex hand-designed features which are derived from various linguistic analyses and maybe only adapted to specified area. In this paper we mainly focus on building a simple and efficient system for bio-NER with the extended Recurrent Neural Network (RNN) which considers the predicted information from the prior node and external context information (topical information & clustering information). Extracting complex hand-designed features is skipped and replaced with word embeddings. The experiments conducted on the BioCreative II GM data set demonstrate RNN models outperform CRF model and deep neural networks (DNN); furthermore, the extended RNN model performs better than the original RNN model.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359761","source":"ieee","year":2015,"key":"ce532ad1-29d6-4290-b49f-495dcb906490","use":1,"doi":"10.1109\/BIBM.2015.7359761"},{"Title":"Multi task sequence learning for depression scale prediction from video","Description":"L. Chao,  J. Tao,  M. Yang,  Y. Li,  J. Tao","ShortDetails":"2015 International Conference on Affective Computing and Intelligent Interaction (ACII). 2015","abstract":"Depression is a typical mood disorder, which affects people in mental and even physical problems. People who suffer depression always behave abnormal in visual behavior and the voice. In this paper, an audio visual based multimodal depression scale prediction system is proposed. Firstly, features are extracted from video and audio are fused in feature level to represent the audio visual behavior. Secondly, long short memory recurrent neural network (LSTM-RNN) is utilized to encode the dynamic temporal information of the abnormal audio visual behavior. Thirdly, emotion information is utilized by multi-task learning to boost the performance further. The proposed approach is evaluated on the Audio-Visual Emotion Challenge (AVEC2014) dataset. Experiments results show the dimensional emotion recognition helps to depression scale prediction.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7344620","source":"ieee","year":2015,"key":"ac836ae0-c3a5-42eb-a138-06b282c406a9","use":1,"doi":"10.1109\/ACII.2015.7344620"},{"Title":"Inferring the dynamics of gene regulatory networks via optimized recurrent neural network and dynamic Bayesian network","Description":"A. Akutekwe,  H. Seker","ShortDetails":"2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB). 2015","abstract":"Inferring gene regulatory networks (GRNs) from time-course expression data is a major challenge in systems biology and comprehensive understanding of its dynamics is difficult. Most temporal inference methods for the dynamics of GRNs assume linear dependencies among genes but this strong assumption of linearity among genes does not truly represent the dynamics of the GRNs which are inherently nonlinear. Other parametric and non-parametric methods for modeling nonlinear dynamical systems such as the S-systems and causal identification structure (CSI) have been proposed for modeling time-course nonlinearities in GRNs; however, these methods are statistically inefficient and analytically intractable especially in high dimensions. To overcome these problems, we propose an algorithm based on optimized recurrent neural network (RNN) and dynamic Bayesian (DBN) network called RNN-DBN. The inference algorithm for our DBN is based on nonlinear state space Elman recurrent neural network. Results on Drosophila Melanogaster nonlinear time-course benchmark dataset shows our method outperforms the G1DBN inference method based on linear model assumptions. The algorithm is further applied to time-course ovarian cancer dataset. The results show that the expression levels of three of five significant hub genes (flap structure-specific endonuclease 1, kinesin family member 11 and CDC6 cell division cycle 6 homolog (S. cerevisiae)) were decreased by oxaliplatin, but remained constant with cisplatin platinum drugs. These may therefore be potential drug candidates for ovarian cancer.","email":["arinze.akutekwe@northumbria.ac.uk","huseyin.seker@northumbria.ac.uk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7300322","source":"ieee","year":2015,"key":"36be8334-4a32-47e4-adad-e9444984ea6c","use":1,"doi":"10.1109\/CIBCB.2015.7300322"},{"Title":"Multi-model data fusion to improve an early warning system for hypo-\/hyperglycemic events","Description":"R. H. Botwey,  E. Daskalaki,  P. Diem,  S. G. Mougiakakou","ShortDetails":"2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 2014","abstract":"Correct predictions of future blood glucose levels in individuals with Type 1 Diabetes (T1D) can be used to provide early warning of upcoming hypo-\/hyperglycemic events and thus to improve the patient's safety. To increase prediction accuracy and efficiency, various approaches have been proposed which combine multiple predictors to produce superior results compared to single predictors. Three methods for model fusion are presented and comparatively assessed. Data from 23 T1D subjects under sensor-augmented pump (SAP) therapy were used in two adaptive data-driven models (an autoregressive model with output correction - cARX, and a recurrent neural network - RNN). Data fusion techniques based on i) Dempster-Shafer Evidential Theory (DST), ii) Genetic Algorithms (GA), and iii) Genetic Programming (GP) were used to merge the complimentary performances of the prediction models. The fused output is used in a warning algorithm to issue alarms of upcoming hypo-\/hyperglycemic events. The fusion schemes showed improved performance with lower root mean square errors, lower time lags, and higher correlation. In the warning algorithm, median daily false alarms (DFA) of 0.25%, and 100% correct alarms (CA) were obtained for both event types. The detection times (DT) before occurrence of events were 13.0 and 12.1 min respectively for hypo-\/hyperglycemic events. Compared to the cARX and RNN models, and a linear fusion of the two, the proposed fusion schemes represents a significant improvement.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6944708","source":"ieee","year":2014,"key":"cb0e2339-553f-4ea1-9dd0-6688545c592f","use":1,"doi":"10.1109\/EMBC.2014.6944708"},{"Title":"Non-invasive detection of hypoglycemic episodes in Type 1 diabetes using intelligent hybrid rough neural system","Description":"S. H. Ling,  P. P. San,  H. K. Lam,  H. T. Nguyen","ShortDetails":"2014 IEEE Congress on Evolutionary Computation (CEC). 2014","abstract":"Insulin-dependent diabetes mellitus is classified as Type 1 diabetes and it can be further classified as immune-mediated or idiopathic. Through the analysis of electrocar-diographic (ECG) signals of 15 children with T1DM, an effective hypoglycemia detection system, hybrid rough set based neural network (RNN) is developed by the use of physiological parameters of ECG signal. In order to detect the status of hypoglycemia, the feature of ECG of type 1 diabetics are extracted and classified according to corresponding glucose levels. In this technique, the applied physiological inputs are partitioned into predicted (certain) or random (uncertain) parts using defined lower and boundary of rough regions. In this way, the neural network is designed to deal only with the boundary region which mainly consists of a random part of applied input signal causing inaccurate modeling of the data set. A global training algorithm, hybrid particle swarm optimization with wavelet mutation (HPSOWM) is introduced for parameter optimization of proposed RNN. The experiment is carried out using real data collected at Department of Health, Government of Western Australia. It indicated that the proposed hybrid architecture is efficient for hypoglycemia detection by achieving better sensitivity and specificity with less number of design parameters.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6900229","source":"ieee","year":2014,"key":"ae9fd40b-a8b7-4b3a-8c40-f441e7330d5b","use":1,"doi":"10.1109\/CEC.2014.6900229"},{"Title":"Capturing Human Body Dynamics Using RNN Based on Persistent Excitation Data Generator","Description":"A. Abdulrahman,  K. Iqbal","ShortDetails":"2014 IEEE 27th International Symposium on Computer-Based Medical Systems. 2014","abstract":"Human body walking movement involves both single and double support phases and is considered difficult to model. The aim of this study was to develop a method to capture human body dynamics during walking using Recurrent Neural Networks (RNN). In addition, a novel method using persistent excitation data generator is proposed to generate kinematic data to train the RNN in the absence of laboratory measurements. Kinematic data were applied to human body mathematical model to obtain required joint torques during bipedal walking. The RNN was used to approximate human body kinematics resulting from the joints torques for the walking movement. In order to test validity of the RNN model, model output was compared with human walking data captured in the laboratory. Simulation results show the model was able to approximate the joint angles during human walk with a low (10<sup>-4<\/sup> m) mean squared error for one stride.","email":["amabdulrahma@ualr.edu","alaa_mohi2002@yahoo.com","kxiqbal@ualr.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6881880","source":"ieee","year":2014,"key":"d1a9a323-c661-4269-aecc-803de46cf634","use":1,"doi":"10.1109\/CBMS.2014.145"},{"Title":"Muscle Fatigue Tracking with Evoked EMG via Recurrent Neural Network: Toward Personalized Neuroprosthetics","Description":"Z. Li,  M. Hayashibe,  C. Fattal,  D. Guiraud","ShortDetails":"IEEE Computational Intelligence Magazine. 2014","abstract":"One of the challenging issues in computational rehabilitation is that there is a large variety of patient situations depending on the type of neurological disorder. Human characteristics are basically subject specific and time variant; for instance, neuromuscular dynamics may vary due to muscle fatigue. To tackle such patient specificity and time-varying characteristics, a robust bio-signal processing and a precise model-based control which can manage the nonlinearity and time variance of the system, would bring break-through and new modality toward computational intelligence (CI) based rehabilitation technology and personalized neuroprosthetics. Functional electrical stimulation (FES) is a useful technique to assist restoring motor capability of spinal cord injured (SCI) patients by delivering electrical pulses to paralyzed muscles. However, muscle fatigue constraints the application of FES as it results in the time-variant muscle response. To perform adaptive closedloop FES control with actual muscle response feedback taken into account, muscular torque is essential to be estimated accurately. However, inadequacy of the implantable torque sensor limits the direct measurement of the time-variant torque at the joint. This motivates the development of methods to estimate muscle torque from bio-signals that can be measured. Evoked electromyogram (eEMG) has been found to be highly correlated with FES-induced torque under various muscle conditions, indicating that it can be used for torque\/force prediction. A nonlinear ARX (NARX) type model is preferred to track the relationship between eEMG and stimulated muscular torque. This paper presents a NARX recurrent neural network (NARX-RNN) model for identification\/prediction of FES-induced muscular dynamics with eEMG. The NARX-RNN model may possess novelty of robust prediction performance. Due to the difficulty of choosing a proper forgetting factor of Kalman filter for predicting time-variant torque with eEMG, the presente- NARX-RNN could be considered as an alternative muscular torque predictor. Data collected from five SCI patients is used to evaluate the proposed NARX-RNN model, and the results show promising estimation performances. In addition, the general importance regarding CI-based motor function modeling is introduced along with its potential impact in the rehabilitation domain. The issue toward personalized neuroprosthetics is discussed in detail with the potential role of CI-based identification and the benefit for motor-impaired patient community.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6786477","source":"ieee","year":2014,"key":"e16d314e-c36d-4505-8ed8-a69d25bc244f","use":1,"doi":"10.1109\/MCI.2014.2307224"},{"Title":"Automatic Detection and Classification of Colorectal Polyps by Transferring Low-Level CNN Features From Nonmedical Domain","Description":"R. Zhang,  Y. Zheng,  T. W. C. Mak,  R. Yu,  S. H. Wong,  J. Y. W. Lau,  C. C. Y. Poon","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7769237","source":"ieee","year":2017,"key":"b1f5a3ef-6d84-46e3-98ee-d5fd69f52629","use":1,"doi":"10.1109\/JBHI.2016.2635662"},{"Title":"Comparison of hand-craft feature based SVM and CNN based deep learning framework for automatic polyp classification","Description":"Y. Shin,  I. Balasingham","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Colonoscopy is a standard method for screening polyps by highly trained physicians. Miss-detected polyps in colonoscopy are potential risk factor for colorectal cancer. In this study, we investigate an automatic polyp classification framework. We aim to compare two different approaches named hand-craft feature method and convolutional neural network (CNN) based deep learning method. Combined shape and color features are used for hand craft feature extraction and support vector machine (SVM) method is adopted for classification. For CNN approach, three convolution and pooling based deep learning framework is used for classification purpose. The proposed framework is evaluated using three public polyp databases. From the experimental results, we have shown that the CNN based deep learning framework shows better classification performance than the hand-craft feature based methods. It achieves over 90% of classification accuracy, sensitivity, specificity and precision.","email":["cpoon@surgery.cuhk.edu.hk"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037556","source":"ieee","year":2017,"key":"99560710-149c-40ca-be3b-03b9161fe12d","use":1,"doi":"10.1109\/EMBC.2017.8037556"},{"Title":"Combining Convolutional and Recurrent Neural Networks for Human Skin Detection","Description":"H. Zuo,  H. Fan,  E. Blasch,  H. Ling","ShortDetails":"IEEE Signal Processing Letters. 2017","abstract":"Skin detection from images, typically used as a preprocessing step, has a wide range of applications such as dermatology diagnostics, human computer interaction designs, and etc. It is a challenging problem due to many factors such as variation in pigment melanin, uneven illumination, and differences in ethnicity geographics. Besides, age and gender introduce additional difficulties to the detection process. It is hard to determine whether a single pixel is skin or nonskin without considering the context. An efficient traditional hand-engineered skin color detection algorithm requires extensive work by domain experts. Recently, deep learning algorithms, especially convolutional neural networks (CNNs), have achieved great success in pixel-wise labeling tasks. However, CNN-based architectures are not sufficient for modeling the relationship between pixels and their neighbors. In this letter, we integrate recurrent neural networks (RNNs) layers into the fully convolutional neural networks (FCNs), and develop an end-to-end network for human skin detection. In particular, FCN layers capture generic local features, while RNN layers model the semantic contextual dependencies in images. Experimental results on the COMPAQ and ECU skin datasets validate the effectiveness of the proposed approach, where RNN layers enhance the discriminative power of skin detection in complex background situations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7820144","source":"ieee","year":2017,"key":"69c327df-1e4b-44ca-bd5c-3b9e7094d1e0","use":1,"doi":"10.1109\/LSP.2017.2654803"},{"Title":"Human induced pluripotent stem cell region recognition in microscopy images using Convolutional Neural Networks","Description":"Y. H. Chang,  K. Abe,  H. Yokota,  K. Sudo,  Y. Nakamura,  C. Y. Lin,  M. D. Tsai","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"We present a deep learning architecture Convolutional Neural Networks (CNNs) for automatic classification and recognition of reprogramming and reprogrammed human Induced Pluripotent Stem (iPS) cell regions in microscopy images. The differentiated cells that possibly undergo reprogramming to iPS cells can be detected by this method for screening reagents or culture conditions in iPS induction. The learning results demonstrate that our CNNs can achieve the Top-1 and Top-2 error rates of 9.2% and 0.84%, respectively, to produce probability maps for the automatic analysis. The implementation results show that this automatic method can successfully detect and localize the human iPS cell formation, thereby yield a potential tool for helping iPS cell culture.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037747","source":"ieee","year":2017,"key":"0323d8bc-bfea-4d91-a735-de81d1be58ad","use":1,"doi":"10.1109\/EMBC.2017.8037747"},{"Title":"Spatiotemporal Joint Mitosis Detection Using CNN-LSTM Network in Time-Lapse Phase Contrast Microscopy Images","Description":"Y. T. Su,  Y. Lu,  M. Chen,  A. A. Liu","ShortDetails":"IEEE Access. 2017","abstract":"We present an approach to jointly detect mitotic events spatially and temporally in time-lapse phase contrast microscopy images. In particular, we combine a Convolutional Neural Network (CNN) and a Long Short Term Memory network (LSTM) to detect mitotic events in patch sequences. The CNN-LSTM network can be trained end-to-end to simultaneously learn convolutional features within each frame and temporal dynamics between frames, without hand-crafted visual or temporal feature design. Owing to the LSTM layer, this approach is able to detect mitotic events in patch sequences of variable length, as well as making use of longer context information among frames in the sequences. To the best of our knowledge, this is the first work to detect mitosis using deep learning in both spatial and temporal domains. Experiments have shown that the CNN-LSTM network can be trained efficiently, and we evaluate this design by applying the network to original raw microscopy image sequences to locate mitotic events both spatially and temporally. The data we validate the proposed method on include C3H10 mesenchymal and C2C12 myoblastic stem cell populations. Our approach achieved the F score of 98.72% on the C2C12 dataset, and the F score of 96.5% on the C3H10 dataset. The results on both datasets outperform traditional graph model based approaches by a large margin, both in terms of detection accuracy and frame localization accuracy. Furthermore, we have developed a framework to aid humans in annotating mitosis with high efficiency and accuracy in raw phase contrast microscopy images based on the joint detection results using the proposed method. Under this framework, expert level annotations can be obtained in raw phase contrast microscopy image sequences, and the annotations have shown to further improve the training performance of CNN-LSTM network.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8019789","source":"ieee","year":2017,"key":"1db246f7-26b4-404f-9659-b690e77af9b4","use":1,"doi":"10.1109\/ACCESS.2017.2745544"},{"Title":"An efficient method for neuronal tracking in electron microscopy images","Description":"L. Yin,  C. Xiao,  Q. Xie,  X. Chen,  L. Shen,  H. Han","ShortDetails":"2017 IEEE International Conference on Mechatronics and Automation (ICMA). 2017","abstract":"With the introduction of deep learning, a wave of artificial intelligence research has been set off again. Scientists focus on brain-inspired intelligence, namely, try to get inspiration from the brain nervous system and cognitive behavior mechanism, to develop intelligent computing models as well as algorithms with stronger information representation, processing and learning ability. So, the study of neurons and the connections between neurons of brain are needed. One major obstacle of reconstruction lies in segmenting and tracking neuronal processes. Electron microscopy is producing neurons images rapidly. In response, we propose an efficient method for neuronal tracking in electron microscopy images to help scientists reconstruct complex neurons. First, we track neurons by kernelized correlation filter to get candidate neuron; then we calculate overlap area and distance of the contours between two consecutive images to get final neuron. We evaluate the performance of our method on a public electron microscopy dataset. The method is superior in accuracy and efficiency.","email":["zhqupc@upc.edu.cn","hengfan@temple.edu","hbling@temple.edu","erik.blasch@rl.af.mil"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8016102","source":"ieee","year":2017,"key":"96a3a0c6-4a24-4fb5-b227-cf6179bb5844","use":1,"doi":"10.1109\/ICMA.2017.8016102"},{"Title":"Deep convolutional neural networks for detecting secondary structures in protein density maps from cryo-electron microscopy","Description":"R. Li,  D. Si,  T. Zeng,  S. Ji,  J. He","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"The detection of secondary structure of proteins using three dimensional (3D) cryo-electron microscopy (cryo-EM) images is still a challenging task when the spatial resolution of cryo-EM images is at medium level (5-10\u212b). Prior researches focused on the usage of local features that may not capture the global information of image objects. In this study, we propose to use deep learning methods to extract high representative global features and then automatically detect secondary structures of proteins. In particular, we build a convolutional neural network (CNN) classifier that predicts the probability of label for every individual voxel in 3D cryo-EM image with respect to the secondary structure elements of proteins such as \u03b1-helix, \u03b2-sheet and background. To effectively incorporate the 3D spatial information in protein structures, we propose to perform 3D convolutions in the convolutional layers of CNNs. We show that the proposed CNN classifier can outperform existing SVM method on identifying the secondary structure elements of proteins from 3D cryo-EM medium resolution images.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822490","source":"ieee","year":2016,"key":"8a0386db-af39-4557-8423-8ef6ef964d48","use":1,"doi":"10.1109\/BIBM.2016.7822490"},{"Title":"Nuclear Architecture Analysis of Prostate Cancer via Convolutional Neural Networks","Description":"J. T. Kwak,  S. M. Hewitt","ShortDetails":"IEEE Access. 2017","abstract":"In this paper, we present an approach of convolutional neural networks (CNNs) to identify prostate cancers. Prostate tissue specimen samples were obtained from tissue microarrays and digitized. For each sample, epithelial nuclear seeds were identified and used to generate a nuclear seed map, i.e., only the location information of epithelial nuclei were utilized. From the nuclear seed maps, CNNs sought to learn the high-level feature representation of nuclear architecture and to detect cancers. Applying data augmentation technique, CNNs were trained on the training dataset including 73 benign and 89 cancer samples and validated on the testing dataset comprising 217 benign and 274 cancer samples. In detecting cancers, CNNs achieved an AUC of 0.974 (95% CI: 0.961-0.985). In comparison to the approaches of utilizing hand-crafted nuclear architecture features and the state of the art deep learning networks with standard machine learning methods, CNNs were significantly superior to them (p-value<5e-2). Moreover, stromal nuclei were incapable of improving the cancer detection performance. The experimental results suggest that our approach offers the ability to aid in improving prostate cancer pathology.","email":["co-correspondingauthors.Thecontactemailsaresji@eecs.wsu.edu","andjhe@cs.odu.edurespectively.Abstract"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8023758","source":"ieee","year":2017,"key":"d33d6bca-163e-4a91-8985-9657a27827b5","use":1,"doi":"10.1109\/ACCESS.2017.2747838"},{"Title":"Microscopic Blood Smear Segmentation and Classification Using Deep Contour Aware CNN and Extreme Machine Learning","Description":"M. I. Razzak,  S. Naz","ShortDetails":"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017","abstract":"Recent advancement in genomics technologies has opened a new realm for early detection of diseases that shows potential to overcome the drawbacks of manual detection technologies. In this work, we have presented efficient contour aware segmentation approach based based on fully conventional network whereas for classification we have used extreme machine learning based on CNN features extracted from each segmented cell. We have evaluated system performance based on segmentation and classification on publicly available dataset. Experiment was conducted on 64000 blood cells and dataset is divided into 80% for training and 20% for testing. Segmentation results are compared with the manual segmentation and found that proposed approach provided with 98.12% and 98.16% for RBC and WBC respectively whereas classification accuracy is shown on publicly available dataset 94.71% and 98.68% for RBC & its abnormalities detection and WBC respectively.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8014845","source":"ieee","year":2017,"key":"e4a574b5-37e7-41ce-9f49-8d6bd08437f5","use":1,"doi":"10.1109\/CVPRW.2017.111"},{"Title":"Crowdsourcing for Chromosome Segmentation and Deep Classification","Description":"M. Sharma,  O. Saha,  A. Sriraman,  R. Hebbalaguppe,  L. Vig,  S. Karande","ShortDetails":"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 2017","abstract":"Metaphase chromosome analysis is one of the primary techniques utilized in cytogenetics. Observations of chromosomal segments or translocations during metaphase can indicate structural changes in the cell genome, and is often used for diagnostic purposes. Karyotyping of the chromosomes micro-photographed under metaphase is done by characterizing the individual chromosomes in cell spread images. Currently, considerable effort and time is spent to manually segment out chromosomes from cell images, and classifying the segmented chromosomes into one of the 24 types, or for diseased cells to one of the known translocated types. Segmenting out the chromosomes in such images can be especially laborious and is often done manually, if there are overlapping chromosomes in the image which are not easily separable by image processing techniques. Many techniques have been proposed to automate the segmentation and classification of chromosomes from spread images with reasonable accuracy, but given the criticality of the domain, a human in the loop is often still required. In this paper, we present a method to segment out and classify chromosomes for healthy patients using a combination of crowdsourcing, preprocessing and deep learning, wherein the non-expert crowd from CrowdFlower is utilized to segment out the chromosomes from the cell image, which are then straightened and fed into a (hierarchical) deep neural network for classification. Experiments are performed on 400 real healthy patient images obtained from a hospital. Results are encouraging and promise to significantly reduce the cognitive burden of segmenting and karyotyping chromosomes.","email":["saeedanaz292@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8014843","source":"ieee","year":2017,"key":"fa5a56f0-10a1-4bba-b555-d51638ffc765","use":1,"doi":"10.1109\/CVPRW.2017.109"},{"Title":"Mitosis Detection in Phase Contrast Microscopy Image Sequences of Stem Cell Populations: A Critical Review","Description":"A. A. Liu,  Y. Lu,  M. Chen,  Y. Su","ShortDetails":"IEEE Transactions on Big Data. 2017","abstract":"Detecting mitosis from cell population is a fundamental problem in many biological researches and biomedical applications. In modern researches, advanced imaging technologies have been applied to generate large amount of microscope images of cells. However, detecting all mitotic cells from these images with human eye is tedious and time-consuming. In recent years, several approaches have been proposed to help humans finish this job automatically with high efficiency and accuracy. In this review paper, we first described some commonly used datasets for mitosis detection, and then discussed different kinds of methods for mitosis detection, like tracking based methods, tracking free methods, hybrid methods, and the most recently proposed works based on deep learning architecture. We compared these methods on same datasets, and found that deep learning based approaches have achieved a great improvement in performance. At last, we discussed the future possible approaches on mitosis detection, to combine the success of previous works and the advantage of big data in modern researches. Considering expertise is highly required in biomedical area, we will further discuss the possibility to learn information from biomedical big data with less expert annotation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7962189","source":"ieee","year":2017,"key":"95a94fab-e8d5-4e68-85b7-ff32337e26bc","use":1,"doi":"10.1109\/TBDATA.2017.2721438"},{"Title":"Malaria Parasite Detection From Peripheral Blood Smear Images Using Deep Belief Networks","Description":"D. Bibin,  M. S. Nair,  P. Punitha","ShortDetails":"IEEE Access. 2017","abstract":"In this paper, we propose a novel method to identify the presence of malaria parasites in human peripheral blood smear images using a deep belief network (DBN). This paper introduces a trained model based on a DBN to classify 4100 peripheral blood smear images into the parasite or non-parasite class. The proposed DBN is pre-trained by stacking restricted Boltzmann machines using the contrastive divergence method for pre-training. To train the DBN, we extract features from the images and initialize the visible variables of the DBN. A concatenated feature of color and texture is used as a feature vector in this paper. Finally, the DBN is discriminatively fine-tuned using a backpropagation algorithm that computes the probability of class labels. The optimum size of the DBN architecture used in this paper is 484-600-600-600-600-2, in which the visible layer has 484 nodes and the output layer has two nodes with four hidden layers containing 600 hidden nodes in every layer. The proposed method has performed significantly better than the other state-of-the-art methods with an F-score of 89.66%, a sensitivity of 97.60%, and specificity of 95.92%. This paper is the first application of a DBN for malaria parasite detection in human peripheral blood smear images.","email":["dh.bibin@gmail.com.","nair2001@yahoo.com.","punithaswamy@gmail.com."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7931565","source":"ieee","year":2017,"key":"f7a43005-dd96-4140-87d0-54fe4be74dbf","use":1,"doi":"10.1109\/ACCESS.2017.2705642"},{"Title":"Cell proposal network for microscopy image analysis","Description":"S. U. Akram,  J. Kannala,  L. Eklund,  J. Heikkil\u00e4","ShortDetails":"2016 IEEE International Conference on Image Processing (ICIP). 2016","abstract":"Robust cell detection plays a key role in the development of reliable methods for automated analysis of microscopy images. It is a challenging problem due to low contrast, variable fluorescence, weak boundaries, conjoined and overlapping cells, causing most cell detection methods to fail in difficult situations. One approach for overcoming these challenges is to use cell proposals, which enable the use of more advanced features from ambiguous regions and\/or information from adjacent frames to make better decisions. However, most current methods rely on simple proposal generation and scoring methods, which limits the performance they can reach. In this paper, we propose a convolutional neural network based method which generates cell proposals to facilitate cell detection, segmentation and tracking. We compare our method against commonly used proposal generation and scoring methods and show that our method generates significantly better proposals, and achieves higher final recall and average precision.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7532950","source":"ieee","year":2016,"key":"4a164ebd-36a5-483b-9a06-0c4419a342d3","use":1,"doi":"10.1109\/ICIP.2016.7532950"},{"Title":"Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions","Description":"G. Carneiro,  T. Peng,  C. Bayer,  N. Navab","ShortDetails":"2015 IEEE International Conference on Computer Vision (ICCV). 2015","abstract":"We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7410438","source":"ieee","year":2015,"key":"3dd93434-cd70-44dc-a827-43bae96a2370","use":1,"doi":"10.1109\/ICCV.2015.81"},{"Title":"Staged Inference using Conditional Deep Learning for energy efficient real-time smart diagnosis","Description":"M. Parsa,  P. Panda,  S. Sen,  K. Roy","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Recent progress in biosensor technology and wearable devices has created a formidable opportunity for remote healthcare monitoring systems as well as real-time diagnosis and disease prevention. The use of data mining techniques is indispensable for analysis of the large pool of data generated by the wearable devices. Deep learning is among the promising methods for analyzing such data for healthcare applications and disease diagnosis. However, the conventional deep neural networks are computationally intensive and it is impractical to use them in real-time diagnosis with low-powered on-body devices. We propose Staged Inference using Conditional Deep Learning (SICDL), as an energy efficient approach for creating healthcare monitoring systems. For smart diagnostics, we observe that all diagnoses are not equally challenging. The proposed approach thus decomposes the diagnoses into preliminary analysis (such as healthy vs unhealthy) and detailed analysis (such as identifying the specific type of cardio disease). The preliminary diagnosis is conducted real-time with a low complexity neural network realized on the resource-constrained on-body device. The detailed diagnosis requires a larger network that is implemented remotely in cloud and is conditionally activated only for detailed diagnosis (unhealthy individuals). We evaluated the proposed approach using available physiological sensor data from Physionet databases, and achieved 38% energy reduction in comparison to the conventional deep learning approach.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036767","source":"ieee","year":2017,"key":"af361845-464e-42af-bf6d-5dc031a3820b","use":1,"doi":"10.1109\/EMBC.2017.8036767"},{"Title":"Comparing deep neural network and other machine learning algorithms for stroke prediction in a large-scale population-based electronic medical claims database","Description":"C. Y. Hung,  W. C. Chen,  P. T. Lai,  C. H. Lin,  C. C. Lee","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Electronic medical claims (EMCs) can be used to accurately predict the occurrence of a variety of diseases, which can contribute to precise medical interventions. While there is a growing interest in the application of machine learning (ML) techniques to address clinical problems, the use of deep-learning in healthcare have just gained attention recently. Deep learning, such as deep neural network (DNN), has achieved impressive results in the areas of speech recognition, computer vision, and natural language processing in recent years. However, deep learning is often difficult to comprehend due to the complexities in its framework. Furthermore, this method has not yet been demonstrated to achieve a better performance comparing to other conventional ML algorithms in disease prediction tasks using EMCs. In this study, we utilize a large population-based EMC database of around 800,000 patients to compare DNN with three other ML approaches for predicting 5-year stroke occurrence. The result shows that DNN and gradient boosting decision tree (GBDT) can result in similarly high prediction accuracies that are better compared to logistic regression (LR) and support vector machine (SVM) approaches. Meanwhile, DNN achieves optimal results by using lesser amounts of patient data when comparing to GBDT method.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037515","source":"ieee","year":2017,"key":"5a6d4dae-e860-4293-b2bd-bc539947ad77","use":1,"doi":"10.1109\/EMBC.2017.8037515"},{"Title":"Oro Vision: Deep Learning for Classifying Orofacial Diseases","Description":"R. Anantharaman,  V. Anantharaman,  Y. Lee","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"This experiment is an attempt to apply deep learning techniques to orofacial image analysis. Health promotion is recognized as a viable approach to preventing diseases and disorders and promoting changes in health behaviors or practices. Each year, oral cancer kills more people in the US than does cervical cancer, malignant melanoma, or Hodgkin's disease. A first line of defense against oral diseases is an orofacial selfexamination. The goal of this experiment titled \"Oro Vision\" is to provide an assessment tool for field workers to perform initial examinations of orofacial diseases, using a camera enabled mobile phone. For this experiment, we chose to implement Oro Vision to detect mouth sores. The goal is to extend this model to identify several other Oral diseases such as Thrush, Leukoplakia, Lichenplanus, etc. One variety of mouth sore, referred to as the \"cold sore\" is highly contagious and an infected person can easily pass on the infection to another person just through skin to skin contact. \"Oro Vision\" is implemented as an HTML5 mobile responsive web app that can be accessed through any mobile or standard browser. Oro Vision uses deep learning to train a model and subsequently uses this trained model to distinguish a cold sore from a canker sore. In addition, an accurate diagnosis by a trained healthcare professional is required before any kind of treatment is discussed since several other conditions of the mouth including oral cancer may mimic canker sores.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031130","source":"ieee","year":2017,"key":"52c5299a-cbf4-4e3f-9a69-318f1bd13a04","use":1,"doi":"10.1109\/ICHI.2017.69"},{"Title":"Continuous Assessment of Children\u2019s Emotional States Using Acoustic Analysis","Description":"Y. Gong,  C. Poellabauer","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Emotional and behavioral disorders (EBD) are a widespread healthcare concern in children and adolescents. Prevention and early intervention are the most powerful tools in ameliorating the problem, and therefore, timely and accurate detection of abnormal emotional patterns is of vital importance. In this paper, we propose a system that detects second-level emotional states of children using hour-level audio recordings. The proposed system consists of an audio segmentation and speaker tracking front-end along with an emotion recognition back-end. Supervised support vector machine is used in the front-end to improve its robustness to short and inconsistent child speech pattern and end-to-end deep learning is used in the emotion recognition back-end to improve its robustness to noise and segmentation error. We further demonstrate the potential of the proposed system as an automated emotion analysis tool.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031145","source":"ieee","year":2017,"key":"ba981377-7d17-4e9b-9a7d-eb3a5d6e0066","use":1,"doi":"10.1109\/ICHI.2017.53"},{"Title":"A Deep-Learning-Based Method of Estimating Water Intake","Description":"Y. Yamada,  T. Saito,  S. Kawasaki,  D. Iketa,  M. Katagiri,  M. Nishimura,  H. Mineno","ShortDetails":"2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC). 2017","abstract":"In Japan, which has become a very aged society, the increasing burden of nursing care is an issue. Services and systems related to automatic recording of healthcare management of elderly people have been proposed in order to reduce the burden of nursing care. Water intake is one of the items necessary for healthcare management of elderly people. However, it is not currently automated, which is a burden on caregivers. In the case of the conventional method, the swallowing sound is used for estimating the water intake. However, the estimation error for each subject is large. Accuracy of estimated water intake is improved by using deep learning. Specifically, three features, namely, mel frequency cepstral coefficient (MFCC), duration of water intake, and a RASTA filter auditory spectrum, are extracted from a subject's swallowing sound (which is thought to be highly correlated with water intake). A method of estimating water intake, which considers abstract features that are difficult for people to find, is proposed and verified. It is experimentally shown that RMSE of water intake estimated by the proposed method using deep learning is reduced compared with that estimated by conventional methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8029900","source":"ieee","year":2017,"key":"253895ad-f6a6-4b2d-ac53-b9fc678f13f3","use":1,"doi":"10.1109\/COMPSAC.2017.14"},{"Title":"Classification of various daily behaviors using deep learning and smart watch","Description":"M. C. Kwon,  M. Ju,  S. Choi","ShortDetails":"2017 Ninth International Conference on Ubiquitous and Future Networks (ICUFN). 2017","abstract":"In traditional healthcare and therapy, human behavior has been classified into only two categories: specific behavior and active behavior. As internet of things and wearable devices become popular, however, it is necessary to classify human behavior into more various categories for providing useful services. In this paper, we propose a novel classification scheme that classifies human behavior into 11 different categories including active and inactive activities in daily life. We collect data with smart watch and use deep learning model with a neural network for the classification. Extensive evaluation shows that various daily human behavior can be classified with 99.24% accuracy, and that the classification of human behavior can be used for various services.","email":["kys4543@kookmin.ac.kr","mcju@kookmin.ac.kr","schoi@kookmin.ac.kr"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7993888","source":"ieee","year":2017,"key":"0790d238-aa17-47eb-97ba-ac74e9f0df4c","use":1,"doi":"10.1109\/ICUFN.2017.7993888"},{"Title":"Deep learning Parkinson's from smartphone data","Description":"C. Stamate,  G. D. Magoulas,  S. Kueppers,  E. Nomikou,  I. Daskalopoulos,  M. U. Luchini,  T. Moussouri,  G. Roussos","ShortDetails":"2017 IEEE International Conference on Pervasive Computing and Communications (PerCom). 2017","abstract":"The cloudUPDRS app is a Class I medical device, namely an active transient non-invasive instrument, certified by the Medicines and Healthcare products Regulatory Agency in the UK for the clinical assessment of the motor symptoms of Parkinson's Disease. The app follows closely the Unified Parkinson's Disease Rating Scale which is the most commonly used protocol in the clinical study of PD; can be used by patients and their carers at home or in the community; and, requires the user to perform a sequence of iterated movements which are recorded by the phone sensors. This paper discusses how the cloudUPDRS system addresses two key challenges towards meeting essential consistency and efficiency requirements, namely: (i) How to ensure high-quality data collection especially considering the unsupervised nature of the test, in particular, how to achieve firm user adherence to the prescribed movements; and (ii) How to reduce test duration from approximately 25 minutes typically required by an experienced patient, to below 4 minutes, a threshold identified as critical to obtain significant improvements in clinical compliance. To address the former, we combine a bespoke design of the user experience tailored so as to constrain context, with a deep learning approach used to identify failures to follow the movement protocol while at the same time limiting false positives to avoid unnecessary repetition. We address the latter by developing a machine learning approach to personalise assessments by selecting those elements of the UPDRS protocol that most closely match individual symptom profiles and thus offer the highest inferential power hence closely estimating the patent's overall UPRDS score.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7917848","source":"ieee","year":2017,"key":"1a90f438-a0e9-4331-ab37-0cd90db8674e","use":1,"doi":"10.1109\/PERCOM.2017.7917848"},{"Title":"Semi-automated annotation of signal events in clinical EEG data","Description":"S. Yang,  S. L\u00f3pez,  M. Golmohammadi,  I. Obeid,  J. Picone","ShortDetails":"2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB). 2016","abstract":"To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7846855","source":"ieee","year":2016,"key":"05f110fe-71f7-4ae6-8ebf-98f167f96454","use":1,"doi":"10.1109\/SPMB.2016.7846855"},{"Title":"The effects of deep network topology on mortality prediction","Description":"H. Du,  M. M. Ghassemi,  M. Feng","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Deep learning has achieved remarkable results in the areas of computer vision, speech recognition, natural language processing and most recently, even playing Go. The application of deep-learning to problems in healthcare, however, has gained attention only in recent years, and it's ultimate place at the bedside remains a topic of skeptical discussion. While there is a growing academic interest in the application of Machine Learning (ML) techniques to clinical problems, many in the clinical community see little incentive to upgrade from simpler methods, such as logistic regression, to deep learning. Logistic regression, after all, provides odds ratios, p-values and confidence intervals that allow for ease of interpretation, while deep nets are often seen as `black-boxes' which are difficult to understand and, as of yet, have not demonstrated performance levels far exceeding their simpler counterparts. If deep learning is to ever take a place at the bedside, it will require studies which (1) showcase the performance of deep-learning methods relative to other approaches and (2) interpret the relationships between network structure, model performance, features and outcomes. We have chosen these two requirements as the goal of this study. In our investigation, we utilized a publicly available EMR dataset of over 32,000 intensive care unit patients and trained a Deep Belief Network (DBN) to predict patient mortality at discharge. Utilizing an evolutionary algorithm, we demonstrate automated topology selection for DBNs. We demonstrate that with the correct topology selection, DBNs can achieve better prediction performance compared to several bench-marking methods.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7591263","source":"ieee","year":2016,"key":"52223dcb-41af-4d7f-bb77-a76ad19dd8fb","use":1,"doi":"10.1109\/EMBC.2016.7591263"},{"Title":"An adaptive deep learning approach for PPG-based identification","Description":"V. Jindal,  J. Birjandtalab,  M. B. Pouyan,  M. Nourani","ShortDetails":"2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2016","abstract":"Wearable biosensors have become increasingly popular in healthcare due to their capabilities for low cost and long term biosignal monitoring. This paper presents a novel two-stage technique to offer biometric identification using these biosensors through Deep Belief Networks and Restricted Boltzman Machines. Our identification approach improves robustness in current monitoring procedures within clinical, e-health and fitness environments using Photoplethysmography (PPG) signals through deep learning classification models. The approach is tested on TROIKA dataset using 10-fold cross validation and achieved an accuracy of 96.1%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7592193","source":"ieee","year":2016,"key":"a0960416-4d46-4842-bc82-abcbd9147d6d","use":1,"doi":"10.1109\/EMBC.2016.7592193"},{"Title":"Deep learning for human activity recognition: A resource efficient implementation on low-power devices","Description":"D. Ravi,  C. Wong,  B. Lo,  G. Z. Yang","ShortDetails":"2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN). 2016","abstract":"Human Activity Recognition provides valuable contextual information for wellbeing, healthcare, and sport applications. Over the past decades, many machine learning approaches have been proposed to identify activities from inertial sensor data for specific applications. Most methods, however, are designed for offline processing rather than processing on the sensor node. In this paper, a human activity recognition technique based on a deep learning methodology is designed to enable accurate and real-time classification for low-power wearable devices. To obtain invariance against changes in sensor orientation, sensor placement, and in sensor acquisition rates, we design a feature generation process that is applied to the spectral domain of the inertial data. Specifically, the proposed method uses sums of temporal convolutions of the transformed input. Accuracy of the proposed approach is evaluated against the current state-of-the-art methods using both laboratory and real world activity datasets. A systematic analysis of the feature generation parameters and a comparison of activity recognition computation times on mobile devices and sensor nodes are also presented.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7516235","source":"ieee","year":2016,"key":"5fd784f5-9ab1-4375-86ac-ff12b86a9674","use":1,"doi":"10.1109\/BSN.2016.7516235"},{"Title":"A restricted Boltzmann machine based two-lead electrocardiography classification","Description":"Y. Yan,  X. Qin,  Y. Wu,  N. Zhang,  J. Fan,  L. Wang","ShortDetails":"2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN). 2015","abstract":"An restricted Boltzmann machine learning algorithm were proposed in the two-lead heart beat classification problem. ECG classification is a complex pattern recognition problem. The unsupervised learning algorithm of restricted Boltzmann machine is ideal in mining the massive unlabelled ECG wave beats collected in the heart healthcare monitoring applications. A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. In this paper a deep belief network was constructed and the RBM based algorithm was used in the classification problem. Under the recommended twelve classes by the ANSI\/AAMI EC57: 1998\/(R)2008 standard as the waveform labels, the algorithm was evaluated on the two-lead ECG dataset of MIT-BIH and gets the performance with accuracy of 98.829%. The proposed algorithm performed well in the two-lead ECG classification problem, which could be generalized to multi-lead unsupervised ECG classification or detection problems.","email":["yan.yan@siat.ac.cn","xb.qin@siat.ac.cn"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7299399","source":"ieee","year":2015,"key":"96005b03-4b7a-4799-8e57-c11877926ad3","use":1,"doi":"10.1109\/BSN.2015.7299399"},{"Title":"Inexpensive user tracking using Boltzmann Machines","Description":"E. Mocanu,  D. C. Mocanu,  H. B. Ammar,  Z. Zivkovic,  A. Liotta,  E. Smirnov","ShortDetails":"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2014","abstract":"Inexpensive user tracking is an important problem in various application domains such as healthcare, human-computer interaction, energy savings, safety, robotics, security and so on. Yet, it cannot be easily solved due to its probabilistic nature, high level of abstraction and uncertainties, on the one side, and to the limitations of our current technologies and learning algorithms, on the other side. In this paper, we tackle this problem by using the Multi-integrated Sensor Technology, which comes at a low price. At the same time, we are aiming to address the lightweight learning requirements by investigating Factored Conditional Restricted Boltzmann Machines (FCRBMs), a form of Deep Learning, that has proven to be an efficient and effective machine learning framework. However, due to their construction properties, the conventional FCRBMs are only capable of performing predictions but are not capable of making classification. Herein, we are proposing extended FCRBMs (eFCRBMs), which incorporate a novel classification scheme, to solve this problem. Experiments performed on both artificially generated as well as real-world data demonstrate the effectiveness and efficiency of the proposed technique. We show that eFCRBMs outperform popular approaches including Support Vector Machines, Naive Bayes, AdaBoost, and Gaussian Mixture Models.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6973875","source":"ieee","year":2014,"key":"ec3b9f29-8503-418a-bdf0-ec536ddc04c2","use":1,"doi":"10.1109\/SMC.2014.6973875"},{"Title":"Predictive Modeling of Therapy Decisions in Metastatic Breast Cancer with Recurrent Neural Network Encoder and Multinomial Hierarchical Regression Decoder","Description":"Y. Yang,  P. A. Fasching,  V. Tresp","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"The increasing availability of novel health-related data sources \u2014e.g., from molecular analysis, health Apps and electronic health records\u2014 might eventually overwhelm the physician, and the community is investigating analytics approaches that might be useful to support clinical decisions. In particular, the success of the latest developments in Deep Learning has demonstrated that machine learning models are capable of handling \u2014and actually profiting from\u2014 high dimensional and possibly sequential data. In this work, we propose an encoder-decoder network approach to model the physician's therapy decisions. Our approach also provides physicians with a list of similar historical patient cases to support the recommended decisions. By using a combination of a Recurrent Neural Network Encoder and a Multinomial Hierarchical Regression Decoder, we specifically tackle two common challenges in modeling clinical data:First, the issue of handling episodic data of variable lengths and, second, the need to represent hierarchical decision procedures. We conduct experiments on a large real-world dataset collected from thousands of metastatic breast cancer patients and show that our model outperforms more traditional approaches.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031131","source":"ieee","year":2017,"key":"170519f7-98e5-4365-bdc6-624b4e61a6b6","use":1,"doi":"10.1109\/ICHI.2017.51"},{"Title":"Language-Based Process Phase Detection in the Trauma Resuscitation","Description":"Y. Gu,  X. Li,  S. Chen,  H. Li,  R. A. Farneth,  I. Marsic,  R. S. Burd","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Process phase detection has been widely used in surgical process modeling (SPM) to track process progression. These studies mostly used video and embedded sensor data, but spoken language also provides rich semantic information directly related to process progression. We present a long-short term memory (LSTM) deep learning model to predict trauma resuscitation phases using verbal communication logs. We first use an LSTM to extract the sentence meaning representations, and then sequentially feed them into another LSTM to extract the mean-ing of a sentence group within a time window. This information is ultimately used for phase prediction. We used 24 manually-transcribed trauma resuscitation cases to train, and the remain-ing 6 cases to test our model. We achieved 79.12% accuracy, and showed performance advantages over existing visual-audio systems for critical phases of the process. In addition to language information, we evaluated a multimodal phase prediction structure that also uses audio input. We finally identified the challenges of substituting manual transcription with automatic speech recognition in trauma resuscitation.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031153","source":"ieee","year":2017,"key":"8551f1d5-5457-4c8c-98ff-89d902dafcc0","use":1,"doi":"10.1109\/ICHI.2017.50"},{"Title":"CVRT: Cognitive Visual Recognition Tracker","Description":"M. Velazquez,  Y. Lee","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Studies on visual attention of patients with Alzheimer's disease and Dementia is a promising way for keeping track of the individual patient's image recognition ability over. This research seeks to expand upon the current applications of combining the Android operating system with TensorFlow by providing a visual question answering platform for image analysis. This application, Cognitive Visual Recognition Tracker (CVRT), provides an entry point by which the user can ask questions concerning any image of their choosing, and then receive cumulative metrics over time to better assess any diminishing cognitive ability (i.e. Alzheimer's patients). In this work, recurrent neural networks as well as semantic analysis are leveraged to provide an interactive VQA experience. One of the main objectives of CVRT is for physicians to be able to determine trends from patient data that could either be applicable to the individual patient, or to many patients if an aggregate is formed from many individual datasets. On an individual level, these metrics would provide a way for the physician to monitor daily cognitive capability, whereas on a grander scale, these joint datasets could be used to provide better overall treatment for the disease with the future inclusion of predictive analytics. The final contribution is an interactive metrics platform by which other users can assess the primary user's cognitive capacity based on features of their questioning, and to then provide them with accurate trending or possible remediation plans based on their condition.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031129","source":"ieee","year":2017,"key":"f7a4f789-294f-47cd-a698-53d3c50310fb","use":1,"doi":"10.1109\/ICHI.2017.65"},{"Title":"Automated EEG-Based Epileptic Seizure Detection Using Deep Neural Networks","Description":"J. Birjandtalab,  M. Heydarzadeh,  M. Nourani","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Millions of people around the world suffer from epilepsy. It is very important to provide a method to efficiently monitor the seizures and alert the caregivers to help patients. It is proven that EEG signals are the best markers for diagnosis of the epileptic seizures. In this paper, we used the frequency domain features (normalized in-band power spectral density) to extract information from EEG signals. We applied a deep learning technique based on multilayer perceptrons to improve the accuracy of seizure detection. The results indicate that our nonlinear technique is able to efficiently and automatically detect seizure and non-seizure episodes with an F-measure accuracy of around 95%.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031211","source":"ieee","year":2017,"key":"3fbcfb86-797c-4da2-a850-b3e0af76d88a","use":1,"doi":"10.1109\/ICHI.2017.55"},{"Title":"Medical Concept Normalization for Online User-Generated Texts","Description":"K. Lee,  S. A. Hasan,  O. Farri,  A. Choudhary,  A. Agrawal","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Social media has become an important tool for sharing content in the last decade. People often talk about their experiences and opinions on different health-related issues e.g. they write reviews on medications, describe symptoms and ask informal questions about various health concerns. Due to the colloquial nature of the languages used in the social media, it is often difficult for an automated system to accurately interpret them for appropriate clinical understanding. To address this challenge, this paper proposes a novel approach for medical concept normalization of user-generated texts to map a health condition described in the colloquial language to a medical concept defined in standard clinical terminologies. We use multiple deep learning architectures such as convolutional neural networks (CNN) and recurrent neural networks (RNN) with input word embeddings trained on various clinical domain-specific knowledge sources. Extensive experiments on two benchmark datasets demonstrate that the proposed models can achieve up to 21.28% accuracy improvements over the existing models when we use the combination of all knowledge sources to learn neural embeddings.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031195","source":"ieee","year":2017,"key":"311b703c-80f1-4a94-9b7c-3767f37460e0","use":1,"doi":"10.1109\/ICHI.2017.59"},{"Title":"Deep Learning Based Recognition of Meltdown in Autistic Kids","Description":"V. S. P. Patnam,  F. T. George,  K. George,  A. Verma","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Children with autism often experience sudden meltdowns which not only makes the moment tough for the caretakers\/parents but also make the children hurt themselves physically. Studies have discovered that children with autistic spectrum disorder exhibit certain actions through which we can anticipate mutilating meltdowns in them. The objective of our project is to build a system that can recognize such kind of actions using deep learning techniques thereby, notifying the caretakers\/parents so that they can get the situation under control in lesser time. Using deep learning RCNNs, we can train the system faster yet reliable because unlike all the machine learning algorithms, deep learning algorithms are more efficient and have more scope into future. We have trained a classifier on images that are gathered from videos and reliable internet sources with most predictive gestures, through which we can detect the meltdowns more precisely. We have trained a model that validated the accuracy by ~93% which is accompanied by a loss\/train classifier with a minimal 0.4% loss. Functional testing was done through feeding the deep neural network with chosen actions performed by five individuals that resulted in an accuracy of ~92% in all cases, which can assure the real-time usage of the system.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031180","source":"ieee","year":2017,"key":"49771c8d-ae11-4f20-81b3-d96c2f598e86","use":1,"doi":"10.1109\/ICHI.2017.35"},{"Title":"Extracting Drug-Drug Interactions with Word and Character-Level Recurrent Neural Networks","Description":"R. Kavuluru,  A. Rios,  T. Tran","ShortDetails":"2017 IEEE International Conference on Healthcare Informatics (ICHI). 2017","abstract":"Drug-drug interactions (DDIs) are known to be responsible for nearly a third of all adverse drug reactions. Hence several current efforts focus on extracting signal from EMRs to prioritize DDIs that need further exploration. To this end, being able to extract explicit mentions of DDIs in free text narratives is an important task. In this paper, we explore recurrent neural network (RNN) architectures to detect and classify DDIs from unstructured text using the DDIExtraction dataset from the SemEval 2013 (task 9) shared task. Our methods are in line with those used in other recent deep learning efforts for relation extraction including DDI extraction. However, to our knowledge, we are the first to investigate the potential of character-level RNNs (Char-RNNs) for DDI extraction (and relation extraction in general). Furthermore, we explore a simple but effective model bootstrapping method to (a). build model averaging ensembles, (b). derive confidence intervals around mean micro-F scores (MMF), and (c). assess the average behavior of our methods. Without any rule based filtering of negative examples, a popular heuristic used by most earlier efforts, we achieve an MMF of 69.13. By adding simple replicable heuristics to filter negative instances we are able to achieve an MMF of 70.38. Furthermore, our best ensembles produce micro F-scores of 70.81 (without filtering) and 72.13 (with filtering), which are superior to metrics reported in published results. Although Char-RNNs turnout to be inferior to regular word based RNN models in overall comparisons, we find that ensembling models from both architectures results in nontrivial gains over simply using either alone, indicating that they complement each other.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8031125","source":"ieee","year":2017,"key":"7351d803-a27b-4909-bb63-eff61518c407","use":1,"doi":"10.1109\/ICHI.2017.15"},{"Title":"Big Social Data Analytics for Public Health: Predicting Facebook Post Performance Using Artificial Neural Networks and Deep Learning","Description":"N. Straton,  R. R. Mukkamala,  R. Vatrapu","ShortDetails":"2017 IEEE International Congress on Big Data (BigData Congress). 2017","abstract":"Facebook \"post popularity\" analysis is fundamental for differentiating between relevant posts and posts with low user engagement and consequently their characteristics. This research study aims at health and care organizations to improve information dissemination on social media platforms by reducing clutter and noise. At the same time, it will help users navigate through vast amount of information in direction of the relevant health and care content. Furthermore, study explores prediction of popularity of healthcare posts on the largest social media platform Facebook. Methodology is presented in this paper to predict user engagement based on eleven characteristics of the post: Post Type, Hour Span, Facebook Wall Category, Level, Country, isHoliday, Season, Created Year, Month, Day of the Week, Time of the Day. Finally, post performance prediction is conducted using Artificial Neural Networks (ANN) and Deep Neural Networks (DNN). Different network topology measures are used to achieve best accuracy prediction followed by examples and discussion on why DNN might not be optimal technique for the given data set.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8029313","source":"ieee","year":2017,"key":"dbec5df3-2996-495a-b96e-369d2c27cbd0","use":1,"doi":"10.1109\/BigDataCongress.2017.21"},{"Title":"Deep Temporal Multimodal Fusion for Medical Procedure Monitoring using Wearable Sensors","Description":"E. A. Bernal,  X. Yang,  Q. Li,  J. Kumar,  S. Madhvanath,  P. Ramesh,  R. Bala","ShortDetails":"IEEE Transactions on Multimedia. 2017","abstract":"Process monitoring and verification have a wide range of uses in the medical and healthcare fields. Currently, such tasks are often carried out by a trained specialist, which makes them expensive, inefficient, and time-consuming. Recent advances in automated video- and multimodal-data-based action and activity recognition have made it possible to reduce the extent of manual intervention required to effectively carry out process supervision tasks. In this paper, we propose algorithms for automated egocentric human action and activity recognition from multimodal data, with a target application of monitoring and assisting a user perform a multi-step medical procedure. We propose a supervised deep multimodal fusion framework that relies on concurrent processing of motion data acquired with wearable sensors and video data acquired with an egocentric or body-mounted camera. We demonstrate the effectiveness of the algorithm on a public multimodal dataset and conclude that automated process monitoring via the use of multiple heterogeneous sensors is a viable alternative to its manual counterpart. Furthermore, we demonstrate that the application of previously proposed adaptive sampling schemes to the video processing branch of the multimodal framework results in significant performance improvements.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7976382","source":"ieee","year":2017,"key":"52bc445b-e953-4258-b9ff-bcb500dc7c33","use":1,"doi":"10.1109\/TMM.2017.2726187"},{"Title":"FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition","Description":"H. Ding,  S. K. Zhou,  R. Chellappa","ShortDetails":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). 2017","abstract":"Relatively small data sets available for expression recognition research make the training of deep networks very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pretrained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization results show that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu- CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7961731","source":"ieee","year":2017,"key":"d1dba7cf-88fc-42f9-83b5-fd6bda1323aa","use":1,"doi":"10.1109\/FG.2017.23"},{"Title":"The chatbot feels you - a counseling service using emotional response generation","Description":"Dongkeon Lee,  Kyo-Joong Oh,  Ho-Jin Choi","ShortDetails":"2017 IEEE International Conference on Big Data and Smart Computing (BigComp). 2017","abstract":"Early study tries to use chatbot for counseling services. They changed drinking habit of who being consulted by leading them via intervene chatbot. However, the application did not concerned about psychiatric status through continuous conversation with user monitoring. Furthermore, they had no ethical judgment method that about the intervention of the chatbot. We argue that more reasonable and continuous emotion recognition will make better mental healthcare experiment. It will be more proper clinical psychiatric consolation in ethical view as well. This paper suggests a introduce a novel chatbot system for psychiatric counseling service. Our system understands content of conversation based on recent natural language processing (NLP) methods with emotion recognition. It senses emotional flow through the continuous observation of conversation. Also, we generate personalized counseling response from user input, to do this, we use additional constrains to generation model for the proper response generation which can detect conversational context, user emotion and expected reaction.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881752","source":"ieee","year":2017,"key":"7a305ad0-9561-48e0-a7c2-d7cf8eccaf32","use":1,"doi":"10.1109\/BIGCOMP.2017.7881752"},{"Title":"Deep Decision Network for Multi-class Image Classification","Description":"V. N. Murthy,  V. Singh,  T. Chen,  R. Manmatha,  D. Comaniciu","ShortDetails":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016","abstract":"In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7780615","source":"ieee","year":2016,"key":"1bbd7f1a-16c9-4f72-a210-fc9e0aca25c8","use":1,"doi":"10.1109\/CVPR.2016.246"},{"Title":"Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy","Description":"J. Liang,  R. Lu,  C. Zhang,  F. Wang","ShortDetails":"2016 IEEE International Conference on Healthcare Informatics (ICHI). 2016","abstract":"Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt \"auras\". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7776343","source":"ieee","year":2016,"key":"2cf27ce4-11f1-450a-8965-552b4b99a0f9","use":1,"doi":"10.1109\/ICHI.2016.27"},{"Title":"Voice Pathology Detection Using Deep Learning: a Preliminary Study","Description":"P. Harar,  J. B. Alonso-Hernandezy,  J. Mekyska,  Z. Galaz,  R. Burget,  Z. Smekal","ShortDetails":"2017 International Conference and Workshop on Bioinspired Intelligence (IWOBI). 2017","abstract":"This paper describes a preliminary investigation of Voice Pathology Detection using Deep Neural Networks (DNN). We used voice recordings of sustained vowel \/a\/ produced at normal pitch from German corpus Saarbruecken Voice Database (SVD). This corpus contains voice recordings and electroglottograph signals of more than 2 000 speakers. The idea behind this experiment is the use of convolutional layers in combination with recurrent Long-Short-Term-Memory (LSTM) layers on raw audio signal. Each recording was split into 64 ms Hamming windowed segments with 30 ms overlap. Our trained model achieved 71.36% accuracy with 65.04% sensitivity and 77.67% specificity on 206 validation files and 68.08% accuracy with 66.75% sensitivity and 77.89% specificity on 874 testing files. This is a promising result in favor of this approach because it is comparable to similar previously published experiment that used different methodology. Further investigation is needed to achieve the state-of-the-art results.","email":["harar@phd.feec.vutbr.cz"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7985525","source":"ieee","year":2017,"key":"c217a472-336c-4c42-aece-ea85e32db146","use":1,"doi":"10.1109\/IWOBI.2017.7985525"},{"Title":"Deep Learning for Automated Extraction of Primary Sites from Cancer Pathology Reports","Description":"J. Qiu,  H. J. Yoon,  P. A. Fearn,  G. D. Tourassi","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"for cancer registries which process high volumes of free-text reports annually. Information extraction and coding is a manual, labor-intensive process. In this study we investigated deep learning and a convolutional neural network (CNN), for extracting ICDO- 3 topographic codes from a corpus of breast and lung cancer pathology reports. We performed two experiments, using a CNN and a more conventional term frequency vector approach, to assess the effects of class prevalence and inter-class transfer learning. The experiments were based on a set of 942 pathology reports with human expert annotations as the gold standard. CNN performance was compared against a more conventional term frequency vector space approach. We observed that the deep learning models consistently outperformed the conventional approaches in the class prevalence experiment, resulting in micro and macro-F score increases of up to 0.132 and 0.226 respectively when class labels were well populated. Specifically, the best performing CNN achieved a micro-F score of 0.722 over 12 ICD-O-3 topography codes. Transfer learning provided a consistent but modest performance boost for the deep learning methods but trends were contingent on CNN method and cancer site. These encouraging results demonstrate the potential of deep learning for automated abstraction of pathology reports.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7918552","source":"ieee","year":2017,"key":"dc413fac-5a27-4492-b82c-4e6e59664361","use":1,"doi":"10.1109\/JBHI.2017.2700722"},{"Title":"Deep learning based Nucleus Classification in pancreas histological images","Description":"Y. H. Chang,  G. Thibault,  O. Madin,  V. Azimi,  C. Meyers,  B. Johnson,  J. Link,  A. Margolin,  J. W. Gray","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"Tumor specimens contain a variety of healthy cells as well as cancerous cells, and this heterogeneity underlies resistance to various cancer therapies. But this problem has not been thoroughly investigated until recently. Meanwhile, technological breakthroughs in imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples, and modern machine learning approaches including deep learning have been shown to produce encouraging results by finding hidden structures and make accurate predictions. In this paper, we propose a Deep learning based Nucleus Classification (DeepNC) approach using paired histopathology and immunofluorescence images (for label), and demonstrate its classification prediction power. This method can solve current issue on discrepancy between genomic- or transcriptomic-based and pathology-based tumor purity estimates by improving histological evaluation. We also explain challenges in training a deep learning model for huge dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8036914","source":"ieee","year":2017,"key":"c3d20c94-1351-4417-9f35-e9b3649f3678","use":1,"doi":"10.1109\/EMBC.2017.8036914"},{"Title":"Epithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images","Description":"Y. Huang,  H. ZHENG,  C. LIU,  X. Ding,  G. Rohde","ShortDetails":"IEEE Journal of Biomedical and Health Informatics. 2017","abstract":"Epithelium-stroma classification is a necessary preprocessing step in histopathological image analysis. Current deep learning based recognition methods for histology data require collection of large volumes of labeled data in order to train a new neural network when there are changes to the image acquisition procedure. However, it is extremely expensive for pathologists to manually label sufficient volumes of data for each pathology study in a professional manner, which results in limitations in real-world applications. A very simple but effective deep learning method, that introduces the concept of unsupervised domain adaptation to a simple convolutional neural network (CNN), has been proposed in this paper. Inspired by transfer learning, our work assumes that the training data and testing data follow different distributions, and there is an adaptation operation to more accurately estimate the kernels in CNN in feature extraction, in order to enhance performance by transferring knowledge from labeled data in source domain to unlabeled data in target domain. The model has been evaluated using three independent public epithelium-stroma datasets by cross-dataset validations. The experimental results demonstrate that for epithelium-stroma classification, the proposed framework outperforms the state-of-the-art deep neural network model, and it also achieves better performance than other existing deep domain adaptation methods. The proposed model can be considered to be a better option for real-world applications in histopathological image analysis, since there is no longer a requirement for large-scale labeled data in each specified domain.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7893702","source":"ieee","year":2017,"key":"c32557ca-57dc-48d2-a626-557c9dc56b99","use":1,"doi":"10.1109\/JBHI.2017.2691738"},{"Title":"Mixed Neural Network Approach for Temporal Sleep Stage Classification","Description":"H. Dong,  A. Supratak,  W. Pan,  C. Wu,  P. M. Matthews,  Y. Guo","ShortDetails":"IEEE Transactions on Neural Systems and Rehabilitation Engineering. 2017","abstract":"This paper proposes a practical approach to addressing limitations posed by using of single-channel electroencephalography (EEG) for sleep stage classification. EEG-based characterizations of sleep stage progression contribute the diagnosis and monitoring of the many pathologies of sleep. Several prior reports explored ways of automating the analysis of sleep EEG and of reducing the complexity of the data needed for reliable discrimination of sleep stages at lower cost in the home. However, these reports have involved recordings from electrodes placed on the cranial vertex or occiput, which are both uncomfortable and difficult to position. Previous studies of sleep stage scoring that used only frontal electrodes with a hierarchical decision tree motivated this paper, in which we have taken advantage of rectifier neural network for detecting hierarchical features and long short-term memory (LSTM) network for sequential data learning to optimize classification performance with single-channel recordings. After exploring alternative electrode placements, we found a comfortable configuration of a single-channel EEG on the forehead and have shown that it can be integrated with additional electrodes for simultaneous recording of the electrooculogram (EOG). Evaluation of data from 62 people (with 494 hours sleep) demonstrated better performance of our analytical algorithm than is available from existing approaches with vertex or occipital electrode placements. Use of this recording configuration with neural network deconvolution promises to make clinically indicated home sleep studies practical.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7995122","source":"ieee","year":2017,"key":"412bfe44-c10f-4d6f-9a3c-82175e295740","use":1,"doi":"10.1109\/TNSRE.2017.2733220"},{"Title":"Evaluations of deep convolutional neural networks for automatic identification of malaria infected cells","Description":"Y. Dong,  Z. Jiang,  H. Shen,  W. David Pan,  L. A. Williams,  V. V. B. Reddy,  W. H. Benjamin,  A. W. Bryan","ShortDetails":"2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). 2017","abstract":"This paper studied automatic identification of malaria infected cells using deep learning methods. We used whole slide images of thin blood stains to compile an dataset of malaria-infected red blood cells and non-infected cells, as labeled by a group of four pathologists. We evaluated three types of well-known convolutional neural networks, including the LeNet, AlexNet and GoogLeNet. Simulation results showed that all these deep convolution neural networks achieved classification accuracies of over 95%, higher than the accuracy of about 92% attainable by using the support vector machine method. Moreover, the deep learning methods have the advantage of being able to automatically learn the features from the input data, thereby requiring minimal inputs from human experts for automated malaria diagnosis.","email":["pand@uah.edu."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7897215","source":"ieee","year":2017,"key":"c4762f03-855d-427d-8a99-bbfdb97e4eff","use":1,"doi":"10.1109\/BHI.2017.7897215"},{"Title":"Automatic Feature Learning Method for Detection of Retinal Landmarks","Description":"B. Al-Bander,  W. Al-Nuaimy,  M. A. Al-Taee,  A. Al-Ataby,  Y. Zheng","ShortDetails":"2016 9th International Conference on Developments in eSystems Engineering (DeSE). 2016","abstract":"This paper presents an automatic deep learning method for location detection of important retinal landmarks, the fovea and optic disc (OD) in digital fundus retinal images with the potential for use in an automated screening and grading system. The proposed method is based on deep convolutional neural networks (CNN) and does not depend the visual appearance or anatomical features of the retinal landmarks. It comprises convolution, max-pooling, fully connected and dropout layers as well as an output layer. The CNN is trained using an existing dataset images along with their annotated locations of the foveal and OD centres. Performance of the network is evaluated using Root Mean Square Error (RMSE). The developed feature learning-based approach presents promising system for retinal landmarks detection.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7930616","source":"ieee","year":2016,"key":"899469b4-a2b9-4d30-9ec4-4dbd08860308","use":1,"doi":"10.1109\/DeSE.2016.4"},{"Title":"Cancer Classification Based on Microarray Gene Expression Data Using Deep Learning","Description":"P. Guillen,  J. Ebalunode","ShortDetails":"2016 International Conference on Computational Science and Computational Intelligence (CSCI). 2016","abstract":"The classification of cancer is a major research topic in Medicine. Cancer microarray data normally contains a small number of samples, which have a large number of gene expression levels as features, however, makes the classification quite challenging. Using a deep learning algorithm based on multilayer perceptron, we show that classification performance at least as good as published results can be obtained for cancer classification.","email":["pgrondon@central.uh.edu","jebaluno@central.uh.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881563","source":"ieee","year":2016,"key":"d0dadcae-03c0-4064-bcfc-7d081c602024","use":1,"doi":"10.1109\/CSCI.2016.0270"},{"Title":"A predictive model of gene expression using a deep learning framework","Description":"Rui Xie,  A. Quitadamo,  J. Cheng,  Xinghua Shi","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"With an unprecedented amount of data available, it is important to explore new methods for developing predictive models to mine this data for scientific discoveries. In this study, we propose a deep learning regression model based on MultiLayer Perceptron and Stacked Denoising Auto-encoder (MLP-SAE) to predict gene expression from genotypes of genetic variation. Specifically, we use a stacked denoising auto-encoder to train our regression model in order to extract useful features, and utilize the multilayer perceptron for backpropagation. We further improve our model by adding a dropout technique to prevent overfitting. Our results on a real genomic dataset show that our MLP-SAE model with dropout outperform Lasso, Random Forests, and MLP-SAE without dropout. Our study provides a new application of deep learning in mining genomics data, and demonstrates that deep learning has great potentials in building predictive models to help understand biological systems.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822599","source":"ieee","year":2016,"key":"6f002345-3ef5-46b6-b359-f23a8a279deb","use":1,"doi":"10.1109\/BIBM.2016.7822599"},{"Title":"Learning structure in gene expression data using deep architectures, with an application to gene clustering","Description":"A. Gupta,  H. Wang,  M. Ganapathiraju","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"Genes play a central role in all biological processes. DNA microarray technology has made it possible to study the expression behavior of thousands of genes in one go. Often, gene expression data is used to generate features for supervised and unsupervised learning tasks. At the same time, advances in the field of deep learning have made available a plethora of architectures. In this paper, we use deep architectures pre-trained in an unsupervised manner using denoising autoencoders as a preprocessing step for a popular unsupervised learning task. Denoising autoencoders (DA) can be used to learn a compact representation of input, and have been used to generate features for further supervised learning tasks. We propose that our deep architectures can be treated as empirical versions of Deep Belief Networks (DBNs). We use our deep architectures to regenerate gene expression time series data for two different data sets. We test our hypothesis on two popular datasets for the unsupervised learning task of clustering and find promising improvements in performance.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359871","source":"ieee","year":2015,"key":"3adceca4-48d8-47d8-8b5c-9675fcce360f","use":1,"doi":"10.1109\/BIBM.2015.7359871"},{"Title":"A Deep Learning Model for Epigenomic Studies","Description":"G. Lo Bosco,  R. Rizzo,  A. Fiannaca,  M. La Rosa,  A. Urso","ShortDetails":"2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS). 2016","abstract":"Epigenetics is the study of heritable changes in gene expression that does not involve changes to the underlying DNA sequence, i.e. a change in phenotype not involved by a change in genotype. At least three main factor seems responsible for epigenetic change including DNA methylation, histone modification and non-coding RNA, each one sharing having the same property to affect the dynamic of the chromatin structure by acting on Nucleosomes position. A nucleosome is a DNA-histone complex, where around 150 base pairs of double-stranded DNA is wrapped. The role of nucleosomes is to pack the DNA into the nucleus of the Eukaryote cells, to form the Chromatin. Nucleosome positioning plays an important role in gene regulation and several studies shows that distinct DNA sequence features have been identified to be associated with nucleosome presence. Starting from this suggestion, the identification of nucleosomes on a genomic scale has been successfully performed by DNA sequence features representation and classical supervised classification methods such as Support Vector Machines, Logistic regression and so on. Taking in consideration the successful application of the deep neural networks on several challenging classification problems, in this paper we want to study how deep learning network can help in the identification of nucleosomes.","email":["giosue.lobosco@unipa.it","riccardo.rizzo@icar.cnr.it"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7907542","source":"ieee","year":2016,"key":"40d760e4-831d-4344-9bab-7b55bee94dda","use":1,"doi":"10.1109\/SITIS.2016.115"},{"Title":"A deep learning model for predicting transcription factor binding location at single nucleotide resolution","Description":"S. Salekin,  J. M. Zhang,  Y. Huang","ShortDetails":"2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). 2017","abstract":"Transcriptional regulation by transcription factors (TFs) plays a pivotal role in controlling the gene expression. However, understanding the mechanism through which the transcription factors regulate the gene expression is a challenging task. This is primarily hindered by the low specificity in identifying transcription factor binding sites (TFBS). The emergence of the ChIP-exonuclease (ChIP-exo) method enables the detection of TFBS at single nucleotide sensitivity, providing us an opportunity to study the detailed mechanisms of TF regulation. Nevertheless, there is still a lack of computational tools that can also provide single base pair (bp) resolution prediction of TFBS. In this paper, we propose DeepSNR, a Deep Learning algorithm for Single Nucleotide Resolution prediction of transcription factor binding site. Our proposed method is inspired by the similarity between predicting the specific binding location from input nucleotide sequence and image segmentation. Particularly, we adopted the deconvolution network (deconvNet); a deep learning model designed for image segmentation, and developed a TFBS specific deconvNet architecture constructed on top of `DeepBind'. We trained a deconvNet for predicting CTCF binding sites using the data from ChIP-exo experiments. The proposed algorithm achieved median precision and recall of 87% and 77% respectively, significantly outperforming motif search based algorithms such as MatInspector.","email":["yufei.huang@utsa.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7897204","source":"ieee","year":2017,"key":"8a9c468d-e7b0-4be6-b15f-94d0c92ea477","use":1,"doi":"10.1109\/BHI.2017.7897204"},{"Title":"DP-miRNA: An improved prediction of precursor microRNA using deep learning model","Description":"J. Thomas,  S. Thomas,  L. Sael","ShortDetails":"2017 IEEE International Conference on Big Data and Smart Computing (BigComp). 2017","abstract":"MicroRNA (miRNA) are small non-coding RNAs regulating gene expression at the post-transcriptional level. Detecting miRNA in a genome is challenging experimentally and results vary depending on their cellular environment. These limitations inspire the development of knowledge-based prediction method. This paper proposes a deep learning based classification model for predicting precursor miRNA sequence that contains the miRNA sequence. The feature set consists of sequence features, folding measures, stem-loop features and statistical features. We evaluate the performance of the proposed method on human dataset. The deep neural network based classification outperformed support vector machine, neural network, naive Bayes classifiers, k-nearest neighbors, random forests as well as hybrid systems combining SVM and genetic algorithm.","email":["jaya.thomas@sunykorea.ac.kr","sonia.thomas@sunykorea.ac.kr","sael@cs.stonybrook.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7881722","source":"ieee","year":2017,"key":"8f833e8f-0401-4934-b6c6-7f36c62eb4ae","use":1,"doi":"10.1109\/BIGCOMP.2017.7881722"},{"Title":"Multimodal Deep Boltzmann Machines for feature selection on gene expression data","Description":"A. F. Syafiandini,  I. Wasito,  S. Yazid,  A. Fitriawan,  M. Amien","ShortDetails":"2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS). 2016","abstract":"In this paper, multimodal Deep Boltzmann Machines (DBM) is employed to learn important genes (biomarkers) on gene expression data from human carcinoma colorectal. The learning process involves gene expression data and several patient phenotypes such as lymph node and distant metastasis occurrence. The proposed framework in this paper uses multimodal DBM to train records with metastasis occurrence. Later, the trained model is tested using records with no metastasis occurrence. After that, Mean Squared Error (MSE) is measured from the reconstructed and the original gene expression data. Genes are ranked based on the MSE value. The first gene has the highest MSE value. After that, k-means clustering is performed using various number of genes. Features that give the highest purity index are considered as the important genes. The important genes obtained from the proposed framework and two sample t-test are being compared. From the accuracy of metastasis classification, the proposed framework gives higher results compared to the top genes from two sample t-test.","email":["arida.ferti@ui.ac.id","ito.wasito@cs.ui.ac.id","setiadi@cs.ui.ac.id","aries.fitriawan@ui.ac.id","mukhlis.amien@ui.ac.id"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7872733","source":"ieee","year":2016,"key":"96f59a54-ea31-4087-981e-986bacb1f135","use":1,"doi":"10.1109\/ICACSIS.2016.7872733"},{"Title":"Cancer subtype identification using deep learning approach","Description":"A. F. Syafiandini,  I. Wasito,  S. Yazid,  A. Fitriawan,  M. Amien","ShortDetails":"2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA). 2016","abstract":"In this paper, a framework using deep learning approach is proposed to identify two subtypes of human colorectal carcinoma cancer. The identification process uses information from gene expression and clinical data which is obtained from data integration process. One of deep learning architecture, multimodal Deep Boltzmann Machines (DBM) is used for data integration process. The joint representation gene expression and clinical is later used as Restricted Boltzmann Machines (RBM) input for cancer subtype identification. Kaplan Meier survival analysis is employed to evaluate the identification result. The curves on survival plot obtained from Kaplan Meier analysis are tested using three statistic tests to ensure that there is a significant difference between those curves. According to Log Rank, Generalized Wilcoxon and Tarone-Ware, the two groups of patients with different cancer subtypes identified using the proposed framework are significantly different.","email":["arida.ferti@ui.ac.id","ito.wasito@cs.ui.ac.id","setiadi@cs.ui.ac.id","aries.fitriawan@ui.ac.id","mukhlis.amien@ui.ac.id"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7863033","source":"ieee","year":2016,"key":"a6ecb31a-f65a-41a4-acbb-f64fcf3a6412","use":1,"doi":"10.1109\/IC3INA.2016.7863033"},{"Title":"Inferring Gene Regulatory Networks by Combining Supervised and Unsupervised Methods","Description":"T. Turki,  J. T. L. Wang,  I. Rajikhan","ShortDetails":"2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA). 2016","abstract":"Supervised methods for inferring gene regulatory networks (GRNs) perform well with good training data. However, when training data is absent, these methods are not applicable. Unsupervised methods do not need training data but their accuracy is low. In this paper, we combine supervised and unsupervised methods to infer GRNs using time-series gene expression data. Specifically, we use results obtained from unsupervised methods to train supervised methods. Since the results contain noise, we develop a data cleaning algorithm to remove noise, hence improving the quality of the training data. These refined training data are then used to guide classifiers including support vector machines and deep learning tools to infer GRNs through link prediction. Experimental results on several data sets demonstrate the good performance of the classifiers and the effectiveness of our data cleaning algorithm.","email":["tturki@kau.edu.sa","wangj@njit.edu","Ibrahim.rajikhan13@stjohns.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7838135","source":"ieee","year":2016,"key":"ffc5be31-2ed0-45f3-9977-44d912c0a5f7","use":1,"doi":"10.1109\/ICMLA.2016.0031"},{"Title":"DeepEnhancer: Predicting enhancers by convolutional neural networks","Description":"Xu Min,  Ning Chen,  Ting Chen,  Rui Jiang","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Enhancers are crucial to the understanding of mechanisms underlying gene transcriptional regulation. Although having been successfully applied in such projects as ENCODE and Roadmap to generate landscape of enhancers in human cell lines, high-throughput biological experimental techniques are still costly and time consuming for even larger scale identification of enhancers across a variety of tissues under different disease status, making computational identification of enhancers indispensable. In this paper, we propose a computational framework, named DeepEnhancer, to classify enhancers from background genomic sequences. We construct convolutional neural networks of various architectures and compare the classification performance with traditional sequence-based classifiers. We first train the deep learning model on the FANTOM5 permissive enhancer dataset, and then fine-tune the model on ENCODE cell type-specific enhancer datasets by adopting the transfer learning strategy. Experimental results demonstrate that DeepEnhancer has superior efficiency and effectiveness in classification tasks, and the use of max-pooling and batch normalization is beneficial to higher accuracy. To make our approach more understandable, we propose a strategy to visualize the convolutional kernels as sequence logos and compare them against the JASPAR database using TOMTOM. In summary, DeepEnhancer allows researchers to train highly accurate deep models and will be broadly applicable in computational biology.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822593","source":"ieee","year":2016,"key":"1c8a3588-cc2e-457d-9d2a-0fb1191a3eca","use":1,"doi":"10.1109\/BIBM.2016.7822593"},{"Title":"DeepSplice: Deep classification of novel splice junctions revealed by RNA-seq","Description":"Yi Zhang,  Xinan Liu,  J. N. MacLeod,  Jinze Liu","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Alternative splicing (AS) is a regulated process that enables the production of multiple mRNA transcripts from a single multi-exon gene. The availability of large-scale RNA-seq datasets has made it possible to predict splice junctions, as well as splice sites through spliced alignment to the reference genome. This greatly enhances the capability to decipher gene structures and explore the diversity of splicing variants. However, existing ab initio aligners are vulnerable to false positive spliced alignments as a result of sequence errors and random sequence matches. These spurious alignments can lead to a significant set of false positive splice junction predictions, confusing downstream analyses of splice variant detection and abundance estimation. In this work, we illustrate that splice junction sequence characteristics can be ascertained from experimental data with deep learning techniques. We employ deep convolutional neural networks for a novel splice junction classification tool named DeepSplice that (i) outperforms state-of-the-art methods for predicting splice sites, (ii) shows high computational efficiency and (iii) can be applied to self-defined training data by users.","email":["liuj@cs.uky.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822541","source":"ieee","year":2016,"key":"b2a09a50-d133-431c-bf2e-3cd61d8725d5","use":1,"doi":"10.1109\/BIBM.2016.7822541"},{"Title":"Towards recognition of protein function based on its structure using deep convolutional networks","Description":"A. Tavanaei,  A. S. Maida,  A. Kaniymattam,  R. Loganantharaj","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"This paper proposes a novel method for protein function recognition using deep learning. Recently, deep convolutional neural networks (DCNNs) demonstrated high performances in many areas of pattern recognition. Protein function is often associated with its tertiary structure denoting the active domain of a protein. This investigation develops a novel DCNN for protein functionality recognition based on its tertiary structure. Two rounds of experiments are performed. The initial experiment on tertiary protein structure alignment shows promising performances (94% accuracy rate) such that it shows the model robustness against rotations, local translations, and scales of the 3D structure. With these results, the main experiments contain five different datasets obtained by similarity measures between pairs of gene ontology terms. The experimental results for protein function recognition on selected datasets show 87.6% and 80.7% maximum and average accuracy rates respectively. The initial success of the DCNN in tertiary protein structure recognition supports further investigations with respect to tertiary protein retrieval and pattern mining on large scale problems.","email":["tavanaei@louisiana.edu","maida@cacs.louisiana.edu","axk9969@louisiana.edu","raja@louisiana.eduAbstract"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822509","source":"ieee","year":2016,"key":"3bf24f74-904b-4ccb-9fb1-db3b9e55146f","use":1,"doi":"10.1109\/BIBM.2016.7822509"},{"Title":"Layerwise feature selection in Stacked Sparse Auto-Encoder for tumor type prediction","Description":"V. Singh,  N. Baranwal,  R. K. Sevakula,  N. K. Verma,  Y. Cui","ShortDetails":"2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2016","abstract":"Transcriptome data has been proved to be very valuable for clinical applications, such as diagnosis and prognosis of various cancers. In this paper, we present layer-wise feature selection in conjunction with stacked sparse auto-encoders (SSAE), a deep learning strategy for tumor classification with gene expression data. While SSAE learns high-level features from data, performing feature selection in every layer is a heuristic to obtain relevant features at every stage and also to assist in reducing the computation during fine-tuning procedure. The data in the new feature representation is finally used by classifier(s) to perform Tumor detection. The algorithm was tested on 36 datasets from the GEMLeR repository and w.r.t. AUC (Area under ROC curve) performance, it was found to outperform the GEMLeR benchmark results on 35 datasets (tied on the other dataset).","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7822750","source":"ieee","year":2016,"key":"bdeef6ed-3d66-4631-b980-cf0ee9687f7d","use":1,"doi":"10.1109\/BIBM.2016.7822750"},{"Title":"Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis","Description":"W. Zhang,  R. Li,  T. Zeng,  Q. Sun,  S. Kumar,  J. Ye,  S. Ji","ShortDetails":"IEEE Transactions on Big Data. 2017","abstract":"A central theme in learning from image data is to develop appropriate representations for the specific task at hand. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila, texture features were particularly effective for determining the developmental stages from in situ hybridization images. Such image representation is however not suitable for controlled vocabulary term annotation. Here, we developed feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain. To account for the differences between the source and target domains, we proposed a partial transfer learning scheme in which only part of the source model is transferred. We employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images. Results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges.","email":["qsun21@asu.edu","s.kumar@temple.edu","jpye@umich.edu","3@224","256@28","512@28","512@14","512@14"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7480825","source":"ieee","year":2017,"key":"f9dd0f4e-7d80-4cbe-9abc-130bf7515c10","use":1,"doi":"10.1109\/TBDATA.2016.2573280"},{"Title":"Learning of Generic Vision Features Using Deep CNN","Description":"K. N. D.,  B. S. P.","ShortDetails":"2015 Fifth International Conference on Advances in Computing and Communications (ICACC). 2015","abstract":"Eminence of learning algorithm applied for computer vision tasks depends on the features engineered from image. It's premise that different representations can interweave and ensnare most of the elucidative genes that are responsible for variations in images, be it rigid, affine or projective. Hence researches give at most attention in hand-engineering features that capture these variations. But problem is, we need subtle domain knowledge to do that. Thereby making researchers elude epitome of representations. Hence learning algorithms never reach their full potential. In recent times there has been a shift from hand-crafting features to representation learning. The resulting features are not only optimal but also generic as in they can be used as off the shelf features for visual recognition tasks. In this paper we design and experiment with a basic deep convolution neural nets for learning generic vision features with an variant of convolving kernels. They operate by giving importance to individual uncorrelated color channels in a color model by convolving each channel with channel specific kernels. We were able to achieve considerable improvement in performance even when using smaller dataset.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7433775","source":"ieee","year":2015,"key":"ef9f4c30-bd8e-4bc4-b758-d3a39945fcbf","use":1,"doi":"10.1109\/ICACC.2015.52"},{"Title":"Deep Convolutional Neural Networks for Multi-instance Multi-task Learning","Description":"T. Zeng,  S. Ji","ShortDetails":"2015 IEEE International Conference on Data Mining. 2015","abstract":"Multi-instance learning studies problems in which labels are assigned to bags that contain multiple instances. In these settings, the relations between instances and labels are usually ambiguous. In contrast, multi-task learning focuses on the output space in which an input sample is associated with multiple labels. In real world, a sample may be associated with multiple labels that are derived from observing multiple aspects of the problem. Thus many real world applications are naturally formulated as multi-instance multi-task (MIMT) problems. A common approach to MIMT is to solve it task-by-task independently under the multi-instance learning framework. On the other hand, convolutional neural networks (CNN) have demonstrated promising performance in single-instance single-label image classification tasks. However, how CNN deals with multi-instance multi-label tasks still remains an open problem. This is mainly due to the complex multiple-to-multiple relations between the input and output space. In this work, we propose a deep leaning model, known as multi-instance multi-task convolutional neural networks (MIMT-CNN), where a number of images representing a multi-task problem is taken as the inputs. Then a shared sub-CNN is connected with each input image to form instance representations. Those sub-CNN outputs are subsequently aggregated as inputs to additional convolutional layers and full connection layers to produce the ultimate multi-label predictions. This CNN model, through transfer learning from other domains, enables transfer of prior knowledge at image level learned from large single-label single-task data sets. The bag level representations in this model are hierarchically abstracted by multiple layers from instance level representations. Experimental results on mouse brain gene expression pattern annotation data show that the proposed MIMT-CNN model achieves superior performance.","email":["nithincvim@gmail.com","pbsk@cb.amrita.edu"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7373362","source":"ieee","year":2015,"key":"d706ffda-c068-4966-9c4c-1142b7e1178d","use":1,"doi":"10.1109\/ICDM.2015.92"},{"Title":"Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets","Description":"M. K. K. Leung,  A. Delong,  B. Alipanahi,  B. J. Frey","ShortDetails":"Proceedings of the IEEE. 2016","abstract":"In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7347331","source":"ieee","year":2016,"key":"ccc76140-0d17-427f-8997-24fe2c5ab973","use":1,"doi":"10.1109\/JPROC.2015.2494198"},{"Title":"Integrative Data Analysis of Multi-Platform Cancer Data with a Multimodal Deep Learning Approach","Description":"M. Liang,  Z. Li,  T. Chen,  J. Zeng","ShortDetails":"IEEE\/ACM Transactions on Computational Biology and Bioinformatics. 2015","abstract":"Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personali- ed cancer therapy.","email":["zengjy321@tsinghua.edu.cn."],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6977954","source":"ieee","year":2015,"key":"9ab00253-0f16-4253-ac08-1964cd6f674e","use":1,"doi":"10.1109\/TCBB.2014.2377729"},{"Title":"Multi-level gene\/MiRNA feature selection using deep belief nets and active learning","Description":"R. Ibrahim,  N. A. Yousri,  M. A. Ismail,  N. M. El-Makky","ShortDetails":"2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 2014","abstract":"Selecting the most discriminative genes\/miRNAs has been raised as an important task in bioinformatics to enhance disease classifiers and to mitigate the dimensionality curse problem. Original feature selection methods choose genes\/miRNAs based on their individual features regardless of how they perform together. Considering group features instead of individual ones provides a better view for selecting the most informative genes\/miRNAs. Recently, deep learning has proven its ability in representing the data in multiple levels of abstraction, allowing for better discrimination between different classes. However, the idea of using deep learning for feature selection is not widely used in the bioinformatics field yet. In this paper, a novel multi-level feature selection approach named MLFS is proposed for selecting genes\/miRNAs based on expression profiles. The approach is based on both deep and active learning. Moreover, an extension to use the technique for miRNAs is presented by considering the biological relation between miRNAs and genes. Experimental results show that the approach was able to outperform classical feature selection methods in hepatocellular carcinoma (HCC) by 9%, lung cancer by 6% and breast cancer by around 10% in F1-measure. Results also show the enhancement in F1-measure of our approach over recently related work in [1] and [2].","email":["Egyptrania.ibrahim.salama@gmail.com","noha.yousri@alexu.edu.eg","drmaismail@gmail.comandnagwamakky"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6944490","source":"ieee","year":2014,"key":"93bda710-be40-4d8f-ac5d-c8466624088d","use":1,"doi":"10.1109\/EMBC.2014.6944490"},{"Title":"MiRTDL: A Deep Learning Approach for miRNA Target Prediction","Description":"S. Cheng,  M. Guo,  C. Wang,  X. Liu,  Y. Liu,  X. Wu","ShortDetails":"IEEE\/ACM Transactions on Computational Biology and Bioinformatics. 2016","abstract":"MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7362158","source":"ieee","year":2016,"key":"7286bcd5-51f7-4f85-a4ff-c074281fa19c","use":1,"doi":"10.1109\/TCBB.2015.2510002"},{"Title":"Gold classification of COPDGene cohort based on deep learning","Description":"J. Ying,  J. Dutta,  N. Guo,  L. Xia,  A. Sitek,  Q. Li,  Q. Li","ShortDetails":"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2016","abstract":"This study aims to employ deep learning for the development of an automatic classifier for the severity of chronic obstructive pulmonary disease (COPD) in patients. A three-layer deep belief network (DBN) with two hidden layers and one visible layer was employed to generate a model for classification, and the model's robustness against exacerbation was analyzed. Subjects from the COPDGene cohort were staged using the GOLD 2011 guidelines. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 97.2% by using a 10-fold cross validation experiment. The most sensitive features as revealed by the DBN weights were consistent with the clinical consensus as per previous studies and clinical diagnosis rules. In summary, we demonstrate that the DBN is a competitive tool for exacerbation risk assessment for patients suffering from, COPD.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7472122","source":"ieee","year":2016,"key":"876b08bd-0b79-4d32-bff7-703afd2f4107","use":1,"doi":"10.1109\/ICASSP.2016.7472122"},{"Title":"Big data analytics in genomics: The point on Deep Learning solutions","Description":"F. Celesti,  A. Celesti,  L. Carnevale,  A. Galletta,  S. Campo,  A. Romano,  P. Bramanti,  M. Villari","ShortDetails":"2017 IEEE Symposium on Computers and Communications (ISCC). 2017","abstract":"Nowadays, Next Generation Sequeencing (NGS) is a catch-all term used to describe different modern DNA sequencing applications that produce big genomics data that can be analysed in a faster fashion than in the past. For this reason, NGS requires more and more sophisticated algorithms and high-performance parallel processing systems able to analyse and extract knowledge from a huge amount of genomics and molecular data. In this context, researchers are beginning to look at emerging deep learning algorithms able to perform efficient big data analytics. In this paper, we analyse and classify the major current deep learning solutions that allow biotechnology researchers to perform big genomics data analytics. Moreover, by means of a taxonomic analysis, we provide a clear picture of the current state of the art also discussing future challenges.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8024547","source":"ieee","year":2017,"key":"91319b21-3d18-4ac9-96ab-723250717627","use":1,"doi":"10.1109\/ISCC.2017.8024547"},{"Title":"Using convolutional neural networks to explore the microbiome","Description":"D. Reiman,  A. Metwally,  Y. Dai","ShortDetails":"2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). 2017","abstract":"The microbiome has been shown to have an impact on the development of various diseases in the host. Being able to make an accurate prediction of the phenotype of a genomic sample based on its microbial taxonomic abundance profile is an important problem for personalized medicine. In this paper, we examine the potential of using a deep learning framework, a convolutional neural network (CNN), for such a prediction. To facilitate the CNN learning, we explore the structure of abundance profiles by creating the phylogenetic tree and by designing a scheme to embed the tree to a matrix that retains the spatial relationship of nodes in the tree and their quantitative characteristics. The proposed CNN framework is highly accurate, achieving a 99.47% of accuracy based on the evaluation on a dataset 1967 samples of three phenotypes. Our result demonstrated the feasibility and promising aspect of CNN in the classification of sample phenotype.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=8037799","source":"ieee","year":2017,"key":"b3d2d865-9513-43ea-aa92-25d428d9d4ca","use":1,"doi":"10.1109\/EMBC.2017.8037799"},{"Title":"Probabilistic Graphical Models and Deep Belief Networks for Prognosis of Breast Cancer","Description":"M. Khademi,  N. S. Nedialkov","ShortDetails":"2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). 2015","abstract":"We propose a probabilistic graphical model (PGM) for prognosis and diagnosis of breast cancer. PGMs are suitable for building predictive models in medical applications, as they are powerful tools for making decisions under uncertainty from big data with missing attributes and noisy evidence. Previous work relied mostly on clinical data to create a predictive model. Moreover, practical knowledge of an expert was needed to build the structure of a model, which may not be accurate. In our opinion, since cancer is basically a genetic disease, the integration of microarray and clinical data can improve the accuracy of a predictive model. However, since microarray data is high-dimensional, including genomic variables may lead to poor results for structure and parameter learning due to the curse of dimensionality and small sample size problems. We address these problems by applying manifold learning and a deep belief network (DBN) to microarray data. First, we construct a PGM and a DBN using clinical and microarray data, and extract the structure of the clinical model automatically by applying a structure learning algorithm to the clinical data. Then, we integrate these two models using softmax nodes. Extensive experiments using real-world databases, such as METABRIC and NKI, show promising results in comparison to Support Vector Machines (SVMs) and k-Nearest Neighbors (k-NN) classifiers, for classifying tumors and predicting events like recurrence and metastasis.","email":["khademm@mcmaster.ca","nedialk@mcmaster.ca"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7424407","source":"ieee","year":2015,"key":"fc623d04-b5cd-498b-a479-1b35c81c318f","use":1,"doi":"10.1109\/ICMLA.2015.196"},{"Title":"The effective diagnosis of schizophrenia by using multi-layer RBMs deep networks","Description":"Chen Qiao,  D. D. Lin,  Shao-Long Cao,  Yu-Ping Wang","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"Schizophrenia is one of the most prevalent mental diseases, and is considered to be caused by the interplay of a number of genetic factors. In this paper, by constructing a multilayer restricted Boltzmann machines (RBMs) deep network, we use the genomic data (i.e., SNP data) for unsupervised feature learning and disease diagnosis of schizophrenia. In order to obtain some more accurate diagnosis results by RBMs, firstly, we transform the SNP data into binary sequences, and then by training the multi-layer RBMs deep network on unlabeled data, the multi-level abstract features of the genomic data are obtained and stored in the network. Finally, by adding a linear classifier to the top of the multi-layer RBMs deep network, the classification results on the testing data are gained. The results show that the average performance of this method is better than that of other methods, e.g., SVM (including linear SVM as well as SVM with multilayer perceptron kernel), sparse representations based classifier and k-nearest neighbors method. It is indicated that the multi-layer RBMs deep network can extract deep hierarchical representations of the genomic data, and then promises a more comprehensive approach for the mental disease diagnosis.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359751","source":"ieee","year":2015,"key":"d942f691-fcc8-47e1-b9ab-5174c0bb5509","use":1,"doi":"10.1109\/BIBM.2015.7359751"},{"Title":"Ensemble of deep long short term memory networks for labelling origin of replication sequences","Description":"U. Singh,  S. Chauhan,  A. Krishnamachari,  L. Vig","ShortDetails":"2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA). 2015","abstract":"Advancement in sequence data generation technologies are churning out voluminous omics data and posing a massive challenge to annotate the biological functional features. Sequence data from the well studied model organism Saccharomyces cerevisiae has been commonly used to test and validate in silico prediction methods. DNA replication is a critical step in the cellular process and the sequence location where this process originates in the genomic landscape is generally referred as origin of replication. In this paper we investigate the application bidirectional Long Short Term (LSTM) Networks to predict origin of replication sequences. Long Short Term Memory (LSTM) networks have recently been shown to yield state of the art performance in speech recognition, and music generation. These networks are capable of learning long term patterns via the use of multiplication gates. This paper utilizes Deep bidirectional LSTM for prediction of origin of replication sequences belonging to the organism Saccharomyces cerevisiae. Results demonstrate that LSTMs outperform the commonly used machine learning classifiers such as Support Vector Machine (SVM), Random Forest (RF), Artificial Neural Network (ANN), and Hidden Markov Model (HMM). An important additional advantage of LSTMs is that they work directly on the sequences and obviate the need for hand coded features.","email":["sit@jnu.ac.in","chauhan2008@yahoo.com","akchari@gmail.com","lovekeshvigin@gmail.com"],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7344871","source":"ieee","year":2015,"key":"89c96d77-4d5e-449a-aa57-5e3851ecce00","use":1,"doi":"10.1109\/DSAA.2015.7344871"},{"Title":"Boosting compound-protein interaction prediction by deep learning","Description":"Kai Tian,  Mingyu Shao,  Shuigeng Zhou,  Jihong Guan","ShortDetails":"2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). 2015","abstract":"The identification of interactions between compounds and proteins plays an important role in network pharmacology and drug discovery. However, experimentally identifying compound-protein interactions (CPIs) is generally expensive and time-consuming, computational approaches are thus introduced. Among these, machine-learning based methods have achieved a considerable success. However, due to the nonlinear and imbalanced nature of biological data, many machine learning approaches have their own limitations. Recently, deep learning techniques show advantages over many state-of-the-art machine learning methods in many applications. In this study, we aim at improving the performance of CPI prediction based on deep learning, and propose a method called DL-CPI (the abbreviation of Deep Learning for Compound-Protein Interactions prediction), which employs deep neural network (DNN) to effectively learn the representations of compound-protein pairs. Extensive experiments show that DL-CPI can learn useful features of compound-protein pairs by a layerwise abstraction, and thus achieves better prediction performance than existing methods on both balanced and imbalanced datasets.","email":[],"fullURL":"http:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=7359651","source":"ieee","year":2015,"key":"bf554964-abd5-493e-9baf-836d9029b2a6","use":1,"doi":"10.1109\/BIBM.2015.7359651"}]