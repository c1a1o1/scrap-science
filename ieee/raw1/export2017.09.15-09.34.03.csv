"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&rowsPerPage%3D10%26queryText%3Ddeep+learning+medical+imaging",2017/09/15 09:34:03
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep learning for tumour classification in homogeneous breast tissue in medical microwave imaging","B. Gerazov; R. C. Conceicao","Faculty of Electrical Engineering and Information Technologies, Ss Cyril and Methodius University in Skopje, Skopje, Macedonia","IEEE EUROCON 2017 -17th International Conference on Smart Technologies","20170817","2017","","","564","569","Deep learning has become the state-of-the-art in the area of biomedical imaging, leading to a large boost in performance that approaches human levels. Medical microwave imaging is an emerging technology that has great potential especially in the area of breast cancer diagnosis. Moreover, the obtained backscatter signals have also been shown to be a good basis for differentiating malignant and benign tumour type. We further analyse these results by applying deep learning methods to a dataset of Finite Difference Time Domain (FDTD) numerical simulations of tumour models embedded in homogeneous breast adipose tissue. Specifically we use Deep and Convolutional Neural Networks and obtain an accuracy of 93.44% which outperforms conventional machine learning previously used on the analysed dataset.","","Electronic:978-1-5090-3843-5; POD:978-1-5090-3844-2; USB:978-1-5090-3842-8","10.1109/EUROCON.2017.8011175","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011175","CNN;DNN;breast tumour classification;deep learning;feature embedding;medical microwave imaging","Biomedical imaging;Finite difference methods;Machine learning;Microwave theory and techniques;Numerical models;Time-domain analysis;Tumors","cancer;convolution;finite difference time-domain analysis;image classification;learning (artificial intelligence);medical image processing;microwave imaging;neural nets;tumours","FDTD numerical simulations;backscatter signals;breast cancer diagnosis;convolutional neural networks;deep learning;deep neural networks;finite difference time domain;homogeneous breast tissue;medical microwave imaging;tumour classification","","","","","","","","6-8 July 2017","","IEEE","IEEE Conference Publications"
"Deep Features Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network","M. Chen; X. Shi; Y. Zhang; D. Wu; M. Guizani","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, HuBei China (e-mail: minchen@ieee.org)","IEEE Transactions on Big Data","","2017","PP","99","1","1","At present, computed tomography (CT) are widely used to assist diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) is an extremely important research field in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it evaluates that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during of artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.","","","10.1109/TBDATA.2017.2717439","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954012","Convolutional autoencoder neural network;Feature learning;Hand-craft feature;Lung nodule;Unsupervised learning","Biomedical imaging;Computed tomography;Convolutional codes;Feature extraction;Image analysis;Lungs;Training","","","","","","","","","20170620","","","IEEE","IEEE Early Access Articles"
"A comparison of deep learning and hand crafted features in medical image modality classification","S. Khan; S. P. Yong","Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Malaysia","2016 3rd International Conference on Computer and Information Sciences (ICCOINS)","20161215","2016","","","633","638","Modality corresponding to medical images is a vital filter in medical image retrieval systems, as radiologists or physicians are interested in only one of radiology images e.g CT scan, MRI, X-ray. Various handcrafted feature schemes have been proposed for medical image modality classification. On the other hand not enough attempts have been made for deep learned feature extraction. A comparative evaluation of both handcrafted and deep learned features for medical image modality classification is presented in this paper. The experiments are performed on IMAGECLEF 2012 data. After carrying out the experiments it is shown that the handcrafted features outperforms the deep learned features and shows the potential of handcrafted feature extraction models in the medical image field.","","Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1","10.1109/ICCOINS.2016.7783289","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783289","Feature representations;deep learned features;handcrafted features;modality classification","Biomedical imaging;Computer architecture;Computers;Feature extraction;Machine learning;Visualization","feature extraction;image classification;image retrieval;learning (artificial intelligence);medical image processing;radiology","IMAGECLEF 2012 data;deep learned feature extraction;hand crafted features;handcrafted feature extraction models;medical image modality classification;medical image retrieval systems;radiology images","","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Deep learning of feature representation with multiple instance learning for medical image analysis","Y. Xu; T. Mo; Q. Feng; P. Zhong; M. Lai; E. I. C. Chang","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20140714","2014","","","1626","1630","This paper studies the effectiveness of accomplishing high-level tasks with a minimum of manual annotation and good feature representations for medical images. In medical image analysis, objects like cells are characterized by significant clinical features. Previously developed features like SIFT and HARR are unable to comprehensively represent such objects. Therefore, feature representation is especially important. In this paper, we study automatic extraction of feature representation through deep learning (DNN). Furthermore, detailed annotation of objects is often an ambiguous and challenging task. We use multiple instance learning (MIL) framework in classification training with deep learning features. Several interesting conclusions can be drawn from our work: (1) automatic feature learning outperforms manual feature; (2) the unsupervised approach can achieve performance that's close to fully supervised approach (93.56%) vs. (94.52%); and (3) the MIL performance of coarse label (96.30%) outweighs the supervised performance of fine label (95.40%) in supervised deep learning features.","1520-6149;15206149","Electronic:978-1-4799-2893-4; POD:978-1-4799-2894-1; USB:978-1-4799-2892-7","10.1109/ICASSP.2014.6853873","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853873","deep learning;feature learning;multiple instance learning;supervised;un-supervised","Biomedical imaging;Cancer;Feature extraction;Manuals;Supervised learning;Training;Vectors","feature extraction;image representation;learning (artificial intelligence);medical image processing","DNN;HARR features;MIL framework;SIFT features;automatic feature representation extraction;classification training;clinical features;feature representation;manual annotation;medical image analysis;multiple instance learning;supervised deep learning features;unsupervised approach","","14","","26","","","","4-9 May 2014","","IEEE","IEEE Conference Publications"
"Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique","H. Greenspan; B. van Ginneken; R. M. Summers","Biomedical Image Computing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel-Aviv University, Tel-Aviv, Israel","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1153","1159","The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.","0278-0062;02780062","","10.1109/TMI.2016.2553401","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094","","Artificial neural networks;Biomedical image processing;Computer vision;Data analysis;Machine learning;Special issues and sections","","","","22","","38","","","","May 2016","","IEEE","IEEE Journals & Magazines"
"Iterative deep convolutional encoder-decoder network for medical image segmentation","J. U. Kim; H. G. Kim; Y. M. Ro","Image and Video Systems Lab., School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Republic of Korea","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","685","688","In this paper, we propose a novel medical image segmentation using iterative deep learning framework. We have combined an iterative learning approach and an encoder-decoder network to improve segmentation results, which enables to precisely localize the regions of interest (ROIs) including complex shapes or detailed textures of medical images in an iterative manner. The proposed iterative deep convolutional encoder-decoder network consists of two main paths: convolutional encoder path and convolutional decoder path with iterative learning. Experimental results show that the proposed iterative deep learning framework is able to yield excellent medical image segmentation performances for various medical images. The effectiveness of the proposed method has been proved by comparing with other state-of-the-art medical image segmentation methods.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8036917","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036917","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Intervertebral disc detection in X-ray images using faster R-CNN","R. Sa; W. Owens; R. Wiegand; M. Studin; D. Capoferri; K. Barooha; A. Greaux; R. Rattray; A. Hutton; J. Cintineo; V. Chaudhary","State University of New York (SUNY) at Buffalo, United States of America","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","564","567","Automatic identification of specific osseous landmarks on the spinal radiograph can be used to automate calculations for correcting ligament instability and injury, which affect 75% of patients injured in motor vehicle accidents. In this work, we propose to use deep learning based object detection method as the first step towards identifying landmark points in lateral lumbar X-ray images. The significant breakthrough of deep learning technology has made it a prevailing choice for perception based applications, however, the lack of large annotated training dataset has brought challenges to utilizing the technology in medical image processing field. In this work, we propose to fine tune a deep network, Faster-RCNN, a state-of-the-art deep detection network in natural image domain, using small annotated clinical datasets. In the experiment we show that, by using only 81 lateral lumbar X-Ray training images, one can achieve much better performance compared to traditional sliding window detection method on hand crafted features. Furthermore, we fine-tuned the network using 974 training images and tested on 108 images, which achieved average precision of 0.905 with average computation time of 3 second per image, which greatly outperformed traditional methods in terms of accuracy and efficiency.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8036887","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036887","X-Ray;deep learning;detection;intervertebral disc","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Automatic Skin Lesion Segmentation Using Deep Fully Convolutional Networks With Jaccard Distance","Y. Yuan; M. Chao; Y. C. Lo","Department of Radiation Oncology, Icahn School of Medicine at Mount Sinai, New York, NY, USA","IEEE Transactions on Medical Imaging","20170830","2017","36","9","1876","1886","Automatic skin lesion segmentation in dermoscopic images is a challenging task due to the low contrast between lesion and the surrounding skin, the irregular and fuzzy lesion borders, the existence of various artifacts, and various imaging acquisition conditions. In this paper, we present a fully automatic method for skin lesion segmentation by leveraging 19-layer deep convolutional neural networks that is trained end-to-end and does not rely on prior knowledge of the data. We propose a set of strategies to ensure effective and efficient learning with limited training data. Furthermore, we design a novel loss function based on Jaccard distance to eliminate the need of sample re-weighting, a typical procedure when using cross entropy as the loss function for image segmentation due to the strong imbalance between the number of foreground and background pixels. We evaluated the effectiveness, efficiency, as well as the generalization capability of the proposed framework on two publicly available databases. One is from ISBI 2016 <italic>skin lesion analysis towards melanoma detection</italic> challenge, and the other is the PH2 database. Experimental results showed that the proposed method outperformed other state-of-the-art algorithms on these two databases. Our method is general enough and only needs minimum pre- and post-processing, which allows its adoption in a variety of medical image segmentation tasks.","0278-0062;02780062","","10.1109/TMI.2017.2695227","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7903636","Deep learning;dermoscopy;fully convolutional neural networks;image segmentation;jaccard distance;melanoma","Biomedical imaging;Databases;Image segmentation;Lesions;Malignant tumors;Skin","","","","","","","","","20170418","Sept. 2017","","IEEE","IEEE Journals & Magazines"
"Looking Under the Hood: Deep Neural Network Visualization to Interpret Whole-Slide Image Analysis Outcomes for Colorectal Polyps","B. Korbar; A. M. Olofson; A. P. Miraflor; C. M. Nicka; M. A. Suriawinata; L. Torresani; A. A. Suriawinata; S. Hassanpour","","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20170824","2017","","","821","827","Histopathological characterization of colorectal polyps is an important principle for determining the risk of colorectal cancer and future rates of surveillance for patients. The process of characterization is time-intensive and requires years of specialized medical training. In this work, we propose a deep-learning-based image analysis approach that not only can accurately classify different types of polyps in whole-slide images, but also generates major regions and features on the slide through a model visualization approach. We argue that this visualization approach will make sense of the underlying reasons for the classification outcomes, significantly reduce the cognitive burden on clinicians, and improve the diagnostic accuracy for whole-slide image characterization tasks. Our results show the efficacy of this network visualization approach in recovering decisive regions and features for different types of polyps on whole-slide images according to the domain expert pathologists.","","Electronic:978-1-5386-0733-6; POD:978-1-5386-0734-3","10.1109/CVPRW.2017.114","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014848","","Agriculture;Backpropagation;Cancer;Computer architecture;Neural networks;Training;Visualization","","","","","","","","","","21-26 July 2017","","IEEE","IEEE Conference Publications"
"Retrieval From and Understanding of Large-Scale Multi-modal Medical Datasets: A Review","H. Müller; D. Unay","Information Systems Institute, HES-SO Valais, Sierre, Switzerland","IEEE Transactions on Multimedia","20170814","2017","19","9","2093","2104","Content-based multimedia retrieval (CBMR) has been an active research domain since the mid 1990s. In medicine visual retrieval started later and has mostly remained a research instrument and less a clinical tool. The limited size of data sets due to privacy constraints is often mentioned as reason for these limitations. Nevertheless, much work has been done in CBMR, including the availability of increasingly large data sets and scientific challenges. Annotated data sets and clinical data for images have now become available and can be combined for multi-modal retrieval. Much has been learned on user behavior and application scenarios. This text is motivated by the advances in medical image analysis and the availability of public large data sets that often include clinical data. It is a systematic review of recent work (concentrating on the period 2011-2017) on multi-modal CBMR and image understanding in the medical domain, where image understanding includes techniques such as detection, localization, and classification for leveraging visual content. With the objective of summarizing the current state of research for multimedia researchers outside the medical field, the text provides ways to get data sets and identifies current limitations and promising research directions. The text highlights advances in the past six years and a trend to use larger scale training data and deep learning approaches that can replace/complement hand-crafted features. Using images alone will likely only work in limited domains but combining multiple sources of data for multi-modal retrieval has the biggest chances of success, particularly for clinical impact.","1520-9210;15209210","","10.1109/TMM.2017.2729400","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984864","Big data;content–based image retrieval;deep learning;large scale datasets;medical images;multi–modality","Machine learning;Medical diagnostic imaging;Multimedia communication;Tools;Visualization","Big Data;data privacy;image retrieval;information retrieval;medical administrative data processing;medical image processing","active research domain;annotated data sets;clinical data;clinical impact;clinical tool;content-based multimedia retrieval;deep learning;hand-crafted features;large-scale multimodal medical datasets;medical domain;medical field;medical image analysis;medicine visual retrieval;multimedia researchers;multimodal CBMR;multimodal retrieval;privacy constraints;public large data sets;research instrument;training data;user behavior","","","","","","","20170719","Sept. 2017","","IEEE","IEEE Journals & Magazines"
"Cell classification using convolutional neural networks in medical hyperspectral imagery","Xiang Li; W. Li; Xiaodong Xu; Wei Hu","College of Information Science & Technology, Beijing University of Chemical Technology, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)","20170720","2017","","","501","504","Hyperspectral imaging is a rising imaging modality in the field of medical applications, and the combination of both spectral and spatial information provides wealth information for cell classification. In this paper, deep convolutional neural network (CNN) is employed to achieve blood cell discrimination in medical hyperspectral images (MHSI). As a deep learning architecture, CNNs are expected to get more discriminative and semantic features, which effect classification accuracy to a certain extent. Experimental results based on two real medical hyperspectral image data sets demonstrate that cell classification using CNNs is effective. In addition, compared to traditional support vector machine (SVM), the proposed method, which jointly exploits spatial and spectral features, can achieve better classification performance, showcasing the CNN-based methods' tremendous potential for accurate medical hyperspectral data classification.","","DVD:978-1-5090-6236-2; Electronic:978-1-5090-6238-6; POD:978-1-5090-6239-3","10.1109/ICIVC.2017.7984606","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984606","blood cell classification;convolutional neural network;deep learning;medical hyperspectral imagery","Blood;Computer architecture;Hyperspectral imaging;Medical diagnostic imaging;Microprocessors;Support vector machines","blood;cellular biophysics;feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);medical image processing;neural nets","CNN;blood cell discrimination;cell classification;deep convolutional neural network;deep learning architecture;discriminative features;hyperspectral imaging;medical applications;medical hyperspectral data classification;medical hyperspectral imagery;semantic features;spatial features;spatial information;spectral features;spectral information","","","","","","","","2-4 June 2017","","IEEE","IEEE Conference Publications"
"Application of neural network based on SIFT local feature extraction in medical image classification","Shuqi Cui; Hong Jiang; Zheng Wang; Chaomin Shen","East China Normal University, Department of Computer Center, Shanghai, China","2017 2nd International Conference on Image, Vision and Computing (ICIVC)","20170720","2017","","","92","97","In the medical image analysis, ROI (Region of Interest) is one of the key features of clinical diagnostic analysis. The applying of local features of ROI to the deep learning of image classification has the advantage of noise eliminating and information reducing. Based on existing research results, using Scale Invariant Feature Transformation (SIFT) algorithm combined with SVM classifier and sliding window to extract the local features and describe ROI precisely in the image. Finally, the extracted feature is used as the input layer of BP neural network in mammary gland X - ray image classification. The experimental results show that the accuracy of neural network classifier based on SIFT is 96.57%, which is 3.44% higher than that of traditional SVM classification accuracy. It is verified that our classifier is important to support clinical diagnosis and diagnosis.","","DVD:978-1-5090-6236-2; Electronic:978-1-5090-6238-6; POD:978-1-5090-6239-3","10.1109/ICIVC.2017.7984525","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984525","BP neural network;ROI;SIFT;SVM;slide the window","Biological neural networks;Feature extraction;Image classification;Medical diagnostic imaging;Neurons","X-ray imaging;backpropagation;feature extraction;image classification;image denoising;medical image processing;neural nets;support vector machines;transforms","BP neural network;ROI;SIFT local feature extraction;SVM classifier;clinical diagnostic analysis;deep learning;information reduction;mammary gland X-ray image classification;medical image analysis;medical image classification;neural network classifier;noise elimination;region of interest;scale invariant feature transformation;sliding window","","","","","","","","2-4 June 2017","","IEEE","IEEE Conference Publications"
"Detecting Anatomical Landmarks From Limited Medical Imaging Data Using Two-Stage Task-Oriented Deep Neural Networks","J. Zhang; M. Liu; D. Shen","Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Image Processing","20170718","2017","26","10","4753","4764","One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.","1057-7149;10577149","","10.1109/TIP.2017.2721106","10.13039/100000002 - NIH; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961205","Anatomical landmark detection;deep convolutional neural networks;limited medical imaging data;real-time;task-oriented","Biological neural networks;Biomedical imaging;Machine learning;Testing;Three-dimensional displays;Training;Training data","biomedical MRI;computerised tomography;feedforward neural nets;learning (artificial intelligence);medical image processing;object detection;regression analysis","3D T1-weighted magnetic resonance images;3D computed tomography images;CNN based regression model;anatomical landmark coordinate prediction;anatomical landmark detection;brain landmarks;deep convolutional neural networks;fully convolutional network;local image patches;medical imaging data;network learning;prostate landmarks;training data;two-stage task-oriented deep learning method;two-stage task-oriented deep neural networks","","","","","","","20170628","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"An Automatic Detection System of Lung Nodule Based on Multi-Group Patch-Based Deep Learning Network","H. Jiang; H. Ma; W. Qian; M. Gao; Y. Li","shenyang China (e-mail: hongyang1020@126.com)","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","High-efficiency lung nodule detection dramatically contributes to the risk assessment of lung cancer. It is a significant and challenging task to quickly locate the exact positions of lung nodules. Extensive work has been done by researchers around this domain for approximately two decades. However, previous computer aided detection (CADe) schemes are mostly intricate and time-consuming since they may require more image processing modules, such as the computed tomography (CT) image transformation, the lung nodule segmentation and the feature extraction, to construct a whole CADe system. It is difficult for those schemes to process and analyze enormous data when the medical images continue to increase. Besides, some state of the art deep learning schemes may be strict in the standard of database. This study proposes an effective lung nodule detection scheme based on multi-group patches cut out from the lung images, which are enhanced by the Frangi filter. Through combining two groups of images, a four-channel convolution neural networks (CNN) model is designed to learn the knowledge of radiologists for detecting nodules of four levels. This CADe scheme can acquire the sensitivity of 80.06% with 4.7 false positives per scan and the sensitivity of 94% with 15.1 false positives per scan. The results demonstrate that the multi-group patch-based learning system is efficient to improve the performance of lung nodule detection and greatly reduce the false positives under a huge amount of image data.","2168-2194;21682194","","10.1109/JBHI.2017.2725903","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981333","Frangi filter;computed tomography (CT) images;computer aided detection (CADe);deep learning network;lung nodule detection","Biomedical imaging;Cancer;Computed tomography;Databases;Feature extraction;Image segmentation;Lungs","","","","","","","","","20170714","","","IEEE","IEEE Early Access Articles"
"Constrained Deep Weak Supervision for Histopathology Image Segmentation","Z. Jia; X. Huang; E. I. C. Chang; Y. Xu","Microsoft Research, Beijing 100080, China and Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-toimage weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.","0278-0062;02780062","","10.1109/TMI.2017.2724070","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7971941","Convolutional neural networks;fully convolutional networks;histopathology image segmentation;multiple instance learning;weakly supervised learning","Biomedical imaging;Cancer;Image segmentation;Neural networks;Prediction algorithms;Supervised learning;Training","","","","","","","","","20170707","","","IEEE","IEEE Early Access Articles"
"Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection","D. Sarikaya; J. J. Corso; K. A. Guru","Department of Computer Science and Engineering, SUNY Buffalo, NY, USA","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1542","1549","Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.","0278-0062;02780062","","10.1109/TMI.2017.2665671","10.13039/100006398 - Roswell Park Alliance Foundation Roswell Park Cancer Institute; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847313","Object detection;image classification;laparoscopes;multi-layer neural network;telerobotics","Neural networks;Object detection;Proposals;Robots;Surgery;Training;Videos","biomedical optical imaging;computer vision;gesture recognition;image fusion;image motion analysis;learning (artificial intelligence);manipulators;medical image processing;medical robotics;neural nets;object detection;surgery;telerobotics;video signal processing","ATLAS Dione;RAS videos;RPN;Roswell Park Cancer Institute;computer vision approach;daVinci Surgical System;deep learning;deep neural networks;effective skill acquisition;frame detection;gestures;human-robot collaborative surgeries;image fusion;localization open problem;mean computation time;medical imaging;multimodal convolutional neural networks;multimodal two stream convolutional network;object detection;objective skill assessment;objectness;real-time feedback;region detection;region proposal network;robot-assisted surgery videos;robotic tools;skill level;temporal motion cues;tool detection;tool localization","","","","","","","20170208","July 2017","","IEEE","IEEE Journals & Magazines"
"A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology","N. Kumar; R. Verma; S. Sharma; S. Bhargava; A. Vahadane; A. Sethi","IIT Guwahati, Guwahati, India","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1550","1560","Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.","0278-0062;02780062","","10.1109/TMI.2017.2677499","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872382","Annotation;boundaries;dataset;deep learning;nuclear segmentation;nuclei","Diseases;Image color analysis;Image segmentation;Machine learning;Measurement;Pathology;Training","biological organs;biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;object recognition;optical microscopy","H&E-stained images;Otsu thresholding;chromatin-sparse;computational pathology;conventional image processing techniques;crowded nuclei;deep learning;digital microscopic tissue images;disease states;generalized nuclear segmentation;hematoxylin and eosin-stained tissue images;high-quality feature extraction;image classification problems;machine learning algorithms;machine learning-based segmentation;nuclear appearances;nuclear boundaries;nuclear morphometrics;object recognition;object-level errors;organs;overlapping nuclei;pixel-level errors;right out-of-the-box;segmentation technique;watershed segmentation","","","","","","","20170306","July 2017","","IEEE","IEEE Journals & Magazines"
"Knowledge transfer for melanoma screening with deep learning","A. Menegola; M. Fornaciali; R. Pires; F. V. Bittencourt; S. Avila; E. Valle","RECOD Lab, DCA, FEEC, University of Campinas (Unicamp), Brazil","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","297","300","Knowledge transfer impacts the performance of deep learning - the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pretrained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950523","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950523","Melanoma screening;deep learning;dermoscopy;transfer learning","Biomedical imaging;Computer architecture;Lesions;Malignant tumors;Retinopathy;Training;Tuning","biomedical optical imaging;cancer;image classification;learning (artificial intelligence);medical image processing;neural nets;skin","AUC;ImageNet;are under curve;automated melanoma screening;deep learning;image classification;knowledge transfer;skin lesion datasets;transfer learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"HEp-2 cell classification based on a Deep Autoencoding-Classification convolutional neural network","J. Liu; B. Xu; L. Shen; J. Garibaldi; G. Qiu","The Universiy of Nottingham Ningbo China, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1019","1023","In this paper, we present a novel deep learning model termed Deep Autoencoding-Classification Network (DACN) for HEp-2 cell classification. The DACN consists of an autoencoder and a normal classification convolutional neural network (CNN), while the two architectures shares the same encoding pipeline. The DACN model is jointly optimized for the classification error and the image reconstruction error based on a multi-task learning procedure. We evaluate the proposed model using the publicly available ICPR2012 benchmark dataset. We show that this architecture is particularly effective when the training dataset is small which is often the case in medical imaging applications. We present experimental results to show that the proposed approach outperforms all known state of the art HEp-2 cell classification methods.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950689","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950689","HEp2 cells;autoencoder;classification;convolutional neural networks;indirect immunofluorescence","Benchmark testing;Computer architecture;Image reconstruction;Machine learning;Microprocessors;Solid modeling;Training","cellular biophysics;image classification;image coding;image reconstruction;learning (artificial intelligence);medical image processing;neural nets","HEp-2 cell classification methods;deep autoencoding-classification convolutional neural network;image reconstruction error;medical imaging applications;multitask learning procedure","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"An easy-to-use image labeling platform for automatic magnetic resonance image quality assessment","T. Küstner; P. Wolf; M. Schwartz; A. Liebgott; F. Schick; S. Gatidis; B. Yang","Institute of Signal Processing and System Theory, University of Stuttgart, Germany","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","754","757","In medical imaging, images are usually evaluated by a human observer (HO) depending on the underlying diagnostic question which can be a time-demanding and cost-intensive process. Model observers (MO) which mimic the human visual system can help to support the HO during this reading process or can provide feedback to the MR scanner and/or HO about the derived image quality. For this purpose MOs are trained on HO-derived image labels with respect to a certain diagnostic task. We propose a non-reference image quality assessment system based on a machine-learning approach with a deep neural network and active learning to keep the amount of needed labeled training data small. A labeling platform is developed as a web application with accounted data security and confidentiality to facilitate the HO labeling procedure. The platform is made publicly available.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950628","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950628","deep neural network;image labeling;machine-learning;magnetic resonance;non-reference image quality assessment","Databases;Feature extraction;Image quality;Imaging;Labeling;Servers;Training","biomedical MRI;learning (artificial intelligence);medical image processing;neural nets;security of data","HO-derived image labels;MR scanner;active learning;automatic magnetic resonance image quality assessment;confidentiality;data security;deep neural network;human observer;image labeling platform;machine learning;medical imaging;model observers","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Lung nodule detection in CT using 3D convolutional neural networks","X. Huang; J. Shan; V. Vaidya","GE Global Research, Niskayuna, NY, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","379","383","We propose a new computer-aided detection system that uses 3D convolutional neural networks (CNN) for detecting lung nodules in low dose computed tomography. The system leverages both a priori knowledge about lung nodules and confounding anatomical structures and data-driven machine-learned features and classifier. Specifically, we generate nodule candidates using a local geometric-model-based filter and further reduce the structure variability by estimating the local orientation. The nodule candidates in the form of 3D cubes are fed into a deep 3D convolutional neural network that is trained to differentiate nodule and non-nodule inputs. We use data augmentation techniques to generate a large number of training examples and apply regularization to avoid overfitting. On a set of 99 CT scans, the proposed system achieved state-of-the-art performance and significantly outperformed a similar hybrid system that uses conventional shallow learning. The experimental results showed benefits of using a priori models to reduce the problem space for data-driven machine learning of complex deep neural networks. The results also showed the advantages of 3D CNN over 2D CNN in volumetric medical image analysis.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950542","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950542","3D convolutional neural networks;CT;Lung nodule;computer-aided detection;deep learning","Computed tomography;Lungs;Neural networks;Solid modeling;Three-dimensional displays;Training;Two dimensional displays","computerised tomography;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","3D CNN;CT scans;complex deep neural networks;computer-aided detection;conventional shallow learning;data augmentation;data-driven machine-learned classifier;data-driven machine-learned features;deep 3D convolutional neural networks;local geometric-model-based filter;low dose computed tomography;lung nodule detection;structure variability;volumetric medical image analysis","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"A study on automated segmentation of blood regions in Wireless Capsule Endoscopy images using fully convolutional networks","X. Jia; M. Q. H. Meng","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","179","182","Wireless Capsule Endoscopy (WCE) is a novel diagnostic modality of endoscopic imaging which facilitates direct visualization of the gastrointestinal (GI) tract. Many computational methods that can automatically detect and/or characterize the abnormalities from WCE sequences are developed to support medical decision-making. This paper presents a new approach for automated segmentation of blood regions in WCE images via a deep learning strategy. The proposed method first classify the bleeding samples into active and inactive subgroups based on the statistical features derived from the histogram probability of the color space. Then for each subgroup, we highlight the blood regions via fully convolutional networks (FCNs). Experimental results on the clinical WCE dataset demonstrate the efficacy of our approach, which achieves comparable or better performance than the state-of-the-art methods.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950496","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950496","Wireless capsule endoscopy;bleeding region segmentation;fully convolutional networks","Blood;Endoscopes;Hemorrhaging;Image color analysis;Image segmentation;Training;Wireless communication","biological organs;blood;decision making;endoscopes;image segmentation;image sequences;learning (artificial intelligence);medical image processing;statistical analysis","WCE sequences;automated segmentation;bleeding samples;blood regions;deep learning strategy;endoscopic imaging;fully convolutional networks;gastrointestinal tract;histogram probability;medical decision-making;statistical features;wireless capsule endoscopy images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Classification of MRI data using deep learning and Gaussian process-based model selection","H. Bertrand; M. Perrot; R. Ardon; I. Bloch","LTCI, T&#x00E9;l&#x00E9;com ParisTech, Universit&#x00E9; Paris-Saclay, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","745","748","The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20% for the most difficult classes).","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950626","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950626","Classification;Convolutional Neural Networks;Deep Learning;Gaussian Process;MRI;Model Selection","Abdomen;Biomedical imaging;Machine learning;Magnetic resonance imaging;Optimization;Pelvis;Training","Gaussian processes;biomedical MRI;computer vision;image classification;learning (artificial intelligence);medical image processing","Gaussian process-based model selection;MRI image classification;adaptive search;anatomical field of view;computer vision problems;deep learning;random search","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN)","H. Chen; Y. Zhang; M. K. Kalra; F. Lin; Y. Chen; P. Liao; J. Zhou; G. Wang","College of Computer Science, Sichuan University, Chengdu 610065, China.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.","0278-0062;02780062","","10.1109/TMI.2017.2715284","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947200","Low-dose CT;auto-encoder;convolutional;deconvolutional;deep learning;residual neural network","Computed tomography;Convolution;Decoding;Feature extraction;Image reconstruction;X-ray imaging","","","","","","","","","20170613","","","IEEE","IEEE Early Access Articles"
"Deep Learning for Health Informatics","D. Ravì; C. Wong; F. Deligianni; M. Berthelot; J. Andreu-Perez; B. Lo; G. Z. Yang","Hamlyn Centre, Imperial College London, London, U.K.","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","4","21","With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.","2168-2194;21682194","","10.1109/JBHI.2016.2636665","EPSRC Smart Sensing for Surgery; EPSRC-NIHR HTC Partnership Award; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801947","Bioinformatics;deep learning;health informatics;machine learning;medical imaging;public health;wearable devices","Artificial neural networks;Biological neural networks;Biomedical imaging;Informatics;Machine learning;Neurons;Training","bioinformatics;learning (artificial intelligence);medical information systems;neural nets","artificial intelligence;deep learning;health informatics;machine learning;medical imaging;medical informatics;multimodality data;parallelization;pervasive sensing;public health;translational bioinformatics","","","","","","","20161229","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks","H. Chen; D. Ni; J. Qin; S. Li; X. Yang; T. Wang; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Journal of Biomedical and Health Informatics","20170520","2015","19","5","1627","1636","Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.","2168-2194;21682194","","10.1109/JBHI.2015.2425041","Hong Kong Innovation and Technology Fund; Research Grants Council of Hong Kong; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943","Convolutional neural network (CNN);Ultrasound;convolutional neural network;deep learning;domain transfer;knowledge transfer;standard plane;ultrasound (US)","Biomedical imaging;Dictionaries;Feature extraction;Informatics;Standards;Training;Videos","biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics","FASP localization;US videos;automatic standard plane localization;classification performance;domain transferred deep convolutional neural network;fetal abdominal standard plane;fetal ultrasound;high-level features;learning-based approach;low-level features;medical image computing problems;natural images;overfitting problem;task-specific CNN;transfer learning strategy;ultrasound videos","0;Abdomen;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;ROC Curve;Ultrasonography, Prenatal","32","","37","","","20150421","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","76","84","Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","2168-2194;21682194","","10.1109/JBHI.2016.2636929","Bern University Hospital; 10.13039/501100001711 - Swiss National Science Foundation (SNSF); ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792","Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning","Biomedical imaging;Computed tomography;Databases;Knowledge engineering;Lungs;Machine learning;Training","biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets","CT images;computed tomography;computer-aided diagnosis;convolutional neural networks;fused knowledge compression;interstitial lung disease diagnosis;lung pattern analysis;lung tissue data;medical image analysis;multisource transfer learning;texture classification;texture databases","","","","","","","20161207","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Evaluation of feature descriptors for cancerous tissue recognition","P. Stanitsas; A. Cherian; Xinyan Li; A. Truskinovsky; V. Morellas; N. Papanikolopoulos","Department of Computer Science and Engineering, University of Minnesota, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1490","1495","Computer-Aided Diagnosis (CAD) has witnessed a rapid growth over the past decade, providing a variety of automated tools for the analysis of medical images. In surgical pathology, such tools enhance the diagnosing capabilities of pathologists by allowing them to review and diagnose a larger number of cases daily. Geared towards developing such tools, the main goal of this paper is to identify useful computer vision based feature descriptors for recognizing cancerous tissues in histopathologic images. To this end, we use images of Hematoxylin & Eosin-stained microscopic sections of breast and prostate carcinomas, and myometrial leiomyosarcomas, and provide an exhaustive evaluation of several state of the art feature representations for this task. Among the various image descriptors that we chose to compare, including representations based on convolutional neural networks, Fisher vectors, and sparse codes, we found that working with covariance based descriptors shows superior performance on all three types of cancer considered. While covariance descriptors are known to be effective for texture recognition, it is the first time that they are demonstrated to be useful for the proposed task and evaluated against deep learning models. Capitalizing on Region Covariance Descriptors (RCDs), we derive a powerful image descriptor for cancerous tissue recognition termed, Covariance Kernel Descriptor (CKD), which consistently outperformed all the considered image representations. Our experiments show that using CKD lead to 92.83%, 91.51%, and 98.10% classification accuracy for the recognition of breast carcinomas, prostate carcinomas, and myometrial leiomyosarcomas, respectively.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899848","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899848","","Cancer;Covariance matrices;Feature extraction;Geometry;Histograms;Image color analysis;Symmetric matrices","cancer;computer vision;feature extraction;image representation;image texture;medical image processing;neural nets;object recognition","CAD;CKD;RCD;breast carcinomas;cancerous tissue recognition;computer vision;computer-aided diagnosis;convolutional neural networks;covariance based descriptors;covariance kernel descriptor;feature descriptors;feature representation;image descriptors;myometrial leiomyosarcomas;pathologists;prostate carcinomas;region covariance descriptors;texture recognition","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Skin lesion classification from dermoscopic images using deep learning techniques","A. Romero Lopez; X. Giro-i-Nieto; J. Burdick; O. Marques","Universitat Politecnica de Catalunya, Barcelona, Spain","2017 13th IASTED International Conference on Biomedical Engineering (BioMed)","20170406","2017","","","49","54","The recent emergence of deep learning methods for medical image analysis has enabled the development of intelligent medical imaging-based diagnosis systems that can assist the human expert in making better decisions about a patients health. In this paper we focus on the problem of skin lesion classification, particularly early melanoma detection, and present a deep-learning based approach to solve the problem of classifying a dermoscopic image containing a skin lesion as malignant or benign. The proposed solution is built around the VGGNet convolutional neural network architecture and uses the transfer learning paradigm. Experimental results are encouraging: on the ISIC Archive dataset, the proposed method achieves a sensitivity value of 78.66%, which is significantly higher than the current state of the art on that dataset.","","Electronic:978-0-88986-990-5; POD:978-1-5090-4908-0","10.2316/P.2017.852-053","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893267","Convolutional Neural Networks;Deep Learning;Machine Learning;Medical Decision Support Systems;Medical Image Analysis;Skin Lesions","Malignant tumors;Medical diagnostic imaging;Skin","","","","","","","","","","20-21 Feb. 2017","","IEEE","IEEE Conference Publications"
"Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks","L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Medical Imaging","20170331","2017","36","4","994","1004","Automated melanoma recognition in dermoscopy images is a very challenging task due to the low contrast of skin lesions, the huge intraclass variation of melanomas, the high degree of visual similarity between melanoma and non-melanoma lesions, and the existence of many artifacts in the image. In order to meet these challenges, we propose a novel method for melanoma recognition by leveraging very deep convolutional neural networks (CNNs). Compared with existing methods employing either low-level hand-crafted features or CNNs with shallower architectures, our substantially deeper networks (more than 50 layers) can acquire richer and more discriminative features for more accurate recognition. To take full advantage of very deep networks, we propose a set of schemes to ensure effective training and learning under limited training data. First, we apply the residual learning to cope with the degradation and overfitting problems when a network goes deeper. This technique can ensure that our networks benefit from the performance gains achieved by increasing network depth. Then, we construct a fully convolutional residual network (FCRN) for accurate skin lesion segmentation, and further enhance its capability by incorporating a multi-scale contextual information integration scheme. Finally, we seamlessly integrate the proposed FCRN (for segmentation) and other very deep residual networks (for classification) to form a two-stage framework. This framework enables the classification network to extract more representative and specific features based on segmented results instead of the whole dermoscopy images, further alleviating the insufficiency of training data. The proposed framework is extensively evaluated on ISBI 2016 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset. Experimental results demonstrate the significant performance gains of the proposed framework, ranking the first in classification and the second in segmentation among 25 teams and 28 teams, r- spectively. This study corroborates that very deep CNNs with effective training mechanisms can be employed to solve complicated medical image analysis tasks, even with limited training data.","0278-0062;02780062","","10.1109/TMI.2016.2642839","Research Grants Council of the Hong Kong Special Administrative Region; 10.13039/501100003453 - Guangdong Natural Science Foundation; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792699","Automated melanoma recognition;fully convolutional neural networks;residual learning;skin lesion analysis;very deep convolutional neural networks","Biomedical imaging;Feature extraction;Image segmentation;Lesions;Malignant tumors;Skin;Training data","biomedical optical imaging;cancer;image recognition;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;skin","automated melanoma recognition;classification network;deep convolutional neural networks;deep residual networks;dermoscopy images;fully convolutional residual network;low-level hand-crafted features;medical image analysis;skin lesion segmentation;training data","","","","","","","20161221","April 2017","","IEEE","IEEE Journals & Magazines"
"Augmenting data when training a CNN for retinal vessel segmentation: How to warp?","A. Oliveira; S. Pereira; C. A. Silva","CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal","2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG)","20170330","2017","","","1","4","The retinal vascular condition is a trustworthy biomarker of several ophthalmologic and cardiovascular diseases, so automatic vessel segmentation is a crucial step to diagnose and monitor these problems. Deep Learning models have recently revolutionized the state-of-the-art in several fields, since they can learn features with multiple levels of abstraction from the data itself. However, these methods can easily fall into overfitting, since a huge number of parameters must be learned. Having bigger datasets may act as regularization and lead to better models. Yet, acquiring and manually annotating images, especially in the medical field, can be a long and costly procedure. Hence, when using regular datasets, people heavily need to apply artificial data augmentation. In this work, we use a fully convolutional neural network capable of reaching the state-of-the-art. Also, we investigate the benefits of augmenting data with new samples created by warping retinal fundus images with nonlinear transformations. Our results hint that may be possible to halve the amount of data, while maintaining the same performance.","","Electronic:978-1-5090-4801-4; POD:978-1-5090-4802-1","10.1109/ENBENG.2017.7889443","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889443","Convolutional neural network;Data augmentation;Retinal blood vessel segmentation","Data mining;Image segmentation;Neural networks;Retinal vessels;Training;Two dimensional displays","biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets;vision defects","CNN;artificial data augmentation;automatic vessel segmentation;biomarker;cardiovascular diseases;deep Learning models;fully convolutional neural network;medical field;nonlinear transformations;ophthalmologic diseases;regular datasets;retinal fundus images;retinal vascular condition;retinal vessel segmentation","","","","","","","","16-18 Feb. 2017","","IEEE","IEEE Conference Publications"
"Predicting high-risk prognosis from diagnostic histories of adult disease patients via deep recurrent neural networks","Jung-Woo Ha; Adrian Kim; Dongwon Kim; J. Kim; Jeong-Whun Kim; Jin Joo Park; Borim Ryu","NAVER Corp., Seongnam, Korea","2017 IEEE International Conference on Big Data and Smart Computing (BigComp)","20170320","2017","","","394","399","It is a critical issue to predict the prognosis of adult disease patients due to the possibility of spreading to high-risk symptoms in medical fields. Most studies for predicting prognosis have used complex data from patients such as biomedical images, biomarkers, and pathological measurements. We demonstrate a language model-like method for predicting high-risk prognosis from diagnosis histories of patients using deep recurrent neural networks (RNNs), i.e., prognosis prediction using RNN (PP-RNN). The proposed PP-RNN uses multiple RNNs for learning from diagnosis code sequences of patients in order to predict occurrences of high-risk diseases. The use of RNNs allows the model to learn the status changes of patients considering time, thus enhancing prediction accuracy. We evaluate our method on real-world diagnosis data of over 67,000 adult disease patients recorded for 14 years. Experimental results show the proposed PP-RNN outperforms other standard classification models. In particular, our method provides competitive performance with respect to recall and F1-score on high-risk diseases compared to other models. Furthermore, we investigate the effects of the parameters on the performances.","","Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9","10.1109/BIGCOMP.2017.7881742","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881742","adult disease;cardiovascular disease;cerebrovascular disease;deep learning;diagnosis sequence;prognosis prediction;recurrent neural network","Biomedical imaging;Diabetes;Diseases;History;Predictive models;Prognostics and health management;Recurrent neural networks","medical diagnostic computing;recurrent neural nets","PP-RNN;adult disease patients;classification models;deep recurrent neural networks;diagnostic histories;high-risk prognosis prediction;language model-like method","","","","","","","","13-16 Feb. 2017","","IEEE","IEEE Conference Publications"
"Automatic Scoring of Multiple Semantic Attributes With Multi-Task Feature Leverage: A Study on Pulmonary Nodules in CT Images","S. Chen; J. Qin; X. Ji; B. Lei; T. Wang; D. Ni; J. Z. Cheng","Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20170301","2017","36","3","802","814","The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of “spiculation”, “texture”, “margin”, etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.","0278-0062;02780062","","10.1109/TMI.2016.2629462","(Key) Project of Department of Education of Guangdong Province; Natural Science Foundation of SZU; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745891","Computer-aided diagnosis (CAD);computed tomography (CT);deep learning;feature learning;lung nodule;multi-task learning","Computational modeling;Computed tomography;Lungs;Machine learning;Medical diagnostic imaging;Semantics","computerised tomography;feature extraction;lung;medical image processing;pneumodynamics;regression analysis","CT images;HoG features;Lung Image Database Consortium data;clinical semantic terms;computer-aided diagnosis performance;convolutional neural network;deep learning models;elastic net regression methods;heterogeneous computational features;lung nodules;medical image;medical search engine;multitask learning schemes;pulmonary nodules;semantic features;single-task LASSO;stacked denoising autoencoder","","1","","","","","20161116","March 2017","","IEEE","IEEE Journals & Magazines"
"Advanced deep learning for blood vessel segmentation in retinal fundus images","Lua Ngo; Jae-Ho Han","Dept. Brain and Cognitive Engineering, Korea University, Seoul, South Korea","2017 5th International Winter Conference on Brain-Computer Interface (BCI)","20170220","2017","","","91","92","Rising of deep learning methodologies draws huge attention to their application in image processing and classification. Catching up the trends, this study briefly presents state-of-the-art of deep learning applications in medical imaging interfered with achievements of blood vessel segmentation methods in neurosensory retinal fundus images. Successful segmentation based on deep learning offers advantage in diagnosing ophthalmological disease or pathology.","","Electronic:978-1-5090-5096-3; POD:978-1-5090-5097-0","10.1109/IWW-BCI.2017.7858169","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858169","Biomedical optical imaging;blood vessels;fundus images;image segmentation;medical image processing","","blood vessels;image segmentation;learning (artificial intelligence);medical image processing","advanced deep learning methodologies;blood vessel segmentation methods;image classification;image processing;ophthalmological disease;pathology;retinal fundus images","","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Assessment of pain in mouse facial images","M. Eral; C. Ç. Aktaş; E. E. Koçak; T. Dalkara; U. Halıcı","Elektrik ve Elektronik M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Orta Do&#x011F;u Teknik &#x00DC;niversitesi, Turkey","2016 20th National Biomedical Engineering Meeting (BIYOMUT)","20170213","2016","","","1","4","Analysing mouse behavior in medical experiments to determine adverse effects of medical drugs requires special expertise and it is a time consuming tedious task. Automatic scaling of facial pain mimics in mice are important for a fast and objective labeling. Although there exists a manual procedure for scaling mouse facial pain expression, a full automatic method does not exist yet. In this paper, a computational method is proposed for assessment of pain through facial expressions of mouses in experiments where pain paradigms are applied. For this purpose, mouse face regions in videos were extracted manually frame by frame and also their pain scales were labeled by experts in order to construct a data set. Then, this data set were used for training a neural network using deep learning. The results obtained in this preliminary study, where a limited dataset of still images was used, are quite encouraging. Our studies in order to make our results more reliable and to develope a fully automatic approach for scaling mouse grimace in videos are still going on.","","Electronic:978-1-5090-5829-7; POD:978-1-5090-5830-3","10.1109/BIYOMUT.2016.7849416","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849416","artificial neural networks;convolutional networks;deep learning;mouse grimace scaling","Biomedical imaging;Kernel;Manuals;Mice;Pain;Rats;Videos","biomedical optical imaging;face recognition;feature extraction;learning (artificial intelligence);medical image processing;neural nets;video signal processing","automatic scaling;computational method;data set;deep learning;facial pain;manual frame extraction;manual procedure;medical drugs;medical experiments;mouse behavior;mouse face regions;mouse facial images;mouse facial pain expression;neural network;pain assessment;pain paradigms;videos","","","","","","","","3-5 Nov. 2016","","IEEE","IEEE Conference Publications"
"Medical Image Denoising Using Convolutional Denoising Autoencoders","L. Gondara","Dept. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","241","246","Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0041","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836672","Image denoising;convolutional autoencoder;denoising autoencoder","Biomedical imaging;Convolutional codes;Image denoising;Noise level;Noise measurement;Noise reduction;Training","convolutional codes;image denoising;image reconstruction;learning (artificial intelligence);medical image processing","convolutional denoising autoencoders;deep learning-based models;denoising performances;heterogeneous images;image reconstruction;medical image analysis;medical image denoising","","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep learning-based pipeline to recognize Alzheimer's disease using fMRI data","S. Sarraf; G. Tofighi","Department of Electrical and Computer Engineering, McMaster University Hamilton, ON, L8S 4L8, Canada, Rotman Research Institue at Baycrest, University of Toronto","2016 Future Technologies Conference (FTC)","20170119","2016","","","816","820","Over the past decade, machine learning techniques and in particular predictive modeling and pattern recognition in biomedical sciences, from drug delivery systems to medical imaging, have become one of the most important methods of assisting researchers in gaining a deeper understanding of issues in their entirety and solving complex medical problems. Deep learning is a powerful machine learning algorithm in classification that extracts low-to high-level features. In this paper, we employ a convolutional neural network to distinguish an Alzheimers brain from a normal, healthy brain. The importance of classifying this type of medical data lies in its potential to develop a predictive model or system in order to recognize the symptoms of Alzheimers disease when compared with normal subjects and to estimate the stages of the disease. Classification of clinical data for medical conditions such as Alzheimers disease has always been challenging, and the most problematic aspect has always been selecting the strongest discriminative features. Using the Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimers subjects from normal controls, where the accuracy of testing data reached 96.85%. This experiment suggests that the shift and scale invariant features extracted by CNN followed by deep learning classification represents the most powerful method of distinguishing clinical data from healthy data in fMRI. This approach also allows for expansion of the methodology to predict more complicated systems.","","Electronic:978-1-5090-4171-8; POD:978-1-5090-4172-5","10.1109/FTC.2016.7821697","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821697","Alzheimer's Disease;Deep learning;fMRI","Alzheimer's disease;Biological neural networks;Biomedical imaging;Feature extraction;Machine learning;Neurons","biomedical MRI;convolution;diseases;feature extraction;learning (artificial intelligence);medical computing;neural nets;pattern classification","Alzheimer disease;Alzheimers brain;CNN;LeNet-5 architecture;biomedical sciences;clinical data classification;convolutional neural network;deep learning-based pipeline;drug delivery systems;functional MRI data classification;machine learning;medical imaging;pattern recognition;predictive modeling;scale invariant features extraction","","","","","","","","6-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"A deep tongue image features analysis model for medical application","Dan Meng; Guitao Cao; Y. Duan; Minghua Zhu; Liping Tu; Jiatuo Xu; D. Xu","School of Computer Scinence and Software Engineering, East China Normal University, Shanghai, China 200062","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1918","1922","With the improvement of people's living standards, there is no doubt that people are paying more and more attention to their health. However, shortage of medical resources is a critical global problem. As a result, an intelligent prognostics system has a great potential to play important roles in computer aided diagnosis. Numerous papers reported that tongue features have been closely related to a human's state. Among them, the majority of the existing tongue image analyses and classification methods are based on the low-level features, which may not provide a holistic view of the tongue. Inspired by a deep convolutional neural network (CNN), we propose a deep tongue image feature analysis system to extract unbiased features and reduce human labor for tongue diagnosis. With the unbalanced sample distribution, it is hard to form a balanced classification model based on feature representations obtained by existing low-level and high-level methods. Our proposed deep tongue image feature analysis model learns high-level features and provide more classification information during training time, which may result in higher accuracy when predicting testing samples. We tested the proposed system on a set of 267 gastritis patients, and a control group of 48 healthy volunteers (labeled according to Western medical practices). Test results show that the proposed deep tongue image feature analysis model can classify a given tongue image into healthy and diseased state with an average accuracy of 91.49%, which demonstrates the relationship between human body's state and its deep tongue image features.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822815","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822815","Tongue image;deep model;feature analysis;weighted SVM","Analytical models;Biomedical imaging;Computational modeling;Radio frequency;Sensitivity;Shape;Support vector machines","convolution;feature extraction;image classification;medical image processing;neural nets","computer aided diagnosis;deep convolutional neural network;deep tongue image classification methods;deep tongue image feature analysis model;intelligent prognostic system;medical application","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Perspective on Deep Imaging","G. Wang","Department of Biomedical Engineering, Biomedical Imaging Center, Center for Biotechnology and Interdisciplinary Studies, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Access","20170104","2016","4","","8914","8924","The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.","2169-3536;21693536","","10.1109/ACCESS.2016.2624938","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733110","Tomographic imaging;big data;data acquisition;deep learning;image analysis;image reconstruction;machine learning;medical imaging","Data acquisition;Deep learning;Image processing;Image reconstruction;Machine learning;Tomography","Big Data;image reconstruction;learning (artificial intelligence)","Big data;deep imaging;deep learning;image analysis;image reconstruction techniques;image reconstruction theories;machine learning;medical imaging;tomographic imaging","","1","","","","","20161103","2016","","IEEE","IEEE Journals & Magazines"
"Super-resolution of medical image using representation learning","X. Yang; S. Zhant; C. Hu; Z. Liang; D. Xie","School of Computer and Information, Hefei University of Technology, Hefei, China 230009","2016 8th International Conference on Wireless Communications & Signal Processing (WCSP)","20161124","2016","","","1","6","Super-resolution (SR) of single image is a meaningful challenge in medical images based diagnosis, while the image resolution is limited. Also, numerous deep neural networks based models were proposed and achieve excellent performance which is superior to the previous handcrafted methods. In this paper, we employ a deep convolutional neural networks for the super-resolution (SR) of single medical image, which learns the nonlinear mapping from the low-resolution space to high-resolution space directly. In addition, we use three sets imaging data (Mammary gland, Prostate tissue and Human brain) training deep network respectively. Firstly, we use Randomized Rectified Linear Unit (RReLU), which incorporates a nonzero slope for negative part to solve the problem of over compression. Secondly, for the purpose of enhancing the quality of reconstructed result and reducing the noise of over-fitting, Nesterov's Accelerated Gradient (NAG) method on the SRCNN is used to accelerate the convergence of loss function and avoid the large oscillations. A comparative performance evaluation is carried out over a set of experiments using real imaging data to verify the validity of proposed algorithm.","","Electronic:978-1-5090-2860-3; POD:978-1-5090-2861-0; USB:978-1-5090-2859-7","10.1109/WCSP.2016.7752617","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752617","Convolutional Neural Networks;Deep Learning;Feature Map;Image Super-Resolution;Representation Learning","Biological neural networks;Image reconstruction;Image resolution;Interpolation;Medical diagnostic imaging;Training","biological tissues;convergence;data handling;image denoising;image resolution;medical image processing;neural nets;patient diagnosis;set theory","NAG;Nesterov accelerated gradient method;RReLU;SR;SRCNN;deep convolutional neural networks;high-resolution space;human brain;imaging data;loss function convergence;low-resolution space;mammary gland;medical image based diagnosis;medical image super-resolution;nonlinear mapping;prostate tissue;randomized rectified linear unit","","","","","","","","13-15 Oct. 2016","","IEEE","IEEE Conference Publications"
"Mass detection using deep convolutional neural network for mammographic computer-aided diagnosis","S. Suzuki; X. Zhang; N. Homma; K. Ichiji; N. Sugita; Y. Kawasumi; T. Ishibashi; M. Yoshizawa","Graduate School of Engineering, Tohoku University, Sendai, Japan","2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","20161121","2016","","","1382","1386","In recent years, a deep convolutional neural network (DCNN) has attracted great attention due to its outstanding performance in recognition of natural images. However, the DCNN performance for medical image recognition is still uncertain because collecting a large amount of training data is difficult. To solve the problem of the DCNN, we adopt a transfer learning strategy, and demonstrate feasibilities of the DCNN and of the transfer learning strategy for mass detection in mammographic images. We adopt a DCNN architecture that consists of 8 layers with weight, including 5 convolutional layers, and 3 fully-connected layers in this study. We first train the DCNN using about 1.2 million natural images for classification of 1,000 classes. Then, we modify the last fully-connected layer of the DCNN and subsequently train the DCNN using 1,656 regions of interest in mammographic image for two classes classification: mass and normal. The detection test is conducted on 198 mammographic images including 99 mass images and 99 normal images. The experimental results showed that the sensitivity of the mass detection was 89.9 % and the false positive was 19.2 %. These results demonstrated that the DCNN trained by transfer learning strategy has a potential to be a key system for mammographic mass detection computer-aided diagnosis (CAD). In addition, to the best of our knowledge, our study is the first demonstration of the DCNN for mammographic CAD application.","","Electronic:978-4-907764-50-0; POD:978-1-5090-0937-4","10.1109/SICE.2016.7749265","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749265","Computer-Aided Diagnosis/Detection;DCNN;Deep Learning;Medical and Welfare Systems;Neural Networks;Signal and/or Image Processing;Transfer Learning;mammogram","Breast cancer;Feature extraction;Mammography;Neurons;Training;Training data","image recognition;learning (artificial intelligence);mammography;medical image processing;neural nets","DCNN architecture;deep convolutional neural network;image classification;mammographic CAD application;mammographic computer-aided diagnosis;mass detection;medical image recognition;normal images;transfer learning strategy","","","","","","","","20-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"Lesion border detection using deep learning","P. Sabouri; H. GholamHosseini","Department of Electrical and Electronics Engineering, School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Private bag 92006, Auckland 1142, New Zealand","2016 IEEE Congress on Evolutionary Computation (CEC)","20161121","2016","","","1416","1421","Computer aided diagnosis of medical images can result in (better) detection in addition to early diagnosis of many symptoms to assist health physicians and therefore reducing the mortality rate. Realization of an efficient mobile device for automatic diagnosis of melanoma would greatly enhance the applicability of medical image classification scheme and make it useful in clinical contexts. In this paper, a deep learning method using convolutional neural networks (CNN) is proposed for border detection of skin lesions based on clinical images. Prepossessing of clinical and dermoscopy images has been common and necessary in the lesion segmentation realm; however, the result of the study shows that CNN can be used with relatively much less prepossessing algorithm compared with previous methods.","","Electronic:978-1-5090-0623-6; POD:978-1-5090-0624-3; USB:978-1-5090-0622-9","10.1109/CEC.2016.7743955","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743955","Deep learning;border detection;convolutional neural networks;lesion segmentation;melanoma detection","Feature extraction;Hair;Image color analysis;Image segmentation;Lesions;Malignant tumors;Skin","cancer;image classification;image segmentation;learning (artificial intelligence);medical image processing;mobile computing;neural nets;skin","CNN;automatic melanoma diagnosis;clinical image prepossessing;computer aided diagnosis;convolutional neural networks;deep learning;dermoscopy image prepossessing;lesion segmentation realm;medical image classification;mobile device;skin lesion border detection","","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Binary codes for tagging x-ray images via deep de-noising autoencoders","A. Sze-To; H. R. Tizhoosh; A. K. C. Wong","Systems Design Engineering, University of Waterloo, Ontario, Canada N2L 3G1","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","2864","2871","A Content-Based Image Retrieval (CBIR) system which identifies similar medical images based on a query image can assist clinicians for more accurate diagnosis. The recent CBIR research trend favors the construction and use of binary codes to represent images. Deep architectures could learn the non-linear relationship among image pixels adaptively, allowing the automatic learning of high-level features from raw pixels. However, most of them require class labels, which are expensive to obtain, particularly for medical images. The methods which do not need class labels utilize a deep autoencoder for binary hashing, but the code construction involves a specific training algorithm and an ad-hoc regularization technique. In this study, we explored using a deep de-noising autoencoder (DDA), with a new unsupervised training scheme using only backpropagation and dropout, to hash images into binary codes. We conducted experiments on more than 14,000 x-ray images. By using class labels only for evaluating the retrieval results, we constructed a 16-bit DDA and a 512-bit DDA independently. Comparing to other unsupervised methods, we succeeded to obtain the lowest total error by using the 512-bit codes for retrieval via exhaustive search, and speed up 9.27 times with the use of the 16-bit codes while keeping a comparable total error. We found that our new training scheme could reduce the total retrieval error significantly by 21.9%. To further boost the image retrieval performance, we developed Radon Autoencoder Barcode (RABC) which are learned from the Radon projections of images using a de-noising autoencoder. Experimental results demonstrated its superior performance in retrieval when it was combined with DDA binary codes.","","","10.1109/IJCNN.2016.7727561","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727561","","Binary codes;Image retrieval;Medical diagnostic imaging;Noise reduction;Training;X-ray imaging","Radon transforms;X-ray imaging;backpropagation;binary codes;content-based retrieval;feature extraction;image coding;image denoising;image matching;image representation;image retrieval;medical image processing;unsupervised learning","CBIR system;DDA binary codes;RABC;Radon autoencoder barcode;Radon projections;X-ray image tagging;ad-hoc regularization;automatic learning;backpropagation;binary hashing;content-based image retrieval system;deep architectures;deep denoising autoencoders;dropout;high-level features;image pixels;medical images;query image;raw pixels;total retrieval error;unsupervised training scheme;word length 16 bit;word length 512 bit","","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"A deep bag-of-features model for the classification of melanomas in dermoscopy images","S. Sabbaghi; M. Aldeen; R. Garnavi","Department of Electrical and Electronic Engineering, University of Melbourne, Australia","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","1369","1372","Deep learning and unsupervised feature learning have received great attention in past years for their ability to transform input data into high level representations using machine learning techniques. Such interest has been growing steadily in the field of medical image diagnosis, particularly in melanoma classification. In this paper, a novel application of deep learning (stacked sparse auto-encoders) is presented for skin lesion classification task. The stacked sparse auto-encoder discovers latent information features in input images (pixel intensities). These high-level features are subsequently fed into a classifier for classifying dermoscopy images. In addition, we proposed a new deep neural network architecture based on bag-of-features (BoF) model, which learns high-level image representation and maps images into BoF space. Then, we examine how using this deep representation of BoF, compared with pixel intensities of images, can improve the classification accuracy. The proposed method is evaluated on a test set of 244 skin images. To test the performance of the proposed method, the area under the receiver operating characteristics curve (AUC) is utilized. The proposed method is found to achieve 95% accuracy.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590962","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590962","","Cancer;Feature extraction;Image color analysis;Lesions;Malignant tumors;Skin;Training","biomedical optical imaging;cancer;image classification;image representation;medical image processing;neural nets;sensitivity analysis;skin;unsupervised learning","AUC;BoF space;area under the receiver operating characteristics curve;classification accuracy;deep bag-of-features model;deep learning;deep neural network architecture;deep representation;dermoscopy image classification;high level representations;high-level image representation;image pixel intensity;input images;machine learning techniques;medical image diagnosis;melanoma classification;skin lesion classification task;stacked sparse auto-encoder;unsupervised feature learning","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Brain MRI segmentation with patch-based CNN approach","Z. Cui; J. Yang; Y. Qiao","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China","2016 35th Chinese Control Conference (CCC)","20160829","2016","","","7026","7031","Brain Magnetic Resonance Image (MRI) plays a non-substitutive role in clinical diagnosis. The symptom of many diseases corresponds to the structural variants of brain. Automatic structure segmentation in brain MRI is of great importance in modern medical research. Some methods were developed for automatic segmenting of brain MRI but failed to achieve desired accuracy. In this paper, we proposed a new patch-based approach for automatic segmentation of brain MRI using convolutional neural network (CNN). Each brain MRI acquired from a small portion of public dataset is firstly divided into patches. All of these patches are then used for training CNN, which is used for automatic segmentation of brain MRI. Experimental results showed that our approach achieved better segmentation accuracy compared with other deep learning methods.","","Electronic:978-9-8815-6391-0; POD:978-1-5090-0910-7","10.1109/ChiCC.2016.7554465","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7554465","Brain MRI Segmentation;CNN;Deep Learning;Patch-based","Biology;Biomedical imaging;Computer architecture;Image segmentation;Magnetic resonance imaging;Visualization","biomedical MRI;brain;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets","CNN training;automatic brain MRI segmentation;automatic structure segmentation;brain magnetic resonance image;clinical diagnosis;convolutional neural network;disease symptoms;patch-based CNN approach;public dataset;segmentation accuracy;structural variants","","","","","","","","27-29 July 2016","","IEEE","IEEE Conference Publications"
"Road crack detection using deep convolutional neural network","L. Zhang; F. Yang; Y. Daniel Zhang; Y. J. Zhu","Department of Electrical and Computer Engineering, Temple University, Philadelphia, PA 19122, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","3708","3712","Automatic detection of pavement cracks is an important task in transportation maintenance for driving safety assurance. However, it remains a challenging task due to the intensity inhomogeneity of cracks and complexity of the background, e.g., the low contrast with surrounding pavement and possible shadows with similar intensity. Inspired by recent success on applying deep learning to computer vision and medical problems, a deep-learning based method for crack detection is proposed in this paper. A supervised deep convolutional neural network is trained to classify each image patch in the collected images. Quantitative evaluation conducted on a data set of 500 images of size 3264 χ 2448, collected by a low-cost smart phone, demonstrates that the learned deep features with the proposed deep learning framework provide superior crack detection performance when compared with features extracted with existing hand-craft methods.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7533052","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533052","Deep learning;convolution neural networks;road crack detection;road survey","Boosting;Feature extraction;Probability;Roads;Support vector machines;Training","computer vision;convolution;crack detection;learning (artificial intelligence);maintenance engineering;neural nets;road safety;traffic engineering computing","computer vision;deep convolutional neural network;deep learning;pavement crack detection;road crack detection;safety assurance;transportation maintenance","","","","23","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Membrane segmentation via active learning with deep networks","U. Gaur; M. Kourakis; E. Newman-Smith; W. Smith; B. S. Manjunath","Department of Computer Science, University of California Santa Barbara","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","1943","1947","Segmentation is a key component of several bio-medical image processing systems. Recently, segmentation methods based on supervised learning such as deep convolutional networks have enjoyed immense success for natural image datasets and biological datasets alike. These methods require large volumes of data to avoid overfitting which limits their applicability. In this work, we present a transfer learning mechanism based on active learning which allows us to utilize pre-trained deep networks for segmenting new domains with limited labelled data. We introduce a novel optimization criterion to allow feedback on the most uncertain, yet abundant image patterns thus provisioning for an expert in the loop albeit with minimum amount of guidance. Our experiments demonstrate the effectiveness of the proposed method in improving segmentation performance with very limited labelled data.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532697","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532697","Active Learning;Deep Networks;Image Segmentation;Transfer Learning","Computer architecture;Image segmentation;Microprocessors;Microscopy;Optimization;Training;Uncertainty","convolution;image segmentation;learning (artificial intelligence);optimisation","active learning;deep convolutional networks;deep networks;membrane segmentation;optimization criterion;supervised learning;transfer learning mechanism","","","","17","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Proposing the Deep Dynamic Bayesian Network as a Future Computer Based Medical System","C. M. Carbery; A. H. Marshall; R. Woods","Centre for Stat. Sci. & Operational Res., Queen's Univ. Belfast, Belfast, UK","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","227","228","The development of new learning models has been of great importance throughout recent years, with a focus on creating advances in the area of deep learning. Deep learning was first noted in 2006, and has since become a major area of research in a number of disciplines. This paper will delve into the area of deep learning to present its current limitations and provide a new idea for a fully integrated deep and dynamic probabilistic system. The new model will be applicable to a vast number of areas initially focusing on applications into medical image analysis with an overall goal of utilising this approach for prediction purposes in computer based medical systems.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.70","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545991","deep learning;dynamic Bayesian network;medical systems;probabilistic graphical model","Analytical models;Bayes methods;Biomedical imaging;Computational modeling;Data models;Hidden Markov models;Machine learning","belief networks;learning (artificial intelligence);medical image processing;prediction theory;probability","deep dynamic bayesian network;deep learning;dynamic probabilistic system;future computer based medical systems;integrated deep;medical image analysis;prediction purpose","","","","","","","","20-24 June 2016","","IEEE","IEEE Conference Publications"
"Colonic Polyp Classification with Convolutional Neural Networks","E. Ribeiro; A. Uhl; M. Häfner","Dept. of Comput. Sci., Univ. of Salzburg, Salzburg, Austria","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","253","258","Texture patch classification is an important task in many different computer-aided medical systems. Convolutional Neural Networks (CNN's) have become state-of-the-art for many computer vision tasks in recent years. In this paper, we propose the use of CNN's for the automated classification of colonic mucosa for colon polyp staging in the context of colon cancer screening. This deep learning approach has the property of extracting features and classifying images in the same architecture by exploiting directly the input image pixels being successful in handling distortions such as different light conditions, presence of partial occlusions, etc. For this type of deep learning approach it is common to require that the database contains large amounts of data, which is quite rare in the medical field. The method proposed allows the use of small patches (subimages) to increase the size of the database as well to classify different regions in the same image. We show experimentally that this model is more efficient than some of the commonly used features for colonic polyp classification.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.39","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545996","Colonic Polyp Classification;Convolutional Neural Networks;Deep Learning","Biological neural networks;Cancer;Colonic polyps;Endoscopes;Feature extraction;Training","biological organs;cancer;computer vision;feature extraction;image classification;image texture;learning (artificial intelligence);medical image processing;neural nets","CNN;automated colonic mucosa classification;colon cancer screening;colon polyp staging;colonic polyp classification;computer vision;computer-aided medical system;convolutional neural networks;deep learning;distortion handling;feature extraction;image classification;image pixels;partial occlusion;texture patch classification","","1","","","","","","20-24 June 2016","","IEEE","IEEE Conference Publications"
"Deep and Structured Robust Information Theoretic Learning for Image Analysis","Y. Deng; F. Bao; X. Deng; R. Wang; Y. Kong; Q. Dai","Automation Department, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Image Processing","20160721","2016","25","9","4209","4221","This paper presents a robust information theoretic (RIT) model to reduce the uncertainties, i.e., missing and noisy labels, in general discriminative data representation tasks. The fundamental pursuit of our model is to simultaneously learn a transformation function and a discriminative classifier that maximize the mutual information of data and their labels in the latent space. In this general paradigm, we, respectively, discuss three types of the RIT implementations with linear subspace embedding, deep transformation, and structured sparse learning. In practice, the RIT and deep RIT are exploited to solve the image categorization task whose performances will be verified on various benchmark data sets. The structured sparse RIT is further applied to a medical image analysis task for brain magnetic resonance image segmentation that allows group-level feature selections on the brain tissues.","1057-7149;10577149","","10.1109/TIP.2016.2588330","10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004608 - National Science Foundation of Jiangsu Province, China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506023","Data embedding;brain MRI segmentation;deep learning;image classification;mutual information;structured-sparse learning","Data models;Entropy;Mutual information;Noise measurement;Probability density function;Robustness;Uncertainty","biological tissues;image segmentation;learning (artificial intelligence);magnetic resonance imaging;medical image processing","RIT implementations;brain magnetic resonance image segmentation;brain tissues;deep robust information theoretic learning;deep transformation;discriminative classifier;group-level feature selections;image analysis;image categorization task;linear subspace embedding;medical image analysis;robust information theoretic model;structured robust information theoretic learning;transformation function","","1","","55","","","20160707","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning","G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen","Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Biomedical Engineering","20160621","2016","63","7","1505","1516","Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.","0018-9294;00189294","","10.1109/TBME.2015.2496253","10.13039/100000071 - National Institute of Child Health and Human Development; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314894","Deep learning;Deformable image registration;deep learning;deformable image registration;hierarchical feature representation","Biomedical imaging;Feature extraction;Image registration;Machine learning;Three-dimensional displays;Unsupervised learning","","","","3","","70","","","20151102","July 2016","","IEEE","IEEE Journals & Magazines"
"Retinal vessel segmentation via deep learning network and fully-connected conditional random fields","H. Fu; Y. Xu; D. W. K. Wong; J. Liu","Ocular Imaging Department, Institute for Infocomm Research, Agency for Science, Technology and Research (A&#8727;STAR), Singapore","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","698","701","Vessel segmentation is a key step for various medical applications. This paper introduces the deep learning architecture to improve the performance of retinal vessel segmentation. Deep learning architecture has been demonstrated having the powerful ability in automatically learning the rich hierarchical representations. In this paper, we formulate the vessel segmentation to a boundary detection problem, and utilize the fully convolutional neural networks (CNNs) to generate a vessel probability map. Our vessel probability map distinguishes the vessels and background in the inadequate contrast region, and has robustness to the pathological regions in the fundus image. Moreover, a fully-connected Conditional Random Fields (CRFs) is also employed to combine the discriminative vessel probability map and long-range interactions between pixels. Finally, a binary vessel segmentation result is obtained by our method. We show that our proposed method achieve a state-of-the-art vessel segmentation performance on the DRIVE and STARE datasets.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493362","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493362","Conditional Random Fields;Convolutional Neural Networks;Vessel segmentation","Computer architecture;Image segmentation;Machine learning;Neural networks;Pathology;Retinal vessels","blood vessels;eye;image segmentation;medical image processing;neural nets","DRIVE dataset;STARE dataset;binary vessel segmentation;boundary detection;convolutional neural networks;deep learning network;fully-connected conditional random fields;fundus image;pathological region;retinal vessel segmentation;vessel probability map","","3","","18","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Deep vessel tracking: A generalized probabilistic approach via deep learning","A. Wu; Z. Xu; M. Gao; M. Buty; D. J. Mollura","Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1363","1367","Analysis of vascular geometry is important in many medical imaging applications, such as retinal, pulmonary, and cardiac investigations. In order to make reliable judgments for clinical usage, accurate and robust segmentation methods are needed. Due to the high complexity of biological vasculature trees, manual identification is often too time-consuming and tedious to be used in practice. To design an automated and computerized method, a major challenge is that the appearance of vasculatures in medical images has great variance across modalities and subjects. Therefore, most existing approaches are specially designed for a particular task, lacking the flexibility to be adapted to other circumstances. In this paper, we present a generic approach for vascular structure identification from medical images, which can be used for multiple purposes robustly. The proposed method uses the state-of-the-art deep convolutional neural network (CNN) to learn the appearance features of the target. A Principal Component Analysis (PCA)-based nearest neighbor search is then utilized to estimate the local structure distribution, which is further incorporated within the generalized probabilistic tracking framework to extract the entire connected tree. Qualitative and quantitative results over retinal fundus data demonstrate that the proposed framework achieves comparable accuracy as compared with state-of-the-art methods, while efficiently producing more information regarding the candidate tree structure.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493520","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493520","Deep Learning;Generalized Probabilistic Tracking;Nearest Neighbor Search;Principal Component Analysis;Vascular Structure","Biomedical imaging;Dictionaries;Image segmentation;Machine learning;Probabilistic logic;Robustness","","","","1","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Segmentation label propagation using deep convolutional neural networks and dense conditional random field","M. Gao; Z. Xu; L. Lu; A. Wu; I. Nogues; R. M. Summers; D. J. Mollura","Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1265","1268","Availability and accessibility of large-scale annotated medical image datasets play an essential role in robust supervised learning of medical image analysis. Missed labeling of regions of interest is a common issue on existing medical image datasets due to the labor intensive nature of the annotation task which requires high levels of clinical proficiency. In this paper, we present a segmentation based label propagation method to a publicly available dataset on interstitial lung disease [3], to address the missing annotation challenge. Upon validation from an expert radiologist, the amount of available annotated training data is largely increased. Such a dataset expansion can can potentially increase the accuracy of Computer-aided Detection (CAD) systems. The proposed constrained segmentation propagation algorithm combines the cues from the initial annotations, deep convolutional neural networks and a dense fully-connected Conditional Random Field (CRF) that achieves high quantitative accuracy levels.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493497","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493497","Convolutional Neural Network;Dense Conditional Random Field;Interstitial Lung Disease;Multi-class Labeling;Segmentation Label Propagation","Biomedical imaging;Computed tomography;Image segmentation;Labeling;Lungs;Message passing;Neural networks","diseases;image segmentation;interstitials;learning (artificial intelligence);lung;medical image processing;neurophysiology","CAD systems;annotation task;computer-aided detection systems;dataset expansion;deep convolutional neural networks;dense conditional random field;dense fully-connected conditional Random field;high quantitative accuracy levels;interstitial lung disease;labor intensive nature;large-scale annotated medical image datasets;medical image analysis;medical image datasets;missing annotation challenge;regions of interest;robust supervised learning;segmentation based label propagation method;segmentation label propagation;segmentation propagation algorithm","","2","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning","H. C. Shin; H. R. Roth; M. Gao; L. Lu; Z. Xu; I. Nogues; J. Yao; D. Mollura; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory","IEEE Transactions on Medical Imaging","20160503","2016","35","5","1285","1298","Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.","0278-0062;02780062","","10.1109/TMI.2016.2528162","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404017","Biomedical imaging;computer aided diagnosis;image analysis;machine learning;neural networks","Biomedical imaging;Computational modeling;Computed tomography;Diseases;Lungs;Lymph nodes;Solid modeling","computerised tomography;diseases;image classification;image representation;learning (artificial intelligence);lung;medical image processing;neurophysiology;reviews","CNN architectures;CNN model analysis;axial CT slices;computer-aided detection;computer-aided detection problems;dataset characteristics;deep convolutional neural networks;fine-tuning CNN models;five-fold cross-validation classification;high performance CAD systems;highly representative hierarchical image features;image recognition;interstitial lung disease classification;learning data-driven;mediastinal LN detection;medical image classification;medical image tasks;medical imaging domain;natural image dataset;off-the-shelf pretrained CNN features;pretrained imagenet;spatial image context;state-of-the-art performance;supervised fine-tuning;thoraco-abdominal lymph node detection;transfer learning;unsupervised CNN pretraining","","35","","73","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing","F. C. Ghesu; E. Krubasik; B. Georgescu; V. Singh; Y. Zheng; J. Hornegger; D. Comaniciu","Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1217","1228","Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% o- er the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","0278-0062;02780062","","10.1109/TMI.2016.2538802","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426845","Deep learning;image parsing;marginal space learning;sparse representations;three-dimensional (3D) object detection and segmentation","Context;Feature extraction;Image segmentation;Machine learning;Robustness;Shape;Three-dimensional displays","biomedical ultrasonics;feature extraction;image classification;image sampling;image segmentation;learning (artificial intelligence);medical image processing;pattern clustering;probability;ultrasonic imaging","3D context;DL-based active shape model;anatomical object detection;anatomical pose estimation;annotated image databases;aortic valve;automated feature design;boundary delineation;clinical workflow;clustered high-probability regions;computational efficiency;deep learning network architectures;deep learning systems;diagnosis;extensive dataset;full 3D data detection;full 3D data segmentation;hierarchical marginal spaces;learning classifiers;machine learning methods;marginal space deep learning;nonrigid object boundary;object localization;object parametrization;parametrized representations;patient stratification;representative image features;restricted affine transformation;run-time performance;scanning high-dimensional parametric spaces;scanning hypotheses;segmentation support;sparse adaptive data sampling patterns;therapy planning;two-step learning problem;ultrasound;volumetric medical image data parsing","","9","","46","","","20160307","May 2016","","IEEE","IEEE Journals & Magazines"
"Multi-Instance Deep Learning: Discover Discriminative Local Anatomies for Bodypart Recognition","Z. Yan; Y. Zhan; Z. Peng; S. Liao; Y. Shinagawa; S. Zhang; D. N. Metaxas; X. S. Zhou","Department of Computer Science, Rutgers University, Piscataway, NJ, USA","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1332","1343","In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. “Bodypart identity” of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.","0278-0062;02780062","","10.1109/TMI.2016.2524985","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398101","CNN;discriminative local information discovery;multi-instance;multi-stage","Algorithm design and analysis;DICOM;Image analysis;Image recognition;Machine learning;Three-dimensional displays","cardiology;computerised tomography;face recognition;image classification;learning (artificial intelligence);medical image processing","aorta arch slice;body-part recognition;cardiac slice;convolutional neural network;discriminative information;discriminative local anatomies;discriminative local appearances;global image context;human faces;human identity information;image classification problem;image recognition problems;image-level classifier;large scale CT dataset;local image information;local image patches;mediastinum region;multiinstance deep learning;multiinstance learning fashion;multistage deep learning framework;prelearned CNN;pretrain stage;synthetic dataset;transversal slice","","10","","51","","","20160203","May 2016","","IEEE","IEEE Journals & Magazines"
"Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images","M. J. J. P. van Grinsven; B. van Ginneken; C. B. Hoyng; T. Theelen; C. I. Sánchez","Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1273","1284","Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.","0278-0062;02780062","","10.1109/TMI.2016.2526689","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401052","Convolutional neural network;deep learning;hemorrhage;selective sampling","Biomedical imaging;Databases;Hemorrhaging;Image analysis;Image color analysis;Observers;Training","biomedical optical imaging;blood;computer vision;image classification;image colour analysis;image sampling;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis","CNN learning process;CNN training iteration;color fundus images;computer vision applications;deep learning network architectures;dynamically selecting misclassified negative samples;fast convolutional neural network training;hemorrhage detection;independent test set;medical image analysis tasks;receiver operating characteristics curve;selective data sampling;selective sampling method","","7","","48","","","20160208","May 2016","","IEEE","IEEE Journals & Magazines"
"Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network","M. Anthimopoulos; S. Christodoulidis; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1207","1216","Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","0278-0062;02780062","","10.1109/TMI.2016.2535865","Bern University hospital Inselspital; Swiss National Science Foundation SNSF; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422082","Convolutional neural networks;interstitial lung diseases;texture classification","Computed tomography;Convolution;Design automation;Diseases;Feature extraction;Lungs;Neural networks","biological tissues;computerised tomography;convolution;diseases;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","CT volume scans;ILD pattern classification;automated tissue characterization;computer aided diagnosis system;computer vision problems;consolidation;deep convolutional neural network;deep learning techniques;feature maps;ground glass opacity;honeycombing;interstitial lung diseases;lung pattern classification;medical image analysis;micronodules;reticulation","","17","","42","","","20160229","May 2016","","IEEE","IEEE Journals & Magazines"
"Privacy-preserving deep learning","R. Shokri; V. Shmatikov","UT Austin, United States","2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20160407","2015","","","909","910","Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extrajudicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we present a practical system that enables multiple parties to jointly learn an accurate neural-network model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our pr- vacy-preserving deep learning on benchmark datasets.","","Electronic:978-1-5090-1824-6; POD:978-1-5090-1825-3","10.1109/ALLERTON.2015.7447103","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447103","","Companies;Data models;Data privacy;Decision support systems;Machine learning;Privacy;Training","data privacy;gradient methods;learning (artificial intelligence);neural nets;optimisation","AI-based services;Internet;artificial intelligence;artificial neural networks;data classification;data collection;data modeling;data privacy;data recognition;optimization algorithms;privacy-preserving deep learning;stochastic gradient descent","","1","","1","","","","Sept. 29 2015-Oct. 2 2015","","IEEE","IEEE Conference Publications"
"The 3-dimensional medical image recognition of right and left kidneys by deep GMDH-type neural network","T. Kondo; S. Takao; J. Ueno","Graduate School of Health Sciences, Tokushima University, 3-18-15 Kuramoto-cho Tokushima, 770-8509 Japan","2015 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)","20160324","2015","","","313","320","In this study, the deep multi-layered Group Method of Data Handling (GMDH)-type neural network algorithm using principal component-regression analysis is applied to recognition problems of the right and left kidney regions. The deep multi-layered GMDH-type neural network algorithm can automatically organize the deep neural network architectures which have many hidden layers and these deep neural networks can identify the characteristics of very complex nonlinear systems. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In this deep GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the deep GMDH-type neural network, and multi-colinearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the right and left kidney regions. The optimum neural network architectures, which fit the complexity of the right and left kidney regions, are automatically organized and the right and left kidney regions are automatically recognized and extracted by the organized deep GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this deep GMDH-type neural networks are useful for the medical image recognition problems of the right and left kidney regions.","","Electronic:978-1-4799-8562-3; POD:978-1-4799-8563-0; USB:978-1-4799-8561-6","10.1109/ICIIBMS.2015.7439548","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439548","Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network","Algorithm design and analysis;Biological neural networks;Biomedical imaging;Computer architecture;Input variables;Kidney;Neurons","data handling;feature extraction;kidney;learning (artificial intelligence);medical image processing;neural nets;principal component analysis;regression analysis","GMDH-type neural network;deep neural network;group method of data handling;kidney region extraction;medical image recognition;principal component analysis;regression analysis","","","","10","","","","28-30 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Histopathological Image Feature Representation Method Based on Deep Learning","G. Zhang; L. Zhong; Y. Huang; Y. Zhang","Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China","2015 7th International Conference on Information Technology in Medicine and Education (ITME)","20160310","2015","","","13","17","Automated annotation and grading for histopathological image plays an important role in CAD systems. It provides valuable information and support for medical diagnosis. Currently, computer-aid analysis of histopathological images mainly relies on some well-designed digital features, which requires abundant human efforts and experiences in problem domain. Learning a good feature representation from data can have positive effects on constructing the target model. We propose a novel method for histopathological image feature representation based on deep learning. The method extracts high level representation of raw pixels of a local region through a network model with several hidden layers, which can learn potential features automatically. The proposed method is evaluated on a real data set from a large local hospital with comparison to two current state-of-the-art methods. The result is promising indicating that it achieves significant improvement of the model performance. Moreover, our study suggests that features learned through deep models can achieve better performance than human designed features.","","CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5","10.1109/ITME.2015.34","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429087","deep learning;feature representation;histopathological image analysis;stacked autoencoder","Biomedical imaging;Data models;Feature extraction;Image color analysis;Medical services;Solid modeling;Training","biological tissues;image representation;learning (artificial intelligence);medical image processing","CAD systems;automated annotation;computer-aid analysis;deep learning;histopathological image feature representation method;large local hospital;medical diagnosis","","","","15","","","","13-15 Nov. 2015","","IEEE","IEEE Conference Publications"
"Interleaved text/image Deep Mining on a large-scale radiology database","H. C. Shin; Le Lu; L. Kim; A. Seff; J. Yao; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","1090","1099","Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning “data-hungry” obstacle in the medical domain.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7298712","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298712","","Machine learning;Medical diagnostic imaging;Radiology;Semantics;Visualization","PACS;computer vision;data mining;image retrieval;learning (artificial intelligence);medical image processing;radiology;recurrent neural nets;text analysis","3D medical volume;CNN;computer vision;data-hungry obstacle;deep convolutional neural network;document-level text;embedded vector label;extracted key image;interleaved text/image deep learning system;interleaved text/image deep mining;large-scale radiology database;latent Dirichlet allocation;national research hospital;picture archiving and communication system;radiology image;recurrent neural net language model;retrieval manner;semantic interaction;semantic label;sentence description;sentence-level text;unsupervised learning;very large-scale medical image database","","3","","47","","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Transformation-Invariant Convolutional Jungles","D. Laptev; J. M. Buhmann","ETH Zurich, Switzerland","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","3043","3051","Many Computer Vision problems arise from information processing of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or - in medical imaging - staining and local warping. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised feature learning approach, which efficiently extracts information from these constraints to produce interpretable, transformation-invariant features. The proposed method can incorporate a large class of transformations, e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel image classification and segmentation method, which we call Transformation-Invariant Convolutional Jungles (TICJ). We test the algorithm on two benchmarks in face recognition and medical imaging, where it achieves state of the art results, while being computationally significantly more efficient than Deep Neural Networks.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7298923","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298923","","Computer vision;Feature extraction;Image segmentation;Kernel;Neural networks;Optimization;Training","computer vision;feature extraction;image classification;image segmentation;learning (artificial intelligence)","TICJ;computer vision;face recognition;image classification;image segmentation;information extraction;interpretable feature;medical imaging;recognition algorithms;supervised feature learning approach;transformation-invariant convolutional jungles;transformation-invariant feature","","2","","29","","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Chest pathology detection using deep learning with non-medical training","Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan","The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","294","297","In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163871","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871","CNN;Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks","Biomedical imaging;Diagnostic radiography;Feature extraction;Machine learning;Pathology;Visualization;X-rays","convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets","AUC;CNN algorithm;CNN deep architecture classification;CNN learning;GIST feature;ImageNet;area under curve;chest X-ray image dataset;chest pathology detection;chest radiograph;convolutional neural network;deep learning;domain specific representation;general medical image recognition task;high level image representation learning;large scale nonmedical image database;mid level image representation learning;nonmedical learning;nonmedical training;pathology identification;pathology type","","10","","15","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Anatomy-specific classification of medical images using deep convolutional nets","H. R. Roth; C. T. Lee; H. C. Shin; A. Seff; L. Kim; J. Yao; L. Lu; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","101","104","Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. “Deep learning” methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163826","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163826","Computed tomography (CT);Convolutional Networks;Deep Learning;Image Classification","Computed tomography;Convolution;Lungs;Medical diagnostic imaging;Neural networks;Training","PACS;biological organs;computerised tomography;image classification;medical image processing","ConvNets;anatomy variability;anatomy-specific classification;anatomy-specific classification error;area-under-the-curve;automated classification;axial 2D key-images;body part-specific anatomical classification;computed tomography;computer-aided diagnosis systems;convolutional networks;data augmentation;data augmentation approach;deep convolutional nets;deep learning methods;hospital PACS archive;human anatomy;image classification;medical images;organ-specific anatomical classification;spatial complexity","","5","","16","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Transfer learning method using multi-prediction deep Boltzmann machines for a small scale dataset","Y. Sawada; K. Kozuka","Panasonic Corporation, 3-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","20150713","2015","","","110","113","In this article, we propose a transfer learning method using the multi-prediction deep Boltzmann machine (MPDBM). In recent years, deep learning has been widely used in many applications such as image classification and object detection. However, it is hard to apply a deep learning method to medical images because the deep learning method needs a large number of training data to train the deep neural network. Medical image datasets such as X-ray CT image datasets do not have enough training data because of privacy. In this article, we propose a method that re-uses the network trained on non-medical images (source domain) to improve performance even if we have a small number of medical images (target domain). Our proposed method firstly trains the deep neural network for solving the source task using the MPDBM. Secondly, we evaluate the relation between the source domain and the target domain. To evaluate the relation, we input the target domain into the deep neural network trained on the source domain. Then, we compute the histograms based on the response of the output layer. After computing the histograms, we select the variables of the output layer corresponding to the target domain. Then, we tune the parameters in such a way that the selected variables respond as the outputs of the target domain. In this article, we use the MNIST dataset as the source domain and the lung dataset of the X-ray CT images as the target domain. Experimental results show that our proposed method can improve classification performance.","","Electronic:978-4-9011-2214-6; POD:978-1-4799-8247-9; USB:978-4-9011-2215-3","10.1109/MVA.2015.7153145","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153145","","Biomedical imaging;Histograms;Learning systems;Lesions;Neural networks;Training;Training data","Boltzmann machines;X-ray imaging;computerised tomography;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","MNIST dataset;MPDBM;X-ray CT image lung dataset;deep learning method;deep neural network;image classification performance improvement;medical image datasets;multiprediction deep Boltzmann machine;network reuse;nonmedical image;small scale dataset;source domain;source task solving;target domain;transfer learning method","","3","","10","","","","18-22 May 2015","","IEEE","IEEE Conference Publications"
"Medical Image Recognition of Abdominal Multi-organs by Hybrid Multi-layered GMDH-type Neural Network Using Principal Component-Regression Analysis","T. Kondo; J. Ueno; S. Takao","Grad. Sch. of Health Sci., Univ. of Tokushima, Tokushima, Japan","2014 Second International Symposium on Computing and Networking","20150302","2014","","","157","163","In this study, hybrid multi-layered Group Method of Data Handling (GMDH) type neural network algorithm using principal component-regression analysis is applied to recognition problems of the abdominal multi-organs such as the liver and spleen. In this GMDH-type neural network, principal component-regression analysis is used as the learning algorithm of the weights in the GMDH-type neural network which is a type of the deep neural network with many hidden layers. The architecture of the deep neural network with many hidden layers is automatically organized using the heuristic self-organization method, so as to minimize the prediction error criterion defined as Akaike's information criterion (AIC) or Prediction Sum of Squares (PSS). The heuristic self-organization method is a type of the evolutional computation. In the GMDH-type neural network, the multi-co linearity occurs and the prediction values become unstable because the architecture of the neural network has many hidden layers whose characteristics are very complex. In the GMDH-type neural networks in this study, multi-co linearity does not occur and stable and accurate prediction values are obtained. This new algorithm is applied to the medical image recognitions of the liver and spleen. The optimum neural network architectures, which fit the complexity of the liver and spleen images, are automatically organized from the multi-detector row CT (MDCT) image of the abdominal regions and the liver and spleen regions are automatically recognized and extracted by the organized GMDH-type neural networks. The recognition results are compared with the conventional sigmoid function neural network trained using back propagation method and it is shown that this GMDH-type neural networks are useful for the medical image recognition problems of the abdominal multi-organs.","2379-1888;23791888","Electronic:978-1-4799-4152-0; POD:978-1-4799-4151-3","10.1109/CANDAR.2014.62","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052176","Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network","Biological neural networks;Computer architecture;Image recognition;Input variables;Liver;Neurons","backpropagation;computerised tomography;data handling;evolutionary computation;image recognition;liver;medical image processing;principal component analysis;regression analysis;self-organising feature maps","Akaike information criterion;abdominal multiorgans;back propagation method;deep neural network;evolutional computation;group method of data handling;heuristic self-organization method;hybrid multilayered GMDH-type neural network;learning algorithm;liver images;medical image recognition;multidetector row CT image;optimum neural network architectures;prediction error criterion;prediction sum of squares;principal component-regression analysis;sigmoid function neural network;spleen images","","0","","10","","","","10-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Hybrid feedback GMDH-type neural network using principal component-regression analysis and its application to medical image recognition of heart regions","T. Kondo; J. Ueno; S. Takao","Grad. Sch. of Health Sci., Univ. of Tokushima, Tokushima, Japan","2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)","20150219","2014","","","1203","1208","Hybrid feedback Group Method of Data Handling (GMDH)-type neural network using principal component-regression analysis is applied to the medical image recognition of the heart regions. In the GMDH-type neural network, the multi-layered deep neural networks are automatically organized so as to fit the complexity of the nonlinear system and, in general, the architectures of the GMDH-type neural network have many hidden layers and become complex for the nonlinear systems. In the multi-layered deep GMDH-type neural network with many hidden layers, the multi-colinearity occurs and the perdition accuracy become worse and the prediction values become unstable. In the GMDH-type neural network used in this study, the principal component-regression analysis is used as the learning algorithm of the neural network and the multi-colinearity do not occur and accurate and stable GMDH-type neural network architectures are automatically organized so as to fit the complexity of the nonlinear system. Furthermore, in this algorithm, three types of neural networks, such as sigmoid function neural network, radial basis function (RBF) neural network and polynomial neural network, can be generated using three types of neuron architectures, and the neural network architecture which fits the complexity of medical images, is selected from these three neural network architectures. This GMDH-type neural network is applied to the medical image recognition of the heart regions and it is shown that this GMDH-type neural network is useful for the medical image recognition of the heart regions.","","Electronic:978-1-4799-5955-6; POD:978-1-4799-5956-3; USB:978-1-4799-5954-9","10.1109/SCIS-ISIS.2014.7044800","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044800","Deep neural network;Evolutional computation;GMDH;Medical image recognition;Neural network","Biological neural networks;Biomedical imaging;Computer architecture;Heart;Image recognition;Input variables;Neurons","cardiology;identification;image recognition;learning (artificial intelligence);medical image processing;neural net architecture;nonlinear systems;principal component analysis;radial basis function networks;regression analysis","GMDH-type neural network architectures;RBF neural network;group method of data handling;heart regions;hybrid feedback GMDH-type neural network;medical image recognition;multilayered deep neural networks;neural network learning algorithm;nonlinear systems;polynomial neural network;principal component-regression analysis;radial basis function neural network;sigmoid function neural network","","0","","9","","","","3-6 Dec. 2014","","IEEE","IEEE Conference Publications"
"Virtual neurosurgical education for image-guided deep brain stimulation neurosurgery","Y. Liu","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA","2014 International Conference on Audio, Language and Image Processing","20150115","2014","","","623","626","Deep brain stimulation is an effective neurosurgical procedure widely used to treat patients suffering from neurological disorders such as Parkinson's disease, essential tremor, and dystonia. Traditional ways of learning to operate such neurosurgery can be non-intuitive, costly, highly pressured, and time limited. With the development of virtual reality technology, such issues may be resolved. Immersive virtual environments use a stereoscopic head-mounted display and data glove to create high fidelity virtual experiences. Users can interact with three-dimensional models by peering over/in/through the object and perceive relationships at their true scale. This stands in contrast to traditional computer-aided infrastructure in which images are viewed as stacks of two-dimensional slices, or, at best, disembodied renderings. Various data sources such as video and audio could also be integrated into the environment to enrich the available information. Despite substantial innovation for entertainment and consumer media, applications of immersive virtual environment technologies in medical applications remain yet to be explored. In this paper, we consider potential applications of such technologies for deep brain stimulation patients with brain magnetic resonance imaging data. Our environment allows users to view brain scans at scale and interact with virtual models including anatomical structures and neural fiber tracts using a data glove. Micro-electrode recordings, stored as wav sound files, are integrated into the system, which permits the user to learn the different patterns of neuronal firing at different brain location. The system creates opportunities to study and optimize interfaces for medical data visualization and neurosurgical education.","","CD-ROM:978-1-4799-3901-5; Electronic:978-1-4799-3903-9; POD:978-1-4799-3904-6","10.1109/ICALIP.2014.7009869","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009869","Immersive virtual reality;head-mounted displays;image-guided surgery;neurosurgical education","Biomedical imaging;Neurosurgery;Solid modeling;Three-dimensional displays;Virtual environments","bioelectric phenomena;biomedical MRI;biomedical education;biomedical electrodes;brain;data gloves;data visualisation;medical disorders;microelectrodes;neurophysiology;surgery;virtual reality","anatomical structures;brain location;brain magnetic resonance imaging data;brain scans;data glove;data sources;image-guided deep brain stimulation neurosurgery;immersive virtual environment technology;medical data visualization;microelectrode recording;neural fiber tracts;neurological disorders;neuronal firing;patient treatment;stereoscopic head-mounted display;three-dimensional models;virtual neurosurgical education;virtual reality technology;wav sound files","","0","","15","","","","7-9 July 2014","","IEEE","IEEE Conference Publications"
"Efficient Training of Convolutional Deep Belief Networks in the Frequency Domain for Application to High-Resolution 2D and 3D Images","T. Brosch; R. Tam","MS/MRI Research Group, Vancouver, BC V6T 2B5, Canada, and Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada brosch.tom@gmail.com","Neural Computation","20141224","2015","27","1","211","227","<para>Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8 times on 2D images and up to 200 times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128 × 128 × 128 voxels practical, which opens new directions for using deep learning for medical image analysis.</para>","0899-7667;08997667","","10.1162/NECO_a_00682","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998135","","","","","","3","","","","","","Jan. 2015","","MIT Press","MIT Press Journals"
"CADdy — Colposcopy learning machine for computer aided diagnosis","M. Traversi; M. Falagario; C. Guaragnella","DEI - Dept. of Electrics and Information, Polytechnic University of Bari, Italy","2013 IEEE Third International Conference on Consumer Electronics ¿¿ Berlin (ICCE-Berlin)","20140102","2013","","","1","4","The study and development of a decision support system for doctors and students is presented, aiming to ease the diagnosis of the cervix cancer through an automated smart system: using a web application, a medical expert can upload colposcopie images feeding an expert system that carries out a deep analysis by a processing system on images uploaded by doctors; results coming out of the processing system are presented by a user friendly system suggesting the decision to the the medical expert who is able to confirm or change it, and annotate information. If changed, the diagnosis is sent to the expert system, developed on a reinforcement learning scheme, to tune decision parameters and enhance detection rates. The paper present the work in progress preliminary results of the system being developed.","2166-6814;21666814","Electronic:978-1-4799-1412-8; POD:978-1-4799-1410-4","10.1109/ICCE-Berlin.2013.6697965","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6697965","Machine learning;cervix cancer;colposcopy;images processing;neural network;web application","IEEE Xplore;Portable document format","","","","1","","","","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data","H. C. Shin; M. R. Orton; D. J. Collins; S. J. Doran; M. O. Leach","Institute of Cancer Rearch Royal Marsden NHS Foundation Trust, Sutton Sutton","IEEE Transactions on Pattern Analysis and Machine Intelligence","20130617","2013","35","8","1930","1943","Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.","0162-8828;01628828","","10.1109/TPAMI.2012.277","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6399478","Edge and feature detection;biomedical image processing;machine learning;object recognition;pixel classification","Feature extraction;Liver;Machine learning;Medical diagnostic imaging;Training;Visualization","biological organs;biomedical MRI;feature extraction;image classification;medical image processing;object detection;probability;unsupervised learning","4D patient data;abnormal dataset;deep-learning model;ground-truth labels;image classifiers;intrinsic abnormalities;magnetic resonance medical images;medical image analysis;multiple organ detection;object class categorization;organ shapes;patient datasets;probabilistic patch-based method;stacked autoencoders;supervised machine learning;temporal hierarchical features;tissue types;unlabeled multimodal DCE-MRI dataset;unsupervised feature learning;visual hierarchical features;weakly supervised training","Artificial Intelligence;Databases, Factual;Humans;Image Enhancement;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Pattern Recognition, Automated;Pilot Projects","45","1","59","","","20121231","Aug. 2013","","IEEE","IEEE Journals & Magazines"
"Autoencoder in Time-Series Analysis for Unsupervised Tissues Characterisation in a Large Unlabelled Medical Image Dataset","H. C. Shin; M. Orton; D. J. Collins; S. Doran; M. O. Leach","Inst. of Cancer Res., R. Marsden NHS Found. Trust, Sutton, UK","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","1","","259","264","The topic of deep-learning has recently received considerable attention in the machine learning research community, having great potential to liberate computer scientists from hand-engineering training datasets, because the method can learn the desired features automatically. This is particularly beneficial in medical research applications of machine learning, where getting good hand labelling of data is especially expensive. We propose application of a single-layer sparse-auto encoder to dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) for fully automatic classification of tissue types in a large unlabelled dataset with minimal human interference -- in a manner similar to data-mining. DCE-MRI analysis, looking at the change of the MR contrast-agent concentration over successively acquired images, is time-series analysis. We analyse the change of brightness (which is related to the contrast-agent concentration) of the DCE-MRI images over time to classify different tissue types in the images. Therefore our system is an application of an auto encoder to time-series analysis while the demonstrated result and further possible successive application areas are in computer vision. We discuss the important factors affecting performance of the system in applying the auto encoder to the time-series analysis of DCE-MRI medical image data.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.38","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146980","DCE-MRI;autoencoder;deep-learning;liver;medical image analysis cancer;time-series analysis","Cancer;Kidney;Liver;Medical diagnostic imaging;Training;Training data","biological tissues;biomedical MRI;image classification;learning (artificial intelligence);medical image processing;time series","DCE-MRI analysis;computer vision;contrast-enhanced magnetic resonance imaging;fully automatic classification;large unlabelled medical image dataset;machine learning;medical research applications;single-layer sparse-autoencoder;time-series analysis;tissue types;unsupervised tissues characterisation","","4","","19","","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Table of contents","","","2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20170424","2016","","","xi","xxxviii","The following topics are dealt with: optical and visible light communications; statistical signal processing; speech processing; source separation and deconvolution; signal decomposition; compressed sensing; deep learning; distributed information processing, optimization, and resource management; transceiver algorithm; millimeter wave technologies; cellular 5G systems; massive MIMO systems; RF technologies; power system state estimation; measurement-based smart grid analytics; cyber-physical attacks and forensics; smart grid control; optimal power flow and power markets; power line and smart grid communications; electric vehicles; storage management and demand response; information theoretic approaches; cognitive communications and radar; machine learning; Big Data analysis; medical imaging; and noncommutative theory.","","Electronic:978-1-5090-4545-7; POD:978-1-5090-4546-4; USB:978-1-5090-4544-0","10.1109/GlobalSIP.2016.7905770","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905770","","","5G mobile communication;Big Data;MIMO communication;biomedical imaging;cellular radio;cognitive radio;compressed sensing;cyber-physical systems;deconvolution;demand side management;digital forensics;electric vehicles;energy storage;free-space optical communication;information theory;learning (artificial intelligence);load flow;millimetre waves;optimisation;power cables;power markets;power system control;power system measurement;power system state estimation;radar;radio transceivers;smart power grids;source separation;speech processing;statistical analysis","Big Data analysis;RF technologies;cellular 5G systems;cognitive communications;compressed sensing;cyber-physical attacks;deconvolution;deep learning;demand response;distributed information processing;electric vehicles;forensics;information theoretic approaches;machine learning;massive MIMO systems;measurement-based smart grid analytics;medical imaging;millimeter wave technologies;noncommutative theory;optical communications;optimal power flow;optimization;power line;power markets;power system state estimation;radar;resource management;signal decomposition;smart grid communications;smart grid control;source separation;speech processing;statistical signal processing;storage management;transceiver algorithm;visible light communications","","","","","","","","7-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Table of contents","","","2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20170323","2016","","","1","16","The following topics are dealt with: software reliability; object-oriented software engineering; interactive visualization; software customization; data mining; support vector machines; Bayesian classifiers; Web security; Internet of Things; medical image segmentation; location-based social networks; recommendation system; encrypted cloud storage system; quantum cryptography; mobile delay tolerant network; mobile augmented reality; Web service; service oriented architecture; image retrieval algorithm; face detection algorithm; deep neural network; biometric authentication system; higher engineering education; online education resource domain ontology; machine learning technique.","","Electronic:978-1-4673-9904-3; POD:978-1-4673-9905-0","10.1109/ICSESS.2016.7883259","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883259","","","Internet of Things;Web services;biology computing;computer aided instruction;cryptography;data mining;data visualisation;delay tolerant networks;face recognition;mobile radio;neural nets;object-oriented programming;recommender systems;service-oriented architecture;social networking (online);software engineering;support vector machines","Bayesian classifiers;Internet of Things;Web security;Web service;biometric authentication system;data mining;deep neural network;encrypted cloud storage system;face detection algorithm;higher engineering education;image retrieval algorithm;interactive visualization;location-based social networks;machine learning technique;medical image segmentation;mobile augmented reality;mobile delay tolerant network;object-oriented software engineering;online education resource domain ontology;quantum cryptography;recommendation system;service oriented architecture;software customization;software reliability;support vector machines","","","","","","","","26-28 Aug. 2016","","IEEE","IEEE Conference Publications"
"Image quality classification for DR screening using deep learning","F. Yu; J. Sun; A. Li; J. Cheng; C. Wan; J. Liu","Nanjing University of Aeronautics and Astronautics, China","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","664","667","The quality of input images significantly affects the outcome of automated diabetic retinopathy (DR) screening systems. Unlike the previous methods that only consider simple low-level features such as hand-crafted geometric and structural features, in this paper we propose a novel method for retinal image quality classification (IQC) that performs computational algorithms imitating the working of the human visual system. The proposed algorithm combines unsupervised features from saliency map and supervised features coming from convolutional neural networks (CNN), which are fed to an SVM to automatically detect high quality vs poor quality retinal fundus images. We demonstrate the superior performance of our proposed algorithm on a large retinal fundus image dataset and the method could achieve higher accuracy than other methods. Although retinal images are used in this study, the methodology is applicable to the image quality assessment and enhancement of other types of medical images.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8036912","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036912","convolutional neural networks;image quality classification;saliency map","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data","Y. Liu; B. Logan; N. Liu; Z. Xu; J. Tang; Y. Wang","","2017 IEEE International Conference on Healthcare Informatics (ICHI)","20170914","2017","","","380","385","In this paper, we propose the first deep reinforce-ment learning framework to estimate the optimal Dynamic Treat-ment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real life complexity in heterogeneous disease progression and treatment choices, with the goal to provide doctor and patients the data-driven personalized decision recommendations. The proposed deep reinforcement learning framework contains a supervised learning step to predict the most possible expert actions; and a deep reinforcement learning step to estimate the long term value function of Dynamic Treatment Regimes. We motivated and implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease. We showed results of the initial implementation that demonstrates promising accuracy in predicting human expert decisions and initial implementation for the reinforcement learning step.","","Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3","10.1109/ICHI.2017.45","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031178","","Biomedical imaging;Decision making;Diseases;Games;Learning (artificial intelligence);Machine learning","","","","","","","","","","23-26 Aug. 2017","","IEEE","IEEE Conference Publications"
"CVRT: Cognitive Visual Recognition Tracker","M. Velazquez; Y. Lee","","2017 IEEE International Conference on Healthcare Informatics (ICHI)","20170914","2017","","","31","38","Studies on visual attention of patients with Alzheimer's disease and Dementia is a promising way for keeping track of the individual patient's image recognition ability over. This research seeks to expand upon the current applications of combining the Android operating system with TensorFlow by providing a visual question answering platform for image analysis. This application, Cognitive Visual Recognition Tracker (CVRT), provides an entry point by which the user can ask questions concerning any image of their choosing, and then receive cumulative metrics over time to better assess any diminishing cognitive ability (i.e. Alzheimer's patients). In this work, recurrent neural networks as well as semantic analysis are leveraged to provide an interactive VQA experience. One of the main objectives of CVRT is for physicians to be able to determine trends from patient data that could either be applicable to the individual patient, or to many patients if an aggregate is formed from many individual datasets. On an individual level, these metrics would provide a way for the physician to monitor daily cognitive capability, whereas on a grander scale, these joint datasets could be used to provide better overall treatment for the disease with the future inclusion of predictive analytics. The final contribution is an interactive metrics platform by which other users can assess the primary user's cognitive capacity based on features of their questioning, and to then provide them with accurate trending or possible remediation plans based on their condition.","","Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3","10.1109/ICHI.2017.65","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031129","Alzheimer's disease and Dementia;Deep Learning;Visual Question Answering","Conferences;Informatics;Medical services","","","","","","","","","","23-26 Aug. 2017","","IEEE","IEEE Conference Publications"
"Medical Concept Normalization for Online User-Generated Texts","K. Lee; S. A. Hasan; O. Farri; A. Choudhary; A. Agrawal","","2017 IEEE International Conference on Healthcare Informatics (ICHI)","20170914","2017","","","462","469","Social media has become an important tool for sharing content in the last decade. People often talk about their experiences and opinions on different health-related issues e.g. they write reviews on medications, describe symptoms and ask informal questions about various health concerns. Due to the colloquial nature of the languages used in the social media, it is often difficult for an automated system to accurately interpret them for appropriate clinical understanding. To address this challenge, this paper proposes a novel approach for medical concept normalization of user-generated texts to map a health condition described in the colloquial language to a medical concept defined in standard clinical terminologies. We use multiple deep learning architectures such as convolutional neural networks (CNN) and recurrent neural networks (RNN) with input word embeddings trained on various clinical domain-specific knowledge sources. Extensive experiments on two benchmark datasets demonstrate that the proposed models can achieve up to 21.28% accuracy improvements over the existing models when we use the combination of all knowledge sources to learn neural embeddings.","","Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3","10.1109/ICHI.2017.59","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031195","deep learning;medical concept normalization;social media","Drugs;Hair;Hidden Markov models;Medical diagnostic imaging;Pain;Recurrent neural networks;Social network services","","","","","","","","","","23-26 Aug. 2017","","IEEE","IEEE Conference Publications"
"CEUS-based classification of liver tumors with deep canonical correlation analysis and multi-kernel learning","L. Guo; D. Wang; H. Xu; Y. Qian; C. Wang; X. Zheng; Q. Zhang; J. Shi","Department of Medical Ultrasound, Shanghai Tenth People's Hospital, Ultrasound Research and Education Institute, School of Medicine, Tongji University, China","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","1748","1751","The contrast-enhanced ultrasound (CEUS) has been a widely accepted imaging modality for diagnosis of liver cancers. In clinical practice, several typical images selected from enhancement patterns of the arterial, portal venous and late phases can provide reliable information basis for diagnosis. In this work, we propose to develop a CEUS-based computer-aided diagnosis (CAD) for liver cancers with only three typical CEUS images selected from three phases, which simulates the clinical diagnosis mode of radiologists. In the proposed CAD, the deep canonical correlation analysis (DCCA) is first performed on three CEUS pairs between arterial and portal venous phases, arterial and late phases, respectively, due to the effectiveness of multi-view fusion of DCCA. The generated six-view features are then fed to a multiple kernel learning (MKL) classifier to further promote the predictive diagnosis result. The experimental results indicate that the proposed DCCA-MKL algorithm achieves best performance for discriminating benign liver tumors from malignant liver cancers.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8037181","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037181","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Single Sensor Techniques for Sleep Apnea Diagnosis Using Deep Learning","R. K. Pathinarupothi; D. P. J; E. S. Rangan; G. E. A; V. R; K. P. Soman","","2017 IEEE International Conference on Healthcare Informatics (ICHI)","20170914","2017","","","524","529","A large number of obstructive sleep apnea (OSA) cases are under-diagnosed due unavailability, inconvenience or expense of sleep labs. Hence, an automated detection by applying computational techniques to multivariate signals has already become a well-researched subject. However, the best-known techniques that use various features have not achieved the gold standard of polysomnography (PSG) tests. In this paper, we substantiate the medical conjecture that OSA directly impacts body parameters such as Instantaneous Heart Rate (IHR) and blood oxygen saturation (SpO2). We then use a deep learning technique called LSTM-RNN (long short-term memory recurrent neural networks) to experimentally prove that OSA severity detection can be solely based on either IHR or SpO2 signals, which can be easily, obtained using off-the-shelf non-intrusive wearable single sensors. The results obtained from LSTM-RNN model shows an area under curve (AUC) of 0.98 associated with very high accuracy on a dataset of more than 16,000 apnea non-apnea minutes. These results have encouraged our collaborating doctors to further come up with a diagnostic protocol that is based on LSTM-RNN, SpO2, and IHR, thereby increasing the chances of larger adoption among medical community.","","Electronic:978-1-5090-4881-6; POD:978-1-5090-4882-3","10.1109/ICHI.2017.37","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031206","Instantaneous Heart Rate and SpO2;LSTM-RNN;Obstructive sleep apnea","Electrocardiography;Heart rate variability;Medical diagnostic imaging;Sleep apnea;Time series analysis","","","","","","","","","","23-26 Aug. 2017","","IEEE","IEEE Conference Publications"
"Deep tessellated retinal image detection using Convolutional Neural Networks","X. Lyu; H. Li; Y. Zhen; X. Ji; S. Zhang","Computer Graphics and Imaging Lab, College of Computer Science and Technology, Zhejiang University, Hangzhou, China","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","676","680","Tessellation in fundus is not only a visible feature for aged-related and myopic maculopathy but also confuse retinal vessel segmentation. The detection of tessellated images is an inevitable processing in retinal image analysis. In this work, we propose a model using convolutional neural network for detecting tessellated images. The input to the model is pre-processed fundus image, and the output indicate whether this photograph has tessellation or not. A database with 12,000 colour retinal images is collected to evaluate the classification performance. The best tessellation classifier achieves accuracy of 97.73% and AUC value of 0.9659 using pretrained GoogLeNet and transfer learning technique.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8036915","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036915","Tessellated fundus;convolutional neural network;tessellation detection;transfer learning","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Automated Analysis of Unregistered Multi-view Mammograms with Deep Learning","G. Carneiro; J. Nascimento; A. P. Bradley","Australian Centre for Visual Technologies, University of Adelaide, Australia.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","We describe an automated methodology for the analysis of unregistered cranio-caudal (CC) and medio-lateral oblique (MLO) mammography views in order to estimate the patient’s risk of developing breast cancer. The main innovation behind this methodology lies in the use of deep learning models for the problem of jointly classifying unregistered mammogram views and respective segmentation maps of breast lesions (i.e., masses and micro-calcifications). This is a holistic methodology that can classify a whole mammographic exam, containing the CC and MLO views and the segmentation maps, as opposed to the classification of individual lesions, which is the dominant approach in the field. We also demonstrate that the proposed system is capable of using the segmentation maps generated by automated mass and micro-calcification detection systems, and still producing accurate results. The semi-automated approach (using manually defined mass and micro-calcification segmentation maps) is tested on two publicly available datasets (INbreast and DDSM), and results show that the volume under ROC surface (VUS) for a 3-class problem (normal tissue, benign and malignant) is over 0.9, the area under ROC curve (AUC) for the 2-class ”benign vs malignant” problem is over 0.9, and for the 2- class breast screening problem (malignancy vs normal/benign) is also over 0.9. For the fully automated approach, the VUS results on INbreast is over 0.7, and the AUC for the 2-class ”benign vs malignant” problem is over 0.78, and the AUC for the 2-class breast screening is 0.86.","0278-0062;02780062","","10.1109/TMI.2017.2751523","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8032490","Deep learning;Mammogram;Multi-view classification;Transfer learning","Breast;Cancer;Lesions;Machine learning;Mammography;Training","","","","","","","","","20170912","","","IEEE","IEEE Early Access Articles"
"Structure Prediction for Gland Segmentation with Hand-Crafted and Deep Convolutional Features","S. Manivannan; W. Li; J. Zhang; E. Trucco; S. McKenna","University of Jaffna, Sri Lanka and University of Dundee during the initial stages of the research reported here.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","We present a novel method to segment instances of glandular structures from colon histopathology images. We use a structure learning approach which represents local spatial configurations of class labels, capturing structural information normally ignored by sliding-window methods. This allows us to reveal different spatial structures of pixel labels (e.g., locations between adjacent glands, or far from glands), and to identify correctly neighbouring glandular structures as separate instances. Exemplars of label structures are obtained via clustering and used to train support vector machine classifiers. The label structures predicted are then combined and post-processed to obtain segmentation maps. We combine hand-crafted, multi-scale image features with features computed by a deep convolutional network trained to map images to segmentation maps. We evaluate the proposed method on the public domain GlaS dataset, which allows extensive comparisons with recent, alternative methods. Using the GlaS contest protocol, our method achieves the overall best performance.","0278-0062;02780062","","10.1109/TMI.2017.2750210","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030141","Gastrointestinal tract;Molecular and cellular imaging;Segmentation","Feature extraction;Glands;Image segmentation;Morphology;Support vector machines;Training","","","","","","","","","20170908","","","IEEE","IEEE Early Access Articles"
"DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks","N. Pezzotti; T. Höllt; J. v. Gemert; B. P. F. Lelieveldt; E. Eisemann; A. Vilanova","Intelligent Systems department, Delft University of Technology, Delft, the Netherlands","IEEE Transactions on Visualization and Computer Graphics","","2017","PP","99","1","1","Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.","1077-2626;10772626","","10.1109/TVCG.2017.2744358","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019872","Progressive visual analytics;deep neural networks;machine learning","Kernel;Layout;Neural networks;Neurons;Three-dimensional displays;Training;Visual analytics","","","","","","","","","20170829","","","IEEE","IEEE Early Access Articles"
"Deep Learning for Categorization of Lung Cancer CT Images","A. M. Rossetto; W. Zhou","Dept. of Comput. Sci., Univ. of Massachusetts Lowell, Lowell, MA, USA","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20170817","2017","","","272","273","Lung cancer is a serious health problem. In the United States alone, approximately 225,000 people each year are diagnosed with lung cancer. Early detection is a crucial part of giving patients the best chance of recovery. Deep learning gives us an opportunity to increase the accuracy of the automated initial diagnosis. Here we present an ensemble of Convolution Neural Networks(CNN) using multiple preprocessing methods to increase the accuracy of the automated labeling of the scans. We have done this by implementing ensembles of CNNs along with a voting system to get the consensus of the two networks. The initial results of our best method show both a consistently high accuracy (97.5%) and a low percentage of false positives (<;10%).","","Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5","10.1109/CHASE.2017.98","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010653","","Cancer;Convolution;Lungs;Machine learning;Pipelines;Testing;Training","computerised tomography;learning (artificial intelligence);medical image processing;neural nets","CNN;automated initial diagnosis;automated labeling;categorization;convolution neural networks;deep learning;false positives;health problem;lung cancer CT images;multiple preprocessing methods","","","","","","","","17-19 July 2017","","IEEE","IEEE Conference Publications"
"HCNN: Heterogeneous Convolutional Neural Networks for Comorbid Risk Prediction with Electronic Health Records","J. Zhang; J. Gong; L. Barnes","Dept. of Syst. & Inf. Eng., Univ. of Virginia, Charlottesville, VA, USA","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20170817","2017","","","214","221","The increasing adoption of electronic health record (EHR) systems has brought tremendous opportunities in medicine enabling more personalized prognostic models. However, most work to date has investigated the binary classification problem for predicting the onset of one chronic disease, but little attention has been given to assessing risk of developing comorbidities that are major causes of morbidity and mortality. For example, type 2 diabetes and chronic kidney disease frequently accompany congestive heart failure. This paper is motivated by the problem of predicting comorbid diseases and aims to answer the following question: can we predict the comorbid risk using a patient's medical history? We propose a new predictive learning framework, Heterogeneous Convolutional Neural Network (HCNN), that represents EHRs as graphs with heterogeneous attributes (e.g. diagnoses, procedures, and medication), and then develop a novel deep learning methodology for risk prediction of multiple comorbid diseases. The main innovation of the framework is that it defines the distance between the heterogeneous attributes of the graph representation extracted from the EHR and develops an appropriate learning infrastructure that is a composition of sparse convolutional layers and local pooling steps that match with the local structure of the space of the heterogeneous attributes. As a result, the new method is capable of capturing features about the relationships between heterogeneous attributes of the graphs. Through a comparative study on patient EHR data, HCNN achieves better performance than traditional convolutional neural networks on the risk prediction of comorbid diseases.","","Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5","10.1109/CHASE.2017.80","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010635","Electronic Health Records;Heterogeneous Convolution;Neural Networks;Risk Prediction","Convolution;Correlation;Diseases;Feature extraction;Machine learning;Medical diagnostic imaging;Neural networks","diseases;electronic health records;feedforward neural nets;learning (artificial intelligence);pattern classification;risk management","EHR systems;HCNN;binary classification problem;chronic disease;comorbid diseases;comorbid risk prediction;deep learning methodology;electronic health records;graph representation;heterogeneous attributes;heterogeneous convolutional neural networks;learning infrastructure;local pooling steps;personalized prognostic models;predictive learning framework;sparse convolutional layers","","","","","","","","17-19 July 2017","","IEEE","IEEE Conference Publications"
"Learning to Read Chest X-Ray Images from 16000+ Examples Using CNN","Y. Dong; Y. Pan; J. Zhang; W. Xu","Inst. for Interdiscipl. Inf. Sci., Tsinghua Univ., Beijing, China","2017 IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20170817","2017","","","51","57","Chest radiography (chest X-ray) is a low-cost yet effective and widely used medical imaging procedures. The lacking of qualified radiologist seriously limits the applicability of the technique. We explore the possibility of designing a computer-aided diagnosis for chest X-rays using deep convolutional neural networks. Using a real-world dataset of 16,000 chest X-rays with natural language diagnosis reports, we can train a multi-class classification model from images and preform accurate diagnosis, without any prior domain knowledge.","","Electronic:978-1-5090-4722-2; POD:978-1-5090-4723-9; USB:978-1-5090-4721-5","10.1109/CHASE.2017.59","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010614","","Biomedical imaging;Convolution;Diseases;Feature extraction;Lungs;Neural networks;X-ray imaging","computerised tomography;diagnostic radiography;image classification;medical image processing;neural nets","CNN;chest X-ray image;chest X-rays;chest radiography;computer-aided diagnosis;deep convolutional neural networks;medical imaging procedures;multiclass image classification;natural language diagnosis reports","","","","","","","","17-19 July 2017","","IEEE","IEEE Conference Publications"
"Leveraging deep preference learning for indexing and retrieval of biomedical images","S. Pang; M. A. Orgun; A. Du; Z. Yu","College of Computer Science and Technology, Jilin University, Qianjin Street: 2699, China","2017 8th International IEEE/EMBS Conference on Neural Engineering (NER)","20170814","2017","","","126","129","This paper presents an original framework based on deep learning and preference learning to retrieve and characterize biomedical images for assisting physicians in diagnosing complex diseases with potentially only small differences between them. In particular, we use deep learning to extract the high-level and compact features for biomedical images. In contrast to the traditional biomedical algorithms or general image retrieval systems that only consider the use of pixel and/or hand-crafted features to represent images, we utilize deep neural networks for feature discovery of biomedical images. Moreover, in order to be able to index the similarly referenced images, we introduce preference learning in a novel way to learn what kinds of images we need so that we can obtain the similarity ranking list of biomedical images. We evaluate the performance of our system in detailed experiments over the well-known available OASIS-MRI database for whole brain neuroimaging as a benchmark and compare it with those of the traditional biomedical and general image retrieval approaches. Our proposed system exhibits an outstanding retrieval ability and efficiency for biomedical image applications.","","Electronic:978-1-5090-4603-4; POD:978-1-5090-4604-1","10.1109/NER.2017.8008308","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008308","","Biological system modeling;Biomedical imaging;Feature extraction;Image retrieval;Indexing;Visualization","biomedical MRI;brain;database indexing;feature extraction;image retrieval;medical image processing;neurophysiology","OASIS-MRI database;biomedical image diagnosing;biomedical image indexing;biomedical image retrieval;brain neuroimaging;deep learning;deep neural networks;deep preference learning;diseases;feature discovery;high-level features;similarity ranking list","","","","","","","","25-28 May 2017","","IEEE","IEEE Conference Publications"
"Automatic localization of the needle target for ultrasound-guided epidural injections","M. Pesteie; V. Lessoway; P. Abolmaesumi; R. N. Rohling","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, B.C., Canada.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Accurate identification of the needle target is crucial for effective epidural anesthesia. Currently, epidural needle placement is administered by a manual technique, relying on the sense of feel, which has a significant failure rate. Moreover, misleading the needle may lead to inadequate anesthesia, post dural puncture headaches and other potential complications. Ultrasound offers guidance to the physician for identification of the needle target, but accurate interpretation and localization remain challenges. A hybrid machine learning system is proposed to automatically localize the needle target for epidural needle placement in ultrasound images of the spine. In particular, a deep network architecture along with a feature augmentation technique is proposed for automatic identification of the anatomical landmarks of the epidural space in ultrasound images. Experimental results of the target localization on planes of 3D as well as 2D images have been compared agianst an expert sonographer. When compared with the expert annotations, the average lateral and vertical error on planes of 3D test data was 1 mm and 0.4 mm, respectively. On 2D test data set, an average lateral error of 1.7 mm and vertical error of 0.8 mm were acquired.","0278-0062;02780062","","10.1109/TMI.2017.2739110","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008783","3D ultrasound;Epidural injection;deep learning;prepuncture scan;target localization","Anesthesia;Needles;Real-time systems;Spine;Three-dimensional displays;Ultrasonic imaging","","","","","","","","","20170811","","","IEEE","IEEE Early Access Articles"
"Ultrasound aided vertebral level localization for lumbar surgery","N. Baka; S. Leenstra; T. v. Walsum","Biomedical Imaging Group Rotterdam, Departments of Radiology &#x0026; Nuclear Medicine and Medical Informatics, Erasmus MC, University Medical Center Rotterdam, The Netherlands.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Localization of the correct vertebral level for surgical entry during lumbar hernia surgery is not straightforward. In this paper we develop and evaluate a solution using free-hand 2D ultrasound (US) imaging in the operation room (OR). Our system exploits the difference in spinous process shapes of the vertebrae. The spinous processes are pre-operatively outlined and labeled in a lateral lumbar X-ray of the patient. Then, in the OR the spinous processes are imaged with 2D sagittal US, and are automatically segmented and registered with the X-ray shapes. After a small number of scanned vertebrae, the system robustly matches the shapes, and propagates the X-ray label to the US images. The main contributions of our work are: We propose a deep convolutional neural network based bone segmentation algorithm from US imaging, that outperforms state-of-the-art methods in both performance and speed. We present a matching strategy that determines the levels of the spinal processes being imaged. And lastly, we evaluate the complete procedure on 19 clinical datasets from two hospitals, and two observers. The final labeling was correct in 92% of the cases, demonstrating the feasibility of US based surgical entry point detection for spinal surgeries.","0278-0062;02780062","","10.1109/TMI.2017.2738612","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007292","bone segmentation;computer aided surgery;deep learning;lumbar X-ray;machine learning;spine;surgical guidance","Bones;Image segmentation;Shape;Surgery;Two dimensional displays;Ultrasonic imaging;X-ray imaging","","","","","","","","","20170810","","","IEEE","IEEE Early Access Articles"
"Deep Learning with Edge Computing for Localization of Epileptogenicity Using Multimodal rs-fMRI and EEG Big Data","M. P. Hosseini; T. X. Tran; D. Pompili; K. Elisevich; H. Soltanian-Zadeh","Dept. of Electr. & Comput. Eng., Rutgers Univ.-New Brunswick, New Brunswick, NJ, USA","2017 IEEE International Conference on Autonomic Computing (ICAC)","20170810","2017","","","83","92","Epilepsy is a chronic brain disorder characterized by the occurrence of spontaneous seizures of which about 30 percent of patients remain medically intractable and may undergo surgical intervention; despite the latter, some may still fail to attain a seizure-free outcome. Functional changes may precede structural ones in the epileptic brain and may be detectable using existing noninvasive modalities. Functional connectivity analysis through electroencephalography (EEG) and resting state-functional magnetic resonance imaging (rs-fMRI), complemented by diffusion tensor imaging (DTI), has provided such meaningful input in cases of temporal lobe epilepsy (TLE). Recently, the emergence of edge computing has provided competent solutions enabling context-aware and real-time response services for users. By leveraging the potential of autonomic edge computing in epilepsy, we develop and deploy both noninvasive and invasive methods for the monitoring, evaluation and regulation of the epileptic brain, with responsive neurostimulation (RNS; Neuropace). First, an autonomic edge computing framework is proposed for processing of big data as part of a decision support system for surgical candidacy. Second, an optimized model for estimation of the epileptogenic network using independently acquired EEG and rs-fMRI is presented. Third, an unsupervised feature extraction model is developed based on a convolutional deep learning structure for distinguishing interictal epileptic discharge (IED) periods from nonIED periods using electrographic signals from electrocorticography (ECoG). Experimental and simulation results from actual patient data validate the effectiveness of the proposed methods.","","Electronic:978-1-5386-1762-5; POD:978-1-5386-1763-2; USB:978-1-5386-1761-8","10.1109/ICAC.2017.41","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005336","Autonomic Computing;Deep Learning;EEG;Edge Computing;Epilepsy Seizure Localization;Health Monitoring and Treatment;Medical Big Data;rs-fMRI","Big Data;Cloud computing;Edge computing;Electroencephalography;Epilepsy;Feature extraction;Logic gates","Big Data;biomedical MRI;decision support systems;electroencephalography;feature extraction;learning (artificial intelligence);medical image processing;unsupervised learning","DTI;ECoG;EEG big data;IED periods;RNS;TLE;autonomic edge computing;chronic brain disorder;convolutional deep learning structure;decision support system;diffusion tensor imaging;electrocorticography;electroencephalography;electrographic signals;epileptogenicity;interictal epileptic discharge;multimodal rs-fMRI;responsive neurostimulation;state-functional magnetic resonance imaging;temporal lobe epilepsy;unsupervised feature extraction model","","","","","","","","17-21 July 2017","","IEEE","IEEE Conference Publications"
"Smartphone-based food category and nutrition quantity recognition in food image with deep learning algorithm","C. L. Chin; C. C. Huang; B. J. Lin; G. R. Wu; T. C. Weng; H. F. Chen","Department of Medical Informatics, Chung Shan Medical University, Taichung, Taiwan","2016 International Conference on Fuzzy Theory and Its Applications (iFuzzy)","20170810","2016","","","1","1","According to the similar nutritional properties, foods could be classified in six groups (Vegetables, Fruits, Dairy, Oils, Grains and Protein foods) and nourish human body respectively. However, people could not understand the nutrients of foods which they obtained generally. Hence, this paper proposes a system based on deep learning for training. Users take pictures on diets by their smartphones and the system will recognize both what kinds of group and how much of nutrients they will take in. With our system, users could recognize the nutrients in their diet and they can administer their health effectively. During training, we not only confirm the architecture of CNN, but also find out that the color feature of foods in the images has significant effect on the identification result about up to seventy percent of the resolution ratio.","","Electronic:978-1-5090-4111-4; POD:978-1-5090-4112-1; USB:978-1-5090-4110-7","10.1109/iFUZZY.2016.8004962","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004962","","Biomedical imaging;Image color analysis;Image recognition;Machine learning;Network architecture;Smart phones;Training","food products;image recognition;learning (artificial intelligence);neural nets;smart phones","CNN;deep learning algorithm;food color feature;food image;food image recognition;nutritional properties;smartphone-based food category recognition;smartphone-based nutrition quantity recognition","","","","","","","","9-11 Nov. 2016","","IEEE","IEEE Conference Publications"
"Optimal Feature Selection and Deep Learning Ensembles Method for Emotion Recognition From Human Brain EEG Sensors","R. Majid Mehmood; R. Du; H. J. Lee","Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea","IEEE Access","20170809","2017","5","","14797","14806","Recent advancements in human–computer interaction research have led to the possibility of emotional communication via brain–computer interface systems for patients with neuropsychiatric disorders or disabilities. In this paper, we efficiently recognize emotional states by analyzing the features of electroencephalography (EEG) signals, which are generated from EEG sensors that noninvasively measure the electrical activity of neurons inside the human brain, and select the optimal combination of these features for recognition. In this paper, the scalp EEG data of 21 healthy subjects (12–14 years old) were recorded using a 14-channel EEG machine while the subjects watched images with four types of emotional stimuli (happy, calm, sad, or scared). After preprocessing, the Hjorth parameters (activity, mobility, and complexity) were used to measure the signal activity of the time series data. We selected the optimal EEG features using a balanced one-way ANOVA after calculating the Hjorth parameters for different frequency ranges. Features selected by this statistical method outperformed univariate and multivariate features. The optimal features were further processed for emotion classification using support vector machine, k-nearest neighbor, linear discriminant analysis, Naive Bayes, random forest, deep learning, and four ensembles methods (bagging, boosting, stacking, and voting). The results show that the proposed method substantially improves the emotion recognition rate with respect to the commonly used spectral power band method.","","","10.1109/ACCESS.2017.2724555","Brain Korea 21 PLUS Project; Ministry of Science, ICT and Future Planning, Korea, under the Information Technology Research Center, supervised by the Institute for Information and Communications Technology Promotion; NUPTSF; 10.13039/100010002 - Basic Science Research Program through the NRF of South Korea, through the Ministry of Education; 10.13039/501100001809 - National Natural Science Foundation for Young Scholars of China; 10.13039/501100003725 - National Research Foundation (NRF) of Korea; 10.13039/501100004608 - Natural Science Foundation for Young Scholars of Jiangsu Province; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997991","EEG emotion recognition;EEG feature extraction;EEG pattern recognition;Hjorth parameter","Electrodes;Electroencephalography;Emotion recognition;Feature extraction;Medical services;Sensors;Support vector machines","","","","","","","","","20170731","2017","","IEEE","IEEE Journals & Magazines"
"Comparison of convolutional neural network models for food image classification","G. Özsert Yi̇ği̇t; B. M. Özyildirim","Computer Engineering Department, Gaziantep University, Turkey","2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)","20170807","2017","","","349","353","According to some estimates of World Health Organization (WHO), in 2014, more than 1.9 billion adults aged 18 years and older were overweight. Overall, about 13% of the world's adult population (11% of men and 15% of women) were obese. 39% of adults aged 18 years and over (38% of men and 40% of women) were overweight. The worldwide prevalence of obesity more than doubled between 1980 and 2014. The purpose of this study is to design a convolutional neural network model and provide a food dataset collection to distinguish the nutrition groups which people take in daily life. For this aim, both two pretrained models Alexnet and Caffenet were finetuned and a similar structure was trained with dataset. Food images were generated from Food-11, FooDD, Food100 datasets and web archives. According to the test results, finetuned models provided better results than trained structure as expected. However, trained model can be improved by using more training examples and can be used as specific structure for classification of nutrition groups.","","Electronic:978-1-5090-5795-5; POD:978-1-5090-5796-2","10.1109/INISTA.2017.8001184","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001184","Convolutional Neural Network;Deep learning;Food Classification;Nutrition Categorization","Convolution;Diabetes;Graphics processing units;Kernel;Neural networks;Obesity;Training","convolution;food products;image classification;medical computing;neural nets","WHO;World Health Organization;convolutional neural network;food image classification;nutrition groups;obesity","","","","","","","","3-5 July 2017","","IEEE","IEEE Conference Publications"
"Bed-exit prediction based on convolutional neural networks","T. X. Chen; R. S. Hsiao; C. H. Kao; W. H. Liao; D. B. Lin","Dept. of Electronic Engineering, National Taipei University of Technology, No. 1, Sec. 3, Zhongxiao E. Rd., 106, Taiwan, R.O.C.","2017 International Conference on Applied System Innovation (ICASI)","20170724","2017","","","188","191","In this paper, we propose a deep convolutional neural network model for in-bed behavior recognition and bed-exit prediction. This model extracts features for training from depth images taken by depth cameras in two categories: in-bed images taken several time intervals before a patient gets out of bed, and usual in-bed activity images. The depth camera-based model features grayscale and low-resolution images, and excels at discarding unnecessary details such as background information beyond the target's contour. The proposed model was proven to be computationally efficient, which is crucial for analyzing and predicting the category of new images in real time.","","Electronic:978-1-5090-4897-7; POD:978-1-5090-4898-4","10.1109/ICASI.2017.7988382","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7988382","bed exit;deep learning;depth image;machine learning","Cameras;Computational modeling;Hospitals;Image recognition;Neural networks;Predictive models;Training","cameras;feature extraction;image resolution;medical image processing;neural nets","bed-exit prediction;deep convolutional neural network model;depth camera-based model;depth images;feature extraction;in-bed activity images;in-bed behavior recognition;low-resolution images;target contour","","","","","","","","13-17 May 2017","","IEEE","IEEE Conference Publications"
"Adaptive smoothing in fMRI data processing neural networks","A. Vilamala; K. H. Madsen; L. K. Hansen","Technical University of Denmark","2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI)","20170720","2017","","","1","4","Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data processing pipelines to accurately determine brain activity; among them, the crucial step of spatial smoothing. These pipelines are commonly suboptimal, given the local optimisation strategy they use, treating each step in isolation. With the advent of new tools for deep learning, recent work has proposed to turn these pipelines into end-to-end learning networks. This change of paradigm offers new avenues to improvement as it allows for a global optimisation. The current work aims at benefitting from this paradigm shift by defining a smoothing step as a layer in these networks able to adaptively modulate the degree of smoothing required by each brain volume to better accomplish a given data analysis task. The viability is evaluated on real fMRI data where subjects did alternate between left and right finger tapping tasks.","","Electronic:978-1-5386-3159-1; POD:978-1-5386-3160-7","10.1109/PRNI.2017.7981499","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981499","","Biological neural networks;Correlation;Data processing;Pipelines;Smoothing methods;Standards;Training","biomedical MRI;brain;data analysis;learning (artificial intelligence);medical image processing;neural nets;optimisation","adaptive smoothing;brain volume;data analysis;deep learning;end-to-end learning networks;fMRI data;fMRI data processing;functional magnetic resonance imaging;left finger tapping tasks;local optimisation strategy;multistep data processing pipelines;neural networks;paradigm shift;right finger tapping tasks","","","","","","","","21-23 June 2017","","IEEE","IEEE Conference Publications"
"Deep convolutional neural networks for motion instability identification using kinect","D. Leightley; S. C. Mukhopadhyay; H. Ghayvat; M. H. Yap","Centre for Military Health Research, King's College London","2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)","20170720","2017","","","310","313","Evaluating the execution style of human motion can give insight into the performance and behaviour exhibited by the participant. This could enable support in developing personalised rehabilitation programmes by providing better understanding of motion mechanics and contextual behaviour. However, performing analyses, generating statistical representations and models which are free from external bins, repeatable and robust is a difficult task. In this work, we propose a framework which evaluates clinically valid motions to identify unstable behaviour during performance using Deep Convolutional Neural Networks. The framework is composed of two parts; 1) Instead of using the whole skeleton as input, we divide the human skeleton into five joint groups. For each group, feature encoding is used to represent spatial and temporal domains to permit high-level abstraction and to remove noise these are then represented using distance matrices. 2) The encoded representations are labelled using an automatic labelling method and evaluated using deep learning. Experimental results demonstrates the ability to correctly classify data compared to classical approaches.","","Electronic:978-4-9011-2216-0; POD:978-1-5386-0495-3","10.23919/MVA.2017.7986863","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986863","","Encoding;Euclidean distance;Feature extraction;Foot;Labeling;Skeleton;Support vector machines","convolution;image coding;image denoising;image motion analysis;image representation;image sensors;learning (artificial intelligence);matrix algebra;medical image processing;neural nets;patient rehabilitation","Kinect;automatic labelling;clinically valid motions;contextual behaviour;deep convolutional neural networks;deep learning;distance matrices;encoded representations;feature encoding;high-level abstraction;human motion;human skeleton;motion instability identification;motion mechanics;noise removal;personalised rehabilitation programmes;spatial domains;temporal domains","","","","","","","","8-12 May 2017","","IEEE","IEEE Conference Publications"
"Deep learning approach for EEG compression in mHealth system","A. Ben Said; A. Mohamed; T. Elfouly","Computer Science and Engineering Department, Qatar Univrsity 2713, Doha, Qatar","2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC)","20170720","2017","","","1508","1512","The emergence of mobile health (mHealth) systems has risen the challenges and concerns due to the sensitivity of the data involved in such systems. It is essential to ensure that these data are well delivered to the health monitoring center for accurate and perfect diagnosis and follow-up. Due to the wireless network constraints, these requirements become more challenging. In this paper, we propose a deep learning approach for EEG data compression in mHealth system. We show that the stacked autoencoder neural network architecture is efficient for EEG data compression. We conduct a comprehensive comparative study that demonstrates the effectiveness of our system for EEG compression in addition to preserving the total energy consumption.","","Electronic:978-1-5090-4372-9; POD:978-1-5090-4373-6","10.1109/IWCMC.2017.7986507","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986507","EEG;compression;mHealth;stacked autoencoder","Data compression;Discrete wavelet transforms;Distortion;Electroencephalography;Image coding;Machine learning;Neural networks","electroencephalography;learning (artificial intelligence);medical signal processing;mobile computing;neural nets;patient monitoring","EEG data compression;deep learning approach;health monitoring center;mHealth system;mobile health systems;patient diagnosis;stacked autoencoder neural network architecture;wireless network constraints","","","","","","","","26-30 June 2017","","IEEE","IEEE Conference Publications"
"Automatic 1D convolutional neural network-based detection of artifacts in MEG acquired without electrooculography or electrocardiography","P. Garg; E. Davenport; G. Murugesan; B. Wagner; C. Whitlow; J. Maldjian; A. Montillo","UT Southwestern Medical Center, Dallas, Texas, USA","2017 International Workshop on Pattern Recognition in Neuroimaging (PRNI)","20170720","2017","","","1","4","Magnetoencephalography (MEG) is a functional neuroimaging tool that records the magnetic fields induced by electrical neuronal activity; however, signal from non-neuronal sources can corrupt the data. Eye-Blinks (EB) and Cardiac Activity (CA) are two of the most common types of non-neuronal artifacts. They can be measured by affixing eye proximal electrodes, as in electrooculography (EOG) and chest electrodes, as in electrocardiography (EKG), however this complicates imaging setup, decreases patient comfort, and often induces further artifacts from facial twitching and postural muscle movement. We propose an EOG- and EKG-free approach to identify eye-blink, cardiac, or neuronal signals for automated artifact suppression. Our contributions are two-fold. First, we combine a data driven, multivariate decomposition approach based on Independent Component Analysis (ICA) and a highly accurate classifier constructed as a deep 1-D Convolutional Neural Network. Second, we visualize the features learned to reveal what features the model uses and to bolster user confidence in our model's training and potential for generalization. We train and test three variants of our method on resting state MEG data from 49 subjects. Our cardiac model achieves a 96% sensitivity and 99% specificity on the set-aside test-set. Our eye-blink model achieves a sensitivity of 85% and specificity of 97%. This work facilitates automated MEG processing for both, clinical and research use, and can obviate the need for EOG or EKG electrodes.","","Electronic:978-1-5386-3159-1; POD:978-1-5386-3160-7","10.1109/PRNI.2017.7981506","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7981506","CNN;EKG;EOG;MEG;artifact;deep learning","Brain modeling;Convolution;Electrocardiography;Electrodes;Electrooculography;Neuroscience;Sensitivity","cardiology;eye;gait analysis;independent component analysis;magnetoencephalography;medical signal detection;medical signal processing;neural nets","ICA;MEG acquisition;MEG processing;affixing eye proximal electrodes;automatic 1D convolutional neural network-based detection;cardiac activity;cardiac model;chest electrodes;electrical neuronal activity;eye-blink model;eye-blinks activity;facial twitching;feature learning;functional neuroimaging tool;independent component analysis;magnetic fields;magnetoencephalography;multivariate decomposition approach;nonneuronal artifacts;nonneuronal sources;postural muscle movement","","","","","","","","21-23 June 2017","","IEEE","IEEE Conference Publications"
"Tongue shape classification integrating image preprocessing and Convolution Neural Network","C. M. Huo; H. Zheng; H. Y. Su; Z. L. Sun; Y. J. Cai; Y. F. Xu","Key Lab of Intelligent Information Technology, Beijing Institute of Technology, Beijing, China","2017 2nd Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","20170720","2017","","","42","46","Tongue diagnosis is one of the most important parts in “inspection diagnosis” of Traditional Chinese Medicine (TCM). Observing tongue shape can help to understand the changes in human body and thereby to estimate the illness. This paper presents a method of recognizing tongue shapes based on Convolution Neural Network. The proposed method enhances the features of tongue images with preprocessing to ensure the data suitable for tongue shape binary classification. In view of the special texture and outline of tongue, the whole tongue images of dot-sting tongue and fissured tongue is transformed by Gabor filter, and the tooth-marked are processed by boundary detection approach. CNN is adopted because it has achieved remarkable results in computer vision and pattern recognition, and the model training through neural network coincides with the Chinese medicine dialectics through experience. Based on commonly used Alex-net, network is optimized with batch normalization to improve efficiency. The experimental results indicate that the preprocessing methods increase the accuracy and decreases the time of training process of tongue shape classification, which proves that the method is effective for the recognition of different tongue shapes.","","DVD:978-1-5090-6791-6; Electronic:978-1-5090-6793-0; POD:978-1-5090-6794-7; Paper:978-1-5090-6792-3","10.1109/ACIRS.2017.7986062","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986062","Gabor filtering;convolution neural network;deep learning;tongue shape classification","Biological neural networks;Convolution;Feature extraction;Gabor filters;Shape;Tongue;Training","Gabor filters;computer vision;image classification;image texture;medical computing;neural nets;patient diagnosis;shape recognition","Alex-net;Chinese medicine dialectics;Gabor filter;TCM;computer vision;convolution neural network;dot-sting tongue;image preprocessing;image texture;inspection diagnosis;pattern recognition;tongue diagnosis;tongue images;tongue shape binary classification;traditional Chinese medicine","","","","","","","","16-18 June 2017","","IEEE","IEEE Conference Publications"
"Computer-aided classification of multi-types of dementia via convolutional neural networks","E. M. Alkabawi; A. R. Hilal; O. A. Basir","Department of Electrical and Computer Engineering, University of Waterloo, Ontario, Canada","2017 IEEE International Symposium on Medical Measurements and Applications (MeMeA)","20170720","2017","","","45","50","With millions of people suffering from dementia worldwide, the global prevalence of dementia has a significant impact on the patients' lives, their caregivers' physical and emotional states, and the global economy. Early diagnosis of dementia helps in finding suitable therapies that reduce or even prevent further deterioration of patients' cognitive abilities. In recent years, state-of-the-art literature has proposed various computer-aided diagnosis systems based on 3-dimensional brain imagery analysis to identify early symptoms of dementia. These systems aim to assist radiologists in increasing the accuracy of diagnoses and reducing false positives. However, the early diagnosis of dementia is a challenging task due to the image quality, noise, and human brain irregularities. The state-of-the-art has focused on differentiating multi-stages of Alzheimer's disease, however, the diagnosis of various types of dementia is still a gap. This paper proposes a deep learning-based computer-aided diagnosis approach for the early detection of multi-type of dementia. To show the performance of the proposed CAD algorithm, three conventional CAD methods are implemented for comparison. The proposed algorithm yields a 74.93% accuracy in early diagnosis of multi-type of dementia and outperforms the state of the art CAD methods.","","Electronic:978-1-5090-2984-6; POD:978-1-5090-2985-3","10.1109/MeMeA.2017.7985847","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985847","Alzheimer's disease;Brain imaging;Computer-Aided Diagnosis;Convolutional Neural Networks;Dementia;Early diagnosis;Magnetic Resonance Imaging","Brain;Dementia;Feature extraction;Image edge detection;Image segmentation;Magnetic resonance imaging","biomedical MRI;cognition;convolution;diseases;learning (artificial intelligence);medical disorders;medical image processing;neural nets;neurophysiology","Alzheimer disease;caregiver physical states;caregiveremotional states;convolutional neural networks;deep learning-based computer-aided diagnosis approach;dementia;image quality;magnetic resonance imaging;patient cognitive abilities;three-dimensional brain imagery analysis","","","","","","","","7-10 May 2017","","IEEE","IEEE Conference Publications"
"Implementing WEKA for medical data classification and early disease prediction","N. Kumar; S. Khatri","Department of Computer Science & Engineering, University Institute of Engineering & Technology, Babasaheb Bhimrao Ambedkar University, Lucknow, India","2017 3rd International Conference on Computational Intelligence & Communication Technology (CICT)","20170713","2017","","","1","6","In recent years, the advent of latest web and data technologies has encouraged massive data growth in almost every sector. Businesses and leading industries are viewing these huge data repositories as a tool to design future strategies, prediction models by analyzing patterns and gaining knowledge from this unstructured data by applying different data mining techniques. Medical domain has now become richer in term of maintaining digital records of patients related to their diagnosis and treatment. These huge data repositories can range from patient personnel data, diagnosis, treatment histories, test diagnosis, images and various scans. This terabytes of medical data is quantity rich but weaker in information in terms of knowledge and robust tools to identify hidden patterns of knowledge specifically in medical sector. Data Mining as a field of research has already well proven capabilities of identifying hidden patterns, analysis and knowledge applied on different research domains, now gaining popularity day by day among researchers and scientist towards generating novel and deep insights of these large biomedical datasets also. Uncovering new biomedical and healthcare related knowledge in order to support clinical decision making, is another dimension of data mining. Through massive literature survey, it is found that early disease prediction is the most demanded area of research in health care sector. As health care domain is bit wider domain and having different disease characteristics, different techniques have their own prediction efficiencies, which can be enhanced and changed in order to get into most optimize way. In this research work, authors have comprehensively compared different data classification techniques and their prediction accuracy for chronic kidney disease. Authors have compared J48, Naive Bayes, Random Forest, SVM and k-NN classifiers using performance measures like ROC, kappa statistics, RMSE and MAE using WEKA tool. Authors have also compar- d these classifiers on various accuracy measures like TP rate, FP rate, precision, recall and f-measure by implementing on WEKA. Experimental result shows that random forest classifier has better classification accuracy over others for chronic kidney disease dataset.","","Electronic:978-1-5090-6218-8; POD:978-1-5090-6219-5","10.1109/CIACT.2017.7977277","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7977277","Classification;Data Mining;Healthcare;WEKA","Algorithm design and analysis;Classification algorithms;Data mining;Diseases;Kidney;Medical diagnostic imaging","data mining;diseases;electronic health records;health care;learning (artificial intelligence);patient diagnosis;patient treatment;pattern classification","J48;MAE;Naive Bayes;RMSE;ROC;SVM;WEKA tool;Waikato environment for knowledge analysis;biomedical datasets;chronic kidney disease dataset;clinical decision making;data mining techniques;data repositories;data technologies;early disease prediction;health care sector;k-NN classifiers;kappa statistics;massive data growth;medical data classification;medical domain;patient diagnosis;patient digital records;patient personnel data;patient treatment;pattern analysis;performance measures;random forest classifier;test diagnosis","","","","","","","","9-10 Feb. 2017","","IEEE","IEEE Conference Publications"
"Deep Convolutional Neural Network for Inverse Problems in Imaging","K. H. Jin; M. T. McCann; E. Froustey; M. Unser","Biomedical Imaging Group, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Image Processing","20170711","2017","26","9","4509","4522","In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.","1057-7149;10577149","","10.1109/TIP.2017.2713099","10.13039/100000070 - National Institute of Biomedical Imaging and Bioengineering; 10.13039/100010661 - European Union¿¿¿s Horizon 2020 Framework Programme for Research and Innovation (call 2015); 10.13039/501100000781 - European Research Council (H2020-ERC Project GlobalBioIm); 10.13039/501100006391 - Center for Biomedical Imaging of the Geneva-Lausanne Universities and EPFL; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949028","Image restoration;biomedical imaging;biomedical signal processing;computed tomography;image reconstruction;magnetic resonance imaging;reconstruction algorithms;tomography","Computed tomography;Convolution;Image reconstruction;Inverse problems;Iterative methods;Neural networks","computerised tomography;feedforward neural nets;image resolution;iterative methods;learning (artificial intelligence);medical image processing","CNN;GPU;adjoint operators;deep convolutional neural network;direct inversion;forward model;forward operators;hyperparameter selection;ill-posed inverse problems;image structure;multiresolution decomposition;normal-convolutional inverse problems;parallel beam X-ray computed tomography;regularized iterative algorithms;residual learning;synthetic phantoms;total variation-regularized iterative reconstruction","","","","","","","20170615","Sept. 2017","","IEEE","IEEE Journals & Magazines"
"Semantic segmentation of microscopic images of H&E stained prostatic tissue using CNN","J. Isaksson; I. Arvidsson; K. Åaström; A. Heyden","Lund University, Centre for Mathematical Sciences, Lund, Sweden","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","1252","1256","There is a need for an automatic Gleason scoring system that can be used for prostate cancer diagnosis. Today the diagnoses are determined by pathologists manually, which is both a complex and a time-consuming task. To reduce the pathologists' workload, but also to reduce variations between different pathologists, an automatic classification system would be of great use. Some previous works have aimed for this, but still more work needs to be done. It is probable that such a tool would benefit from having access to individually segmented, pathologically relevant objects from the images. Therefore, we have developed an algorithm for semantic segmentation of the microscopic images of H&E stained prostate tissue into Background, Stroma, Epithelial Cytoplasm and Nuclei. This algorithm is based on deep learning, or more specifically a convolutional neural network. The network design is inspired by architectures that previously have been proved successful in different applications. It consists of a contracting and an expanding part, which are symmetrical. We have reached an accuracy of 80 %, as measured by the mean of the intersection over union, for segmentation into four classes. Previous works have only investigated nuclei segmentation, and our network performed similar but for the more challenging task of four class segmentation.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7965996","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965996","","Cancer;Gold;Image segmentation;Microscopy;Neural networks;Semantics;Standards","biological tissues;cancer;image classification;image segmentation;medical image processing;neural nets;patient diagnosis","CNN;H&E stained prostatic tissue;automatic Gleason scoring system;automatic classification system;epithelial cytoplasm;microscopic image semantic segmentation;prostate cancer diagnosis","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"Automating Papanicolaou Test Using Deep Convolutional Activation Feature","J. Hyeon; H. J. Choi; K. N. Lee; B. D. Lee","Sch. of Comput., KAIST, Daejeon, South Korea","2017 18th IEEE International Conference on Mobile Data Management (MDM)","20170703","2017","","","382","385","Cervical cancer is the women's fourth most common cancer worldwide, with 266,000 deaths in a year. Cervical cancer can be diagnosed by the Papanicolaou test. In this test, a cytopathologist observes a microscopic image of the cervix cells and decides whether the patient is abnormal or not. According to research, the accuracy of the cervical cytology is reported as 89.7%. Because it is associated with the patient's life, it is important to improve the accuracy of this test. Many systems have been proposed to help judge experts to improve the accuracy of tests in the medical field, but development has been limited to areas where there are cleanly quantified test data. In this paper, we design and train a model to automatically classify the normal/abnormal state of cervical cells from microscopic images by using a convolutional neural network and several machine learning classifiers. As a result, the support vector machine achieves the highest performance with 78% F1 score.","","Electronic:978-1-5386-3932-0; POD:978-1-5386-3933-7","10.1109/MDM.2017.66","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962484","Cervical Cancer;Cervical Cancer Screening Test;Convolutional Neural Network;Deep Convolutional Activation Feature;Papanicolaou Test","Biological neural networks;Cervical cancer;Feature extraction;Microscopy;Support vector machines","biomedical optical imaging;cancer;cellular biophysics;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;microscopes;support vector machines","F1 score;Papanicolaou test automation;automatic abnormal state classification;automatic normal state classification;cervical cancer;cervical cytology;cervix cells;convolutional neural network;deep convolutional activation feature;machine learning classifiers;medical field;microscopic image;support vector machine","","","","","","","","May 29 2017-June 1 2017","","IEEE","IEEE Conference Publications"
"Classification of radiology reports using neural attention models","B. Shin; F. H. Chokshi; T. Lee; J. D. Choi","Mathematics and Computer Science, Emory University, Atlanta, GA 30322","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","4363","4370","The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure “black-box” models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966408","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966408","","Computational modeling;Convolution;Machine learning;Neural networks;Radiology;Sentiment analysis;Support vector machines","computerised tomography;convolution;data analysis;electronic health records;neural nets;pattern classification;radiology","CNN;EHR;attention analysis;convolutional neural networks;double-annotated dataset;electronic health record;neural attention mechanism;neural attention models;radiology head computed tomography reports classification","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"Similarities and differences between stimulus tuning in the inferotemporal visual cortex and convolutional networks","B. P. Tripp","Department of Systems Design Engineering & Centre for Theoretical Neuroscience, Waterloo, Ontario, Canada","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","3551","3560","Deep convolutional neural networks (CNNs) trained for object classification have a number of striking similarities with the primate ventral visual stream. In particular, activity in early, intermediate, and late layers is closely related to activity in V1, V4, and the inferotemporal cortex (IT). This study further compares activity in late layers of object-classification CNNs to activity patterns reported in the IT electrophysiology literature. There are a number of close similarities, including the distributions of population response sparseness across stimuli, and the distribution of size tuning bandwidth. Statisics of scale invariance, responses to clutter and occlusion, and orientation tuning are less similar. Statistics of object selectivity are quite different. These results agree with recent studies that highlight strong parallels between object-categorization CNNs and the ventral stream, and also highlight differences that could perhaps be reduced in future CNNs.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966303","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966303","","Bandwidth;Correlation;Neurons;Sociology;Tuning;Visualization","bioelectric phenomena;image classification;learning (artificial intelligence);medical image processing;object detection;scaling phenomena;statistics","CNN training;IT electrophysiology literature;activity patterns;clutter responses;deep convolutional neural networks;inferotemporal visual cortex;object classification;object selectivity statistics;object-categorization CNNs;object-classification CNNs;occlusion responses;orientation tuning;primate ventral visual stream;scale invariance statistics;size tuning bandwidth;stimulus tuning;ventral stream","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"End-to-end learning of brain tissue segmentation from imperfect labeling","A. Fedorov; J. Johnson; E. Damaraju; A. Ozerin; V. Calhoun; S. Plis","The Mind Research Network, Albuquerque, USA","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","3785","3792","Segmenting a structural magnetic resonance imaging (MRI) scan is an important pre-processing step for analytic procedures and subsequent inferences about longitudinal tissue changes. Manual segmentation defines the current gold standard in quality but is prohibitively expensive. Automatic approaches are computationally intensive, incredibly slow at scale, and error prone due to usually involving many potentially faulty intermediate steps. In order to streamline the segmentation, we introduce a deep learning model that is based on volumetric dilated convolutions, subsequently reducing both processing time and errors. Compared to its competitors, the model has a reduced set of parameters and thus is easier to train and much faster to execute. The contrast in performance between the dilated network and its competitors becomes obvious when both are tested on a large dataset of unprocessed human brain volumes. The dilated network consistently outperforms not only another state-of-the-art deep learning approach, the up convolutional network, but also the ground truth on which it was trained. Not only can the incredible speed of our model make large scale analyses much easier but we also believe it has great potential in a clinical setting where, with little to no substantial delay, a patient and provider can go over test results.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966333","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966333","","Brain modeling;Image segmentation;Kernel;Magnetic resonance imaging;Mathematical model;Training","biological tissues;biomedical MRI;brain;image segmentation;learning (artificial intelligence);medical image processing","MRI scan;brain tissue segmentation;deep learning model;dilated network;end-to-end learning;gold standard;imperfect labeling;longitudinal tissue changes;preprocessing step;structural magnetic resonance imaging scan;subsequent inferences;unprocessed human brain volumes;volumetric dilated convolutions","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"Deep learning of texture and structural features for multiclass Alzheimer's disease classification","C. V. Dolph; M. Alam; Z. Shboul; M. D. Samad; K. M. Iftekharuddin","Vision Lab at Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA 23529","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","2259","2266","This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6% and 58.0% tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4% and 56.8%. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2% in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966129","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966129","ADNI;Alzheimer's disease;biomarkers;deep learning;dropout learning;hippocampus;neuroimaging classification","Brain modeling;Computational modeling;Diseases;Feature extraction;Machine learning;Magnetic resonance imaging;Sensitivity","biomedical MRI;feature extraction;feature selection;image classification;image texture;matrix algebra;medical image processing;neural nets","CADDementia competition;SAE deep neural network;cognitive normal;cortical surface area;cortical thickness;feature selection;fractal Brownian motion co-occurrence matrices;gray matter volumes;magnetic resonance imaging;mild cognitive impairment;multiclass Alzheimer's disease classification;multiclass deep learning classification;stacked auto-encoder;structural MRI;structural feature;subcortical area specific feature extraction;texture feature;true positive fraction;white matter volumes","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"ConvNet-Based Localization of Anatomical Structures in 3-D Medical Images","B. D. de Vos; J. M. Wolterink; P. A. de Jong; T. Leiner; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands","IEEE Transactions on Medical Imaging","20170630","2017","36","7","1470","1481","Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.","0278-0062;02780062","","10.1109/TMI.2017.2673121","10.13039/100007065 - NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research; 10.13039/501100003958 - Netherlands Organization for Scientific Research Foundation for Technology Sciences Project 12726; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862905","CT;Localization;convolutional neural networks;deep learning;detection","Abdomen;Anatomical structure;Computed tomography;Heart;Three-dimensional displays;Two dimensional displays","angiocardiography;computerised tomography;feature extraction;liver;medical image processing;neural nets;stereo image processing","3D medical images;ConvNet-based localization;abdomen CT scans;anatomical structure;aortic arch;ascending aorta;cardiac CT angiography;chest CT;convolutional neural network;descending aorta;heart;left cardiac ventricle;liver","","","","","","","20170223","July 2017","","IEEE","IEEE Journals & Magazines"
"A deep learning based approach for classification of CerbB2 tumor cells in breast cancer","G. A. Tataroğlu; A. Genç; K. A. Kabakçı; A. Çapar; B. U. Töreyin; H. K. Ekenel; İ. Türkmen; A. Çakır","Bili&#x015F;im Enstit&#x00FC;s&#x00FC;, &#x0130;stanbul Teknik &#x00DC;niversitesi","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","This study proposes a unique approach to classify CerbB2 tumor cell scores in breast cancer based on deep learning models. Another contribution of the study is the creation of a dataset from original breast cancer tissues. On the purpose of training, validating and testing with deep learning models cell fragments were generated from sample tissue images. CerbB2 tumor scores were generated for the cell fragments were classified with high performance by the aid of convolutional neural networks (CNN).","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960587","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960587","CerbB2 marker;Convolutional Neural Networks (CNN);classification;score;tumor","Breast cancer;Fasteners;Fish;Machine learning;Neural networks;Proteins;Tumors","cancer;learning (artificial intelligence);medical image processing;neural nets;tumours","CNN;CerbB2 tumor cells;breast cancer;breast cancer tissues;cell fragments;convolutional neural networks;deep learning;deep learning models;sample tissue images","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation","J. Egede; M. Valstar; B. Martinez","Sch. of Comput. Sci., Univ. of Nottingham, Ningbo, China","2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","20170629","2017","","","689","696","Automatic continuous time, continuous value assessment of a patient's pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth.","","Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7","10.1109/FG.2017.87","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961808","","Estimation;Face;Feature extraction;Machine learning;Pain;Physiology;Shape","learning (artificial intelligence);mean square error methods;medical image processing;pose estimation;time series;video coding","RMSE error;automatic pain estimation;deep learning;deep-learned features;face video;hand-crafted features;medical diagnosis;shape encoding;time series","","","","","","","","May 30 2017-June 3 2017","","IEEE","IEEE Conference Publications"
"Cerebral vessel classification with convolutional neural networks","Y. H. Şahin; G. Ünal","Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, &#x0130;stanbul Teknik &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","Analysing brain magnetic resonance angiography (MRA) images is important for detecting arteriovenous malformations and aneurysms. To detect these diseases, extracting the vessel structure in the image can be seen as a first step. In this paper, it was aimed to classify the cubic image parts obtained from brain MRA images according to whether they belong to vein structure or not. For this purpose, a 9 layers deep convolutional neural network (CNN) architecture is designed. With the model trained using this architecture, 85% accuracy was obtained in the classification performed on the test data.","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960697","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960697","cerebral vessel classification;convolutional neural networks;deep learning;magnetic resonance angiography (MRA)","Biological neural networks;Biomedical imaging;Brain modeling;Dogs;Image segmentation;Magnetic resonance;Nanoelectromechanical systems","biomedical MRI;diseases;feature extraction;feedforward neural nets;image classification;medical image processing;object detection","CNN architecture;aneurysm detection;arteriovenous malformation detection;brain MRA images;brain magnetic resonance angiography image analysis;cerebral vessel classification;cubic image part classification;deep convolutional neural network architecture;disease detection;vein structure;vessel structure extraction","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Cells classification with deep learning","A. Sezer; U. Çekmez","Bilgisayar M&#x00FC;hendisligi B&#x00F6;l&#x00FC;m&#x00FC;, Y&#x0131;ld&#x0131;z Teknik &#x00DC;niversitesi, 34220 Istanbul, T&#x00FC;rkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","Proteomic analysis is a rapidly developing research field that has recently been used in the diagnosis and treatment of various diseases by analyzing the structure and functions of protein patterns in the cell. Numerous computer based decision support mechanisms implemented in this context have mostly used special image processing techniques until now. Recently, high performance self-learning deep learning methods have taken place in the classification studies over the conventional methods examining the structural features of the patterns, shapes and the texture properties in the images. In this study, different intracellular patterns of HeLa cells taken by the microscope used in the testing of pattern analysis and the output is compared by classifying these patterns by using both deep learning methods and bag-of-features method. As a result of the experiments, it is seen that the success of the proposed deep learning model has a very high performance in classifying compared to the existing models and bag-of-features technique.","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960647","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960647","classification;convolutional deep neural networks;deep learning;genome;protein;proteomic analysis","Bioinformatics;Dogs;Genomics;Machine learning;Neural networks;Pattern recognition;Proteins","cellular biophysics;diseases;image classification;image texture;learning (artificial intelligence);medical image processing;patient diagnosis;patient treatment;proteins","HeLa cell intracellular patterns;cell classification;computer-based decision support mechanisms;high-performance self-learning deep learning;image processing;pattern structural features;proteomic analysis;texture properties","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Small Sample Deep Learning for Newborn Gestational Age Estimation","M. T. Torres; M. F. Valstar; C. Henry; C. Ward; D. Sharkey","Sch. of Comput. Sci., Univ. of Nottingham, Nottingham, UK","2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","20170629","2017","","","79","86","A baby's gestational age determines whether or not they are preterm, which helps clinicians decide on suitable post-natal treatment. The most accurate dating methods use Ultrasound Scan (USS) machines, but these machines are expensive, require trained personnel and cannot always be deployed to remote areas. In the absence of USS, the Ballard Score can be used, which is a manual postnatal dating method. However, this method is highly subjective and results can vary widely depending on the experience of the rater. In this paper, we present an automatic system for postnatal gestational age estimation aimed to be deployed on mobile phones, using small sets of images of a newborn's face, foot and ear. We present a novel two-stage approach that makes the most out of Convolutional Neural Networks trained on small sets of images to predict broad classes of gestational age, and then fuse the outputs of these discrete classes with a baby's weight to make fine-grained predictions of gestational age. On a purpose=collected dataset of 88 babies, experiments show that our approach attains an expected error of 6 days and is three times more accurate than the manual postnatal method (Ballard). Making use of images improves predictions by 30% compared to using weight only. This indicates that even with a very small set of data, our method is a viable candidate for postnatal gestational age estimation in areas were USS is not available.","","Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7","10.1109/FG.2017.19","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961726","","Biomedical imaging;Ear;Estimation;Face;Image segmentation;Machine learning;Pediatrics","biomedical ultrasonics;convolution;image classification;image segmentation;learning (artificial intelligence);medical image processing;mobile computing;paediatrics","Ballard score;USS machines;convolutional neural network training;deep learning;image segmentation;mobile phones;newborn gestational age estimation;post-natal treatment;postnatal dating method;postnatal gestational age estimation;ultrasound scan machines","","","","","","","","May 30 2017-June 3 2017","","IEEE","IEEE Conference Publications"
"Investigation of transfer learning on pulmonary nodule characteristics","A. Kaya; A. S. Keçeli; A. B. Can","Hacettepe &#x00DC;niversitesi, Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Ankara, T&#x00FC;rkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","Studies on the classification of small pulmonary nodules generally focus on the prediction of malignancy of the nodule. In the recent years, publicly available databases provided different types of data to researchers, such as nodule characteristics, apart from the lung image and malignancy degree. In this paper, a study on the classification of pulmonary nodule characteristics using conventional features and deep features obtained from transfer learning method has been proposed. The results were assessed by sensitivity, specificity, and classification accuracy. The results of the study can be used to form multi-level classifiers in predicting malignancy by combining different types of features.","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960357","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960357","malignancy prediction;nodule characteristics;pulmonary nodules;tansfer learning","Biomedical imaging;Computed tomography;Image databases;Lungs;Machine learning;Radio frequency;Support vector machines","cancer;image classification;learning (artificial intelligence);lung;medical image processing;radiology","conventional features;deep features;lung image;malignancy prediction;pulmonary nodule characteristics classification;transfer learning","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Deep neural network based diagnosis system for melanoma skin cancer","A. Baştürk; M. E. Yüksei; H. Badem; A. Çalışkan","Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Erciyes &#x00DC;niversitesi, Kayseri","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","Melanoma is a serious cancer that causes many people to lose their lives. This disease can be diagnosed by a dermatologist as a result of interpretation of the dermoscopy images by the ABCD rule. In this study, a deep neural network (DNN) is used as a new method for diagnosis of melanoma skin cancer. This method is compared with the-state-art-methods in literature. According to the obtained results, DNN was more successful than the comparative methods.","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960563","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960563","ABCD rule;deep learning;deep neural network;melanoma","Cancer;Dogs;Lesions;Malignant tumors;Neural networks;Niobium;Support vector machines","cancer;medical diagnostic computing;neural nets;skin","ABCD rule;DNN;deep neural network based diagnosis system;dermoscopy images;melanoma skin cancer","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Automatic Detection of ADHD and ASD from Expressive Behaviour in RGBD Data","S. Jaiswal; M. F. Valstar; A. Gillott; D. Daley","","2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","20170629","2017","","","762","769","Attention Deficit Hyperactivity Disorder (ADHD) and Autism Spectrum Disorder (ASD) are neurodevelopmental conditions which impact on a significant number of children and adults. Currently, the diagnosis of such disorders is done by experts who employ standard questionnaires and look for certain behavioural markers through manual observation. Such methods for their diagnosis are not only subjective, difficult to repeat, and costly but also extremely time consuming. In this work, we present a novel methodology to aid diagnostic predictions about the presence/absence of ADHD and ASD by automatic visual analysis of a persons behaviour. To do so, we conduct the questionnaires in a computer-mediated way while recording participants with modern RGBD (Colour+Depth) sensors. In contrast to previous automatic approaches which have focussed only on detecting certain behavioural markers, our approach provides a fully automatic end-to-end system to directly predict ADHD and ASD in adults. Using state of the art facial expression analysis based on Dynamic Deep Learning and 3D analysis of behaviour, we attain classification rates of 96% for Controls vs Condition (ADHD/ASD) groups and 94% for Comorbid (ADHD+ASD) vs ASD only group. We show that our system is a potentially useful time saving contribution to the clinical diagnosis of ADHD and ASD.","","Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7","10.1109/FG.2017.95","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961818","","Autism;Cameras;Databases;Face;Magnetic heads;Tracking","emotion recognition;face recognition;image classification;image colour analysis;learning (artificial intelligence);medical disorders;medical image processing;patient diagnosis","ADHD automatic detection;ADHD clinical diagnosis;ASD automatic detection;ASD clinical diagnosis;aid diagnostic predictions;attention deficit hyperactivity disorder;autism spectrum disorder;automatic visual analysis;behaviour 3D analysis;colour-depth sensors;dynamic deep learning;expressive behaviour;facial expression analysis;modern RGBD sensors;neurodevelopmental conditions","","","","","","","","May 30 2017-June 3 2017","","IEEE","IEEE Conference Publications"
"Tutorials","","","2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","20170629","2017","","","xxv","xxvii","These tutorials discuss the following: Remote physiological measurement from images and videos; Multi-view face representation; From deep unsupervised to supervised models for face analysis; and Statistical methods for affective computing.","","Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7","10.1109/FG.2017.10","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961715","","Affective computing;Computer vision;Face;Image resolution;Machine learning;Physiology;Tutorials","affective computing;biomedical measurement;face recognition;medical image processing;physiology;statistical analysis","affective computing;deep unsupervised models;face analysis;multiview face representation;remote physiological measurement;statistical methods;supervised models","","","","","","","","May 30 2017-June 3 2017","","IEEE","IEEE Conference Publications"
"Deep Learning Segmentation of Optical Microscopy Images Improves 3-D Neuron Reconstruction","R. Li; T. Zeng; H. Peng; S. Ji","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1533","1541","Digital reconstruction, or tracing, of 3-D neuron structure from microscopy images is a critical step toward reversing engineering the wiring and anatomy of a brain. Despite a number of prior attempts, this task remains very challenging, especially when images are contaminated by noises or have discontinued segments of neurite patterns. An approach for addressing such problems is to identify the locations of neuronal voxels using image segmentation methods, prior to applying tracing or reconstruction techniques. This preprocessing step is expected to remove noises in the data, thereby leading to improved reconstruction results. In this paper, we proposed to use 3-D convolutional neural networks (CNNs) for segmenting the neuronal microscopy images. Specifically, we designed a novel CNN architecture, that takes volumetric images as the inputs and their voxel-wise segmentation maps as the outputs. The developed architecture allows us to train and predict using large microscopy images in an end-to-end manner. We evaluated the performance of our model on a variety of challenging 3-D microscopy images from different organisms. Results showed that the proposed methods improved the tracing performance significantly when combined with different reconstruction algorithms.","0278-0062;02780062","","10.1109/TMI.2017.2679713","10.13039/100000001 - National Science Foundation; 10.13039/100007588 - Washington State University; 10.13039/100009980 - Old Dominion University; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7874113","BigNeuron;Deep learning;image denoising;image segmentation;neuron reconstruction","Convolution;Image reconstruction;Image segmentation;Microscopy;Morphology;Neurons;Three-dimensional displays","biomedical optical imaging;brain;image denoising;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optical microscopy","3-D convolutional neural networks;3-D microscopy images;3-D neuron reconstruction;3-D neuron structure;CNN architecture;brain anatomy;brain wiring;deep learning segmentation;digital reconstruction;digital tracing;discontinued segments;image segmentation methods;neurite patterns;neuronal microscopy images;neuronal voxels;noise removal;optical microscopy images;organisms;preprocessing step;reconstruction algorithms;reversing engineering;tracing performance;volumetric images;voxel-wise segmentation maps","","","","","","","20170308","July 2017","","IEEE","IEEE Journals & Magazines"
"Automatic Quantification of Tumour Hypoxia From Multi-Modal Microscopy Images Using Weakly-Supervised Learning Methods","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, SA, Australia","IEEE Transactions on Medical Imaging","20170628","2017","36","7","1405","1417","In recently published clinical trial results, hypoxia-modified therapies have shown to provide more positive outcomes to cancer patients, compared with standard cancer treatments. The development and validation of these hypoxia-modified therapies depend on an effective way of measuring tumor hypoxia, but a standardized measurement is currently unavailable in clinical practice. Different types of manual measurements have been proposed in clinical research, but in this paper we focus on a recently published approach that quantifies the number and proportion of hypoxic regions using high resolution (immuno-)fluorescence (IF) and hematoxylin and eosin (HE) stained images of a histological specimen of a tumor. We introduce new machine learning-based methodologies to automate this measurement, where the main challenge is the fact that the clinical annotations available for training the proposed methodologies consist of the total number of normoxic, chronically hypoxic, and acutely hypoxic regions without any indication of their location in the image. Therefore, this represents a weakly-supervised structured output classification problem, where training is based on a high-order loss function formed by the norm of the difference between the manual and estimated annotations mentioned above. We propose four methodologies to solve this problem: 1) a naive method that uses a majority classifier applied on the nodes of a fixed grid placed over the input images; 2) a baseline method based on a structured output learning formulation that relies on a fixed grid placed over the input images; 3) an extension to this baseline based on a latent structured output learning formulation that uses a graph that is flexible in terms of the amount and positions of nodes; and 4) a pixel-wise labeling based on a fully-convolutional neural network. Using a data set of 89 weakly annotated pairs of IF and HE images from eight tumors, we show that the quantitativ- results of methods (3) and (4) above are equally competitive and superior to the naive (1) and baseline (2) methods. All proposed methodologies show high correlation values with respect to the clinical annotations.","0278-0062;02780062","","10.1109/TMI.2017.2677479","10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Experienced Researchers; 10.13039/100005156 - Alexander von Humboldt Foundation for the Fellowship for Postdoctoral Researchers; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869416","Microscopy;deep learning;high-order loss functions;structured output learning;weakly-supervised training","Biomedical imaging;Cancer;Computational modeling;Manuals;Medical treatment;Training;Tumors","biomedical optical imaging;cancer;fluorescence;image classification;learning (artificial intelligence);medical image processing;neural nets;optical microscopy;tumours","HE images;IF images;acutely hypoxic regions;automatic quantification;baseline method;cancer patients;chronically hypoxic regions;clinical annotations;estimated annotations;fixed grid;fully-convolutional neural network;hematoxylin and eosin stained images;high resolution immunofluorescence images;high-order loss function;histological specimen;hypoxia-modified therapies;input images;latent structured output learning formulation;machine learning-based methodologies;majority classifier;manual annotations;multimodal microscopy images;naive method;normoxic regions;pixel-wise labeling;standard cancer treatments;standardized measurement;tumor hypoxia;tumour hypoxia;weakly-supervised learning methods;weakly-supervised structured output classification problem","","","","","","","20170302","July 2017","","IEEE","IEEE Journals & Magazines"
"Residual and plain convolutional neural networks for 3D brain MRI classification","S. Korolev; A. Safiullin; M. Belyaev; Y. Dodonova","Skolkovo Institute of Science and Technology, Russia","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","835","838","In the recent years there have been a number of studies that applied deep learning algorithms to neuroimaging data. Pipelines used in those studies mostly require multiple processing steps for feature extraction, although modern advancements in deep learning for image classification can provide a powerful framework for automatic feature generation and more straightforward analysis. In this paper, we show how similar performance can be achieved skipping these feature extraction steps with the residual and plain 3D convolutional neural network architectures. We demonstrate the performance of the proposed approach for classification of Alzheimer's disease versus mild cognitive impairment and normal controls on the Alzheimers Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950647","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950647","Alzheimer's Disease;Convolutional Neural Network;Deep Learning;MRI;Residual Neural Network","Alzheimer's disease;Biological neural networks;Feature extraction;Machine learning;Magnetic resonance imaging;Three-dimensional displays","biomedical MRI;brain;cognition;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural net architecture","3D brain MRI classification;3D plain convolutional neural network architecture;3D residual convolutional neural network architecture;3D structural MRI brain scans;ADNI dataset;Alzheimers disease national initiative dataset;cognitive impairment;deep learning algorithms;feature extraction;feature generation;image classification;neuroimaging data","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Classification of thyroid nodules in ultrasound images using deep model based transfer learning and hybrid features","T. Liu; S. Xie; J. Yu; L. Niu; W. Sun","Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","919","923","Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful semantic features to the classification. Firstly, a CNN model trained with a massive natural dataset is transferred to the ultrasound image domain, to generate semantic deep features and handle the small sample problem. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Local Binary Patterns (LBP) together, to form a hybrid feature space. Finally, a positive-sample-first majority voting and a feature-selected based strategy are employed for the hybrid classification. Experimental results on 1037 images show that the accuracy of our proposed method is 0.931, which outperformed other relative methods by over 10%.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952290","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952290","classification;deep learning;feature fusion;transfer learning;ultrasound image","Biomedical imaging;Cancer;Feature extraction;Indexes;Machine learning;Semantics;Ultrasonic imaging","biomedical ultrasonics;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;ultrasonic imaging","CNN model trained;HOG;LBP;benign nodules;convolution neural networks;deep model based transfer learning;feature extraction method;feature-selected based strategy;histogram of oriented gradient;hybrid feature space;local binary patterns;malignant nodules;positive-sample-first majority voting;semantic deep features;thyroid nodule classification;thyroid nodules;ultrasonography;ultrasound images","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Automated characterization of the fetal heart in ultrasound images using fully convolutional neural networks","V. Sundaresan; C. P. Bridge; C. Ioannou; J. A. Noble","Institute of Biomedical Engineering, University of Oxford, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","671","674","Automatic analysis of fetal echocardiography screening images could aid in the identification of congenital heart diseases. The first step towards automatic fetal echocardiography analysis is locating the fetal heart in an image and identifying the viewing (imaging) plane. This is highly challenging since the fetal heart is small with relatively indistinct anatomical structural appearance. This is further compounded by the presence of artefacts in ultrasound images. Herein we provide a state-of-art solution for detecting the fetal heart and classifying each individual frame as belonging to one of the standard viewing planes using fully convolutional neural networks (FCNs). Our FCN model achieves a classification error rate of 23.48% on real-world clinical ultrasound data. We also present comparative performance for analysis of different FCN architectures.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950609","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950609","Fetal ultrasound images;deep learning;fetal echocardiography;fully convolutional neural networks","Echocardiography;Fetal heart;Neural networks;Standards;Training;Ultrasonic imaging","echocardiography;image classification;medical image processing;neural nets","FCN model;classification error rate;congenital heart diseases;fetal echocardiography screening images;fetal heart;fully convolutional neural networks;real-world clinical ultrasound data;ultrasound images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Nuclei segmentation in histopathology images using deep neural networks","P. Naylor; M. Laé; F. Reyal; T. Walter","MINES ParisTech, PSL Research University, CBIO - Centre de Bioinformatique, 77300 Fontainebleau, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","933","936","Analysis and interpretation of stained tumor sections is one of the main tools in cancer diagnosis and prognosis, which is mainly carried out manually by pathologists. The avent of digital pathology provides us with the challenging opportunity to automatically analyze large amounts of these complex image data in order to draw biological conclusions from them and to study cellular and tissular phenotypes at a large scale. One of the bottlenecks for such approaches is the automatic segmentation of cell nuclei from this type of image data. Here, we present a fully automated workflow to segment nuclei from histopathology image data by using deep neural networks trained from a set of manually annotated images and by processing the posterior probability maps in order to split jointly segmented nuclei. Further, we provide the image data set that has been generated for this study as a benchmark set to the scientific community.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950669","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950669","Breast Cancer;Cellular Phenotyping;Convolutional Neural Networks;Deep Learning;Digital Pathology;Histopathology;Nuclei Segmentation","Cancer;Computer architecture;Image segmentation;Machine learning;Microprocessors;Neural networks;Semantics","biomedical optical imaging;cancer;cellular biophysics;image segmentation;medical image processing;neural nets;probability;tumours","automatic segmentation;cancer diagnosis;cancer prognosis;cell nuclei;cellular phenotypes;complex image data;deep neural networks;digital pathology;fully automated workflow;histopathology image data;image data set;manually annotated images;nuclei segmentation;pathologists;posterior probability maps;scientific community;split jointly segmented nuclei;stained tumor sections;tissular phenotypes","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Biopsy-guided learning with deep convolutional neural networks for Prostate Cancer detection on multiparametric MRI","Y. Tsehay; N. Lay; X. Wang; J. T. Kwak; B. Turkbey; P. Choyke; P. Pinto; B. Wood; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Department of Radiology and Imaging Science, National Institute of Health, Clinical Center, Bethesda, MD 20892, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","642","645","Prostate Cancer (PCa) is highly prevalent and is the second most common cause of cancer-related deaths in men. Multiparametric MRI (mpMRI) is robust in detecting PCa. We developed a weakly supervised computer-aided detection (CAD) system that uses biopsy points to learn to identify PCa on mpMRI. Our CAD system, which is based on a deep convolutional neural network architecture, yielded an area under the curve (AUC) of 0.903±0.009 on a receiver operation characteristic (ROC) curve computed on 10 different models in a 10 fold cross-validation. 9 of the 10 ROCs were statistically significantly different from a competing support vector machine based CAD, which yielded a 0.86 AUC when tested on the same dataset (α = 0.05). Furthermore, our CAD system proved to be more robust in detecting high-grade transition zone lesions.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950602","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950602","Biopsy Database;Computer-Aided Detection;Holistically-nested Edge Detection;Prostate;Prostate-CAD;Radiology","Biopsy;Databases;Lesions;Principal component analysis;Solid modeling;Training","biomedical MRI;cancer;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis;support vector machines","biopsy-guided learning;computer-aided detection;deep convolutional neural network;multiparametric MRI;prostate cancer detection;receiver operation characteristic curve;support vector machine","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images","B. Ehteshami Bejnordi; J. Lin; B. Glass; M. Mullooly; G. L. Gierach; M. E. Sherman; N. Karssemeijer; J. van der Laak; A. H. Beck","Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, Netherlands","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","929","932","Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950668","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950668","Breast Cancer;Convolutional Neural Networks;Digital pathology;Tumor Associated Stroma","Breast cancer;Feature extraction;Training;Tumors","biomedical optical imaging;cancer;cellular biophysics;learning (artificial intelligence);mammography;medical image processing;patient diagnosis;tumours","breast cancer diagnosis;breast carcinomas diagnosis;breast tissue biopsies;convolutional neural networks;deep learning-based assessment;epithelial cells;epithelial regions;epithelial tissue architecture;hematoxyli-and-eosin stained breast specimens;histopathology images;tumor-associated stroma","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Automatic 3D ultrasound segmentation of the first trimester placenta using deep learning","P. Looney; G. N. Stevenson; K. H. Nicolaides; W. Plasencia; M. Molloholli; S. Natsis; S. L. Collins","Nuffield Department of Obstetrics and Gynaecology, University of Oxford, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","279","282","Placental volume measured with 3D ultrasound in the first trimester has been shown to be correlated to adverse pregnancy outcomes. This could potentially be used as a screening test to predict the “at risk” pregnancy. However, manual segmentation whilst previously shown to be accurate and repeatable is very time consuming and semi-automated methods still require operator input. To generate a screening tool, fully automated placental segmentation is required. In this work, a deep convolutional neural network (cNN), DeepMedic, was trained using the output of the semi-automated Random Walker method as ground truth. 300 3D ultrasound scans of first trimester placentas were used to train, validate and test the cNN. Compared against the semi-automated segmentation, resultant median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Dice Similarity Coefficient was 0.73 (0.66, 0.76). The median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Hausdorff distance was 27 mm (18 mm, 36 mm). We present the first attempt at using a deep cNN for segmentation of 3D ultrasound of the placenta. This work shows that feasible results compared to ground truth were obtained that could form the basis of a fully automatic segmentation method.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950519","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950519","3D;automatic segmentation;deep learning;neural network;placenta;random walker;ultrasound","Biological neural networks;Image segmentation;Magnetic resonance imaging;Pregnancy;Three-dimensional displays;Ultrasonic imaging","biomedical ultrasonics;image segmentation;learning (artificial intelligence);medical image processing;neural nets;obstetrics","DeepMedic;automatic 3D ultrasound segmentation;deep convolutional neural network;deep learning;first trimester placenta","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Combining fully convolutional networks and graph-based approach for automated segmentation of cervical cell nuclei","L. Zhang; M. Sonka; L. Lu; R. M. Summers; J. Yao","Radiology and Imaging Sciences Department, National Institutes of Health (NIH), Bethesda MD, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","406","409","Cervical nuclei carry substantial diagnostic information for cervical cancer. Therefore, in automation-assisted reading of cervical cytology, automated and accurate segmentation of nuclei is essential. This paper proposes a novel approach for segmentation of cervical nuclei that combines fully convolutional networks (FCN) and graph-based approach (FCNG). FCN is trained to learn the nucleus high-level features to generate a nucleus label mask and a nucleus probabilistic map. The mask is used to construct a graph by image transforming. The map is formulated into the graph cost function in addition to the properties of the nucleus border and nucleus region. The prior constraints regarding the context of nucleus-cytoplasm position are also utilized to modify the local cost functions. The globally optimal path in the constructed graph is identified by dynamic programming. Validation of our method was performed on cell nuclei from Herlev Pap smear dataset. Our method shows a Zijdenbos similarity index (ZSI) of 0.92 ± 0.09, compared to the best state-of-the-art approach of 0.89 ± 0.15. The nucleus areas measured by our method correlated strongly with the independent standard (r<sup>2</sup> = 0.91).","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950548","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950548","Deep learning;FCN;Pap smear;graph-based segmentation","Computer architecture;Context;Cost function;Image segmentation;Imaging;Microprocessors;Shape","cancer;cellular biophysics;dynamic programming;image segmentation;medical image processing","Herlev Pap smear dataset;Zijdenbos similarity index;automated segmentation;automation-assisted reading;cervical cancer;cervical cell nuclei;cervical cytology;diagnostic information;dynamic programming;fully convolutional networks;graph cost function;graph-based approach;image transforming;nucleus high-level features;nucleus label mask;nucleus probabilistic map;nucleus-cytoplasm position","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Multi-stage segmentation of the fovea in retinal fundus images using fully Convolutional Neural Networks","S. Sedai; R. Tennakoon; P. Roy; K. Cao; R. Garnavi","IBM Research - Australia, Melbourne, VIC, Australia","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1083","1086","The fovea is one of the most important anatomical landmarks in the eye and its localization is required in automated analysis of retinal diseases due to its role in sharp central vision. In this paper, we propose a two-stage deep learning framework for accurate segmentation of the fovea in retinal colour fundus images. In the first stage, coarse segmentation is performed to localize the fovea in the fundus image. The location information from the first stage is then used to perform fine-grained segmentation of the fovea region in the second stage. The proposed method performs end-to-end pixelwise segmentation by creating a deep learning model based on fully convolutional neural networks, which does not require the prior knowledge of the location of other retinal structures such as optic disc (OD) and vasculature geometry. We demonstrate the effectiveness of our method on a dataset with 400 retinal images with average localization error of 14 ± 7 pixels.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950704","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950704","Convolution neural network;Fovea Segmentation;Retinal Imaging","Convolution;Image segmentation;Machine learning;Neural networks;Optical imaging;Retina;Training","biomedical optical imaging;eye;image segmentation;medical image processing;neural nets","end-to-end pixelwise segmentation;fovea;fully convolutional neural networks;multistage segmentation;optic disc;retinal colour fundus images;retinal structures;two-stage deep learning framework;vasculature geometry","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Wide residual networks for mitosis detection","E. Zerhouni; D. Lányi; M. Viana; M. Gabrani","IBM Research Zurich, S&#x00E4;umerstrasse 4, 8803 R&#x00FC;schlikon, Switzerland","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","924","928","One of the most important prognostic markers to assess proliferation activity of breast tumors is estimating the number of mitotic figures in H&E stained tissue. We propose the use of a recently published convolutional neural network architecture, Wide Residual Networks, for mitosis detection in breast histology images. The model is trained to classify each pixel of on an image using as context a patch centered on the pixel. We apply post-processing on the network output in order to filter out noise and select true mitosis. Finally, we combine the output of several networks using majority vote. Our approach ranked 2nd in the MICCAI TUPAC 2016 competition for mitosis detection, outperforming most other contestants by a significant margin.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950667","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950667","Mitotic activity;convolutional network;deep learning;tumor proliferation;wide-residual network","Image processing;Machine learning;Neural networks;Pathology;Shape;Testing;Training","cellular biophysics;diagnostic radiography;image classification;image filtering;mammography;medical image processing;neural nets;tumours","breast histology images;breast tumors;convolutional neural network architecture;hematoxylin-and-eosin-stained-tissue;image classification;image filtering;mitosis detection;mitotic figures;proliferation activity;wide residual networks","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Toward automatic diagnosis of hip dysplasia from 2D ultrasound","A. R. Hareendranathan; D. Zonoobi; M. Mabee; D. Cobzas; K. Punithakumar; M. Noga; J. L. Jaremko","Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Canada","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","982","985","Developmental dysplasia of the hip (DDH) is a congenital deformity occurring in ~3% of infants. If diagnosed early most cases of DDH can be effectively treated using a Pavlik harness. However, current diagnosis of DDH using 2D ultrasound is and can have high inter-operator variability. In this paper we propose a method to automatically segment the acetabulum bone and derive geometric indices of hip dysplasia from this model. In the proposed method, using multi-scale superpixels, we incorporate global and local image features into a Deep Learning framework to obtain a probability map of the bone to be segmented and then use this map in probabilistic graph search to guide the segmentation. Clinically relevant geometric measures of hip dysplasia, including a new index of acetabular rounding, are then automatically calculated from the segmented acetabulum contour. We tested this method on 2D ultrasound of 50 infant hips and the contours generated matched closely with manual segmentations at root mean square error 1.8±0.7 mm and Hausdorff distance 2.1±0.9 mm. In this pilot data, the measured indices of dysplasia give an area under the curve of 86.2% for classifying normal vs dysplastic hips. The proposed approach could be used clinically for accurate and automatic diagnosis of hip dysplasia in infants.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950680","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950680","deep learning;hip dysplasia;segmentation;superpixels;ultrasound","Bones;Feature extraction;Hip;Image segmentation;Machine learning;Pediatrics;Ultrasonic imaging","biomedical ultrasonics;bone;diseases;image classification;image segmentation;learning (artificial intelligence);medical image processing;paediatrics","2D ultrasound;DDH diagnosis;Pavlik harness;acetabulum bone;acetabulum contour;automatic hip dysplasia diagnosis;congenital deformity;deep learning framework;developmental hip dysplasia;image feature;manual segmentation;multiscale superpixel","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Epithelium-stroma classification in histopathological images via convolutional neural networks and self-taught learning","Y. Huang; H. Zheng; C. Liu; G. Rohde; D. Zeng; J. Wang; X. Ding","School of Information Science and Engineering, Xiamen University, 361005, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","1073","1077","Epithelium-stroma classification is always considered as an important preprocessing step for morphological quantitative analysis in image-based histological researches of oncologic diseases. However, large-scale accurate ground-truth labeling is expensive in histopathological image analysis, thus the classification performances will still be limited with the insufficient labeled training samples. Considering that acquisition of public unlabeled histopathological images is much cheaper, an epithelium-stroma classification framework is developed, based on the deep convolutional neural network framework and the strategies of self-taught learning. The method has the ability of taking advantage of large-scale unlabeled public histopathological data as auxiliary data, and then transferring the knowledge to enhance the performances in epithelium-stroma classification with limited labeled training data. The experiments demonstrate that the proposed method outperforms traditional CNNs when the labeled training data size is decreasing dramatically.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952321","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952321","convolutional neural networks;epithelium-stroma classification;histopathological image analysis;self-taught learning;transfer learning","Dictionaries;Feature extraction;Kernel;Neural networks;Testing;Training;Training data","diseases;image classification;medical image processing;neural nets;tumours","auxiliary data;convolutional neural networks;deep convolutional neural network framework;epithelium-stroma classification framework;histopathological image analysis;image-based histological researches;labeled training data size;large-scale accurate ground-truth labeling;large-scale unlabeled public histopathological data;morphological quantitative analysis;oncologic diseases;public unlabeled histopathological images;self-taught learning","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Deep residual learning for compressed sensing MRI","D. Lee; J. Yoo; J. C. Ye","Bio Imaging and Signal Processing Lab., Dep. of Bio and Brain Engineering, KAIST, South Korea","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","15","18","Compressed sensing (CS) enables significant reduction of MR acquisition time with performance guarantee. However, computational complexity of CS is usually expensive. To address this, here we propose a novel deep residual learning algorithm to reconstruct MR images from sparsely sampled k-space data. In particular, based on the observation that coherent aliasing artifacts from downsampled data has topologically simpler structure than the original image data, we formulate a CS problem as a residual regression problem and propose a deep convolutional neural network (CNN) to learn the aliasing artifacts. Experimental results using single channel and multi channel MR data demonstrate that the proposed deep residual learning outperforms the existing CS and parallel imaging algorithms. Moreover, the computational time is faster in several orders of magnitude.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950457","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950457","CNN;Compressed sensing MRI;deep learning;residual learning","Complexity theory;Image reconstruction;Machine learning;Magnetic resonance imaging;Manifolds;Topology","biomedical MRI;compressed sensing;data acquisition;image reconstruction;learning (artificial intelligence);medical image processing;neural nets;regression analysis","MR acquisition time;MR image reconstruction;compressed sensing MRI;deep convolutional neural network;deep residual learning algorithm;residual regression problem;sparsely sampled k-space data","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Disease grading of heterogeneous tissue using convolutional autoencoder","E. Zerhouni; B. Prisacari; Q. Zhong; P. Wild; M. Gabrani","IBM Research-Z&#x00FC;rich, Saeumerstrasse 4, 8803 Rueschlikon, Switzerland","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","596","599","One of the main challenges of histological image analysis is the high dimensionality of the images. This can be addressed via summarizing techniques or feature engineering. However, such approaches can limit the performance of subsequent machine learning models, particularly when dealing with highly heterogeneous tissue samples. One possible alternative is to employ unsupervised learning to determine the most relevant features automatically. In this paper, we propose a method of generating representative image signatures that are robust to tissue heterogeneity. At the core of our approach lies a novel deep-learning based mechanism to simultaneously produce representative image features as well as perform dictionary learning to further reduce dimensionality. By integrating this mechanism in a broader framework for disease grading, we show significant improvement in terms of grading accuracy compared to alternative local feature extraction methods.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950591","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950591","Convolutional Autoencoder;Dimensionality Reduction;Image Description;Tissue Heterogeneity","Dictionaries;Diseases;Feature extraction;Image color analysis;Image reconstruction;Morphology;Training","biological tissues;convolutional codes;diseases;feature extraction;image coding;learning (artificial intelligence);medical image processing","convolutional autoencoder;deep-learning;dictionary learning;disease grading;feature engineering;feature extraction;heterogeneous tissue;histological image analysis;image features;machine learning;tissue heterogeneity;unsupervised learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep learning based multi-label classification for surgical tool presence detection in laparoscopic videos","S. Wang; A. Raju; J. Huang","The University of Texas at Arlington, Dept. of Computer Science and Engineering, USA","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","620","623","Automatic recognition of surgical workflow is an unresolved problem among the community of computer-assisted interventions. Among all the features used for surgical workflow recognition, one important feature is the presence of the surgical tools. Extracting this feature leads to the surgical tool presence detection problem to detect what tools are used at each time in surgery. This paper proposes a deep learning based multi-label classification method for surgical tool presence detection in laparoscopic videos. The proposed method combines two state-of-the-art deep neural networks and uses ensemble learning to solve the tool presence detection problem as a multi-label classification problem. The performance of the proposed method has been evaluated in the surgical tool presence detection challenge held by Modeling and Monitoring of Computer Assisted Interventions workshop. The proposed method shows superior performance compared to other methods and has won the first place of the challenge.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950597","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950597","Deep learning;Ensemble;Multi-label classification;Surgical tool detection","Machine learning;Neural networks;Surgery;Testing;Tools;Training;Videos","image classification;learning (artificial intelligence);medical image processing;neural nets;surgery","automatic surgical workflow recognition;computer-assisted intervention;deep learning-based multilabel classification;deep neural networks;ensemble learning;laparoscopic video;surgical tool presence detection","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Feature selection and thyroid nodule classification using transfer learning","T. Liu; S. Xie; Y. Zhang; J. Yu; L. Niu; W. Sun","Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1096","1099","Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful and specific features to the classification. A CNN model trained with ImageNet data is transferred to the ultrasound image domain, to generate semantic deep features under small sample condition. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Scale Invariant Feature Transform (SIFT) together to form a hybrid feature space. Furthermore, to make the general deep features more pertinent to our problem, a feature subset selection process is employed for the hybrid nodule classification, followed by a detailed discussion on the influence of feature number and feature composition method. Experimental results on 1037 images show that the accuracy of our proposed method is 0.929, which outperforms other relative methods by over 10%.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950707","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950707","feature subset selection;thyroid nodules classification;transfer learning;ultrasound image","Biomedical imaging;Cancer;Convolution;Feature extraction;Indexes;Training;Ultrasonic imaging","biomedical ultrasonics;feature extraction;feature selection;image classification;learning (artificial intelligence);medical image processing;neural nets;programming language semantics","ImageNet data;benign nodules;convolution neural networks;diagnosis accuracy;feature extraction method;feature selection;histogram-of-oriented-gradient;malignant nodules;scale-invariant-feature-transform;semantic deep features;thyroid nodule classification;transfer learning;ultrasonography;ultrasound images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Low-dose CT denoising with convolutional neural network","H. Chen; Y. Zhang; W. Zhang; P. Liao; K. Li; J. Zhou; G. Wang","College of Computer Science, Sichuan University, Chengdu 610065, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","143","146","To reduce the potential radiation risk, low-dose CT has attracted much attention. However, simply lowering the radiation dose will lead to significant deterioration of the image quality. In this paper, we propose a noise reduction method for low-dose CT via deep neural network without accessing original projection data. A deep convolutional neural network is trained to transform low-dose CT images towards normal-dose CT images, patch by patch. Visual and quantitative evaluation demonstrates a competing performance of the proposed method.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950488","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950488","Low-dose CT;convolutional neural network;deep learning;noise reduction","Computed tomography;Dictionaries;Filtering;Image reconstruction;Neural networks;Noise reduction;Training","computerised tomography;image denoising;medical image processing;neural nets","deep convolutional neural network;image quality;low-dose CT denoising;low-dose CT images;noise reduction method;normal-dose CT images;original projection data;quantitative evaluation;radiation dose;visual evaluation","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Classification of breast lesions using cross-modal deep learning","O. Hadad; R. Bakalo; R. Ben-Ari; S. Hashoul; G. Amit","IBM Research, Haifa, Israel","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","109","112","Automatic detection and classification of lesions in medical images is a desirable goal, with numerous clinical applications. In breast imaging, multiple modalities such as X-ray, ultrasound and MRI are often used in the diagnostic workflow. Training robust classifiers for each modality is challenging due to the typically small size of the available datasets. We propose to use cross-modal transfer learning to improve the robustness of the classifiers. We demonstrate the potential of this approach on a problem of identifying masses in breast MRI images, using a network that was trained on mammography images. Comparison between cross-modal and cross-domain transfer learning showed that the former improved the classification performance, with overall accuracy of 0.93 versus 0.90, while the accuracy of de-novo training was 0.94. Using transfer learning within the medical imaging domain may help to produce standard pre-trained shared models, which can be utilized to solve a variety of specific clinical problems.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950480","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950480","breast imaging;computer-aided diagnosis;deep learning;multimodal analysis;transfer learning","Biomedical imaging;Breast;Data models;Lesions;Magnetic resonance imaging;Training","biomedical MRI;image classification;learning (artificial intelligence);mammography;medical image processing","breast MRI images;breast imaging;breast lesion classification;cross-domain transfer learning;cross-modal deep learning;mammography images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"A fully automatic deep learning method for atrial scarring segmentation from late gadolinium-enhanced MRI images","G. Yang; X. Zhuang; H. Khan; S. Haldar; E. Nyktari; X. Ye; G. Slabaugh; T. Wong; R. Mohiaddin; J. Keegan; D. Firmin","Cardiovascular Biomedical Research Unit, Royal Brompton Hospital, SW3 6NP, London, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","844","848","Precise and objective segmentation of atrial scarring (SAS) is a prerequisite for quantitative assessment of atrial fibrillation using non-invasive late gadolinium-enhanced (LGE) MRI. This also requires accurate delineation of the left atrium (LA) and pulmonary veins (PVs) geometry. Most previous studies have relied on manual segmentation of LA wall and PVs, which is a tedious and error-prone procedure with limited reproducibility. There are many attempts on automatic SAS using simple thresholding, histogram analysis, clustering and graph-cut based approaches; however, in general, these methods are considered as unsupervised learning thus subject to limited segmentation accuracy. In this study, we present a fully-automated multi-atlas based whole heart segmentation method to derive the LA and PVs geometry objectively that is followed by a fully automatic deep learning method for SAS. Our deep learning method consists of a feature extraction step via super-pixel over-segmentation and a supervised classification step via stacked sparse auto-encoders. We demonstrate the efficacy of our method on 20 clinical LGE MRI scans acquired from a longstanding persistent atrial fibrillation cohort. Both quantitative and qualitative results show that our fully automatic method obtained accurate segmentation results compared to the manual segmentation based ground truths.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950649","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950649","","Feature extraction;Geometry;Image segmentation;Magnetic resonance imaging;Manuals;Myocardium;Synthetic aperture sonar","biomedical MRI;blood vessels;cardiology;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing","atrial fibrillation;atrial scarring segmentation;clustering approach;feature extraction step;fully automatic deep learning method;fully-automated multiatlas based whole heart segmentation method;graph-cut based approach;histogram analysis;late gadolinium-enhanced MRI images;left atrium;pulmonary veins;simple thresholding;stacked sparse autoencoders;super-pixel over-segmentation;supervised classification step;unsupervised learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Domain specific convolutional neural nets for detection of architectural distortion in mammograms","R. Ben-Ari; A. Akselrod-Ballin; L. Karlinsky; S. Hashoul","IBM Research - Haifa, Israel","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","552","556","Detection of Architectural distortion (AD) is important for ruling out possible pre-malignant lesions in breast, but due to its subtlety, it is often missed on the screening mammograms. In this work we suggest a novel AD detection method based on region proposal convolution neural nets (R-CNN). When the data is scarce, as typically the case in medical domain, R-CNN yields poor results. In this study, we suggest a new R-CNN method addressing this shortcoming by using a pretrained network on a candidate region guided by clinical observations. We test our method on the publicly available DDSM data set, with comparison to the latest faster R-CNN and previous works. Our detection accuracy allows binary image classification (normal vs. containing AD) with over 80% sensitivity and specificity, and yields 0.46 false-positives per image at 83% true-positive rate, for localization accuracy. These measures significantly improve the best results in the literature.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950581","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950581","Architectural Distortion;Breast Mammography;Computer-Aided Diagnosis;Convolution Neural Net;Deep Learning;Region Proposal","Breast;Convolution;Mammography;Neural networks;Proposals;Sensitivity;Training","cancer;image classification;mammography;medical image processing;neural nets","AD detection method;R-CNN method;architectural distortion detection;image classification;mammograms;region proposal convolution neural nets","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Lung nodule segmentation using deep learned prior based graph cut","S. Mukherjee; X. Huang; R. R. Bhagalia","GE Global Research, Bangalore, India","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1205","1208","We propose an automated framework for lung nodule segmentation from pulmonary CT scan using graph cut with a deep learned prior. The segmentation problem is formulated as a hybrid cost function minimization task, which combines a domain specific data term with a deep learned probability map. The proposed segmentation framework embodies the robustness of deep learning in object localization, while retaining the hallmark of traditional segmentation models in addressing the morphological intricacies of elaborate objects. The proposed solution offers more than 20% performance improvement over a contemporary data driven model, and also outperforms traditional graph cuts especially in situations where model initialization is slightly inaccurate.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950733","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950733","CNN;CT;graph cuts;segmentation","Cost function;Image segmentation;Lungs;Neural networks;Robustness;Solids;Two dimensional displays","computerised tomography;image segmentation;learning (artificial intelligence);lung;medical image processing;physiological models;probability","contemporary data driven model;deep learned prior based graph cut;deep learned probability map;lung nodule segmentation;pulmonary CT scan","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Handcrafted features vs ConvNets in 2D echocardiographic images","C. Raynaud; H. Langet; M. S. Amzulescu; E. Saloux; H. Bertrand; P. Allain; P. Piro","Philips Research Medisys, Paris, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1116","1119","In this paper, we address the problem of automated pose classification and segmentation of the left ventricle (LV) in 2D echocardiographic images. For this purpose, we compare two complementary approaches. The first one is based on engineering ad-hoc features according to the traditional machine learning paradigm. Namely, we extract phase features to build an unsupervised LV pose estimator, as well as a global image descriptor for view type classification. We also apply the Supervised Descent Method (SDM) to iteratively refine the LV contour. The second approach follows the deep learning framework, where a Convolutional Network (ConvNet) learns the visual features automatically. Our experiments on a large database of apical sequences show that the two approaches yield comparable results on view classification, but SDM outperforms ConvNet on LV segmentation at a significantly lower training computational cost.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950712","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950712","2D ultrasound;ConvNets;Supervised Descent Method;left ventricle segmentation;phase features","Databases;Image segmentation;Pose estimation;Robustness;Shape;Training;Two dimensional displays","echocardiography;feature extraction;image classification;image segmentation;image sequences;iterative methods;learning (artificial intelligence);medical image processing","2D echocardiographic images;ConvNet;LV contour;LV segmentation;SDM;ad-hoc features;apical sequences;automated pose classification;convolutional network;deep learning framework;handcrafted features;image classification;iterative method;left ventricle segmentation;machine learning paradigm;phase feature extraction;supervised descent method;unsupervised LV pose estimator;visual features","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Parcellation of visual cortex on high-resolution histological brain sections using convolutional neural networks","H. Spitzer; K. Amunts; S. Harmeling; T. Dickscheid","Institute of Neuroscience and Medicine (INM-1), Forschungszentrum J&#x00FC;lich, Germany","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","920","923","Microscopic analysis of histological sections is considered the “gold standard” to verify structural parcellations in the human brain. Its high resolution allows the study of laminar and columnar patterns of cell distributions, which build an important basis for the simulation of cortical areas and networks. However, such cytoarchitectonic mapping is a semiautomatic, time consuming process that does not scale with high throughput imaging. We present an automatic approach for parcellating histological sections at 2μm resolution. It is based on a convolutional neural network that combines topological information from probabilistic atlases with the texture features learned from high-resolution cell-body stained images. The model is applied to visual areas and trained on a sparse set of partial annotations. We show how predictions are transferable to new brains and spatially consistent across sections.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950666","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950666","Brain Parcellation;Convolutional Networks;Deep Learning;Human Brain;Mapping","Brain modeling;Data models;Image resolution;Image segmentation;Probabilistic logic;Training;Visualization","biomedical optical imaging;brain;cellular biophysics;image texture;medical image processing;neural nets;probability","cell distributions;columnar patterns;convolutional neural networks;cortical areas;cortical networks;cytoarchitectonic mapping;gold standard;high throughput imaging;high-resolution cell-body stained images;high-resolution histological brain sections;histological sections;human brain;laminar patterns;microscopic analysis;partial annotations;probabilistic atlases;structural parcellations;texture features;topological information;visual areas;visual cortex parcellation","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Automated assessment of endometrium from transvaginal ultrasound using Deep Learned Snake","N. Singhal; S. Mukherjee; C. Perrey","GE Global Research, Bangalore, India","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","283","286","Endometrium assessment via thickness measurement is commonly performed in routine gynecological ultrasound examination for assessing the reproductive health of patients undergoing fertility related treatments and endometrium cancer screening in women with post-menopausal bleeding. This paper introduces a fully automated technique for endometrium thickness measurement from three-dimensional transvaginal ultrasound (TVUS) images. The algorithm combines the robustness of deep neural networks with the more interpretable level set method for segmentation. We propose a hybrid variational curve propagation model which embeds a deep-learned endometrium probability map in the segmentation energy functional. This solution provides approximately 30% performance improvement over a contemporary supervised learning method on a database of 59 TVUS images and the thickness measurement is found to be within ±2mm of the manual measurement in 87% of the cases.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950520","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950520","Endometrium;deep learning;level set;segmentation;ultrasound;uterus","Image segmentation;Level set;Shape;Thickness measurement;Three-dimensional displays;Training;Ultrasonic imaging","biomedical measurement;biomedical ultrasonics;cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines;thickness measurement","3D TVUS images;3D transvaginal ultrasound images;automated endometrium assessment;deep learned snake;deep neural networks;deep-learned endometrium probability map;endometrium cancer screening;endometrium thickness measurement;gynecological ultrasound examination;image segmentation;post-menopausal bleeding;supervised learning method;variational curve propagation model","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"A generalized MRI-based CAD system for functional assessment of renal transplant","F. Khalifa; M. Shehata; A. Soliman; M. Abou El-Ghar; T. El-Diasty; A. C. Dwyer; M. El-Melegy; G. Gimel'farb; R. Keynton; A. El-Baz","Bioengineering Department, University of Louisville, KY, USA","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","758","761","In recent years, magnetic resonance imaging (MRI) has been explored for non-invasive assessment of renal transplant function. This paper proposes a computer-aided diagnostic (CAD) system for the assessment of renal transplant status, which integrates both clinical and MRI-derived biomarkers. The latter are derived from either 3D (2D + time) dynamic contrast-enhanced MRI or 4D (3D + b-value) diffusion-weighted (DW) MRI. In order to extract the MRI-based biomarkers, our framework performs multiple image processing steps, including MRI data alignment to handle the motion effects, kidney segmentation using a geometric deformable model, local motion correction, and estimation of image-based biomarkers. These biomarkers are fused with clinical biomarkers (creatinine clearance and serum plasma creatinine) for the classification of transplant status using a machine learning classifier. Our CAD system has been tested on a cohort of 100 subjects (50 DCE-MRI and 50 DW-MRI) using a “leave-one-subject-out” approach and distinguished rejection from non-rejection transplants with an overall accuracy of 98% for both DCE-MRI and DW-MRI data sets. These preliminary results demonstrate the promise of the proposed CAD system as a reliable non-invasive diagnostic tool for renal transplant assessment.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950629","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950629","CAD;Deep Learning;MRI;Renal rejection","Biomarkers;Image segmentation;Kidney;Machine learning;Magnetic resonance imaging;Motion segmentation;Three-dimensional displays","biomedical MRI;image classification;learning (artificial intelligence);medical image processing","MRI-based CAD system;MRI-based biomarkers;computer-aided diagnostic system;machine learning classifier;magnetic resonance imaging;renal transplant assessment;renal transplant function","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Automated 5-year mortality prediction using deep learning and radiomics features from chest computed tomography","G. Carneiro; L. Oakden-Rayner; A. P. Bradley; J. Nascimento; L. Palmer","Australian Centre for Visual Technologies, The University of Adelaide, Australia","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","130","134","In this paper, we propose new prognostic methods that predict 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifier that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a unified framework based on two state-of-the-art deep learning models extended to 3-D inputs, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection and extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning models produces a mean 5-year mortality prediction AUC in [68.8%,69.8%] and accuracy in [64.5%,66.5%], while radiomics produces a mean AUC of 64.6% and accuracy of 64.6%. The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950485","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950485","computed tomography;deep learning;feature learning;five-year mortality;hand-designed features;radiomics","Biomedical imaging;Computed tomography;Fats;Feature extraction;Image segmentation;Machine learning;Training","computerised tomography;feature extraction;geriatrics;health care;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation","3-D inputs;CT image;annotated chest CT;automated 5-year mortality prediction;chest computed tomography;classifier learning process;deep learning models;elderly individuals;feature extraction;hand-crafted radiomics features;mean 5-year mortality prediction AUC;multiple anatomic structures;multistage framework;personalised healthcare;preventive healthcare;prognostic methods;segmentation maps;single optimisation process;unified framework","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"The importance of stain normalization in colorectal tissue classification with convolutional networks","F. Ciompi; O. Geessink; B. E. Bejnordi; G. S. de Souza; A. Baidoshvili; G. Litjens; B. van Ginneken; I. Nagtegaal; J. van der Laak","Dept. of Pathology, Radboud University Medical Center, Nijmegen, Netherlands","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","160","163","The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950492","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950492","Colorectal Cancer;Deep learning;Digital pathology","Algorithm design and analysis;Biomarkers;Blood;Cancer;Image color analysis;Training;Tumors","biological tissues;cancer;image classification;medical image processing;neural nets","ConvNets;colorectal tissue classification;convolutional networks;hematoxylin-eosin stained histopathology images;imaging biomarkers;stain normalization","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Automatic detection of motion artifacts in MR images using CNNS","K. Meding; A. Loktyushin; M. Hirsch","Max Planck Institute for Intelligent Systems, Department of Empirical Inference, Germany","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","811","815","Considerable practical interest exists in being able to automatically determine whether a recorded magnetic resonance image is affected by motion artifacts caused by patient movements during scanning. Existing approaches usually rely on the use of navigators or external sensors to detect and track patient motion during image acquisition. In this work, we present an algorithm based on convolutional neural networks that enables fully automated detection of motion artifacts in MR scans without special hardware requirements. The approach is data driven and uses the magnitude of MR images in the spatial domain as input. We evaluate the performance of our algorithm on both synthetic and real data and observe adequate performance in terms of accuracy and generalization to different types of data. Our proposed approach could potentially be used in clinical practice to tag an MR image as motion-free or motion-corrupted immediately after a scan is finished. This process would facilitate the acquisition of high-quality MR images that are often indispensable for accurate medical diagnosis.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952268","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952268","Convolutional Neural Networks;Deep Learning;MRI;Motion Artifacts;Quality Assessment","Brain;Kernel;Magnetic resonance imaging;Motion artifacts;Testing;Three-dimensional displays;Training","biomedical MRI;convolution;feature extraction;image motion analysis;neural nets;patient diagnosis","CNNS;MR images;MR scans;automatic motion artifacts detection;clinical practice;convolutional neural networks;data accuracy;data driven;data generalization;image acquisition;magnetic resonance image;medical diagnosis;motion-corrupted MR image;motion-free MR image;patient motion;patient movements","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Skin melanoma segmentation using recurrent and convolutional neural networks","M. Attia; M. Hossny; S. Nahavandi; A. Yazdabadi","Institute for Intelligent Systems Research and Innovation, Deakin University, Australia","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","292","296","Skin melanoma is one of the highly addressed health problems in many countries. Dermatologists diagnose melanoma by visual inspections of mole using clinical assessment tools such as ABCD. However, computer vision tools have been introduced to assist in quantitative analysis of skin lesions. Deep learning is one of the trending machine learning techniques that have been successfully utilized to solve many difficult computer vision tasks. We proposed using a hybrid method that utilizes two popular deep learning methods: convolutional and recurrent neural networks. The proposed method was trained using 900 images and tested on 375 images. Images were obtained from “Skin Lesion Analysis Toward Melanoma Detection” challenge which was hosted by ISBI 2016 conference. We achieved segmentation average accuracy of 0.98 and Jaccard index of 0.93. Results were compared with other state-of-the-art methods, including winner of ISBI 2016 challenge for skin melanoma segmentation, along with the same evaluation criteria.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950522","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950522","convolutional neural networks;deep learning;dermoscopy;melanoma;recurrent neural networks;segmentation;skin lesion","Feature extraction;Image segmentation;Lesions;Malignant tumors;Recurrent neural networks;Skin","cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;skin","convolutional neural networks;deep learning;recurrent neural networks;skin melanoma segmentation","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Realistic human action recognition: When CNNS meet LDS","L. Zhang; Y. Feng; X. Xiang; X. Zhen","College of Information and Communication Engineering, Harbin Engineering University, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","1622","1626","In this paper, we proposed new framework for human action representation, which leverages the strengths of convolutional neural networks (CNNs) and the linear dynamical system (LDS) to represent both spatial and temporal structures of actions in videos. We make two principal contributions: first, we incorporate image-trained CNNs to detect action clip concepts, which takes advantage of different levels of information by combining the two layers in CNNs trained from images; Second, we further propose adopting a linear dynamical system (LDS) to model the relationships between these clip concepts, which captures temporal structures of actions. We have applied the proposed method on two challenging realistic benchmark datasets, and our method achieves high performance up to 86.16% on the YouTube and 82.76% UCF50 datasets, which largely outperforms most of the state-of-the-art algorithms with more sophisticated techniques.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952431","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952431","Concept confidence;Deep learning;Image-trained CNNs;Linear dynamical system","Data mining;Feature extraction;Heuristic algorithms;Machine learning;Trajectory;Videos;YouTube","convolution;image recognition;image representation;learning (artificial intelligence);neural nets;principal component analysis;video signal processing","CNN;LDS;convolutional neural networks;deep learning;human action representation;linear dynamical system;realistic human action recognition","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Fast predictive multimodal image registration","X. Yang; R. Kwitt; M. Styner; M. Niethammer","Department of Computer Science, UNC Chapel Hill, USA","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","858","862","We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950652","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950652","deep learning;deformation prediction;multimodal image similarity","Bayes methods;Convolutional codes;Deformable models;Image registration;Optimization;Three-dimensional displays;Training","Bayes methods;biomedical MRI;biomedical measurement;brain;image registration;learning (artificial intelligence);medical image processing;neural nets;optimisation","3D brain MRI dataset;Bayesian probabilistic version;LDDMM registration model;T1-weighted images;T2-weighted images;deep encoder-decoder architecture;image deformation prediction;image registration formulations;image-patch-based deep network;large deformation diffeomorphic metric mapping;multimodal image registration;multimodal image similarity measurement;multimodal images;optimization-based LDDMM","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"3-D functional brain network classification using Convolutional Neural Networks","D. Ren; Y. Zhao; H. Chen; Q. Dong; J. Lv; T. Liu","College of Computer Science and Information Engineering, Tianjin University of Science and Technology, 300222, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1217","1221","Several recent studies have shown that dictionary learning and sparse representation can effectively reconstruct hundreds of interacting functional brain networks simultaneously from whole-brain fMRI data. However, accurate classification and recognition of those hundreds of functional networks from an individual or a population of many subjects is still a challenging and open problem due to the intrinsic variability of functional networks and other noise sources. To tackle this problem, this paper presents an effective deep learning framework to train convolutional neural networks from a large dataset of hundreds of thousands of available brain network volume maps, which was then applied on testing samples for network classification and recognition. We effectively applied computer-labeled data as training set so the whole process can be automated. Experimental results showed that the proposed method is quite robust in handling noisy patterns in the dataset, which suggests that our work offers a new computational framework for modeling functional connectomes from fMRI big data in the future.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950736","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950736","3D convolutional neural networks;classification;deep learning;functional brain networks","Biological neural networks;Machine learning;Noise measurement;Testing;Three-dimensional displays;Training;Visualization","Big Data;biomedical MRI;brain;image classification;image denoising;learning (artificial intelligence);neural nets;neurophysiology","3-D functional brain network classification;brain network volume maps;computational framework;computer-labeled data;convolutional neural networks;deep learning framework;dictionary learning;fMRI big data;functional connectomes;functional networks;intrinsic variability;network recognition;noise sources;noisy patterns;sparse representation;training set;whole-brain fMRI data","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Segmentation of Organs at Risk in thoracic CT images using a SharpMask architecture and Conditional Random Fields","R. Trullo; C. Petitjean; S. Ruan; B. Dubray; D. Nie; D. Shen","Normandie Univ, UNIROUEN, UNIHAVRE, INSA Rouen, LITIS, 76000, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1003","1006","Cancer is one of the leading causes of death worldwide. Radiotherapy is a standard treatment for this condition and the first step of the radiotherapy process is to identify the target volumes to be targeted and the healthy organs at risk (OAR) to be protected. Unlike previous methods for automatic segmentation of OAR that typically use local information and individually segment each OAR, in this paper, we propose a deep learning framework for the joint segmentation of OAR in CT images of the thorax, specifically the heart, esophagus, trachea and the aorta. Making use of Fully Convolutional Networks (FCN), we present several extensions that improve the performance, including a new architecture that allows to use low level features with high level information, effectively combining local and global information for improving the localization accuracy. Finally, by using Conditional Random Fields (specifically the CRF as Recurrent Neural Network model), we are able to account for relationships between the organs to further improve the segmentation results. Experiments demonstrate competitive performance on a dataset of 30 CT scans.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950685","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950685","CRF;CRFasRNN;CT Segmentation;Fully Convolutional Networks (FCN)","Cancer;Computed tomography;Computer architecture;Esophagus;Heart;Image segmentation;Semantics","biological organs;cancer;computerised tomography;image segmentation;learning (artificial intelligence);medical image processing;neural nets;radiation therapy","CT scan;aorta;automatic organs-at-risk segmentation;cancer;conditional random fields;deep learning framework;esophagus;fully convolutional networks;heart;radiotherapy process;recurrent neural network model;sharpmask architecture;thoracic CT image;thorax;trachea","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Globally optimal breast mass segmentation from DCE-MRI using deep semantic segmentation as shape prior","G. Maicas; G. Carneiro; A. P. Bradley","ACVT, School of Computer Science, The University of Adelaide, South Australia","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","305","309","We introduce a new fully automated breast mass segmentation method from dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). The method is based on globally optimal inference in a continuous space (GOCS) using a shape prior computed from a semantic segmentation produced by a deep learning (DL) model. We propose this approach because the limited amount of annotated training samples does not allow the implementation of a robust DL model that could produce accurate segmentation results on its own. Furthermore, GOCS does not need precise initialisation compared to locally optimal methods on a continuous space (e.g., Mumford-Shah based level set methods); also, GOCS has smaller memory complexity compared to globally optimal inference on a discrete space (e.g., graph cuts). Experimental results show that the proposed method produces the current state-of-the-art mass segmentation (from DCEMRI) results, achieving a mean Dice coefficient of 0.77 for the test set.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950525","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950525","breast MRI;breast cancer;breast mass segmentation;deep learning;energy-based segmentation;global optimization;shape prior","Breast;Image segmentation;Machine learning;Magnetic resonance imaging;Semantics;Shape;Training","biomedical MRI;image segmentation;learning (artificial intelligence);mammography;medical image processing","DCE-MRI;automated breast mass segmentation method;deep learning model;deep semantic segmentation;discrete space;dynamic contrast-enhanced magnetic resonance imaging;globally optimal breast mass segmentation;globally-optimal-inference-in-a-continuous-space;shape prior","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Age estimation from brain MRI images using deep learning","T. W. Huang; H. T. Chen; R. Fujimoto; K. Ito; K. Wu; K. Sato; Y. Taki; H. Fukuda; T. Aoki","Department of Computer Science, National Tsing-Hua University, Taiwan","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","849","852","Estimating human age from brain MR images is useful for early detection of Alzheimer's disease. In this paper we propose a fast and accurate method based on deep learning to predict subject's age. Compared with previous methods, our algorithm achieves comparable accuracy using fewer input images. With our GPU version program, the time needed to make a prediction is 20 ms. We evaluate our methods using mean absolute error (MAE) and our method is able to predict subject's age with MAE of 4.0 years.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950650","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950650","MRI;T1-weighted image;age estimation;brain-aging;deep learning","Biological neural networks;Graphics processing units;Kernel;Magnetic resonance imaging;Testing;Training","biomedical MRI;diseases;image recognition;learning (artificial intelligence)","Alzheimer's disease early detection;GPU version program;age estimation;brain MRI images;deep learning;mean absolute error","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Modeling medical texts for distributed representations based on Skip-Gram model","Z. Zhou; B. Fu; H. Qiu; Y. Zhang; X. Liu","Dept. Big Data Research Center, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, P. R. China","2017 3rd International Conference on Information Management (ICIM)","20170619","2017","","","279","283","Medical text is complex while its semantic expression is variable. Producing a vector for each medical word to encode its semantic information, as the cornerstone of medical semantic understanding, is quite different from the tasks of natural language processing. In this paper, we focus on the vectorization of Chinese medical text words. To recognize Chinese medical entities, a framework is constructed for Chinese word segmentations by jointly using a built medical dictionary and a trained hidden Markov model. Then, each medical text word is learned by Skip-Gram model into a 128 dimension vector for the distributed representation, and the robustness of the vectorization model is strengthened by negative sampling. Comparing with the one-hot method, the word vectorization method based on Skip-Gram model can improve the performance of the CNN classifier significantly due to the data sparsity and vector dimension decreased. The accuracy is also improved about 15% than that of the one-hot method.","","Electronic:978-1-5090-6306-2; POD:978-1-5090-6307-9; Paper:978-1-5090-6304-8; USB:978-1-5090-6305-5","10.1109/INFOMAN.2017.7950392","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950392","deep learning;negative sampling;neural network;word vector","Convolution;Dictionaries;Hidden Markov models;Medical diagnostic imaging;Neural networks;Semantics;Vocabulary","hidden Markov models;learning (artificial intelligence);medical information systems;sampling methods;text analysis","CNN classifier;Chinese medical entity recognition;Chinese medical text words;Skip-Gram model;built medical dictionary;distributed representation;hidden Markov model;medical text modeling;medical word;natural language processing;negative sampling;one-hot method;semantic expression;word vectorization method","","","","","","","","21-23 April 2017","","IEEE","IEEE Conference Publications"
"Detection of lumen and media-adventitia borders in IVUS images using sparse auto-encoder neural network","S. Su; Z. Gao; H. Zhang; Q. Lin; W. K. Hau; S. Li","College of Sciences, Zhejiang University of Technology, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1120","1124","This paper describes an artificial neural network (ANN) method that employs a feature-learning algorithm to detect the lumen and MA borders in intravascular ultrasound (IVUS) images. Three types of imaging features including spatial, neighboring, and gradient features were used as the input features to the neural network, and then the different vascular layers were distinguished using two sparse autoencoders and one softmax classifier. To smooth the lumen and MA borders detected by the ANN method, we used the active contour model. The performance of our approach was compared with the manual drawing method and another existing method on 538 IVUS images from six subjects. Results showed that our approach had a high correlation (r = 0.9284 ~ 0.9875 for all measurements) and good agreement (bias = 0.0148 ~ 0.4209 mm) with the manual drawing method, and small detection error (lumen border: 0.0928±0.0935 mm, MA border: 0.1056±0.1088 mm). The average time to process each image was 14±4.6 seconds. The obtained results indicate that our proposed approach can be used to efficiently and accurately detect the lumen and MA borders in IVUS images.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950713","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950713","Deep neural network;Image segmentation;Intravascular image;Sparse autoencoder","Active contours;Artificial neural networks;Biomedical imaging;Feature extraction;Manuals;Training","biomedical ultrasonics;blood vessels;feature extraction;image classification;medical image processing;neural nets","ANN method;IVUS image;artificial neural network;feature-learning algorithm;imaging feature;intravascular ultrasound;lumen detection;media-adventitia border detection;softmax classifier;sparse autoencoder neural network;vascular layer","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Prostate segmentation in MR images using ensemble deep convolutional neural networks","H. Jia; Y. Xia; W. Cai; M. Fulham; D. D. Feng","Shaanxi Key Lab of Speech & Image Information Processing (SAIIP), School of Computer Science, Northwestern Polytechnical University, Xi'an 710072, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","762","765","The automated segmentation of the prostate gland from MR images is increasingly used for clinical diagnosis. Since deep learning demonstrates superior performance in computer vision applications, we propose a coarse-to-fine segmentation strategy using ensemble deep convolutional neural networks (DCNNs) to address prostate segmentation in MR images. First, we use registration-based coarse segmentation on pre-processed prostate MR images to define the potential boundary region. We then train four DCNNs as voxel-based classifiers and classify the voxel in the potential region is a prostate voxel when at least three DCNNs made that decision. Finally, we use boundary refinement to eliminate the outliers and smooth the boundary. We evaluated our approach on the MICCAI PROMIS12 challenge dataset and our experimental results verify the effectiveness of the proposed algorithms.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950630","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950630","MR prostate segmentation;deep convolutional neural network;voxel classification","Biomedical imaging;Convolution;Glands;Image segmentation;Magnetic resonance imaging;Probabilistic logic;Training","biomedical MRI;image registration;image segmentation;learning (artificial intelligence);medical image processing;neural nets;pattern classification","MICCAI PROMIS12 challenge dataset;MR images;automated prostate gland segmentation;computer vision applications;deep learning;ensemble deep convolutional neural networks;prostate MR image preprocessing;prostate voxel;registration-based coarse segmentation;voxel-based classifiers","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"MIMO-Net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images","S. E. A. Raza; L. Cheung; D. Epstein; S. Pelengaris; M. Khan; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","337","340","We propose a novel multiple-input multiple-output convolution neural network (MIMO-Net) for cell segmentation in fluorescence microscopy images. The proposed network trains the network parameters using multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters. The MIMO-Net allows us to deal with variable intensity cell boundaries and highly variable cell size in the mouse pancreatic tissue by adding extra convolutional layers which bypass the max-pooling operation. The results show that our method outperforms state-of-the-art deep learning based approaches for segmentation.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950532","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950532","Cell Segmentation;Deep Learning;Fluorescence Microscopy","Biomembranes;Computer architecture;Convolution;Image segmentation;Machine learning;Microprocessors;Microscopy","biomedical optical imaging;fluorescence;image resolution;image segmentation;medical image processing;neural nets;optical microscopy","cell segmentation;deep learning based approaches;fluorescence microscopy images;mouse pancreatic tissue;multipleinput multipleoutput convolution neural network;multiresolution deconvolution filters;variable intensity cell boundaries","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep residual Hough voting for mitotic cell detection in histopathology images","T. Wollmann; K. Rohr","University of Heidelberg, BIOQUANT, IPMB, and DKFZ Heidelberg, Dept. Bioinformatics and Functional Genomics, Biomedical Computer Vision Group, Im Neuenheimer Feld 267, 69120, Germany","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","341","344","Cell detection in microscopy images is a common and challenging task. We propose a new approach for mitotic cell detection in histopathology images, which is based on a Deep Residual Network architecture combined with Hough voting. We propose a voting layer for neural networks and introduce a novel loss function. Our approach is learned from scratch using cell centroids and the original images. We benchmarked our approach on the challenging AMIDA13 dataset containing histology images of invasive breast carcinoma. It turned out that our approach achieved competitive results.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950533","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950533","Deep Learning;Dilated Convolution;Hough transform;Microscopy;Residual Network","Biomedical imaging;Computer architecture;Convolution;Microprocessors;Neural networks;Training;Transforms","Hough transforms;biological organs;cellular biophysics;learning (artificial intelligence);medical image processing;neural net architecture","AMIDA13 dataset;cell centroids;deep residual Hough voting;deep residual network architecture;histopathology images;invasive breast carcinoma;microscopy images;mitotic cell detection","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"M-net: A Convolutional Neural Network for deep brain structure segmentation","R. Mehta; J. Sivaswamy","Center for Visual Information Technology (CVIT), IIIT-Hyderabad, India","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","437","440","In this paper, we propose an end-to-end trainable Convolutional Neural Network (CNN) architecture called the M-net, for segmenting deep (human) brain structures from Magnetic Resonance Images (MRI). A novel scheme is used to learn to combine and represent 3D context information of a given slice in a 2D slice. Consequently, the M-net utilizes only 2D convolution though it operates on 3D data, which makes M-net memory efficient. The segmentation method is evaluated on two publicly available datasets and is compared against publicly available model based segmentation algorithms as well as other classification based algorithms such as Random Forrest and 2D CNN based approaches. Experiment results show that the M-net outperforms all these methods in terms of dice coefficient and is at least 3 times faster than other methods in segmenting a new volume which is attractive for clinical use.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950555","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950555","Convolutional Neural Networks;Deep Brain Structures;Magnetic Resonance Images;Segmentation","Brain;Convolution;Image segmentation;Magnetic resonance imaging;Three-dimensional displays;Training;Two dimensional displays","biomedical MRI;brain;convolution;image segmentation;medical image processing;neural nets","2D convolution;M-net;convolutional neural network;deep brain structure segmentation;magnetic resonance images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Deep learning model based breast cancer histopathological image classification","Benzheng Wei; Zhongyi Han; Xueying He; Yilong Yin","College of Science and Technology, Shandong University of Traditional Chinese Medicine, Jinan, China","2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)","20170619","2017","","","348","353","The automatic and precision classification for breast cancer histopathological image has a great significance in clinical application. However, the existing analysis approaches are difficult to addressing the breast cancer classification problem because the feature subtle differences of inter-class histopathological image and the classification accuracy still hard to meet the clinical application. Recent advancements in data-driven sharing processing and multi-level hierarchical feature learning have made available considerable chance to dope out a solution to this problem. To address the challenging problem, we propose a novel breast cancer histopathological image classification method based on deep convolutional neural networks, named as BiCNN model, to address the two-class breast cancer classification on the pathological image. This deep learning model considers class and sub-class labels of breast cancer as prior knowledge, which can restrain the distance of features of different breast cancer pathological images. In addition, an advanced data augmented method is proposed to fit tolerance whole slide image recognition, which can full reserve image edge feature of cancerization region. The transfer learning and fine-tuning method are adopted as an optimal training strategy to improve breast cancer histopathological image classification accuracy. The experiment results show that the proposed method leads to a higher classification accuracy (up to 97%) and displays good robustness and generalization, which provides efficient tools for breast cancer clinical diagnosis.","","CD:978-1-5090-4497-9; Electronic:978-1-5090-4499-3; POD:978-1-5090-4500-6; Paper:978-1-5090-4498-6","10.1109/ICCCBDA.2017.7951937","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7951937","CNN;breast cancer;classification;deep learning;histopathological image;massive image data","Cancer;Feature extraction;Image color analysis;Image recognition;Image reconstruction;Neurons;Robustness","cancer;convolution;image classification;learning (artificial intelligence);medical image processing;neural nets","BiCNN model;advanced data augmented method;breast cancer clinical diagnosis;breast cancer histopathological image classification method;cancerization region;class labels;classification accuracy;data-driven sharing processing;deep convolutional neural networks;deep learning model;fine-tuning method;fit tolerance whole slide image recognition;image edge feature;multilevel hierarchical feature learning;optimal training strategy;subclass labels;transfer learning","","","","","","","","28-30 April 2017","","IEEE","IEEE Conference Publications"
"Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images","T. H. Song; V. Sanchez; H. EIDaly; N. M. Rajpoot","Department of Computer Science, University of Warwick, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1040","1043","Automated cell detection is a critical step for a number of computer-assisted pathology related image analysis algorithm. However, automated cell detection is complicated due to the variable cytomorphological and histological factors associated with each cell. In order to efficiently resolve the challenge of automated cell detection, deep learning strategies are widely applied and have recently been shown to be successful in histopathological images. In this paper, we concentrate on bone marrow trephine biopsy images and propose a hybrid deep autoencoder (HDA) network with Curvature Gaussian model for efficient and precise bone marrow hematopoietic stem cell detection via related high-level feature correspondence. The accuracy of our proposed method is up to 94%, outperforming other supervised and unsupervised detection approaches.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950694","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950694","Autoencoder;Bone marrow;Deep learning;Nuclei Detection","Biological system modeling;Biopsy;Bones;Decoding;Feature extraction;Shape;Training","Gaussian processes;bone;cellular biophysics;learning (artificial intelligence);medical image processing","automated cell detection;bone marrow hematopoietic stem cell detection;bone marrow trephine biopsy images;curvature Gaussian model;deep learning;hybrid deep autoencoder","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Convolutional neural networks for predicting molecular profiles of non-small cell lung cancer","D. Yu; M. Zhou; F. Yang; D. Dong; O. Gevaert; Z. Liu; J. Shi; J. Tian","The Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","569","572","Quantitative imaging biomarkers identification has become a powerful tool for predictive diagnosis given increasingly available clinical imaging data. In parallel, molecular profiles have been well documented in non-small cell lung cancers (NSCLCs). However, there has been limited studies on leveraging the two major sources for improving lung cancer computer-aided diagnosis. In this paper, we investigate the problem of predicting molecular profiles with CT imaging arrays in NSCLC. In particular, we formulate a discriminative convolutional neural network to learn deep features for predicting epidermal growth factor receptor (EGFR) mutation states that are associated with cancer cell growth. We evaluated our approach on two independent datasets including a discovery set with 595 patients (Datset1) and a validation set with 89 patients (Dataset2). Extensive experimental results demonstrated that the learned CNN-based features are effective in predicting EGFR mutation states (AUC=0.828, ACC=76.16%) on Dataset1, and it further demonstrated generalized predictive performance (AUC=0.668, ACC=67.55%) on Dataset2.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950585","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950585","Computed tomography;Computed-aided diagnosis;Convolutional neural networks;Non-Small Cell Lung Carcinoma","Cancer;Computed tomography;Convolution;Feature extraction;Lungs;Neural networks","cancer;cellular biophysics;computerised tomography;feature extraction;lung;medical image processing;molecular biophysics;neural nets;proteins","CT imaging arrays;EGFR mutation state prediction;biomarker identification;cancer cell growth;clinical imaging data;discriminative convolutional neural network;epidermal growth factor receptor;learned CNN-based feature;lung cancer computer-aided diagnosis;molecular profile;nonsmall cell lung cancer;quantitative imaging","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Exploring texture Transfer Learning for Colonic Polyp Classification via Convolutional Neural Networks","E. Ribeiro; M. Häfner; G. Wimmer; T. Tamaki; J. J. W. Tischendorf; S. Yoshida; S. Tanaka; A. Uhl","University of Salzburg - Department of Computer Sciences, AT","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1044","1048","This work addresses Transfer Learning via Convolutional Neural Networks (CNN's) for the automated classification of colonic polyps in eight HD-endoscopic image databases acquired using different modalities. For this purpose, we explore if the architecture, the training approach, the number of classes, the number of images as well as the nature of the images in the training phase can influence the results. The experiments show that when the number of classes and the nature of the images are similar to the target database, the results are improved. Also, the better results obtained by the transfer learning compared to the most used features in the literature suggest that features learned by CNN's can be highly relevant for automated classification of colonic polyps.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950695","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950695","Colonic Polyp Classification;Convolutional Neural Networks;Deep Learning;Texture Transfer Learning","Biomedical imaging;Colonic polyps;Computers;Feature extraction;Image databases;Training","biomedical optical imaging;endoscopes;image classification;learning (artificial intelligence);medical image processing;neural nets","HD-endoscopic image databases;colonic polyp classification;convolutional neural networks;deep learning;transfer learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Coronary luminal and wall mask prediction using convolutional neural network","Y. Hong; Y. M. Hong; Y. Jang; S. Kim; B. Jeon; S. Jung; S. Ha; D. Han; H. Shim; H. J. Chang","Brain Korea 21 PLUS Project for Medical Science, Yonsei University, South Korea","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1049","1052","A significant amount of research has been done on the segmentation of coronary arteries. However, the resulting automated boundary delineation is still not suitable for clinical utilization. The convolutional neural network was driving advances in the medical image processing. We propose the brief convolutional network (BCN) that automatically produces the labeled mask with the luminal and wall boundaries of the coronary artery. We utilized 50 patients of CCTA - intravascular ultrasound matched image data sets. Training and testing were performed on 40 and 10 patient data sets, respectively. The prediction of luminal and wall mask was performed using stacked BCN on the each image view: axial, coronal, and sagittal of straightened curved planar reformation. We defined the vector that includes probability from BCN result on each image view and proposed amplified probability. We used an Adaptive Boost regressor with an extremely randomized tree regressor to determine the label for unknown probability vector.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950696","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950696","Classification;Convolutional neural network;Coronary artery;Deep learning;Plaque quantification","Arteries;Computer architecture;Feature extraction;Image segmentation;Neural networks;Training;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiology;image segmentation;learning (artificial intelligence);medical image processing;neural nets;probability;regression analysis","CCTA-intravascular ultrasound image;adaptive boost regressor;brief convolutional network;convolutional neural network;coronary artery segmentation;coronary luminal;coronary wall mask prediction;extremely randomized tree regressor;image view;medical image processing","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Self supervised deep representation learning for fine-grained body part recognition","P. Zhang; F. Wang; Y. Zheng","Medical Imaging Technologies, Siemens Medical Solutions USA Inc., Princeton, NJ 08540, USA","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","578","582","Difficulty on collecting annotated medical images leads to lack of enough supervision and makes discrimination tasks challenging. However, raw data, e.g., spatial context information from 3D CT images, even without annotation, may contain rich useful information. In this paper, we exploit spatial context information as a source of supervision to solve discrimination tasks for fine-grained body part recognition with conventional 3D CT and MR volumes. The proposed pipeline consists of two steps: 1) pre-train a convolutional network for an auxiliary task of 2D slices ordering in a self-supervised manner; 2) transfer and fine-tune the pre-trained network for fine-grained body part recognition. Without any use of human annotation in the first stage, the pre-trained network can still outperform CNN trained from scratch on CT as well as M-R data. Moreover, by comparing with pre-trained CNN from ImageNet, we discover that the distance between source and target tasks plays a crucial role in transfer learning. Our experiments demonstrate that our approach can achieve high accuracy with a slice location estimation error of only a few slices on CT and MR data. To the best of our knowledge, our work is the first attempt studying the problem of robust body part recognition at a continuous level.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950587","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950587","Body Part Recognition;Self Supervised Learning;Slice Ordering","Biomedical imaging;Computed tomography;Context;Image recognition;Three-dimensional displays;Training;Two dimensional displays","biomedical MRI;computerised tomography;image recognition;learning (artificial intelligence);medical image processing","3D CT;MR volumes;fine-grained body part recognition;self supervised deep representation learning;spatial context information;transfer learning","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Modeling Task fMRI Data via Deep Convolutional Autoencoder","H. Huang; X. Hu; Y. Zhao; M. Makkie; Q. Dong; S. Zhao; L. Guo; T. Liu","School of Automation, Northwestern Polytechnical University, Xi&#x2019;an, 710072, China.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Task-based fMRI (tfMRI) has been widely used to study functional brain networks under task performance. Modeling tfMRI data is challenging due to at least two problems: the lack of the ground truth of underlying neural activity and the highly complex intrinsic structure of tfMRI data. To better understand brain networks based on fMRI data, data-driven approaches have been proposed, for instance, Independent Component Analysis (ICA) and Sparse Dictionary Learning (SDL). However, both ICA and SDL only build shallow models, and they are under the strong assumption that original fMRI signal could be linearly decomposed into time series components with their corresponding spatial maps. As growing evidence shows that human brain function is hierarchically organized, new approaches that can infer and model the hierarchical structure of brain networks are widely called for. Recently, deep convolutional neural network (CNN) has drawn much attention, in that deep CNN has proven to be a powerful method for learning high-level and mid-level abstractions from low-level raw data. Inspired by the power of deep CNN, in this study, we developed a new neural network structure based on CNN, called Deep Convolutional Auto-Encoder (DCAE), in order to take the advantages of both data-driven approach and CNN’s hierarchical feature abstraction ability for the purpose of learning mid-level and high-level features from complex, large-scale tfMRI time series in an unsupervised manner. The DCAE has been applied and tested on the publicly available human connectome project (HCP) tfMRI datasets, and promising results are achieved.","0278-0062;02780062","","10.1109/TMI.2017.2715285","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949140","CNN;Task fMRI;deep learning;unsupervised","Brain modeling;Convolution;Data models;Decoding;Hidden Markov models;Machine learning;Time series analysis","","","","","","","","","20170615","","","IEEE","IEEE Early Access Articles"
"Multilevel Contextual 3-D CNNs for False Positive Reduction in Pulmonary Nodule Detection","Q. Dou; H. Chen; L. Yu; J. Qin; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Biomedical Engineering","20170615","2017","64","7","1558","1567","Objective: False positive reduction is one of the most crucial components in an automated pulmonary nodule detection system, which plays an important role in lung cancer diagnosis and early treatment. The objective of this paper is to effectively address the challenges in this task and therefore to accurately discriminate the true nodules from a large number of candidates. Methods: We propose a novel method employing three-dimensional (3-D) convolutional neural networks (CNNs) for false positive reduction in automated pulmonary nodule detection from volumetric computed tomography (CT) scans. Compared with its 2-D counterparts, the 3-D CNNs can encode richer spatial information and extract more representative features via their hierarchical architecture trained with 3-D samples. More importantly, we further propose a simple yet effective strategy to encode multilevel contextual information to meet the challenges coming with the large variations and hard mimics of pulmonary nodules. Results: The proposed framework has been extensively validated in the LUNA16 challenge held in conjunction with ISBI 2016, where we achieved the highest competition performance metric (CPM) score in the false positive reduction track. Conclusion: Experimental results demonstrated the importance and effectiveness of integrating multilevel contextual information into 3-D CNN framework for automated pulmonary nodule detection in volumetric CT data. Significance: While our method is tailored for pulmonary nodule detection, the proposed framework is general and can be easily extended to many other 3-D object detection tasks from volumetric medical images, where the targeting objects have large variations and are accompanied by a number of hard mimics.","0018-9294;00189294","","10.1109/TBME.2016.2613502","Shenzhen-Hong Kong Innovation Circle; The Hong Kong Special Administrative Region; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576695","3-D convolutional neural networks;Computer-aided diagnosis;deep learning;false positive reduction;pulmonary nodule detection","Cancer;Computed tomography;Feature extraction;Kernel;Lungs;Three-dimensional displays;Two dimensional displays","cancer;computerised tomography;feature extraction;lung;medical image processing;neural nets","3D convolutional neural network;3D object detection tasks;CPM score;ISBI 2016;LUNA16 challenge;automated pulmonary nodule detection system;competition performance metric score;contextual 3D CNN;false positive reduction track;feature extraction;lung cancer diagnosis;multilevel contextual information;volumetric CT scans;volumetric computed tomography;volumetric medical images","","","","","","","20160926","July 2017","","IEEE","IEEE Journals & Magazines"
"Poster Abstract: Maximizing Accuracy of Fall Detection and Alert Systems Based on 3D Convolutional Neural Network","S. Hwang; D. Ahn; H. Park; T. Park","Hanyang Univ., Seoul, South Korea","2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI)","20170615","2017","","","343","344","We present a deep-learning-based approach to maximize the accuracy and reliability of vision-based fall detection and alert systems. The proposed approach utilizes a 3D convolutional neural network (3D-CNN) to analyze the continuous motion data obtained from depth cameras and exploits a data augmentation method to do away with overfitting. Our preliminary evaluation results demonstrate that it achieves the classification accuracy of up to 96.9%.","","Electronic:978-1-4503-4966-6; POD:978-1-4673-9146-7","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946918","3D convolutional neural network;IoT applications;deep learning;elderly care;fall detection","Medical services;Neural networks;Reliability;Senior citizens;Shape;Three-dimensional displays;Training","convolution;geriatrics;image classification;learning (artificial intelligence);medical computing;neural nets;stereo image processing","3D convolutional neural network;3D-CNN;activity classification;activity recognition;alert systems;data augmentation;deep learning;elderly care;fall detection","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Automatic Quality Assessment of Echocardiograms Using Convolutional Neural Networks: Feasibility on the Apical Four-Chamber View","A. H. Abdi; C. Luong; T. Tsang; G. Allan; S. Nouranian; J. Jue; D. Hawley; S. Fleming; K. Gin; J. Swift; R. Rohling; P. Abolmaesumi","Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Medical Imaging","20170602","2017","36","6","1221","1230","Echocardiography (echo) is a skilled technical procedure that depends on the experience of the operator. The aim of this paper is to reduce user variability in data acquisition by automatically computing a score of echo quality for operator feedback. To do this, a deep convolutional neural network model, trained on a large set of samples, was developed for scoring apical four-chamber (A4C) echo. In this paper, 6,916 end-systolic echo images were manually studied by an expert cardiologist and were assigned a score between 0 (not acceptable) and 5 (excellent). The images were divided into two independent training-validation and test sets. The network architecture and its parameters were based on the stochastic approach of the particle swarm optimization on the training-validation data. The mean absolute error between the scores from the ultimately trained model and the expert's manual scores was 0.71 ± 0.58. The reported error was comparable to the measured intra-rater reliability. The learned features of the network were visually interpretable and could be mapped to the anatomy of the heart in the A4C echo, giving confidence in the training result. The computation time for the proposed network architecture, running on a graphics processing unit, was less than 10 ms per frame, sufficient for real-time deployment. The proposed approach has the potential to facilitate the widespread use of echo at the point-of-care and enable early and timely diagnosis and treatment. Finally, the approach did not use any specific assumptions about the A4C echo, so it could be generalizable to other standard echo views.","0278-0062;02780062","","10.1109/TMI.2017.2690836","10.13039/501100000024 - Canadian Institutes of Health Research; 10.13039/501100000038 - Natural Sciences and Engineering Research Council; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892028","Convolutional neural network;apical four-chamber;deep learning;echocardiography;quality assessment;swarm optimization","Convolutional neural networks;Echocardiography;Machine learning;Particle swarm optimization;Quality assessment","data acquisition;echocardiography;medical image processing;neural nets;particle swarm optimisation","A4C echo;apical four-chamber view;automatic quality assessment;data acquisition;deep convolutional neural networks;echo quality;echocardiograms;graphics processing unit;intra-rater reliability;operator feedback;particle swarm optimization;stochastic approach;training-validation data","","","","","","","20170404","June 2017","","IEEE","IEEE Journals & Magazines"
"Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning","W. Xue; A. Islam; M. Bhaduri; S. Li","Department of Medical Imaging, Western University, London, ON N6A 3K7, Canada and also with Digital Imaging Group of London, London, ON N6A 3K7, Canada.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.440.71mm) and areas of cavity and myocardium (204133mm2). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment","0278-0062;02780062","","10.1109/TMI.2017.2709251","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934404","cardiac MR;deep convolution autoencoder;direct estimation;joint learning;multitype cardiac indices","Convolution;Estimation;Feature extraction;Image representation;Image segmentation;Myocardium;Volume measurement","","","","","","","","","20170526","","","IEEE","IEEE Early Access Articles"
"Generative Adversarial Networks for Noise Reduction in Low-Dose CT","J. M. Wolterink; T. Leiner; M. A. Viergever; I. Isgum","","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Noise is inherent to low-dose CT acquisition. We propose to train a convolutional neural network (CNN) jointly with an adversarial CNN to estimate routine-dose CT images from low-dose CT images and hence reduce noise. A generator CNN was trained to transform low-dose CT images into routine-dose CT images using voxel-wise loss minimization. An adversarial discriminator CNN was simultaneously trained to distinguish the output of the generator from routinedose CT images. The performance of this discriminator was used as an adversarial loss for the generator. Experiments were performed using CT images of an anthropomorphic phantom containing calcium inserts, as well as patient non-contrast-enhanced cardiac CT images. The phantom and patients were scanned at 20% and 100% routine clinical dose. Three training strategies were compared: the first used only voxel-wise loss, the second combined voxel-wise loss and adversarial loss, and the third used only adversarial loss. The results showed that training with only voxel-wise loss resulted in the highest peak signal-to-noise ratio with respect to reference routine-dose images. However, the CNNs trained with adversarial loss captured image statistics of routine-dose images better. Noise reduction improved quantification of low-density calcified inserts in phantom CT images and allowed coronary calcium scoring in low-dose patient CT images with high noise levels. Testing took less than 10 seconds per CT volume. CNN-based low-dose CT noise reduction in the image domain is feasible. Training with an adversarial network improves the CNN’s ability to generate images with an appearance similar to that of reference routine-dose CT images.","0278-0062;02780062","","10.1109/TMI.2017.2708987","Netherlands Organization for Health Research and Development ZonMw; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934380","Low-dose cardiac CT;coronary calcium scoring;deep learning,;generative adversarial networks;noise reduction","Calcium;Computed tomography;Convolution;Generators;Noise reduction;Training;Transforms","","","","","","","","","20170526","","","IEEE","IEEE Early Access Articles"
"Mapping, Learning, Visualization, Classification, and Understanding of fMRI Data in the NeuCube Evolving Spatiotemporal Data Machine of Spiking Neural Networks","N. K. Kasabov; M. G. Doborjeh; Z. G. Doborjeh","Auckland University of Technology, Auckland, New Zealand","IEEE Transactions on Neural Networks and Learning Systems","20170520","2017","28","4","887","899","This paper introduces a new methodology for dynamic learning, visualization, and classification of functional magnetic resonance imaging (fMRI) as spatiotemporal brain data. The method is based on an evolving spatiotemporal data machine of evolving spiking neural networks (SNNs) exemplified by the NeuCube architecture [1]. The method consists of several steps: mapping spatial coordinates of fMRI data into a 3-D SNN cube (SNNc) that represents a brain template; input data transformation into trains of spikes; deep, unsupervised learning in the 3-D SNNc of spatiotemporal patterns from data; supervised learning in an evolving SNN classifier; parameter optimization; and 3-D visualization and model interpretation. Two benchmark case study problems and data are used to illustrate the proposed methodology - fMRI data collected from subjects when reading affirmative or negative sentences and another one - on reading a sentence or seeing a picture. The learned connections in the SNNc represent dynamic spatiotemporal relationships derived from the fMRI data. They can reveal new information about the brain functions under different conditions. The proposed methodology allows for the first time to analyze dynamic functional and structural connectivity of a learned SNN model from fMRI data. This can be used for a better understanding of brain activities and also for online generation of appropriate neurofeedback to subjects for improved brain functions. For example, in this paper, tracing the 3-D SNN model connectivity enabled us for the first time to capture prominent brain functional pathways evoked in language comprehension. We found stronger spatiotemporal interaction between left dorsolateral prefrontal cortex and left temporal while reading a negated sentence. This observation is obviously distinguishable from the patterns generated by either reading affirmative sentences or seeing pictures. The proposed NeuCube-- ased methodology offers also a superior classification accuracy when compared with traditional AI and statistical methods. The created NeuCube-based models of fMRI data are directly and efficiently implementable on high performance and low energy consumption neuromorphic platforms for real-time applications.","2162-237X;2162237X","","10.1109/TNNLS.2016.2612890","Knowledge Engineering and Discovery Research Institute of the Auckland University of Technology; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585110","Brain data classification;NeuCube;brain data modeling;brain data visualization;evolving spatiotemporal data machines (eSTDMs);functional magnetic resonance imaging (fMRI);neuro information processing;spatiotemporal brain data (STBD);spiking neural networks (SNNs)","Brain modeling;Data models;Data visualization;Solid modeling;Spatiotemporal phenomena;Unsupervised learning","biomedical MRI;data visualisation;image classification;medical image processing;neural nets;unsupervised learning","3-D SNNc;3-D visualization;NeuCube evolving spatiotemporal data machine;NeuCube-based methodology;SNN;affirmative sentences;brain functional pathways;brain functions;brain template;classification accuracy;dynamic learning;dynamic spatiotemporal relationships;evolving spiking neural networks;fMRI data;functional magnetic resonance imaging;input data transformation;language comprehension;left dorsolateral prefrontal cortex;low energy consumption neuromorphic platforms;negative sentences;neurofeedback;spatiotemporal brain data;spatiotemporal interaction;spiking neural networks;unsupervised learning","","","","","","","20161006","April 2017","","IEEE","IEEE Journals & Magazines"
"Multilinear Principal Component Analysis Network for Tensor Object Classification","J. Wu; S. Qiu; R. Zeng; Y. Kong; L. Senhadji; H. Shu","LIST, Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, Nanjing, China","IEEE Access","20170520","2017","5","","3322","3331","The recently proposed principal component analysis network (PCANet) has performed well with respect to the classification of 2-D images. However, feature extraction may perform less well when dealing with multi-dimensional images, since the spatial relationships within the structures of the images are not fully utilized. In this paper, we develop a multilinear principal component analysis network (MPCANet), which is a tensor extension of PCANet, to extract the high-level semantic features from multi-dimensional images. The extracted features largely minimize the intraclass invariance of tensor objects by making efficient use of spatial relationships within multi-dimensional images. The proposed MPCANet outperforms traditional methods on a benchmark composed of three data sets, including the UCF sports action database, the UCF11 database, and a medical image database. It is shown that even a simple one-layer MPCANet may outperform a two-layer PCANet.","2169-3536;21693536","","10.1109/ACCESS.2017.2675478","333 Project; Open Fund of China-USA Computer Science Research Center; Project Sponsored by the Scientific Research Foundation for the Returned Overseas Chinese Scholars; Qing Lan Project; State Education Ministry; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867073","Deep learning;MPCANet;PCANet;medical image classification;tensor object classification","Convolution;Databases;Feature extraction;Histograms;Principal component analysis;Tensile stress;Visualization","feature extraction;image classification;learning (artificial intelligence);principal component analysis;tensors;visual databases","MPCANet;UCF sports action database;UCF11 database;high-level semantic feature extraction;intraclass invariance minimization;medical image database;multidimensional images;multilinear principal component analysis network;spatial relationships;tensor object classification","","","","","","","20170301","2017","","IEEE","IEEE Journals & Magazines"
"Integrating Online and Offline Three-Dimensional Deep Learning for Automated Polyp Detection in Colonoscopy Videos","L. Yu; H. Chen; Q. Dou; J. Qin; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","65","75","Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way for colorectal cancer prevention and diagnosis. Traditional manual screening is time consuming, operator dependent, and error prone; hence, automated detection approach is highly demanded in clinical practice. However, automated polyp detection is very challenging due to high intraclass variations in polyp size, color, shape, and texture, and low interclass variations between polyps and hard mimics. In this paper, we propose a novel offline and online three-dimensional (3-D) deep learning integration framework by leveraging the 3-D fully convolutional network (3D-FCN) to tackle this challenging problem. Compared with the previous methods employing hand-crafted features or 2-D convolutional neural network, the 3D-FCN is capable of learning more representative spatio-temporal features from colonoscopy videos, and hence has more powerful discrimination capability. More importantly, we propose a novel online learning scheme to deal with the problem of limited training data by harnessing the specific information of an input video in the learning process. We integrate offline and online learning to effectively reduce the number of false positives generated by the offline network and further improve the detection performance. Extensive experiments on the dataset of MICCAI 2015 Challenge on Polyp Detection demonstrated the better performance of our method when compared with other competitors.","2168-2194;21682194","","10.1109/JBHI.2016.2637004","Hong Kong Special Administrative Region; Shenzhen Science and Technology Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776845","Automated polyp detection;colonoscopy video;computer-aided diagnosis;convolutional neural networks (CNNs);deep learning","Cancer;Colonoscopy;Feature extraction;MIMICs;Shape;Three-dimensional displays;Videos","cancer;endoscopes;learning (artificial intelligence);medical image processing;neural nets;video signal processing","3D fully convolutional network;automated polyp detection;colonoscopy videos;colorectal cancer diagnosis;colorectal cancer prevention;offline three-dimensional deep learning integration framework;online three-dimensional deep learning integration framework;spatiotemporal features","","","","","","","20161207","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"An Ensemble of Fine-Tuned Convolutional Neural Networks for Medical Image Classification","A. Kumar; J. Kim; D. Lyndon; M. Fulham; D. Feng","Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Camperdown, NSW, Australia","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","31","40","The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than - ethods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.","2168-2194;21682194","","10.1109/JBHI.2016.2635663","10.13039/100000163 - ARC; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769199","Convolutional neural network (CNN);deep learning;ensembles;fine-tuning;image classification","Biomedical imaging;Computer architecture;Feature extraction;Informatics;Neural networks;Training;Training data","feature extraction;image classification;image representation;medical image processing;neural nets;probability","ImageCLEF 2016 medical image public dataset;feature extraction;fine-tuned convolutional neural networks;image features;medical image classification;natural image dataset;posterior probability;semantic image representation","","","","","","","20161205","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Ultrasound Standard Plane Detection Using a Composite Neural Network Framework","H. Chen; L. Wu; Q. Dou; J. Qin; S. Li; J. Z. Cheng; D. Ni; P. A. Heng","Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Cybernetics","20170520","2017","47","6","1576","1586","Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.","2168-2267;21682267","","10.1109/TCYB.2017.2685080","National Basic Research Program of China, 973 Program; National Natural Science Foundation of China; Research Grants Council of Hong Kong Special Administrative Region; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445","Convolutional neural network (CNN);deep learning;knowledge transfer;recurrent neural network (RNN);standard plane;ultrasound (US)","Biomedical imaging;Feature extraction;Fetus;Machine learning;Standards;Training data;Videos","","","","","","","","","20170330","June 2017","","IEEE","IEEE Journals & Magazines"
"$mathtt {Deepr}$: A Convolutional Net for Medical Records","P. Nguyen; T. Tran; N. Wickramasinghe; S. Venkatesh","Centre for Pattern Recognition and Data Analytics, Faculty of Science and Technology, Deakin University, Geelong, Vic, Australia","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","22","30","Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space.","2168-2194;21682194","","10.1109/JBHI.2016.2633963","Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762861","Convolutional neural networks;deep learning;medical records","Diseases;Electronic medical records;Feature extraction;Hospitals;Machine learning;Medical diagnostic imaging","diseases;electronic health records;feature extraction;hospitals;learning (artificial intelligence);neural nets","Deepr;coded time gaps;convolutional neural networks;disease;electronic medical records;end-to-end deep learning system;feature extraction;hospital data;hospital transfers","","","","","","","20161201","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Liver Fibrosis Classification Based on Transfer Learning and FCNet for Ultrasound Images","D. Meng; L. Zhang; G. Cao; W. Cao; G. Zhang; B. Hu","MOE Research Center for Software/Hardware Co-Design Engineering, East China Normal University, Shanghai, China","IEEE Access","20170520","2017","5","","5804","5810","Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.","2169-3536;21693536","","10.1109/ACCESS.2017.2689058","NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Informatization; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890483","Deep neural networks;fully connected layers;liver fibrosis;transfer learning","Feature extraction;Heating systems;Liver;Medical services;Neural networks;Training;Ultrasonic imaging","","","","","","","","","20170330","2017","","IEEE","IEEE Journals & Magazines"
"A Robust Deep Model for Improved Classification of AD/MCI Patients","F. Li; L. Tran; K. H. Thung; S. Ji; D. Shen; J. Li","Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2015","19","5","1610","1616","Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.","2168-2194;21682194","","10.1109/JBHI.2015.2429556","10.13039/501100001677 - NIH grants; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101222","Alzheimer’s Disease;Alzheimer's disease (AD);Deep Learning;Early Diagnosis;MRI;PET;deep learning;early diagnosis;magnetic resonance imaging (MRI);positron emission tomography (PET)","Computational modeling;Feature extraction;Magnetic resonance imaging;Positron emission tomography;Principal component analysis;Support vector machines;Training","biomedical MRI;cognition;diseases;learning (artificial intelligence);medical computing;neurophysiology;positron emission tomography","AD conversion diagnosis;AD diagnosis;AD patients;ADNI dataset;Alzheimer's disease;MCI conversion diagnosis;MCI patients;MRI;PET scans;adaptive learning factor;classical deep learning method;classification accuracy;deep learning framework;dropout technique;improved classification;memory impairment;mild cognitive impairment;multitask learning strategy;noninvasive imaging biomarkers;prodromal stage;progression stages;quality of life;robust deep learning system;stability selection;weight coadaptation","0;Alzheimer Disease;Early Diagnosis;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Mild Cognitive Impairment;Models, Theoretical;Positron-Emission Tomography;Principal Component Analysis;Support Vector Machine","11","","29","","","20150504","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel","Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","48","55","Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","2168-2194;21682194","","10.1109/JBHI.2016.2631401","European Regions Development; FIS; Marie-Curie Actions Program of the European Union; 10.13039/100000002 - NIH; 10.13039/100007065 - NVIDIA; 10.13039/501100000783 - REA; 10.13039/501100003741 - ICREA; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798","Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound","Atherosclerosis;Feature extraction;Imaging;Lipidomics;Machine learning;Neural networks;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","calcified tissue;cardiovascular events;carotid plaque composition;carotid ultrasound;cerebrovascular events;convolutional neural network;deep learning framework;fibrous cap;fibrous tissue;image noise;imaging features;lipid core;plaque constituents;thresholding values","","","","","","","20161122","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Deep Filter Banks for Land-Use Scene Classification","H. Wu; B. Liu; W. Su; W. Zhang; J. Sun","Institute of Medical Equipment, Academy of Military Medical Science, Tianjin, China","IEEE Geoscience and Remote Sensing Letters","20170519","2016","13","12","1895","1899","Land-use (LU) scene classification is one of the most challenging tasks in the field of remote sensing (RS) image processing due to its high intraclass variability and low interclass distance. Motivated by the challenge posed by this problem, we propose a novel hybrid architecture, deep filter banks, combining multicolumn stacked denoising sparse autoencoder (SDSAE) and Fisher vector (FV) to automatically learn the representative and discriminative features in a hierarchical manner for LU scene classification. SDSAE kernels describe local patches and a robust global feature of the RS image is built through the FV pooling layer. Unlike previous handcrafted features, we use machine-learning mechanisms to optimize our proposed feature extractor so that it can learn more suitable internal features from the RS data, boosting the final performance. Our approach achieves superior performance compared with the state-of-the-art methods, obtaining average classification accuracies of 92.7% and 90.4%, respectively, on the UC Merced and RSSCN7 data sets.","1545-598X;1545598X","","10.1109/LGRS.2016.2616440","Science and Technology Pillar Program, Tianjin, China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7676333","Deep filter banks;Fisher vector (FV);land-use (LU) scene classification;stacked denoising sparse autoencoder (SDSAE)","Data models;Encoding;Feature extraction;Kernel;Noise reduction;Robustness;Semantics","feature extraction;geophysical image processing;image classification;image denoising;image filtering;land use;learning (artificial intelligence);remote sensing","Fisher vector;deep filter bank;feature extractor;image classification accuracy;image processing;land-use scene classification;machine-learning mechanisms;multicolumn SDSAE kernel;remote sensing image;stacked denoising sparse autoencoder","","2","","","","","20161025","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"A Deep Convolutional Neural Network Based Framework for Automatic Fetal Facial Standard Plane Recognition","Z. Yu; E. L. Tan; D. Ni; J. Qin; S. Chen; S. Li; B. Lei; T. Wang","","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Ultrasound imaging has become a prevalent examination method in prenatal diagnosis. Accurate acquisition of fetal facial standard plane (FFSP) is the most important precondition for subsequent diagnosis and measurement. In the past few years, considerable effort has been devoted to FFSP recognition using various hand-crafted features, but the recognition performance is still unsatisfactory due to the high intra-class variation of FFSPs and the high degree of visual similarity between FFSPs and other non-FFSPs. To improve the recognition performance, we propose a method to automatically recognize FFSP via a deep convolutional neural network (DCNN) architecture. The proposed DCNN consists of 16 convolutional layers with small 3×3 size kernels and three fully connected layers. A global average pooling (GAP) is adopted in the last pooling layer to significantly reduce network parameters, which alleviates the overfitting problems and improves the performance under limited training data. Both the transfer learning strategy and a data augmentation technique tailored for FFSP are implemented to further boost the recognition performance. Extensive experiments demonstrate the advantage of our proposed method over traditional approaches and the effectiveness of DCNN to recognize FFSP for clinical diagnosis.","2168-2194;21682194","","10.1109/JBHI.2017.2705031","10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930382","Deep convolutional neural network;Standard plane recognition;Transfer learning;Ultrasound image","Biomedical imaging;Biomedical measurement;Feature extraction;Image recognition;Neural networks;Standards;Ultrasonic imaging","","","","","","","","","20170517","","","IEEE","IEEE Early Access Articles"
"Diabetic retinopathy detection using deep convolutional neural networks","D. Doshi; A. Shenoy; D. Sidhpura; P. Gharpure","Sardar Patel Institute of Technology, India","2016 International Conference on Computing, Analytics and Security Trends (CAST)","20170501","2016","","","261","266","Diabetic retinopathy is when damage occurs to the retina due to diabetes, which affects up to 80 percent of all patients who have had diabetes for 10 years or more. The expertise and equipment required are often lacking in areas where diabetic retinopathy detection is most needed. Most of the work in the field of diabetic retinopathy has been based on disease detection or manual extraction of features, but this paper aims at automatic diagnosis of the disease into its different stages using deep learning. This paper presents the design and implementation of GPU accelerated deep convolutional neural networks to automatically diagnose and thereby classify high-resolution retinal images into 5 stages of the disease based on severity. The single model accuracy of the convolutional neural networks presented in this paper is 0.386 on a quadratic weighted kappa metric and ensembling of three such similar models resulted in a score of 0.3996.","","Electronic:978-1-5090-1338-8; POD:978-1-5090-1339-5","10.1109/CAST.2016.7914977","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7914977","Computer vision;Convolutional Neural Networks;Deep learning;Diabetic Retinopathy;Quadratic weighted kappa metric","Convolution;Diabetes;Neural networks;Retina;Retinopathy;Training","diseases;eye;feedforward neural nets;graphics processing units;image classification;image resolution;learning (artificial intelligence);medical image processing","GPU accelerated deep convolutional neural networks;automatic disease diagnosis;deep learning;diabetic retinopathy detection;disease detection;ensembling;high-resolution retinal image classification;quadratic weighted kappa metric","","","","","","","","19-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"Detecting Cardiovascular Disease from Mammograms With Deep Learning","J. Wang; H. Ding; F. A. Bidgoli; B. Zhou; C. Iribarren; S. Molloi; P. Baldi","Department of Computer Science, Institute for Genomics and Bioinformatics, University of California at Irvine, Irvine, CA, USA","IEEE Transactions on Medical Imaging","20170501","2017","36","5","1172","1181","Coronary artery disease is a major cause of death in women. Breast arterial calcifications (BACs), detected in mammograms, can be useful risk markers associated with the disease. We investigate the feasibility of automated and accurate detection of BACs in mammograms for risk assessment of coronary artery disease. We develop a 12-layer convolutional neural network to discriminate BAC from non-BAC and apply a pixelwise, patch-based procedure for BAC detection. To assess the performance of the system, we conduct a reader study to provide ground-truth information using the consensus of human expert radiologists. We evaluate the performance using a set of 840 full-field digital mammograms from 210 cases, using both free-response receiver operating characteristic (FROC) analysis and calcium mass quantification analysis. The FROC analysis shows that the deep learning approach achieves a level of detection similar to the human experts. The calcium mass quantification analysis shows that the inferred calcium mass is close to the ground truth, with a linear regression between them yielding a coefficient of determination of 96.24%. Taken together, these results suggest that deep learning can be used effectively to develop an automated system for BAC detection in mammograms to help identify and assess patients with cardiovascular risks.","0278-0062;02780062","","10.1109/TMI.2017.2655486","10.13039/100000001 - National Science Foundation; 10.13039/100000050 - National Heart, Lung, and Blood Institute (Bethesda, MD) to CI and SM; 10.13039/100006785 - Google Faculty Research Award under Grant; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827150","Breast arterial calcification (BAC);coronary artery disease;deep learning;mammography","Arteries;Breast;Calcium;Diseases;Machine learning;Mammography;Neural networks","cardiovascular system;diseases;learning (artificial intelligence);mammography;medical image processing;neural nets","12-layer convolutional neural network;BAC detection;FROC analysis;breast arterial calcifications;calcium mass quantification analysis;cardiovascular disease detection;coronary artery disease;death;deep learning;digital mammograms;patch based procedure","","","","","","","20170119","May 2017","","IEEE","IEEE Journals & Magazines"
"A computational approach to relative aesthetics","V. Gattupalli; P. S. Chandakkar; Baoxin Li","School of Computing, Informatics and Decision Systems Engineering, Arizona State University, United States of America","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2446","2451","Computational visual aesthetics has recently become an active research area. Existing state-of-art methods formulate this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image retrieval and enhancement, it is more important to rank images based on their aesthetic quality instead of binary-categorizing them. Furthermore, in such applications, it may be possible that all images belong to the same category. Hence determining the aesthetic ranking of the images is more appropriate. To this end, we formulate a novel problem of ranking images with respect to their aesthetic quality. We construct a new dataset of image pairs with relative labels by carefully selecting images from the popular AVA dataset. Unlike in aesthetics classification, there is no single threshold which would determine the ranking order of the images across our entire dataset. We propose a deep neural network based approach that is trained on image pairs by incorporating principles from relative learning. Results show that such relative training procedure allows our network to rank the images with a higher accuracy than a state-of-art network trained on the same set of images using binary labels.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7900003","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900003","","Convolutional codes;Data models;Image color analysis;Neural networks;Semantics;Training;Visualization","data analysis;medical image processing;neural nets","AVA dataset;active research area;aesthetic quality;aesthetic ranking;binary classification task;binary labels;binary-category;computational approach;deep neural network based approach;image enhancement;image pairs;image retrieval;ranking images;relative aesthetics;relative labels;relative learning;relative training procedure","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Fast Fully Automatic Segmentation of the Severely Abnormal Human Right Ventricle from Cardiovascular Magnetic Resonance Images Using a Multi-Scale 3D Convolutional Neural Network","A. Giannakidis; K. Kamnitsas; V. Spadotto; J. Keegan; G. Smith; B. Glocker; D. Rueckert; S. Ernst; M. A. Gatzoulis; D. J. Pennell; S. Babu-Narayan; D. N. Firmin","NIHR Cardiovascular Biomed. Res. Unit, R. Brompton Hosp., London, UK","2016 12th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","20170424","2016","","","42","46","Cardiac magnetic resonance (CMR) is regarded as the reference examination for cardiac morphology in tetralogy of Fallot (ToF) patients allowing images of high spatial resolution and high contrast. The detailed knowledge of the right ventricular anatomy is critical in ToF management. The segmentation of the right ventricle (RV) in CMR images from ToF patients is a challenging task due to the high shape and image quality variability. In this paper we propose a fully automatic deep learning-based framework to segment the RV from CMR anatomical images of the whole heart. We adopt a 3D multi-scale deep convolutional neural network to identify pixels that belong to the RV. Our robust segmentation framework was tested on 26 ToF patients achieving a Dice similarity coefficient of 0.8281±0.1010 with reference to manual annotations performed by expert cardiologists. The proposed technique is also computationally efficient, which may further facilitate its adoption in the clinical routine.","","Electronic:978-1-5090-5698-9; POD:978-1-5090-5699-6","10.1109/SITIS.2016.16","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907443","3D convolutional neural network;cardiavascular magnetic resonance;deep learning;right ventricle;segmentation;tetralogy of Fallot","Heart;Image segmentation;Magnetic resonance;Manuals;Neurons;Three-dimensional displays;Training","biomedical MRI;image segmentation;learning (artificial intelligence);medical image processing;neural nets","CMR;Dice similarity coefficient;ToF patients;cardiovascular magnetic resonance images;deep learning-based framework;fast fully automatic segmentation;human right ventricle;image quality variability;multiscale 3D convolutional neural network;right ventricle;tetralogy of Fallot patients","","","","","","","","Nov. 28 2016-Dec. 1 2016","","IEEE","IEEE Conference Publications"
"Severity grading of psoriatic plaques using deep CNN based multi-task learning","A. Pal; A. Chaturvedi; U. Garain; A. Chandra; R. Chatterjee","CVPR Unit, Indian Statistical Unit, Kolkata 700108, West Bengal, India","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1478","1483","This paper addresses the problem of automatic machine analysis based severity scoring of psoriasis skin disease. Three different disease parameters namely, erythema, scaling and induration are considered for such severity grading. Given an image containing a psoriatic plaque the task is to predict severity scores for all the three parameters. This paper presents a novel deep CNN based architecture for achieving the task. Apart from viewing this task as three different single task learning (STL) problems (i.e. three different classification problems), a new multi-task learning (MTL) is also presented where the three classification tasks are treated as interdependent and thereby the neural net is trained accordingly. A new annotated dataset consisting of seven hundred and seven (707) images has been constructed on which the performance of the severity scoring algorithms have been reported. Several competing baselines are considered to compare the performance of STL and MTL approaches. Experimental result shows that the deep CNN based architectures (both the STL and MTL) achieve promising performances, MTL producing slightly superior results to that of STL.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899846","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899846","","Computer architecture;Convolution;Diseases;Drugs;Estimation;Kernel;Skin","diseases;image classification;learning (artificial intelligence);medical image processing;neural net architecture","MTL;automatic machine analysis;deep CNN based architecture;deep CNN based multitask learning;disease parameters;erythema;image classification tasks;induration;neural net;psoriasis skin disease;psoriatic plaque severity grading;scaling;severity scoring algorithms;single task learning problem","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep learning for magnification independent breast cancer histopathology image classification","N. Bayramoglu; J. Kannala; J. Heikkilä","Center for Machine Vision and Signal Analysis, University of Oulu, Finland","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2440","2445","Microscopic analysis of breast tissues is necessary for a definitive diagnosis of breast cancer which is the most common cancer among women. Pathology examination requires time consuming scanning through tissue images under different magnification levels to find clinical assessment clues to produce correct diagnoses. Advances in digital imaging techniques offers assessment of pathology images using computer vision and machine learning methods which could automate some of the tasks in the diagnostic pathology workflow. Such automation could be beneficial to obtain fast and precise quantification, reduce observer variability, and increase objectivity. In this work, we propose to classify breast cancer histopathology images independent of their magnifications using convolutional neural networks (CNNs). We propose two different architectures; single task CNN is used to predict malignancy and multi-task CNN is used to predict both malignancy and image magnification level simultaneously. Evaluations and comparisons with previous results are carried out on BreaKHis dataset. Experimental results show that our magnification independent CNN approach improved the performance of magnification specific model. Our results in this limited set of training data are comparable with previous state-of-the-art results obtained by hand-crafted features. However, unlike previous methods, our approach has potential to directly benefit from additional training data, and such additional data could be captured with same or different magnification levels than previous data.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7900002","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900002","","Breast cancer;Databases;Microscopy;Pathology;Training;Training data","cancer;computer vision;image classification;learning (artificial intelligence);medical image processing;neural nets","BreaKHis dataset;breast tissues;computer vision;convolutional neural networks;deep learning;diagnostic pathology workflow;digital imaging techniques;image classification;machine learning;magnification independent breast cancer histopathology image classification;microscopic analysis;multitask CNN;single task CNN","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Skin lesion segmentation in clinical images using deep learning","M. H. Jafari; N. Karimi; E. Nasr-Esfahani; S. Samavi; S. M. R. Soroushmehr; K. Ward; K. Najarian","Department of Electrical and Computer Engineering, Isfahan University of Technology, 84156-83111 Iran","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","337","342","Melanoma is the most aggressive form of skin cancer and is on rise. There exists a research trend for computerized analysis of suspicious skin lesions for malignancy using images captured by digital cameras. Analysis of these images is usually challenging due to existence of disturbing factors such as illumination variations and light reflections from skin surface. One important stage in diagnosis of melanoma is segmentation of lesion region from normal skin. In this paper, a method for accurate extraction of lesion region is proposed that is based on deep learning approaches. The input image, after being preprocessed to reduce noisy artifacts, is applied to a deep convolutional neural network (CNN). The CNN combines local and global contextual information and outputs a label for each pixel, producing a segmentation mask that shows the lesion region. This mask will be further refined by some post processing operations. The experimental results show that our proposed method can outperform the existing state-of-the-art algorithms in terms of segmentation accuracy.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899656","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899656","Melanoma;convolutional neural network;deep learning;medical image segmentation;skin cancer","Feature extraction;Image segmentation;Lesions;Lighting;Machine learning;Malignant tumors;Skin","cancer;convolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets","CNN;clinical images;computerized analysis;convolutional neural network;deep learning approaches;digital cameras;global contextual information;illumination variations;lesion region extraction;light reflections;local contextual information;melanoma;noisy artifacts reduction;normal skin;post processing operations;segmentation accuracy;segmentation mask;skin cancer;skin lesion segmentation","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Hybrid deep learning for Reflectance Confocal Microscopy skin images","P. Kaur; K. J. Dana; G. O. Cula; M. C. Mack","Department of Electrical and Computer Engineering, Rutgers University, NJ, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1466","1471","Reflectance Confocal Microscopy (RCM) is used for evaluation of human skin disorders and the effects of skin treatments by imaging the skin layers at different depths. Traditionally, clinical experts manually categorize the images captured into different skin layers. This time-consuming labeling task impedes the convenient analysis of skin image datasets. In recent automated image recognition tasks, deep learning with convolutional neural nets (CNN) has achieved remarkable results. However in many clinical settings, training data is often limited and insufficient for CNN training. For recognition of RCM skin images, we demonstrate that a CNN trained on a moderate size dataset leads to low accuracy. We introduce a hybrid deep learning approach which uses traditional texton-based feature vectors as input to train a deep neural network. This hybrid method uses fixed filters in the input layer instead of tuned filters, yet superior performance is achieved. Our dataset consists of 1500 images from 15 RCM stacks belonging to six different categories of skin layers. We show that our hybrid deep learning approach performs with a test accuracy of 82% compared with 51% for CNN. We also compare the results with additional proposed methods for RCM image recognition and show improved accuracy.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899844","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899844","","Epidermis;Histograms;Image recognition;Libraries;Machine learning;Neural networks","convolution;data analysis;filtering theory;image recognition;learning (artificial intelligence);medical image processing;microscopy;vectors;visual databases","CNN training;RCM;automated image recognition tasks;clinical experts;convolutional neural nets;fixed filters;human skin disorders;hybrid deep learning approach;reflectance confocal microscopy skin images;skin image datasets;skin layers;traditional texton-based feature vectors","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Cloud-based deep learning of big EEG data for epileptic seizure prediction","M. P. Hosseini; H. Soltanian-Zadeh; K. Elisevich; D. Pompili","Dept. of Electrical and Computer Engineering, Rutgers University-New Brunswick, NJ, USA","2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20170424","2016","","","1151","1155","Developing a Brain-Computer Interface (BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.","","Electronic:978-1-5090-4545-7; POD:978-1-5090-4546-4; USB:978-1-5090-4544-0","10.1109/GlobalSIP.2016.7906022","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906022","Big Data;Brain-Computer Interface;Cloud Computing;Deep Learning;EEG;Epilepsy;Seizure Prediction","Big Data;Cloud computing;Electrodes;Electroencephalography;Epilepsy;Feature extraction;Real-time systems","Big Data;biomedical electrodes;brain-computer interfaces;cloud computing;data analysis;data reduction;electroencephalography;feature extraction;medical signal processing;signal classification;unsupervised learning","Big EEG Data analysis;benchmark clinical dataset;brain data recording;brain-computer interface;classification accuracy;cloud-based BCI system;cloud-based deep learning approach;cloud-computing;communication bandwidth;dimensionality-reduction technique;epileptic patients;epileptic seizure prediction;implanted electrodes;manual feature extraction;nonstationary EEG signals;stacked autoencoder;unsupervised feature extraction","","","","","","","","7-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Lossless compression of curated erythrocyte images using deep autoencoders for malaria infection diagnosis","H. Shen; W. David Pan; Y. Dong; M. Alim","Dept. of Electrical and Computer Engineering, University of Alabama in Huntsville, Huntsville, AL 35899, USA","2016 Picture Coding Symposium (PCS)","20170424","2016","","","1","5","While autoencoders have been used as an unsupervised machine learning technique for classification and dimensionality reduction of the input data, they are lossy in nature when used alone in data compression. In this work, we proposed an image coding scheme by using stacked autoencoders, where the reconstruction residuals were entropy-coded to achieve lossless compression. As a case study, we compressed labeled red blood cell images from a database curated by pathologists for malaria infection diagnosis. Specifically, we trained two separate stacked autoencoders to automatically learn the discriminative features from input images of infected and non-infected cells. Subsequently, the residuals of these two classes of images were coded by two independent Golomb-Rice encoders. Testing results showed that this deep learning approach provided remarkably higher compression on average than several other lossless coding methods including JPEG-LS, JPEG 2000 lossless mode, and CALIC.","","Electronic:978-1-5090-5966-9; POD:978-1-5090-5967-6","10.1109/PCS.2016.7906393","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906393","","Decoding;Diseases;Feature extraction;Image coding;Image reconstruction;Red blood cells;Training","cellular biophysics;data compression;data reduction;entropy codes;image classification;image coding;image reconstruction;medical image processing;unsupervised learning","CALIC;JPEG 2000 lossless mode;JPEG-LS;curated erythrocyte image lossless compression;data compression;deep autoencoders;deep learning approach;discriminative feature learning;entropy code;image coding scheme;independent Golomb-Rice encoders;infected cells;input data classification;input data dimensionality reduction;labeled red blood cell image compression;lossless coding methods;malaria infection diagnosis;noninfected cells;reconstruction residuals;stacked autoencoders;unsupervised machine learning technique","","","","","","","","4-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"A temporal deep learning approach for MR perfusion parameter estimation in stroke","K. C. Ho; F. Scalzo; K. V. Sarma; S. El-Saden; C. W. Arnold","Medical Imaging Informatics Group, Department of Radiological Sciences, University of California Los Angeles, 90024, USA","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1315","1320","Perfusion magnetic resonance (MR) images are often used in the assessment of acute ischemic stroke to distinguish between salvageable tissue and infarcted core. Deconvolution methods such as singular value decomposition have been used to approximate model-based perfusion parameters from these images. However, studies have shown that these existing deconvolution algorithms can introduce distortions that may negatively influence the utility of these parameter maps. There is limited previous work on utilizing machine learning algorithms to estimate perfusion parameters. In this work, we present a novel bi-input convolutional neural network (bi-CNN) to approximate four perfusion parameters without using an explicit deconvolution method. These bi-CNNs produced good approximations for all four parameters, with relative average root-mean-square errors (ARMSEs) ≤ 5% of the maximum values. We further demonstrate the utility of the estimated perfusion maps for quantifying the salvageable tissue volume in stroke, with more than 80% agreement with the ground truth. These results show that deep learning techniques are a promising tool for perfusion parameter estimation without requiring a standard deconvolution process.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899819","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899819","","Biological neural networks;Biological tissues;Convolution;Deconvolution;Estimation;Imaging;Parameter estimation","approximation theory;biological tissues;biomedical MRI;deconvolution;feedforward neural nets;haemorheology;learning (artificial intelligence);medical signal processing;parameter estimation;singular value decomposition","MR perfusion parameter estimation;acute ischemic stroke assessment;bi-CNN;bi-input convolutional neural network;infarcted core;model-based perfusion parameters;perfusion magnetic resonance images;perfusion parameter approximation;relative average root-mean-square errors;salvageable tissue volume;singular value decomposition;temporal deep learning","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep learning analytics for diagnostic support of breast cancer disease management","T. He; M. Puppala; R. Ogunti; J. J. Mancuso; X. Yu; S. Chen; J. C. Chang; T. A. Patel; S. T. C. Wong","Systems Medicine and Bioengineering Department of Houston Methodist Research Institute and Informatics Development Department of Houston Methodist Hospital, Houston, TX 77030 USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","365","368","Breast cancer continues to be one of the leading causes of cancer death among women. Mammogram is the standard of care for screening and diagnosis of breast cancer. The American College of Radiology developed the Breast Imaging Reporting and Data System (BI-RADS) lexicon to standardize mammographic reporting to assess cancer risk and facilitate biopsy decision-making. However, because substantial inter-observer variability remains in the application of the BI-RADS lexicon, including inappropriate term usage and missing data, current biopsy decision-making accuracy using the unstructured free text or semi-structured reports varies greatly. Hence, incorporating novel and accurate technique into breast cancer decision-making data is critical. Here, we combined natural language processing and deep learning methods to develop an analytic model that targets well-characterized and defined specific breast suspicious patient subgroups rather than a broad heterogeneous group for diagnostic support of breast cancer management.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897281","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897281","","Biological system modeling;Biopsy;Breast cancer;Feature extraction;Maximum likelihood detection;Nonlinear filters","cancer;decision making;learning (artificial intelligence);mammography;medical computing;natural language processing","BI-RADS lexicon;Breast Imaging Reporting and Data System;breast cancer decision-making data;breast cancer diagnosis;breast cancer disease management;breast cancer screening;cancer risk;decision-making accuracy;deep learning analytics;deep learning method;diagnostic support;natural language processing","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Low quality dermal image classification using transfer learning","M. S. Elmahdy; S. S. Abdeldayem; I. A. Yassine","Systems and Biomedical Department, Faculty of Engineering, Cairo University. Giza, Egypt","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","373","376","In this study, we investigate three class skin lesion classification problem of a low quality and small size dataset using transfer learning using AlexNet deep Convolutional Neural Network (CNN). Our approach involves modifying the pre-trained AlexNet model; through replacing the decision layer to be compatible with our three class problem. In addition, we propose adding two dropout layers to overcome the over fitting problem. The fine tuning process of the complete network, based on stochastic gradient descent, is performed using skin lesion dataset. Furthermore, we investigated augmenting the original dataset through three flipping directions and sixteen rotation angles processes using a new methodology. The proposed algorithm has been compared with a hand crafted features, based on Local Binary Pattern (LBP) representation followed by Support Vector Machine (SVM) classifier. Increasing the dataset size has dramatically boosted the performance of classifiers achieving accuracy of 98.67% for the modified AlexNet compared to 96.8% using the LBP based system.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897283","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897283","","Diseases;Feature extraction;Lesions;Neural networks;Skin;Support vector machines;Tuning","biomedical optical imaging;convolutional codes;image classification;medical image processing;neural nets;skin;support vector machines","AlexNet deep convolutional neural network;CNN;LBP;SVM;class skin lesion classification;local binary pattern representation;low quality dermal image classification;stochastic gradient descent;support vector machine classifier;transfer learning","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Predicting heart rejection using histopathological whole-slide imaging and deep neural network with dropout","L. Tong; R. Hoffman; S. R. Deshpande; M. D. Wang","Dept. of Biomedical Engineering, Georgia Institute of Technology and Emory University, Atlanta, GA 30332, USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","1","4","Cardiac allograft rejection is one major limitation for long-term survival for patients with heart transplants. The endomyocardial biopsy is one gold standard to screen heart rejection for patients that have heart transplantation. However, manual identification of heart rejection is expensive and time-consuming. With the development of imaging processing techniques and machine learning tools, automatic prediction of heart rejection using whole-slide images is one promising approach to improve the care of patients with heart transplants. In this paper, we first develop a histopathological whole-slide image processing pipeline to extract features automatically. Then, we construct deep neural networks with and without regularization and dropout to classify the patients into non-rejection and rejection respectively. Our results show that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracies.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897190","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897190","","Biopsy;Cost function;Feature extraction;Heart;Neural networks;Testing;Training","biomedical optical imaging;cardiology;diseases;feature extraction;learning (artificial intelligence);medical image processing;pipeline processing;prosthetics","cardiac allograft rejection;classification algorithms;deep neural network;endomyocardial biopsy;feature extraction;heart rejection;heart transplants;histopathological whole-slide imaging;machine learning","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Automatic polyp detection in endoscopy videos: A survey","B. Taha; N. Werghi; J. Dias","Department of Electrical and Computer Engineering, Khalifa University, UAE","2017 13th IASTED International Conference on Biomedical Engineering (BioMed)","20170406","2017","","","233","240","Early detection of polyps play an essential role for the prevention of colorectal cancer. Manual clinical inspection have many limitations and could result to either false or missed polyps. Computer aided diagnosis system has been used to help the medical expert and to provide more accurate diagnosis. Since their introduction, many types of algorithms have been proposed in the literature using different types of features and classifiers. This paper provides a state-of-the-art for the automatic detection of polyps using endoscopic videos. Given the increasing evolution of medical imaging technologies and algorithms, it is important to have a recent review in order to know the current state of the art, and the opportunities for improving existing algorithms, or developing innovative ones. The paper divides the work done on this research area according to the type of features and classification methods implemented. The features have been divided into shape, texture or fusion features. Future directions and challenges for more accurate polyp detection in endoscopy videos are also discussed.","","Electronic:978-0-88986-990-5; POD:978-1-5090-4908-0","10.2316/P.2017.852-031","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893296","Deep learning;Endoscopy videos;Feature fusion;Polyp detection;Shape features;Texture features","Biomedical imaging;Computers;Endoscopes;Histograms;Image segmentation;Support vector machines;Videos","","","","","","","","","","20-21 Feb. 2017","","IEEE","IEEE Conference Publications"
"Medic: An artificially intelligent system to provide healthcare services to society and medical assistance to doctors","S. Jayawant","","2016 International Conference on Communication and Electronics Systems (ICCES)","20170330","2016","","","1","6","Since the last decade, number of applications of Artificial Intelligence in daily livelihood has drastically increased. This is primarily because of the inclusion of high-tech gadgets in our day-to-day lives. These gadgets provide high computational capabilities and geographical reach. These two features could be exerted to provide medical services to the society. This paper is based on a project which emphasizes on creating a software infrastructure which would provide healthcare services like diagnosis of diseases, advising medical tests to patients, providing medical prescription to patients by making use of personalized medicine problem solving algorithms etc., and providing medical assistance to doctors. This project, Medic, makes use of natural language processing, fuzzy logic, deep learning and a constantly evolving knowledge base to correctly diagnose diseases. It also provides various services to doctors which would help them while making decisions regarding any patient's medical treatment.","","DVD:978-1-5090-1064-6; Electronic:978-1-5090-1066-0; POD:978-1-5090-1067-7; Paper:978-1-5090-1065-3","10.1109/CESYS.2016.7889878","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889878","Artificial Intelligence;Artificial neural networks;Convolution neural networks;Deep learning;Image recognition;Natural language processing","Biological neural networks;Convolution;Diseases;Medical diagnostic imaging;Natural language processing","diseases;fuzzy logic;health care;image recognition;learning (artificial intelligence);medical information systems;natural language processing;problem solving","Medic;artificially intelligent system;constantly evolving knowledge base;decision making;deep learning;disease diagnosis;fuzzy logic;healthcare services;high-tech gadgets;image recognition;medical assistance;medical prescription;medical services;medical tests;medical treatment;natural language processing;patient treatment;personalized medicine problem solving algorithms;software infrastructure","","","","","","","","21-22 Oct. 2016","","IEEE","IEEE Conference Publications"
"Multimodal learning using convolution neural network and Sparse Autoencoder","Tien Duong Vu; Hyung-Jeong Yang; V. Q. Nguyen; A-Ran Oh; Mi-Sun Kim","Department of Electronics and Computer Engineering, Chonnam National University, Gwangju, South Korea","2017 IEEE International Conference on Big Data and Smart Computing (BigComp)","20170320","2017","","","309","312","In the last decade, pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease (AD) have been the subject of extensive research. Deep learning has recently been a great interest in AD classification. Previous works had done almost on single modality dataset, such as Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET), shown high performances. However, identifying the distinctions between Alzheimer's brain data and healthy brain data in older adults (age > 75) is challenging due to highly similar brain patterns and image intensities. The corporation of multimodalities can solve this issue since it discovers and uses the further complementary of hidden biomarkers from other modalities instead of only one, which itself cannot provide. We therefore propose a deep learning method on fusion multimodalities. In details, our approach includes Sparse Autoencoder (SAE) and convolution neural network (CNN) train and test on combined PET-MRI data to diagnose the disease status of a patient. We focus on advantages of multimodalities to help providing complementary information than only one, lead to improve classification accuracy. We conducted experiments in a dataset of 1272 scans from ADNI study, the proposed method can achieve a classification accuracy of 90% between AD patients and healthy controls, demonstrate the improvement than using only one modality.","","Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9","10.1109/BIGCOMP.2017.7881683","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881683","Alzheimer's disease;MRI;PET;autoencoder;convolutional neural network;deep learning","Biological neural networks;Convolution;Feature extraction;Magnetic resonance imaging;Positron emission tomography;Support vector machines;Three-dimensional displays","biomedical MRI;brain;convolution;diseases;image classification;image coding;image fusion;learning (artificial intelligence);medical image processing;neural nets;positron emission tomography","Alzheimer disease diagnosis;convolution neural network;deep learning method;image fusion multimodalities;magnetic resonance imaging;multimodal learning method;neuroimaging data;pattern recognition methods;positron emission tomography;sparse autoencoder","","","","","","","","13-16 Feb. 2017","","IEEE","IEEE Conference Publications"
"Automated blood vessel segmentation based on de-noising auto-encoder and neural network","Z. Fan; J. J. Mo","Guangdong Key Laboratory of Digital Signal and Image Processing, Shantou University, Shantou 515063, China","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170309","2016","2","","849","856","Retinal vessel segmentation has been widely used for screening, diagnosis and treatment of cardiovascular and ophthalmologic diseases. In this paper, we propose an automated approach for vessel segmentation in digital retinal images based on de-noising auto-encoders layer-wise initialized neural networks. The proposed method utilized a deep neural network, which is layer-wise initialized by de-noising auto-encoders and fine-tuned by BP algorithm, to segment vessel structures in retinal images. The proposed method is very competitive with the state-of-the-art methods. It achieves an average accuracy of 0.9612, 0.9614, 0.6761, sensitivity of 0.7814, 0.7234, 0.9702, and specificity of 0.9788, 0.9799, 0.9702 on 3 public databases DRIVE, STARE, and CHASE_DB1 respectively. The proposed method is promising for automated blood vessel segmentation.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7872998","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872998","Denoising auto-encoders;Neural networks;Retinal images;Vessel segmentation","Biological neural networks;Databases;Feature extraction;Image segmentation;Neurons;Retina;Training","backpropagation;cardiovascular system;diseases;image denoising;medical image processing;neural nets","BP algorithm;STARE;autoencoder denoising;automated blood vessel segmentation;cardiovascular;deep neural networks;digital retinal images;ophthalmologic diseases;public databases DRIVE;retinal vessel segmentation","","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Simultaneous reconstruction and restoration of sparsely sampled optical coherence tomography image through learning separable filters for deep architectures","S. P. K. Karri; N. Garai; D. Nawn; S. Ghosh; D. Chakraborty; J. Chatterjee","IIT Kharagpur, Kharagpur, India 721302","2016 IEEE Students&#8217; Technology Symposium (TechSym)","20170309","2016","","","52","55","Spectral domain optical coherence tomography (SD-OCT) is widely employed across ophthalmology practices for visual investigation of live tissues. The involuntary movements of subjects frequently infuse motion artifacts to SD-OCT images. Sub-sampling of signals is introduced in imaging protocol to avoid such artifacts which causes fall in spatial resolution and peak signal to noise ratio (PSNR). Sparse coding (SC) is opted for restoration and rectification of complete signals from sparse samples through constructing complete and sparse space dictionaries independently. Convolutional neural networks (CNN) can be casted as SC for jointly learning dictionaries resulting less number of CNN filters (equivalence of SC dictionaries) to be trained. The proposed approach extends the separable filters to CNN through architectural constrain. This results in a parallel architecture and reduced number of parameters without compromising on performance. The approach scaled down trainable parameters by 46% with a trade-off of 0.108 PSNR during training and 0.107 PSNR during testing in comparison to conventional CNN.","","Electronic:978-1-5090-5163-2; POD:978-1-5090-5164-9","10.1109/TechSym.2016.7872654","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872654","convolutional neural network;denoising;image restoration;optical coherence tomography;separable filters;sparse coding","Convolution;Dictionaries;High definition video;Image restoration;Optical coherence tomography;Testing;Training","biological tissues;image restoration;medical image processing;neural nets;optical tomography","SD-OCT images;convolutional neural networks;deep architectures;learning separable filters;live tissues;ophthalmology;simultaneous image reconstruction;simultaneous image restoration;sparse coding;sparsely sampled optical coherence tomography image;spatial resolution;spectral domain optical coherence tomography","","","","","","","","Sept. 30 2016-Oct. 2 2016","","IEEE","IEEE Conference Publications"
"A deep learning network for right ventricle segmentation in short-axis MRI","G. Luo; R. An; K. Wang; S. Dong; H. Zhang","Harbin Institute of Technology, Harbin, China","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","485","488","The segmentation of the right ventricle (RV) myocardium on MRI is a prerequisite step for the evaluation of RV structure and function, which is of great importance in the diagnose of most cardiac diseases, such as pulmonary hypertension, congenital heart disease, coronary heart disease, and dysplasia. However, RV segmentation is considered challenging, mainly because of the complex crescent shape of the RV across slices and phases. Hence this study aims to propose a new approach to segment RV endocardium and epicardium based on deep learning. The proposed method contains two subtasks: (1) localizing the region of interest (ROI), the biventricular region which contains more meaningful features and can facilitate the RV segmentation, and (2) segmenting the RV myocardium based on the localization. The two subtasks are integrated into a joint task learning framework, in which each task is solved via two multilayer convolutional neural networks. The experiments results show that the proposed method has big potential to be further researched and applied in clinical diagnosis.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","10.23919/CIC.2016.7868785","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868785","","Cardiac disease;Heart;Image segmentation;Machine learning;Magnetic resonance imaging;Neural networks;Training","biomedical MRI;cardiology;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets","RV endocardium;RV epicardium;RV function;RV structure;biventricular region;cardiac diseases;clinical diagnosis;congenital heart disease;convolutional neural networks;coronary heart disease;deep learning network;dysplasia;pulmonary hypertension;region-of-interest;right ventricle segmentation;short-axis MRI","","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"A combined multi-scale deep learning and random forests approach for direct left ventricular volumes estimation in 3D echocardiography","S. Dong; G. Luo; G. Sun; K. Wang; H. Zhang","Harbin Institute of Technology, Harbin, China","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","889","892","Estimation of left ventricular (LV) volumes from 3D echocardiography (3DE) is a popular clinical approach in accurate assessment of left ventricular function for the diagnosis of cardiac disease. The segmentation of 3DE volumes is a crucial step in traditional methods. Nevertheless, segmentation itself is an extremely challenging problem due to the presence of speckle noise and discontinuous edges. Therefore, direct left ventricular volumes estimation methods without the segmentation become attractive in cardiac function analysis. The aim of this paper is to present a fully learning framework to estimate the left ventricular volume in 3DE. The proposed method combined unsupervised multi-scale convolutional deep network and random forests. The multi-scale convolution deep network adopted multi-scale convolutional filters to represent features of unlabeled end-diastolic and end-systolic 3DE volumes (EDV and ESV). And then we formulated left ventricular volume estimation as a regression problem and used random forests for efficient volume estimation. The experiments results suggested that our proposed method is feasible and can achieve higher accuracy, even in case of echocardiography images with irregular geometry.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","10.23919/CIC.2016.7868886","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868886","","Active contours;Echocardiography;Image segmentation;Training;Vegetation;Volume measurement","diseases;echocardiography;image denoising;image filtering;image segmentation;medical image processing;speckle","3D echocardiography;3DE volume segmentation;cardiac disease diagnosis;cardiac function analysis;clinical approach;combined multiscale deep learning;direct left ventricular volumes estimation;discontinuous edges;echocardiography images;end-systolic 3DE volumes;fully learning framework;multiscale convolution deep network adopted multiscale convolutional filters;random forests approach;speckle noise;unlabeled end-diastolic 3DE volumes","","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Automatic segmentation of left ventricular myocardium by deep convolutional and de-convolutional neural networks","X. L. Yang; L. Gobeawan; S. Y. Yeo; W. T. Tang; Z. Z. Wu; Y. Su","Institute of High Performance Computing, A&#x2217;STAR, Singapore","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","81","84","Deep learning has been integrated into several existing left ventricle (LV) endocardium segmentation methods to yield impressive accuracy improvements. However, challenges remain for segmentation of LV epicardium due to its fuzzier appearance and complications from the right ventricular insertion points. Segmenting the myocardium collectively (i.e., endocardium and epicardium together) confers the potential for better segmentation results. In this work, we develop a computational platform based on deep learning to segment the whole LV myocardium simultaneously from a cardiac magnetic resonance (CMR) image. The deep convolutional network is constructed using Caffe platform, which consists of 6 convolutional layers, 2 pooling layers, and 1 de-convolutional layer. A preliminary result with Dice metric of 0.75±0.04 is reported on York MR dataset. While in its current form, our proposed one-step deep learning method cannot compete with state-of-art myocardium segmentation methods, it delivers promising first pass segmentation results.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","10.23919/CIC.2016.7868684","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868684","","Image segmentation;Machine learning;Magnetic resonance;Magnetic resonance imaging;Measurement;Myocardium;Neural networks","biomedical MRI;cardiology;image segmentation;medical image processing;neural nets","Caffe platform;automatic left ventricular myocardium segmentation;cardiac magnetic resonance image;deconvolutional neural network;deep convolutional neural network;deep learning;left ventricle endocardium segmentation method;right ventricular insertion","","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"A left ventricular segmentation method on 3D echocardiography using deep learning and snake","S. Dong; G. Luo; G. Sun; K. Wang; H. Zhang","Harbin Institute of Technology, Harbin, China","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","473","476","Segmentation of left ventricular (LV) endocardium from 3D echocardiography is important for clinical diagnosis because it not only can provide some clinical indices (e.g. ventricular volume and ejection fraction) but also can be used for the analysis of anatomic structure of ventricle. In this work, we proposed a new full-automatic method, combining the deep learning and deformable model, for the segmentation of LV endocardium. We trained convolutional neural networks to generate a binary cuboid to locate the region of interest (ROI). And then, using ROI as the input, we trained stacked autoencoder to infer the LV initial shape. At last, we adopted snake model initiated by inferred shape to segment the LV endocardium. In the experiments, we used 3DE data, from CETUS challenge 2014 for training and testing by segmentation accuracy and clinical indices. The results demonstrated the proposed method is accuracy and efficiency respect to expert's measurements.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","10.23919/CIC.2016.7868782","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868782","","Deformable models;Echocardiography;Image segmentation;Machine learning;Shape;Three-dimensional displays;Ultrasonic imaging","echocardiography;image segmentation;learning (artificial intelligence);medical image processing;neural nets","LV endocardium;anatomic structure;autoencoder;binary cuboid;clinical diagnosis;convolutional neural networks;deep learning;deformable model;left ventricular segmentation method;region-of-interest;snake model;three dimensional echocardiography","","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"A novel left ventricular volumes prediction method based on deep learning network in cardiac MRI","G. Luo; G. Sun; K. Wang; S. Dong; H. Zhang","Harbin Institute of Technology, Harbin, China","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","89","92","Accurate estimation of left ventricle (LV) volumes plays an essential role in clinical diagnosis of cardiac diseases using MRI. Conventional methods of estimating ventricular volumes depend on the results of manual or automatic segmentation of MRI. However, manual segmentation of MRI sequences is extremely time-consuming and subjective, and automatic segmentation is still a challenging task. Therefore, this study aims to develop a new LV volumes prediction method without segmentation, motivated by deep learning technology and the large scale cardiac MRI (CMR) datasets from the second Annual Data Science Bowl (ADSB) in 2016. The experiments results shows that the predicted LV volumes have high correlation with the ground truth. These results prove that the proposed method has big potential to be researched and applied in clinical diagnosis and screening of cardiac diseases.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","10.23919/CIC.2016.7868686","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868686","","Cardiac disease;Image segmentation;Machine learning;Magnetic resonance imaging;Predictive models;Training","biomedical MRI;cardiology;diseases;learning (artificial intelligence);medical image processing;patient diagnosis","ADSB;Annual Data Science Bowl;MRI sequences;automatic MRI image segmentation;cardiac MRI dataset;cardiac disease diagnosis;deep learning network;deep learning technology;left ventricle volume estimation;left ventricular volume prediction method","","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Macrophage learning-tracking algorithm in time-lapse MR images","A. Tashita; S. Kobashi; M. Nii; Y. Mori; Y. Yoshioka; Y. Hata","Graduate School of Engineering, University of Hyogo, Japan","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170223","2016","1","","421","426","It is difficult to observe the movement of immune cells in vivo deep. However, our recent study, by using 11.7 T magnetic resonance imaging (MRI), shows that it has become possible to observe the macrophages in living brain of mouse. Macrophages are a type of immune cell. In this paper, we propose a three dimensional tracking method of macrophages in the 11.7 T time lapse MR images and consider the application of machine learning for the detection of macrophages. This method was applied to a stroke model mouse. The result showed that we are able to track the macrophages in three dimensions. We applied Support Vector Machine (SVM) for the detection of macrophages. The proposed method and the two-dimensional tracking method were applied to the artificial data. The results showed that SVM have a good success to detect macrophages.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7860938","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860938","Hungarian algorithm;Immune system;Machine learning;Macrophage;Magnetic resonance image;Multiple object tracking;Support vector machine;Tracking","Brain modeling;Cybernetics;Magnetic resonance imaging;Mice;Support vector machines;Tracking","biomedical MRI;cellular biophysics;feature extraction;medical image processing;support vector machines","MRI;SVM;brain macrophages;immune cell movement;machine learning;macrophage detection;macrophage learning-tracking algorithm;magnetic resonance imaging;stroke model mouse;support vector machine;time-lapse MR images","","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Tracking mice face in video","B. Akkaya; Y. R. Tabar; F. Gharbalchi; İ. Ulusoy; U. Halıcı","Elektrik ve Elektronik M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Orta Do&#x011F;u Teknik &#x00DC;niversitesi, Ankara, T&#x00FC;rkiye","2016 20th National Biomedical Engineering Meeting (BIYOMUT)","20170213","2016","","","1","4","Laboratory mice are frequently used in biomedical studies. Facial expressions of mice provides important data about various issues. For this reason real time tracking of mice provide output to both researcher and software that operate on face image directly. Since body and face of laboratory mice is the same color and mice moves fast, tracking of face of mice is a challenging task. In recent years, methods that uses artificial neural networks provide effective solutions to problems such as classification, decision making and object recognition due to their ability to abstract training from data. In this study, a method based on deep learning is proposed for real time tracking of face of mice and successful results are obtained. Our studies are still going on in order to improve our results obtained using a limited dataset.","","Electronic:978-1-5090-5829-7; POD:978-1-5090-5830-3","10.1109/BIYOMUT.2016.7849406","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849406","Convolutional Neural Networks;Machine Learning;Object Tracking","Art;Convolutional codes;Face;Mice;Neural networks;Real-time systems;Tracking","biomedical optical imaging;decision making;face recognition;image classification;medical image processing;object recognition;object tracking;video signal processing","abstract training;artificial neural networks;biomedical studies;decision making;deep learning;face image;facial expressions;image classification;laboratory mice;mice face tracking;object recognition;real time tracking;software","","","","","","","","3-5 Nov. 2016","","IEEE","IEEE Conference Publications"
"Mitosis detection using convolutional neural network based features","A. Albayrak; G. Bilgin","Department of Computer Engineering, Signal and Image Processing Lab. (SIMPLAB), Yildiz Technical University, 34220 Istanbul, Turkey","2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI)","20170209","2016","","","000335","000340","Breast cancer is the second leading cause of cancer death in women according to World Health Organization (WHO). Development of computer aided diagnostic (CAD) systems has great importance as a secondary reader systems for a correct diagnosis and treatment process. In this paper, a deep learning based feature extraction method by convolutional neural network (CNN) is proposed for automated mitosis detection for cancer diagnosis and grading by histopathological images. The proposed framework is tested on the MITOS data set provided for a contest on mitosis detection in breast cancer histological images released for research purposes in International Conference on Pattern Recognition (ICPR'2014). By using provided histopathological images, cellular structures are initially found by combined clustering based segmentation and blob analysis after preprocessing step. Then, obtained cellular image patches are cropped automatically from the histopathological images for feature extraction stage. CNN, which is a prominent deep learning method on image processing tasks, is utilized for extracting discriminative features. Due to the high dimensional output of the CNN, combination of PCA and LDA dimension reduction methods are performed respectively for regularization and dimension reduction process. Afterwards, a robust kernel based classifier, support vector machine (SVM), is used for final classification of mitotic and non-mitotic cells. The test results on MITOS data set prove that the proposed framework achieved promising results for mitosis detection on histopathological images.","","Electronic:978-1-5090-3909-8; POD:978-1-5090-3910-4; USB:978-1-5090-3908-1","10.1109/CINTI.2016.7846429","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846429","","Breast cancer;Clustering algorithms;Feature extraction;Image segmentation;Kernel;Neural networks","cancer;feature extraction;image segmentation;medical image processing;neural nets;pattern clustering;principal component analysis;support vector machines","CAD systems;CNN;International Conference on Pattern Recognition;LDA dimension reduction methods;MITOS data set;PCA;SVM;WHO;World Health Organization;automated mitosis detection;blob analysis;breast cancer histological images;cancer death;cancer diagnosis;cellular image patches;cellular structures;clustering based segmentation;computer aided diagnostic systems;convolutional neural network;deep learning based feature extraction;deep learning method;dimension reduction process;histopathological images;image processing tasks;nonmitotic cells;regularization;robust kernel based classifier;secondary reader systems;support vector machine","","","","","","","","17-19 Nov. 2016","","IEEE","IEEE Conference Publications"
"Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification","P. Rodriguez; G. Cucurull; J. Gonzàlez; J. M. Gonfaus; K. Nasrollahi; T. B. Moeslund; F. X. Roca","Computer Vision Center, Universitat Aut&#x00F2;noma de Barcelona, 08193 Barcelona, Spain.","IEEE Transactions on Cybernetics","","2017","PP","99","1","11","Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively, there is the need for automatic systems to measure it. In this paper, contrary to current state-of-the-art techniques in pain assessment, which are based on facial features only, we suggest that the performance can be enhanced by feeding the raw frames to deep learning models, outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline, our approach first uses convolutional neural networks (CNNs) to learn facial features from VGG_Faces, which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result, we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition, to evaluate the generalization properties of our proposed methodology on facial motion recognition, we also report competitive results in the Cohn Kanade+ facial expression database.","2168-2267;21682267","","10.1109/TCYB.2017.2662199","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849133","Affective computing;computer applications;cybercare industry applications;human factors engineering in medicine and biology;medical services;monitoring;patient monitoring computers and information processing;pattern recognition","Databases;Estimation;Face;Face recognition;Feature extraction;Hidden Markov models;Pain","","","","","","","","","20170209","","","IEEE","IEEE Early Access Articles"
"Clinical named entity recognition: Challenges and opportunities","S. R. Kundeti; J. Vijayananda; S. Mujjiga; M. Kalyan","Data Science, Philips Healthcare, Bangalore, India","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1937","1945","Information Extraction (IE), one of the important tasks in text analysis and Natural Language Processing (NLP), involves extracting meaningful pieces of knowledge from unstructured information sources, as unstructured data is computationally opaque. The intent of IE is to produce a knowledge base i.e. organize the information in a way that it is useful to people and arrange the information in a semantic way so that algorithms can make certain useful inferences from it. Named Entity Recognition (NER) is a sub-task of IE which finds and classifies the names/entities. Once these Named Entities (NE) are extracted, they can then be indexed and made searchable, relations can be derived, questions can be answered and many more. NER techniques are different for different domains, because of the uniqueness that exists in each of the domains, although the process depends on a number of fundamental Natural Language Processing (NLP) steps such as tokenization, part-of-speech tagging, parsing and model building. As an example, NER in the medical domain involves handling of a number of vital tasks such as identification of medical terms, attributes such as negation, severity, identification of relationships between entities and mapping terms in the document to concepts in domain specific ontologies. There is also a heavy dependence on domain specific resources such as medical dictionaries and ontologies such as the Unified Medical Language System (UMLS)[34]. In this paper, we focus on NER in the clinical domain. In particular, we will focus on the NER challenges and the qualitative analysis of clinical reports on the approaches we took for the named entities: anatomies, findings, location qualifier, and procedures.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840814","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840814","Deep Learning;Language Modelling;Long short-term memory;Machine Learning;Natural Language Processing;Ontologies;Text Analytics","Data mining;Dictionaries;Medical diagnostic imaging;Medical services;Ontologies;Unified modeling language","knowledge acquisition;learning (artificial intelligence);medical computing;natural language processing;ontologies (artificial intelligence);pattern classification;text analysis","IE;NER techniques;NLP;UMLS;clinical named entity recognition;domain specific ontology;entity classification;information extraction;knowledge extraction;machine learning;medical domain;medical term identification;name classification;natural language processing;term mapping;text analysis;unified medical language system;unstructured data;unstructured information sources","","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Augmented LSTM Framework to Construct Medical Self-Diagnosis Android","C. Liu; H. Sun; N. Du; S. Tan; H. Fei; W. Fan; T. Yang; H. Wu; Y. Li; C. Zhang","Baidu Res. Big Data Lab., Sunnyvale, CA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","251","260","Given a health-related question (such as ""I have a bad stomach ache. What should I do?""), a medical self-diagnosis Android inquires further information from the user, diagnoses the disease, and ultimately recommend best solutions. One practical challenge to build such an Android is to ask correct questions and obtain most relevant information, in order to correctly pinpoint the most likely causes of health conditions. In this paper, we tackle this challenge, named ""relevant symptom question generation"": Given a limited set of patient described symptoms in the initial question (e.g., ""stomach ache""), what are the most critical symptoms to further ask the patient, in order to correctly diagnose their potential problems? We propose an augmented long short-term memory (LSTM) framework, where the network architecture can naturally incorporate the inputs from embedding vectors of patient described symptoms and an initial disease hypothesis given by a predictive model. Then the proposed framework generates the most important symptom questions. The generation process essentially models the conditional probability to observe a new and undisclosed symptom, given a set of symptoms from a patient as well as an initial disease hypothesis. Experimental results show that the proposed model obtains improvements over alternative methods by over 30% (both precision and mean ordinal distance).","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0036","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837849","Deep learning;LSTM;Medical self-diagnosis Android;Relevant symptom generation","Androids;Correlation;Diseases;Electronic mail;Humanoid robots;Medical diagnostic imaging;Predictive models","Android (operating system);diseases;medical computing;patient diagnosis;probability","LSTM;augmented LSTM;augmented long short-term memory;conditional probability;disease diagnosis;disease hypothesis;health conditions;health-related question;initial disease hypothesis;mean ordinal distance;medical self-diagnosis Android;network architecture;patient described symptoms;relevant symptom question generation","","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Feature Fusion for Denoising and Sparse Autoencoders: Application to Neuroimaging Data","A. Moussavi-Khalkhali; M. Jamshidi; S. Wijemanne","Dept. of Electr. & Comput. Eng., Univ. of Texas at San Antonio, San Antonio, TX, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","605","610","Although there is no cure to date, Alzheimer's disease detection in early stages has a significant impact on the patient's life in terms of cost, the progress, and helping to plan in advance for an appropriate healthcare in the life ahead as well as providing clinical etiologies for further research. This paper discusses implementing a feature fusion method utilizing sparse and denoising autoencoders to reveal the stage of Alzheimer's disease. Four cohorts consisted of individuals with Alzheimer's disease, late mild cognitive impairment, early mild cognitive impairment, and normal control groups are classified using multinomial logistic regression fueled by the fusion of high-level and low-level features. The high-level features are extracted from the stacked autoencoders. The results show that feature fusion enhance the performance of typical autoencoders. However, the performance of feature fusion using denoising autoencoders is superior to that of the sparse training of autoencoders in terms of overall accuracy, precision, and recall.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0106","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838210","Alzheimer's disease stage detection;deep learning;feature fusion;sacked sparse autoencoders;stacked denoising autoencoders","Alzheimer's disease;Classification algorithms;Feature extraction;Magnetic resonance imaging;Noise reduction;Training","diseases;feature extraction;image coding;image denoising;image fusion;medical image processing;neurophysiology;regression analysis","Alzheimer disease detection;clinical etiologies;denoising autoencoders;feature fusion method;high-level features;low-level features;mild cognitive impairment;multinomial logistic regression;neuroimaging data;normal control groups;sparse autoencoder training;stacked autoencoders","","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automated atrial fibrillation detection based on deep learning network","C. Yuan; Y. Yan; L. Zhou; J. Bai; L. Wang","Information and communication engineering, Wuhan university of technology, Wuhan, Hubei Province China","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1159","1164","Aiming at the shorting of the existing atrial fibrillation (AF) detection algorithms and improve the ability of intelligent recognition and extraction of AF signals. Recently, deep learning theory with massive data has been used on image, voice and other filed widely. In this paper, a method based on the stack sparse autoencoder neural network, a instance of deep learning strategy, was proposed for AF detection. Greedy layer-wise training algorithms and massive unlabeled hotter data from a hospital were used to train the deep learning system, and Back Propagation algorithm and half of the MIT-BIH standard databases were applied to optimized the whole system. Another half of the standard data were used to evaluated the performance of this method. The autoencoder learns the high level features which can describe the necessary information better from the raw data The experimental results show that the accuracy of the algorithm based on stack sparse autoencoder is 98.309%, so this approach is of great significance on the real-time monitoring of atrial fibrillation signal in electrocardiogram.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7831994","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831994","AF detection;autoencoder;deep learning;intelligent recognition","Atrial fibrillation;Biological neural networks;Databases;Electrocardiography;Feature extraction;Machine learning;Training","backpropagation;diseases;electrocardiography;feature extraction;graph theory;medical signal detection;network theory (graphs);neural nets;patient diagnosis","AF detection;AF signal extraction;AF signal recognition;atrial fibrillation detection;back propagation algorithm;deep learning network;electrocardiogram;greedy layer-wise training algorithm;stack sparse autoencoder neural network","","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"Measuring Patient Similarities via a Deep Architecture with Medical Concept Embedding","Z. Zhu; C. Yin; B. Qian; Y. Cheng; J. Wei; F. Wang","Xi'an Jiaotong Univ., Xi'an, China","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","749","758","Evaluating the clinical similarities between pairwise patients is a fundamental problem in healthcare informatics. Aproper patient similarity measure enables various downstream applications, such as cohort study and treatment comparative effectiveness research. One major carrier for conducting patient similarity research is the Electronic Health Records(EHRs), which are usually heterogeneous, longitudinal, and sparse. Though existing studies on learning patient similarity from EHRs have shown being useful in solving real clinical problems, their applicability is limited due to the lack of medical interpretations. Moreover, most previous methods assume a vector based representation for patients, which typically requires aggregation of medical events over a certain time period. As aconsequence, the temporal information will be lost. In this paper, we propose a patient similarity evaluation framework based on temporal matching of longitudinal patient EHRs. Two efficient methods are presented, unsupervised and supervised, both of which preserve the temporal properties in EHRs. The supervised scheme takes a convolutional neural network architecture, and learns an optimal representation of patient clinical records with medical concept embedding. The empirical results on real-world clinical data demonstrate substantial improvement over the baselines.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0086","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837899","Deep Matching;Medical Concept Embedding;Patient Similarity","Context;Diseases;Medical diagnostic imaging;Natural language processing;Neural networks","electronic health records;health care;patient care;pattern matching;unsupervised learning","EHR;clinical similarities;convolutional neural network architecture;deep architecture;electronic health records;healthcare informatics;medical concept embedding;medical interpretations;patient similarity evaluation;patient similarity measurement;patient similarity research;real-world clinical data;temporal longitudinal patient EHR matching;vector based representation","","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Residual Deconvolutional Networks for Brain Electron Microscopy Image Segmentation","A. Fakhry; T. Zeng; S. Ji","Department of Computer Science, Old Dominion University, Norfolk, VA, USA","IEEE Transactions on Medical Imaging","20170201","2017","36","2","447","456","Accurate reconstruction of anatomical connections between neurons in the brain using electron microscopy (EM) images is considered to be the gold standard for circuit mapping. A key step in obtaining the reconstruction is the ability to automatically segment neurons with a precision close to human-level performance. Despite the recent technical advances in EM image segmentation, most of them rely on hand-crafted features to some extent that are specific to the data, limiting their ability to generalize. Here, we propose a simple yet powerful technique for EM image segmentation that is trained end-to-end and does not rely on prior knowledge of the data. Our proposed residual deconvolutional network consists of two information pathways that capture full-resolution features and contextual information, respectively. We showed that the proposed model is very effective in achieving the conflicting goals in dense output prediction; namely preserving full-resolution predictions and including sufficient contextual information. We applied our method to the ongoing open challenge of 3D neurite segmentation in EM images. Our method achieved one of the top results on this open challenge. We demonstrated the generality of our technique by evaluating it on the 2D neurite segmentation challenge dataset where consistently high performance was obtained. We thus expect our method to generalize well to other dense output prediction problems.","0278-0062;02780062","","10.1109/TMI.2016.2613019","10.13039/100000153 - National Science Foundation, Old Dominion University, and Washington State University; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575638","Brain circuit reconstruction;deconvolutional networks;deep learning;electron microscopy;image segmentation;residual learning","Convolution;Deconvolution;Feature extraction;Image reconstruction;Image segmentation;Predictive models;Three-dimensional displays","brain;deconvolution;electron microscopy;image segmentation;medical image processing;neural nets;neurophysiology","3D neurite segmentation;EM image segmentation;anatomical connection reconstruction;brain electron microscopy;circuit mapping;contextual information;dense output prediction;electron microscopy images;full-resolution features;full-resolution predictions;hand-crafted features;human-level performance;information pathways;neurite segmentation challenge dataset;neurons;residual deconvolutional networks","","","","","","","20160923","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Extraction of GGO candidate regions from the LIDC database using deep learning","K. Hirayama; J. K. Tan; H. Kim","Kyushu Institute of Technology, 1-1, Sensui, Tobata, Kitakyushu 804-8550, Japan","2016 16th International Conference on Control, Automation and Systems (ICCAS)","20170126","2016","","","724","727","In recent years, development of the computer-aided diagnosis (CAD) systems for the purpose of reducing the false positive on visual screening and improving accuracy of lesion detection has been advanced. Lung cancer is the leading cause of cancer death in the world. Among them, GGO (Ground Glass Opacity) that exhibited early in the before cancer lesion and carcinoma in situ shows a pale concentration, have been concerned about the possibility of undetected on the screening. In this paper, we propose an automatic extraction method of GGO candidate regions from the chest CT image. Our proposed image processing algorithms is consist of four main steps; (1) segmentation of volume of interest from the chest CT image and removing the blood vessel regions, bronchus regions based on 3D line filter, (2) first detection of GGO regions based on density and gradient which is selected the initial GGO candidate regions, (3) identification of the final GGO candidate regions based on DCNN (Deep Convolutional Neural Network) algorithms. Finally, we calculates the statistical features for reducing the false-positive (FP) shadow by the rule-based method, performs identification of the final GGO candidate regions by SVM (Support Vector Machine). Our proposed method performed on to the 31 cases of the LIDC (Lung Image Database Consortium) database, and final identification performance of TP: 93.02[%], FP: 128.52[/case] are obtained respectively.","","Electronic:978-89-93215-11-3; POD:978-1-4673-9058-3; USB:978-89-93215-12-0","10.1109/ICCAS.2016.7832398","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832398","Computer Aided Diagnosis;Deep Convolutional Neural Network;Ground Glass Opacity;Lung Image Database Consortium;Support Vector Machine","Biomedical imaging;Blood vessels;Cancer;Lungs;Machine learning;Support vector machines;Three-dimensional displays","blood vessels;cancer;computerised tomography;feature extraction;feedforward neural nets;image filtering;image segmentation;learning (artificial intelligence);lung;medical image processing;opacity;statistical analysis;support vector machines;visual databases","3D line filter;DCNN algorithms;FP shadow reduction;GGO region detection;GGO region extraction;LIDC database;Lung Image Database Consortium database;SVM;automatic extraction method;blood vessel region removal;bronchus regions;cancer lesion;carcinoma;chest CT image;computer-aided diagnosis systems;deep learning;deep-convolutional neural network algorithms;false-positive shadow reduction;ground glass opacity;image processing algorithm;lung cancer;pale concentration;rule-based method;statistical features;support vector machine;volume-of-interest segmentation","","","","","","","","16-19 Oct. 2016","","IEEE","IEEE Conference Publications"
"Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images","Y. Zhang; M. T. C. Ying; L. Yang; A. T. Ahuja; D. Z. Chen","Department of Computer Science and Engineering, University of Notre Dame, IN 46556, USA","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","443","448","Ultrasound as a well-established imaging modality is widely used in imaging lymph nodes for clinical diagnosis and disease analysis. Quantitative analysis of lymph node features, morphology, and relations can provide valuable information for diagnosis and immune system studies. For such analysis, it is necessary to first accurately segment the lymph node areas in ultrasound images. In this paper, we develop a new deep learning method, called Coarse-to-Fine Stacked Fully Convolutional Nets (CFS-FCN), for automatically segmenting lymph nodes in ultrasound images. Our method consists of multiple stages of FCN modules. We train the CFS-FCN model to learn the segmentation knowledge from a coarse-to-fine, simple-to-complex manner. A data set of 80 ultrasound images containing both normal and diseased lymph nodes is used in our experiments, which show that our method considerably outperforms the state-of-the-art deep learning methods for lymph node segmentation.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822557","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822557","","Biological system modeling;Biomedical imaging;Image segmentation;Lymph nodes;Machine learning;Training;Ultrasonic imaging","biomedical ultrasonics;diseases;image segmentation;learning (artificial intelligence);medical image processing","CFS-FCN;FCN module;clinical diagnosis;coarse-to-fine stacked fully convolutional net;deep learning method;disease analysis;lymph node imaging;lymph node segmentation;ultrasound images;ultrasound imaging modality","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Size-Invariant Fully Convolutional Neural Network for vessel segmentation of digital retinal images","Y. Luo; H. Cheng; L. Yang","Machine Intelligence Institute, School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu","2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","20170119","2016","","","1","7","Vessel segmentation of digital retinal images plays an important role in diagnosis of diseases such as diabetics, hypertension and retinopathy of prematurity due to these diseases impact the retina. In this paper, a novel Size-Invariant Fully Convolutional Neural Network (SIFCN) is proposed to address the automatic retinal vessel segmentation problems. The input data of the network is the patches of images and the corresponding pixel-wise labels. A consecutive convolution layers and pooling layers follow the input data, so that the network can learn the abstract features to segment retinal vessel. Our network is designed to hold the height and width of data of each layer with padding and assign pooling stride so that the spatial information maintain and up-sample is not required. Compared with the pixel-wise retinal vessel segmentation approaches, our patch-wise segmentation is much more efficient since in each cycle it can predict all the pixels of the patch. Our overlapped SIFCN approach achieves accuracy of 0.9471, with the AUC of 0.9682. And our non-overlap SIFCN is the most efficient approach among the deep learning approaches, costing only 3.68 seconds per image, and the overlapped SIFCN costs 31.17 seconds per image.","","Electronic:978-9-8814-7682-1; POD:978-1-5090-2401-8","10.1109/APSIPA.2016.7820677","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820677","","Convolution;Diabetes;Diseases;Image segmentation;Kernel;Retinal vessels","eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets","diabetics;digital retinal images;disease diagnosis;hypertension;patch-wise segmentation;pixel-wise retinal vessel segmentation;retinopathy;size-invariant fully convolutional neural network","","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automated human physical function measurement using constrained high dispersal network with SVM-linear","Dan Meng; G. Cao; Xinyu Song; W. Chen; Wenming Cao","School of Computer Scinence and Software Engineering, East China Normal University, Shanghai, China 200062","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1520","1526","Physical measurement have been becoming increasingly helpful in monitoring the humans health status. Manual measurement of physical status is time consuming and may result in misdiagnosing, so an automatic method for identification the status of physical is urgently needed. This paper presents a novel feature extraction method based on using constrained high dispersal network for depth images and coped with Support Vector Machines (SVM) to measure human physical function. The proposed method can catch the most representative features of depth images belonging to different actions and statuses. We analyze the representation efficiency of hand-crafted features (HOG features, and LBP features), deep learning features (CNN features, and PCANet features) and our proposed deep learning features separately in order to validate the efficiency and accuracy of our proposed method. The results show superior performance of 85.19% on 3840 samples (three actions, each with four different statuses, and every status contains sixteen sequences) when the proposed deep features combined with SVM.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822747","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822747","PCA lter;Physical function measurement;SVM;deep learning;high dispersal;local normalization;multi-scale feature","Artificial neural networks;Biomedical imaging;Histograms;Manuals;Principal component analysis;Support vector machines;Training","feature extraction;image representation;learning (artificial intelligence);medical image processing;patient monitoring;support vector machines","CNN features;HOG features;LBP features;PCANet features;SVM-linear;automated human physical function measurement;constrained high dispersal network;deep learning features;depth images;feature extraction;hand-crafted features;human physical function;humans health status monitoring;representation efficiency;support vector machine","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automatic fall detection of human in video using combination of features","Kun Wang; Guitao Cao; Dan Meng; Weiting Chen; Wenming Cao","School of Computer Science and Software Engineering, EAST CHINA NORMAL UNIVERSITY, Shanghai, China 200062","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1228","1233","The problem of automatically fall detection of older people living alone is a popular research topic since falls are one of the major health hazards among the aging population aged 65 and above and the population of them in China is more than 100 million. In this paper, we present an automatic human fall detection framework based on video surveillance which can improve safety of elders in indoor environments. First, a vision component was used to detect and extract moving people in videos from static cameras. Then, we combine Histograms of Oriented Gradients(HOG),Local Binary Pattern(LBP)and feature extracted by the Deep Learning Framework Caffe to form a new augmented feature and the feature is named HLC. We use HLC to represent a person's motion state in a frame of a video sequence. Because the process of fall is a sequence of movements, we use HLC features which were extracted from continuous frames of a video sequence to implement the fall detection. With the help of the HLC feature, we achieve an average fall detection result of 93.7% sensitivity and 92.0% specificity on three different datasets.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822694","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822694","Combination of features;Fall detection;Visual surveillance","Adaptation models;Analytical models;Cameras;Feature extraction;Hazards;Legged locomotion;Software","biomechanics;biomedical optical imaging;feature extraction;geriatrics;image sequences;medical image processing;video signal processing;video surveillance","HLC feature extraction;HOG;Histograms of Oriented Gradient;automatic human fall detection;deep learning framework caffe;indoor environment;local binary pattern;static camera;video sequence;video surveillance","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"CNN-based image analysis for malaria diagnosis","Z. Liang; A. Powell; I. Ersoy; M. Poostchi; K. Silamut; K. Palaniappan; P. Guo; M. A. Hossain; A. Sameer; R. J. Maude; J. X. Huang; S. Jaeger; G. Thoma","School of Information Technology, York University, Toronto, ON, M3J1P3, Canada","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","493","496","Malaria is a major global health threat. The standard way of diagnosing malaria is by visually examining blood smears for parasite-infected red blood cells under the microscope by qualified technicians. This method is inefficient and the diagnosis depends on the experience and the knowledge of the person doing the examination. Automatic image recognition technologies based on machine learning have been applied to malaria blood smears for diagnosis before. However, the practical performance has not been sufficient so far. This study proposes a new and robust machine learning model based on a convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. In a ten-fold cross-validation based on 27,578 single cell images, the average accuracy of our new 16-layer CNN model is 97.37%. A transfer learning model only achieves 91.99% on the same images. The CNN model shows superiority over the transfer learning model in all performance indicators such as sensitivity (96.99% vs 89.00%), specificity (97.75% vs 94.98%), precision (97.73% vs 95.12%), F1 score (97.36% vs 90.24%), and Matthews correlation coefficient (94.75% vs 85.25%).","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822567","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822567","computer-aided diagnosis;convolutional neural network;deep learning;machine learning;malaria","Blood;Data models;Diseases;Machine learning;Mathematical model;Microscopy;Training","blood;cellular biophysics;diseases;image recognition;learning (artificial intelligence);medical image processing;neural nets","CNN-based image analysis;Matthews correlation coefficient;automatic image recognition technology;convolutional neural network;machine learning model;malaria diagnosis;parasite-infected red blood cell;single cell classification;single cell images","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep convolutional neural network for survival analysis with pathological images","X. Zhu; J. Yao; J. Huang","Department of Computer Science and Engineering, The University of Texas at Arlington, USA","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","544","547","Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822579","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822579","Deep learning;Lung cancer;Pathological images;Survival analysis","Analytical models;Data models;Feature extraction;Hazards;Lungs;Pathology;Predictive models","cancer;feature extraction;lung;medical image processing;neural nets","DeepConvSurv;NLST lung cancer data;cox proportional hazard model;deep convolutional neural network;hand-crafted feature;medical imaging technology;national lung screening trial;pathological image;survival analysis;survival prediction;unstructured medical image","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Combining deep learning and hand-crafted features for skin lesion classification","T. Majtner; S. Yildirim-Yayilgan; J. Y. Hardeberg","Faculty of Computer Science and Media Technology, NTNU Norwegian University of Science and Technology, Gj&#x2298;vik, Norway","2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)","20170119","2016","","","1","6","Melanoma is one of the most lethal forms of skin cancer. It occurs on the skin surface and develops from cells known as melanocytes. The same cells are also responsible for benign lesions commonly known as moles, which are visually similar to melanoma in its early stage. If melanoma is treated correctly, it is very often curable. Currently, much research is concentrated on the automated recognition of melanomas. In this paper, we propose an automated melanoma recognition system, which is based on deep learning method combined with so called hand-crafted RSurf features and Local Binary Patterns. The experimental evaluation on a large publicly available dataset demonstrates high classification accuracy, sensitivity, and specificity of our proposed approach when it is compared with other classifiers on the same dataset.","","Electronic:978-1-4673-8910-5; POD:978-1-4673-8911-2","10.1109/IPTA.2016.7821017","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821017","Convolutional Neural Network;Local Binary Patterns;RSurf Features;SVM;Skin Lesion Classification","Feature extraction;Histograms;Image color analysis;Lesions;Malignant tumors;Skin;Support vector machines","biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;skin","Local Binary Patterns;automated melanoma recognition system;benign lesions;cells;classification accuracy;deep learning method;hand-crafted RSurf features;melanocytes;moles;skin cancer;skin lesion classification","","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Cardiac left ventricular volumes prediction method based on atlas location and deep learning","G. Luo; S. Dong; K. Wang; H. Zhang","School of Computer Science and Technology, Harbin Institute of Technology, China","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1604","1610","In this paper, we proposed a novel left ventricular volumes prediction method. This method is a cascade architecture which is based on multi-scale LV atlas location and deep convolutional neural networks (CNN). Firstly, we adopted LV atlas mapping method to achieve accurate location of LV region in cardiac magnetic resonance (CMR) images. And then, the CNN were used to train an end-to-end LV volumes prediction model to achieve the direct prediction. What's more, the large number of CMR images data (1140 subjects, more than 1026000 images) make the proposed deep CNN have relatively better feature representation and robust prediction ability. The experiment results on the large-scale CMR datasets prove that the proposed method has higher accuracy than the state-of-the-art prediction methods in terms of the end-diastole volumes (EDV), the end-systole volumes (ESV), and the ejection fraction (EF). Besides, we make the proposed method open accessible to public for wide application in other biomedical image processing fields.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822759","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822759","CMR images;LV atlas;deep convolutional neural networks;volumes prediction","Fitting;Image segmentation;Kernel;Manuals","biomedical MRI;cardiology;convolution;feature extraction;image representation;learning (artificial intelligence);medical image processing;neural nets","biomedical image processing fields;cardiac left ventricular volume prediction method;cardiac magnetic resonance images;deep convolutional neural networks;deep learning;end-diastole volumes;end-systole volumes;feature representation;multiscale LV atlas mapping method","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"New Deep Neural Nets for Fine-Grained Diabetic Retinopathy Recognition on Hybrid Color Space","H. H. Vo; A. Verma","Dept. of Comput. Sci., California State Univ., Fullerton, CA, USA","2016 IEEE International Symposium on Multimedia (ISM)","20170119","2016","","","209","215","Automatic diabetes retinopathy (DR) recognition can help DR carriers to receive treatment in early stages and avoid the risk of vision loss. In this paper, we emphasize the role of multiple filter sizes in learning fine-grained discriminant features and propose: (i) two deep convolutional neural networks - Combined Kernels with Multiple Losses Network (CKML Net) and VGGNet with Extra Kernel (VNXK), which are an improvement upon GoogLeNet and VGGNet in context of DR tasks. Learning from existing research, (ii) we propose a hybrid color space, LGI, for DR recognition via proposed nets. (iii) Transfer learning is applied to solve the challenge of imbalanced dataset. The effectiveness of proposed new nets and color space is evaluated using two grand challenge retina datasets: EyePACS and Messidor. Our experimental results show: (iv) CKML Net improves upon GoogLeNet and VNXK improves upon VGGNet on both datasets using the LGI color space. Additionally, proposed methodology improves upon other state of the art results on Messidor dataset for referable/non-referable screening.","","Electronic:978-1-5090-4571-6; POD:978-1-5090-4572-3","10.1109/ISM.2016.0049","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823616","CKML Net;LGI;VNXK;convolutional neural networks;diabetic retinopathy recognition;hybrid color space;transfer learning","Diabetes;Feature extraction;Image color analysis;Neural networks;Retina;Retinopathy;Training","computer vision;diseases;filtering theory;image colour analysis;learning (artificial intelligence);medical image processing;neural nets","CKML Net;DR carriers;DR recognition;LGI color space;Messidor dataset;VGGNet with Extra Kernel;VNXK;automatic diabetes retinopathy;combined kernels with multiple losses network;fine grained diabetic retinopathy recognition;hybrid color space;imbalanced dataset;learning fine-grained discriminant features;multiple filter sizes;new deep neural nets;vision loss","","","","","","","","11-13 Dec. 2016","","IEEE","IEEE Conference Publications"
"Facial expression recognition based on LLENet","Dan Meng; Guitao Cao; Zhihai He; Wenming Cao","School of Computer Scinence and Software Engineering, East China Normal University, Shanghai, China 200062","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1915","1917","Facial expression recognition plays an important role in lie detection, and computer-aided diagnosis. Many deep learning facial expression feature extraction methods have a great improvement in recognition accuracy and robutness than traditional feature extraction methods. However, most of current deep learning methods need special parameter tuning and ad hoc fine-tuning tricks. This paper proposes a novel feature extraction model called Locally Linear Embedding Network (LLENet) for facial expression recognition. The proposed LLENet first reconstructs image sets for the cropped images. Unlike previous deep convolutional neural networks that initialized convolutional kernels randomly, we learn multi-stage kernels from reconstructed image sets directly in a supervised way. Also, we create an improved LLE to select kernels, from which we can obtain the most representative feature maps. Furthermore, to better measure the contribution of these kernels, a new distance based on kernel Euclidean is proposed. After the procedure of multi-scale feature analysis, feature representations are finally sent into a linear classifier. Experimental results on facial expression datasets (CK+) show that the proposed model can capture most representative features and thus improves previous results.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822814","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822814","Face expression recognition;deep learning;kernel distance;locally linear embedding (LLE)","Computational modeling;Euclidean distance","face recognition;feature extraction;image reconstruction;learning (artificial intelligence);medical image processing;operating system kernels","LLENet method;ad hoc fine-tuning tricks;computer-aided diagnosis;convolutional kernels;deep convolutional neural networks;deep learning facial expression feature extraction methods;facial expression recognition;image reconstruction;kernel Euclidean;lie detection;locally linear embedding network;parameter tuning","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Sparse Autoencoder Based Deep Neural Network for Voxelwise Detection of Cerebral Microbleed","Y. D. Zhang; X. X. Hou; Y. D. Lv; H. Chen; Y. Zhang; S. H. Wang","Sch. of Comput. Sci. & Technol., Nanjing Normal Univ., Nanjing, China","2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)","20170119","2016","","","1229","1232","In order to detect cerebral microbleed more efficiently, we developed a novel computer-aided detection method based on susceptibility-weighted imaging. We enrolled five CADASIL patients and five healthy controls. We used a 20x20 neighboring window to generate samples on each slice of the volumetric brain images. The sparse autoencoder (SAE) was used to unsupervised feature learning. Then, a deep neural network was established using the learned features. The results over 10x10-fold cross validation showed our method yielded a sensitivity of 93.20±1.37%, a specificity of 93.25±1.38%, and an accuracy of 93.22±1.37%. Our result is better than Roy's method, which was proposed in 2015.","1521-9097;15219097","Electronic:978-1-5090-4457-3; POD:978-1-5090-5382-7","10.1109/ICPADS.2016.0166","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823881","cerebral microbleed;cross validation;deep neural network;sparse autoencoder;susceptibility weighted imaging","Biological neural networks;Biomedical imaging;Blood vessels;Brain models;Image processing;Neural networks;Neurology;Patient monitoring;Sensitivity;Standards","biomedical MRI;learning (artificial intelligence);medical image processing;neural nets","CADASIL patients;MRI;SWI;cerebral microbleed detection;computer-aided detection method;deep neural network;sparse autoencoder;susceptibility-weighted imaging;unsupervised feature learning;voxelwise detection","","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Novel Deep Model for Biopsy Image Grading","G. Zhang; Z. H. Liang; H. D. Lai; Y. Y. Lin; D. Lin; Z. P. Li","Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China","2016 International Conference on Information System and Artificial Intelligence (ISAI)","20170116","2016","","","323","326","We propose in this paper a deep learning model based on convolutional neural network (CNN) for biopsy image grading. The model outputs a vector of scores indicating presence or severity of the target histopathological characteristics. Within the model, we first design a 7-layer CNN for feature representation and high level concept extraction. Each biopsy image is expressed as a feature vector through our CNN processor. We then place a sigmoid function into the output layer so as to generate a score for each target characteristic. The proposed model is evaluated on a benchmark dataset and a real biopsy image dataset to show its effectiveness.","","Electronic:978-1-5090-1585-6; POD:978-1-5090-1586-3","10.1109/ISAI.2016.0075","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816728","biopsy image grading;convolutional neural network;deep learning;histopathological image analysis;sigmoid function","Biological system modeling;Biopsy;Computational modeling;Feature extraction;Machine learning;Solid modeling;Training","feature extraction;learning (artificial intelligence);medical image processing;neural nets","CNN;biopsy image grading;convolutional neural network;deep learning model;feature representation;high level concept extraction;histopathological characteristics;sigmoid function","","","","","","","","24-26 June 2016","","IEEE","IEEE Conference Publications"
"Deep Learning-Aided Parkinson's Disease Diagnosis from Handwritten Dynamics","C. R. Pereira; S. A. T. Weber; C. Hook; G. H. Rosa; J. P. Papa","Dept. of Comput., Fed. Univ. of Sao Carlos, Sao Carlos, Brazil","2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","20170116","2016","","","340","346","Parkinson's Disease (PD) automatic identification in early stages is one of the most challenging medicine-related tasks to date, since a patient may have a similar behaviour to that of a healthy individual at the very early stage of the disease. In this work, we cope with PD automatic identification by means of a Convolutional Neural Network (CNN), which aims at learning features from a signal extracted during the individual's exam by means of a smart pen composed of a series of sensors that can extract information from handwritten dynamics. We have shown CNNs are able to learn relevant information, thus outperforming results obtained from raw data. Also, this work aimed at building a public dataset to be used by researchers worldwide in order to foster PD-related research.","","Electronic:978-1-5090-3568-7; POD:978-1-5090-3569-4","10.1109/SIBGRAPI.2016.054","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813053","Convolutional Neural Networks;Parkinson's Disease","Convolution;Feature extraction;Neural networks;Parkinson's disease;Sensors;Spirals","convolution;diseases;feature extraction;handwriting recognition;learning (artificial intelligence);medical diagnostic computing;neural nets","Parkinson's disease automatic identification;convolutional neural network;deep learning-aided Parkinson's disease diagnosis;feature extraction;handwritten dynamics;smart pen","","","","","","","","4-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Early diagnosis of Alzheimer's disease: A multi-class deep learning framework with modified k-sparse autoencoder classification","P. Bhatkoti; M. Paul","School of Computing and Mathematics, Charles Sturt University, Australia","2016 International Conference on Image and Vision Computing New Zealand (IVCNZ)","20170105","2016","","","1","5","Successful, timely diagnosis of neuropsychiatry diseases is key to management. Research efforts in the area of diagnosis of Alzheimer's disease have used various aspects of computer-aided multi-class diagnosis approaches with varied degrees of success. However, there is still need for more efficient and reliable approaches to successful diagnosis of the disease. This research used deep learning framework with modified k-sparse autoencoder (oKSA) classification to locate neutrally degenerated areas of the brain magnetic resonance imaging (MRI), low amyloid beta 1-42 imaging in cerebrospinal fluid (CSF) and positron emission tomography (PET) imaging of amyloid; each with a sample of 150 images. Results show a correlation between computational demarcation of infected regions and the images. Degeneration in the studied areas was evidenced by high phosphorylated t-/p-tau levels in CSF, regional fluorodeoxyglucose PET, and the presence of atrophy patterns. The use of σKSA algorithm in boosting classification helped to improve the classifier performance. The KSA method with deep learning framework is used for the first time to produce accurate results in diagnosis of Alzheimer's disease.","","Electronic:978-1-5090-2748-4; POD:978-1-5090-2749-1; USB:978-1-5090-2747-7","10.1109/IVCNZ.2016.7804459","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804459","Alzheimer's;aKSA;deep learning;diagnosis;k-sparse;neuroimaging","Alzheimer's disease;Classification algorithms;Encoding;Feature extraction;Machine learning;Magnetic resonance imaging","biomedical MRI;diseases;image classification;image coding;learning (artificial intelligence);medical image processing;positron emission tomography","σKSA algorithm;CSF;MRI;PET imaging;atrophy patterns;brain magnetic resonance imaging;cerebrospinal fluid;classifier performance;computational demarcation;computer-aided multiclass diagnosis;early Alzheimer's disease diagnosis;infected images;infected regions;low amyloid beta 1-42 imaging;modified k-sparse autoencoder classification;multiclass deep learning;neuropsychiatric disease diagnosis;neutrally degenerated areas;phosphorylated t-/p-tau levels;positron emission tomography;regional hypometabolism fluorodeoxyglucose PET","","","","","","","","21-22 Nov. 2016","","IEEE","IEEE Conference Publications"
"A deep learning-based segmentation method for brain tumor in MR images","Zhe Xiao; Ruohan Huang; Yi Ding; Tian Lan; RongFeng Dong; Zhiguang Qin; Xinjie Zhang; Wei Wang","School of Information and Software Engineering, University of Electronic Science and Technology of China, No.4, Section 2, North Jianshe Road, Chengdu, Sichuan, 610054, China","2016 IEEE 6th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)","20170102","2016","","","1","6","Accurate tumor segmentation is an essential and crucial step for computer-aided brain tumor diagnosis and surgical planning. Subjective segmentations are widely adopted in clinical diagnosis and treating, but they are neither accurate nor reliable. An automatical and objective system for brain tumor segmentation is strongly expected. But they are still facing some challenges such as lower segmentation accuracy, demanding a priori knowledge or requiring the human intervention. In this paper, a novel and new coarse-to-fine method is proposed to segment the brain tumor. This hierarchical framework consists of preprocessing, deep learning network based classification and post-processing. The preprocessing is used to extract image patches for each MR image and obtains the gray level sequences of image patches as the input of the deep learning network. The deep learning network based classification is implemented by a stacked auto-encoder network to extract the high level abstract feature from the input, and utilizes the extracted feature to classify image patches. After mapping the classification result to a binary image, the post-processing is implemented by a morphological filter to get the final segmentation result. In order to evaluate the proposed method, the experiment was applied to segment the brain tumor for the real patient dataset. The final performance shows that the proposed brain tumor segmentation method is more accurate and efficient.","","Electronic:978-1-5090-4199-2; POD:978-1-5090-4200-5","10.1109/ICCABS.2016.7802771","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802771","Brain tumor detection;Brain tumor segmentation;Computer Aided Diagnosis (CAD);Deep Learning;Stacked Auto-Encoder (SAE);Stacked Denoising Auto-Encoder (SDAE)","Image reconstruction;Image segmentation;Surgery","biomedical MRI;brain;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours","MR images;brain tumor segmentation;clinical diagnosis;computer-aided brain tumor diagnosis;deep learning network based classification;deep learning-based segmentation method;gray level sequences;image patches;morphological filter;stacked autoencoder network;surgical planning","","","","","","","","13-15 Oct. 2016","","IEEE","IEEE Conference Publications"
"Automatic analysis of neonatal video data to evaluate resuscitation performance","Yue Guo; J. Wrammert; K. Singh; A. KC; K. Bradford; A. Krishnamurthy","University of North Carolina at Chapel Hill, USA","2016 IEEE 6th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)","20170102","2016","","","1","6","Approximately 3% of births require neonatal resuscitation, which has a direct impact on the immediate survival of these infants. This report proposes an automatic video analysis method for neonatal resuscitation performance evaluation, which helps improve the quality of this procedure. More specifically, we design a deep learning based action model which incorporates motion and spatial information in order to classify neonatal resuscitation actions in videos. First, we use a Convolutional Neural Network to select regions containing infants and only keep those that are motion salient. Second, we extract deep spatial-temporal features to train a linear SVM classifier. Finally, we propose a pair-wise model to ensure consistent classification in consecutive frames. We evaluate the proposed method on a dataset consisting of 17 videos and compare the result against the state-of-the-art method for action classification in videos. To our best knowledge, this work is the first to attempt automatic evaluation of neonatal resuscitation videos and identifies several issues that require further work.","","Electronic:978-1-5090-4199-2; POD:978-1-5090-4200-5","10.1109/ICCABS.2016.7802775","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802775","","Cameras;Feature extraction;Neural networks;Optical imaging;Pediatrics;Proposals;Support vector machines","biomedical optical imaging;data analysis;image motion analysis;medical image processing;paediatrics;support vector machines;video signal processing","automatic neonatal video data analysis;automatic video analysis method;convolutional neural network;deep learning-based action model;deep spatialtemporal feature;infant survival;linear SVM classifier;motion salient;neonatal resuscitation action classification;neonatal resuscitation video;support vector machines","","","","","","","","13-15 Oct. 2016","","IEEE","IEEE Conference Publications"
"Accurate Cervical Cell Segmentation from Overlapping Clumps in Pap Smear Images","Y. Song; E. L. Tan; X. Jiang; J. Z. Cheng; D. Ni; S. Chen; B. Lei; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20161230","2017","36","1","288","300","Accurate segmentation of cervical cells in Pap smear images is an important step in automatic pre-cancer identification in the uterine cervix. One of the major segmentation challenges is overlapping of cytoplasm, which has not been well-addressed in previous studies. To tackle the overlapping issue, this paper proposes a learning-based method with robust shape priors to segment individual cell in Pap smear images to support automatic monitoring of changes in cells, which is a vital prerequisite of early detection of cervical cancer. We define this splitting problem as a discrete labeling task for multiple cells with a suitable cost function. The labeling results are then fed into our dynamic multi-template deformation model for further boundary refinement. Multi-scale deep convolutional networks are adopted to learn the diverse cell appearance features. We also incorporated high-level shape information to guide segmentation where cell boundary might be weak or lost due to cell overlapping. An evaluation carried out using two different datasets demonstrates the superiority of our proposed method over the state-of-the-art methods in terms of segmentation accuracy.","0278-0062;02780062","","10.1109/TMI.2016.2606380","(Key) Project of Department of Education of Guangdong Province; National Key Research and Development Project; Shenzhen Key Basic Research Project; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562400","Cervical cancer;Pap smear screening;dynamic multi-template deformation model;multi-scale convolutional networks;overlapping cells splitting","Cervical cancer;Deformable models;Image color analysis;Image edge detection;Image segmentation;Labeling;Shape","biomedical optical imaging;cancer;cellular biophysics;feature extraction;gynaecology;image segmentation;medical image processing;tumours","Pap smear images;accurate cervical cell segmentation;automatic monitoring;automatic precancer identification;boundary refinement;cell boundary;cell overlapping;cervical cancer detection;cervical cells;cytoplasm;datasets;discrete labeling task;diverse cell appearance features;dynamic multitemplate deformation model;high-level shape information;learning-based method;multiple cells;multiscale deep convolutional networks;overlapping clumps;uterine cervix","","1","","","","","20160907","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos","A. P. Twinanda; S. Shehata; D. Mutter; J. Marescaux; M. de Mathelin; N. Padoy","ICube, University of Strasbourg, CNRS, IHU, Strasbourg, France","IEEE Transactions on Medical Imaging","20161230","2017","36","1","86","97","Surgical workflow recognition has numerous potential medical applications, such as the automatic indexing of surgical video databases and the optimization of real-time operating room scheduling, among others. As a result, surgical phase recognition has been studied in the context of several kinds of surgeries, such as cataract, neurological, and laparoscopic surgeries. In the literature, two types of features are typically used to perform this task: visual features and tool usage signals. However, the used visual features are mostly handcrafted. Furthermore, the tool usage signals are usually collected via a manual annotation process or by using additional equipment. In this paper, we propose a novel method for phase recognition that uses a convolutional neural network (CNN) to automatically learn features from cholecystectomy videos and that relies uniquely on visual information. In previous studies, it has been shown that the tool usage signals can provide valuable information in performing the phase recognition task. Thus, we present a novel CNN architecture, called EndoNet, that is designed to carry out the phase recognition and tool presence detection tasks in a multi-task manner. To the best of our knowledge, this is the first work proposing to use a CNN for multiple recognition tasks on laparoscopic videos. Experimental comparisons to other methods show that EndoNet yields state-of-the-art results for both tasks.","0278-0062;02780062","","10.1109/TMI.2016.2593957","French state funds managed by the ANR within the Investissements d¿Avenir program; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519080","Laparoscopic videos;cholecystectomy;convolutional neural network;phase recognition;tool presence detection","Computer architecture;Feature extraction;Image recognition;Laparoscopes;Surgery;Videos;Visualization","biomedical optical imaging;endoscopes;feature extraction;medical image processing;neural nets;optimisation;surgery","CNN architecture;EndoNet;automatic indexing;cataract surgeries;cholecystectomy videos;convolutional neural network;deep architecture;laparoscopic surgeries;laparoscopic videos;manual annotation process;medical applications;neurological surgeries;optimization;phase recognition task;real-time operating room scheduling;recognition tasks;surgical phase recognition;surgical video databases;surgical workflow recognition;tool presence detection tasks;tool usage signals;visual features","","1","","","","","20160722","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Deep learning application trial to lung cancer diagnosis for medical sensor systems","R. Shimizu; S. Yanagawa; Y. Monde; H. Yamagishi; M. Hamada; T. Shimizu; T. Kuroda","Faculty of Science and Technology, Keio University, Yokohama, Japan","2016 International SoC Design Conference (ISOCC)","20161229","2016","","","191","192","Personal and easy-to-use health checking system is an attractive application of sensor systems. Sensing data analysis for diagnosis is important as well as preparing small and mobile sensor nodes because sensing data include variations and noises reflecting individual difference of people and sensing conditions. Deep Neural Network, or Deep Learning, is a well-known method of machine learning and it is effective for feature extraction from pictures. Then, we thought Deep Learning also can extract features from sensing data. In this paper, we tried to build a diagnosis system of lung cancer based on Deep Learning. Input data of the system was generated from human urine by Gas Chromatography Mass Spectrometer (GC-MS) and our system achieved 90% accuracy in judging whether the patient had lung cancer or not. This system will be useful for pre- and personal diagnosis because collecting urine is very easy and not harmful to human body. We are targeting installation of this system not only to gas chromatography systems but also to some combination of multiple sensors for detecting gases of low concentration.","","Electronic:978-1-5090-3219-8; POD:978-1-5090-3220-4; USB:978-1-5090-3218-1","10.1109/ISOCC.2016.7799852","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7799852","Deep Learning;Deep Neural Network;Gas Chromatography Mass Spectrometer(GC-MS);Stacked Autoencoder","Cancer;Feature extraction;Lungs;Machine learning;Medical diagnostic imaging;Neural networks;Sensors","biosensors;cancer;chromatography;learning (artificial intelligence);lung;mass spectrometers;neural nets;patient diagnosis","deep learning application;deep neural network;easy-to-use health checking system;feature extraction;gas chromatography mass spectrometer;gas detection;human urine;lung cancer diagnosis;machine learning;medical sensor systems;mobile sensor nodes;personal diagnosis;personal health checking system;pre-diagnosis;sensing data analysis","","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Segmentation of the Left Ventricle in Echocardiography Using Contextual Shape Model","G. Belous; A. Busch; D. Rowlands; Y. Gao","Sch. of Eng., Griffith Univ., Brisbane, QLD, Australia","2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","20161226","2016","","","1","7","Accurate localization of the left ventricle (LV) boundary from echocardiogram images is of vital importance for the diagnosis and treatment of heart disease. Statistical shape models such as active shape models (ASM) have been commonly used to perform automatic detection of this boundary. Such models perform well when there is low variability in the underlying shape subspace and an accurate initialization can be provided, however in the absence of these conditions results are often much poorer. In the case of LV echocardiogram images, such variability is often encountered in patients with abnormal LV function. In this paper we propose a fully automatic segmentation technique using deep learning in a Bayesian nonparametric framework. Our model uses a dynamic statistical shape model comprised of training shapes from select weighted subsets of the feature subspace. Subsets are chosen during the iterative segmentation process according to a latent temporal component allocation variable, determined from joint deep features and LV landmark information using a Dirichlet process mixture model with Chinese restaurant process prior. Testing is performed on a data set comprising images of the LV acquired from patients exhibiting both normal and abnormal LV function, and the results using our technique compared to both the ASM and other state of the art techniques. Results from this testing show an improvement in the LV localization accuracy, particularly when LV function is abnormal.","","Electronic:978-1-5090-2896-2; POD:978-1-5090-2897-9","10.1109/DICTA.2016.7797080","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797080","","Active shape model;Bayes methods;Image segmentation;Machine learning;Mixture models;Shape;Training","Bayes methods;echocardiography;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;mixture models;patient treatment;shape recognition","ASM;Bayesian nonparametric framework;Chinese restaurant process prior;Dirichlet process mixture model;LV echocardiogram images;active shape models;automatic detection;contextual shape model;deep learning;dynamic statistical shape model;echocardiography;feature subspace;heart disease diagnosis;heart disease treatment;iterative segmentation process;latent temporal component allocation variable;left ventricle localization;left ventricle segmentation;shape subspace;statistical shape models","","","","","","","","Nov. 30 2016-Dec. 2 2016","","IEEE","IEEE Conference Publications"
"Classification of Exacerbation Frequency in the COPDGene Cohort Using Deep Learning with Deep Belief Networks","J. Ying; J. Dutta; N. Guo; C. Hu; D. Zhou; A. Sitek; Q. Li","","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","This study aims to develop an automatic classifier based on deep learning for exacerbation frequency in patients with chronic obstructive pulmonary disease (COPD). A threelayer deep belief network (DBN) with two hidden layers and one visible layer was employed to develop classification models and the models’ robustness to exacerbation was analyzed. Subjects from the COPDGene cohort were labeled with exacerbation frequency, defined as the number of exacerbation events per year. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 91.99%, using a 10-fold cross validation experiment. The analysis of DBN weights showed that there was a good visual spatial relationship between the underlying critical features of different layers. Our findings show that the most sensitive features obtained from the DBN weights are consistent with the consensus showed by clinical rules and standards for COPD diagnostics. We thus demonstrate that DBN is a competitive tool for exacerbation risk assessment for patients suffering from COPD.","2168-2194;21682194","","10.1109/JBHI.2016.2642944","10.13039/100000049 - National Institute on Aging; 10.13039/100000050 - National Heart Lung and Blood Institute; 10.13039/100000070 - National Institute of Biomedical Imaging and Bioengineering; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792616","Chronic obstructive pulmonary disease (COPD);Fisher score;deep belief network (DBN);deep learning;exacerbation","Diseases;Electronic mail;Feature extraction;Gold;Hospitals;Lungs;Machine learning","","","","","","","","","20161221","","","IEEE","IEEE Early Access Articles"
"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation","F. Milletari; N. Navab; S. A. Ahmadi","","2016 Fourth International Conference on 3D Vision (3DV)","20161219","2016","","","565","571","Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.","","Electronic:978-1-5090-5407-7; POD:978-1-5090-5408-4","10.1109/3DV.2016.79","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785132","Deep learning;convolutional neural networks;machine learning;prostate;segmentation","Biomedical imaging;Feature extraction;Image segmentation;Magnetic resonance imaging;Neural networks;Three-dimensional displays;Two dimensional displays","biomedical MRI;image segmentation;medical image processing;neural nets","3D image segmentation;CNN;Dice coefficient;MRI volumes;V-Net;background voxel;clinical practice;computer vision;foreground voxel;fully convolutional neural networks;histogram matching;magnetic resonance imaging;medical image analysis;random nonlinear transformations;volumetric medical image segmentation","","1","","","","","","25-28 Oct. 2016","","IEEE","IEEE Conference Publications"
"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation","H. C. Shin; K. Roberts; L. Lu; D. Demner-Fushman; J. Yao; R. M. Summers","Imaging Biomarkers & Comput. Aided Diagnosis Lab., Nat. Inst. of Health, Bethesda, MD, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2497","2506","Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normalvs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.274","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780643","","Biomedical imaging;Context;Diseases;Radiology;Recurrent neural networks;Training;X-rays","X-rays;convolution;diagnostic radiography;diseases;image annotation;medical image processing;radiology;recurrent neural nets","CNNs;automated image annotation;chest X-rays;convolutional neural networks;deep learning model;image caption datasets;natural images;normalvs-diseased cases bias;radiology dataset;recurrent neural cascade model","","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Online process phase detection using multimodal deep learning","X. Li; Y. Zhang; M. Li; S. Chen; F. R. Austin; I. Marsic; R. S. Burd","Rutgers University, Piscataway, NJ, USA","2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","20161212","2016","","","1","7","We present a multimodal deep-learning structure that automatically predicts phases of the trauma resuscitation process in real-time. The system first pre-processes the audio and video streams captured by a Kinect's built-in microphone array and depth sensor. A multimodal deep learning structure then extracts video and audio features, which are later combined through a “slow fusion” model. The final decision is then made from the combined features through a modified softmax classification layer. The model was trained on 20 trauma resuscitation cases (>13 hours), and was tested on 5 other cases. Our results showed over 80% online detection accuracy with 0.7 F-Score, outperforming previous systems.","","Electronic:978-1-5090-1496-5; POD:978-1-5090-1497-2","10.1109/UEMCON.2016.7777912","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777912","Activity recognition;Kinect;deep learning;multimodal sensing;process phase recognition;trauma resuscitation","Biomedical imaging;Feature extraction;Hidden Markov models;Image segmentation;Machine learning;Phase detection;Surgery","feature extraction;health care;learning (artificial intelligence)","audio feature extraction;modified softmax classification layer;multimodal deep-learning structure;online process phase detection;slow fusion model;trauma resuscitation process;video feature extraction","","1","","","","","","20-22 Oct. 2016","","IEEE","IEEE Conference Publications"
"DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation","H. Chen; X. Qi; L. Yu; P. A. Heng","Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2487","2496","The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.273","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780642","","Biomedical imaging;Cancer;Feature extraction;Glands;Image segmentation;Neural networks;Training","biomedical optical imaging;cancer;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;probability;statistical analysis;tumours","2015 MICCAI Gland Segmentation Challenge;DCAN;deep contour-aware networks;gland morphology;gland segmentation;hierarchical architecture;histology images;large-scale histopathological data;malignancy degree;morphological statistics;multitask regularization;probability maps;unified multitask learning","","3","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Convolutional neural networks for deep feature learning in retinal vessel segmentation","A. F. Khalaf; I. A. Yassine; A. S. Fahmy","Systems and Biomedical Engineering Department, Faculty of Engineering, Cairo University","2016 IEEE International Conference on Image Processing (ICIP)","20161208","2016","","","385","388","Analysis of retinal vessels in fundus images provides a valuable tool for characterizing many retinal and systemic diseases. Accurate automatic segmentation of these vessels is usually required as an essential analysis step. In this work, we propose a new formulation of deep Convolutional Neural Networks that allows simple and accurate segmentation of the retinal vessels. A major modification in this work is to reduce the intra-class variance by formulating the problem as a Three-class problem that differentiates: large vessels, small vessels, and background areas. In addition, different sizes of the convolutional kernels have been studied and it was found that a combination of kernels with different sizes achieve the best sensitivity and specificity. The proposed method was tested using DRIVE dataset and it showed superior performance compared to several other state of the art methods. The segmentation sensitivity, specificity and accuracy were found to be 83.97%, 95.62% and 94.56% respectively.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532384","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532384","Convolutional Neural Networks;Deep Learning;Pattern Classification;Retinal Blood Vessel Segmentation","","diseases;image segmentation;medical image processing;neural nets","DRIVE dataset;convolutional neural networks;deep feature learning;fundus images;retinal blood vessel segmentation;retinal diseases;retinal vessel analysis;segmentation sensitivity;systemic diseases","","","","","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Computer-aided diagnostic tool for early detection of prostate cancer","I. Reda; A. Shalaby; F. Khalifa; M. Elmogy; A. Aboulfotouh; M. A. El-Ghar; E. Hosseini-Asl; N. Werghi; R. Keynton; A. El-Baz","Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt","2016 IEEE International Conference on Image Processing (ICIP)","20161208","2016","","","2668","2672","In this paper, we propose a novel non-invasive framework for the early diagnosis of prostate cancer from diffusion-weighted magnetic resonance imaging (DW-MRI). The proposed approach consists of three main steps. In the first step, the prostate is localized and segmented based on a new level-set model. In the second step, the apparent diffusion coefficient (ADC) of the segmented prostate volume is mathematically calculated for different b-values. To preserve continuity, the calculated ADC values are normalized and refined using a Generalized Gauss-Markov Random Field (GGMRF) image model. The cumulative distribution function (CDF) of refined ADC for the prostate tissues at different b-values are then constructed. These CDFs are considered as global features describing water diffusion which can be used to distinguish between benign and malignant tumors. Finally, a deep learning auto-encoder network, trained by a stacked non-negativity constraint algorithm (SNCAE), is used to classify the prostate tumor as benign or malignant based on the CDFs extracted from the previous step. Preliminary experiments on 53 clinical DW-MRI data sets resulted in 100% correct classification, indicating the high accuracy of the proposed framework and holding promise of the proposed CAD system as a reliable non-invasive diagnostic tool.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532843","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532843","CAD;MGRF;NMF;Prostate cancer","Design automation;Feature extraction;Magnetic resonance imaging;Prostate cancer;Solid modeling;Training","Gaussian processes;Markov processes;biological organs;biomedical MRI;cancer;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;statistical distributions;tumours","ADC value normalization;ADC value refinement;CAD system;CDF extraction;GGMRF image model;SNCAE;apparent diffusion coefficient;benign tumor;clinical DW-MRI data set;computer-aided diagnostic tool;cumulative distribution function;deep learning autoencoder network;diffusion-weighted magnetic resonance imaging;generalized Gauss-Markov random field image model;level-set model;malignant tumor;noninvasive diagnostic tool;noninvasive prostate cancer early diagnosis;prostate cancer early detection;prostate localization;prostate segmentation;prostate tissue;prostate tumor classification;prostate volume segmentation;stacked nonnegativity constraint algorithm;water diffusion","","","","","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Deepmole: Deep neural networks for skin mole lesion classification","V. Pomponiu; H. Nejati; N. M. Cheung","Information System, Technology and Desing (SUTD) Singapore University of Technology and Design (ISTD) Somapah Road 8, Singapore, 487372","2016 IEEE International Conference on Image Processing (ICIP)","20161208","2016","","","2623","2627","Nowadays, the occurrence of skin cancer cases has grown worldwide due to the extended exposure to the harmful radiation from the Sun. Most common approach to detect the malignancy of skin moles is by visual inspection performed by an expert dermatologist, using a set of specific clinical rules. Computer-aided diagnosis, based on skin mole imaging, is another concurrent method which has experienced major advancements due to improvement of imaging sensors and processing power. However, these schemes use hand-crafted features which are difficult to tune and perform poorly on new cases due to lack of generalization power. In this study we present a method that use a pretrained deep neural network (DNN) to automatically extract a set of representative features that can be later used to diagnose a sample of skin lesion for malignancy. The experimental tests carried out on a clinical dataset show that the classification performance using DNN-based features performs better than the state-of-the-art techniques.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532834","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532834","Skin mole classification;deep neural networks;feature extraction;malignant melanoma;transfer learning","Feature extraction;Image color analysis;Lesions;Malignant tumors;Neural networks;Skin;Skin cancer","cancer;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;skin","DNN-based features;automatic representative feature extraction;clinical dataset;computer-aided diagnosis;deepmole;malignancy;pretrained deep neural network;skin cancer;skin mole imaging;skin mole lesion classification","","1","","","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"A new non-invasive approach for early classification of renal rejection types using diffusion-weighted MRI","M. Shehata; F. Khalifa; E. Hollis; A. Soliman; E. Hosseini-Asl; M. A. El-Ghar; M. El-Baz; A. C. Dwyer; A. El-Baz; R. Keynton","Bioengineering Department, University of Louisville, Louisville, KY, USA","2016 IEEE International Conference on Image Processing (ICIP)","20161208","2016","","","136","140","Although renal biopsy remains the gold standard for diagnosing the type of renal rejection, it is not preferred due to its invasiveness, recovery time (1-2 weeks), and potential for complications, e.g., bleeding and/or infection. Therefore, there is an urgent need to explore a non-invasive technique that can early classify renal rejection types. In this paper, we develop a computer-aided diagnostic (CAD) system that can classify acute renal transplant rejection (ARTR) types early via the analysis of apparent diffusion coefficients (ADCs) extracted from diffusion-weighted (DW) MRI data acquired at low-(accounting for perfusion) and high-(accounting for diffusion) b-values. The developed framework mainly consists of three steps: (i) data co-alignment using a 3D B-spline-based approach (to handle local deviations due to breathing and heart beat motions) and segmentation of kidney tissue with an evolving geometric (level-set based) deformable model guided by a voxel-wise stochastic speed function, which follows a joint kidney-background Markov-Gibbs random field model accounting for an adaptive kidney shape prior and visual kidney-background appearances of DW-MRI data (image intensities and spatial interactions); (ii) construction of a cumulative empirical distribution of ADC at low and high b-values of the segmented kidney accounting for blood perfusion and water diffusion, respectively, to be our discriminatory ARTR types feature; and (iii) classification of ARTR types (acute tubular necrosis (ATN) anti-body- and T-cell-mediated rejection) based on deep learning of a non-negative constrained stacked autoencoder. Results show that 98% of the subjects were correctly classified in our “leave-one-subject-out” experiments on 39 subjects (namely, 8 out of 8 of the ATN group and 30 out of 31 of the T-cell group). Thus, the proposed approach holds promise as a reliable non-invasive diagnostic tool.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532334","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532334","ADC;ARTR types;DW-MRI;Deep learning","Blood;Design automation;Image segmentation;Kidney;Magnetic resonance imaging;Motion segmentation;Three-dimensional displays","Markov processes;biomedical MRI;image classification;image segmentation;medical image processing;patient diagnosis;splines (mathematics)","3D B-spline-based approach;ADC;ARTR;ATN;CAD;DW;T-cell-mediated rejection;acute renal transplant rejection;acute tubular necrosis;adaptive kidney shape prior;antibody-mediated rejection;apparent diffusion coefficients;blood perfusion;computer-aided diagnostic system;deformable model;diffusion-weighted MRI;kidney tissue segmentation;kidney-background Markov-Gibbs random field model;leave-one-subject-out experiments;noninvasive approach;noninvasive diagnostic tool;recovery time;renal biopsy;renal rejection types classification;voxel-wise stochastic speed function","","","","","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Histopathological image classification using random binary hashing based PCANet and bilinear classifier","J. Wu; J. Shi; Y. Li; J. Suo; Q. Zhang","School of Communication and Information Engineering Shanghai University Shanghai, China","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","2050","2054","The computer-aided histopathological image diagnosis has attracted considerable attention. Principal component analysis network (PCANet) is a novel deep learning algorithm with a simple network architecture and parameters. In this work, we propose a random binary hashing (RBH) based PCANet (RBH-PCANet), which can generate multiple randomly encoded binary codes to provide more information. Moreover, we rearrange the local features derived from PCANet to the matrix-form features in order to reduce feature dimensionality, and then we apply the low-rank bilinear classifier (LRBC) to perform effective classification for matrix features. The proposed classification framework using RBH-PCANet and LRBC (RBH-PCANet-LRBC) is adopted for histopathological image classification. The experimental results on both a hepatocellular carcinoma image dataset and a breast cancer image dataset show that the RBH-PCANet-LRBC algorithm achieves best performance compared with other unsupervised deep learning algorithms.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760609","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760609","Histopathological Image;Low Rank Bilinear Classifier;PCANet;Random Binary Hashing","Breast cancer;Classification algorithms;Europe;Histograms;Principal component analysis;Signal processing;Signal processing algorithms","cancer;feature extraction;image classification;image coding;learning (artificial intelligence);matrix algebra;medical image processing;principal component analysis;random processes","RBH-PCANet-LRBC;breast cancer image dataset;computer-aided histopathological image diagnosis;deep learning;feature dimensionality;hepatocellular carcinoma image dataset;histopathological image classification;local features;low-rank bilinear classifier;matrix features;matrix-form features;multiple randomly encoded binary codes;principal component analysis network;random binary hashing based PCANet","","","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling","A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers","Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Image Processing","20161124","2017","26","1","386","399","Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.","1057-7149;10577149","","10.1109/TIP.2016.2624198","10.13039/100000098 - Intramural Research Program of the National Institutes of Health Clinical Center; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966","Abdominal computed tomography (CT);cascaded random forest;deep convolutional neural networks;dense image patch labeling;pancreas segmentation","Computed tomography;Image segmentation;Labeling;Liver;Pancreas;Shape","biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability","CT slice image decomposition;abdominal computed tomography scans;bottom-up approach;cascaded superpixels;computer-aided diagnosis;deep convolutional neural network classification;deep image patch labeling;dense patch labeling;disjoint boundary-preserving superpixels;image histogram;image location;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach;pancreas class probability maps;pancreas segmentation;pathology detection;probability features;quantitative imaging analysis;random forest classification;superpixel classification;surgical assistance;texture features","","","","","","","20161101","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Severe burns assessment by joint color-thermal imagery and ensemble methods","M. S. Badea; C. Vertan; C. Florea; L. Florea; S. Bădoiu","The Image Processing and Analysis Lab, Politehnica University of Bucharest, Bucharest, Romania","2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)","20161121","2016","","","1","5","Burns are some of the most severe forms of accidental trauma across the world. Burn injuries require specialized care and an early and accurate distinction between superficial dermal burns and deep dermal burns which require further surgical procedures, as they do not heal spontaneously. This paper proposes a multispectral imaging based diagnosis support system for the identification of severe burns. The acquisition is performed in both visual and infrared domains, simultaneously, with a thermal camera recording in parallel color and thermal images of the unconstrained patient and patient environment. The classification of burns is developed under a supervised scenario, according to a ground truth defined by specialist surgeons from a large pediatric case database, by an ensemble of methods that gather votes from both convolutional neural network and standard pattern recognition systems.","","Electronic:978-1-5090-3370-6; POD:978-1-5090-3371-3","10.1109/HealthCom.2016.7749450","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749450","burn identification;color imaging;deep learning convolutional networks;diagnosis support;ensemble method;infrared imaging;segmentation","Color;Databases;Feature extraction;Image color analysis;Skin;Surgery;Wounds","accidents;biomedical optical imaging;cameras;image classification;image colour analysis;infrared imaging;injuries;medical image processing;neural nets;patient care;patient diagnosis;pattern recognition;surgery","accidental trauma;burn assessment;burn classification;burn injuries;color-thermal imagery;convolutional neural network;deep dermal burns;ground truth;large pediatric case database;multispectral imaging-based diagnosis support system;parallel color images;parallel thermal images;standard pattern recognition systems;superficial dermal burns;surgical procedures;thermal camera recording","","","","","","","","14-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"A multiclass classification method based on deep learning for named entity recognition in electronic medical records","X. Dong; L. Qian; Y. Guan; L. Huang; Q. Yu; J. Yang","Center of Excellence in Research and Education for Big Military Data Intelligence (CREDIT), Department of Electrical and Computer Engineering, Prairie View A&M University Houston, USA","2016 New York Scientific Data Summit (NYSDS)","20161121","2016","","","1","10","Research of named entity recognition (NER) on electrical medical records (EMRs) focuses on verifying whether methods to NER in traditional texts are effective for that in EMRs, and there is no model proposed for enhancing performance of NER via deep learning from the perspective of multiclass classification. In this paper, we annotate a real EMR corpus to accomplish the model training and evaluation. And, then, we present a Convolutional Neural Network (CNN) based multiclass classification method for mining named entities from EMRs. The method consists of two phases. In the phase 1, EMRs are pre-processed for representing samples with word embedding. In the phase 2, the method is built by segmenting training data into many subsets and training a CNN binary classification model on each of subset. Experimental results showed the effectiveness of our method.","","Electronic:978-1-4673-9051-4; POD:978-1-4673-9052-1","10.1109/NYSDS.2016.7747810","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747810","convolutional neural network;electrical medical records;machine learning;named entity recognition;natural language processing","Diseases;Feature extraction;Medical diagnostic imaging;Support vector machines;Unified modeling language","data mining;electronic health records;learning (artificial intelligence);neural nets;pattern classification","CNN based multiclass classification method;CNN binary classification model;EMR corpus;NER;convolutional neural network based multiclass classification method;deep learning;electronic medical records;named entity mining;named entity recognition;training data segmentation","","","","","","","","14-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Temporal Pattern Recognition in Gait Activities Recorded With a Footprint Imaging Sensor System","O. Costilla-Reyes; P. Scully; K. B. Ozanyan","School of Electrical and Electronic Engineering, Photon Science Institute, University of Manchester, Manchester, U.K.","IEEE Sensors Journal","20161117","2016","16","24","8815","8822","In this paper, we assess the capability of a unique unobtrusive footprint imaging sensor system, based on plastic optical fiber technology, to allow efficient gait analysis from time domain sensor data by pattern recognition techniques. Trial gait classification experiments are executed as ten manners of walking, affecting the amplitude and frequency characteristics of the temporal signals. The data analysis involves the design of five temporal features, subsequently analyzed in 14 different machine learning models, representing linear, non-linear, ensemble, and deep learning models. The model performance is presented as cross-validated accuracy scores for the best model-feature combinations, along with the optimal hyper-parameters for each of them. The best classification performance was observed for a random forest model with the adjacent mean feature, yielding a mean validation score of 90.84% ± 2.46%. We conclude that the floor sensor system is capable of detecting changes in gait by means of pattern recognition techniques applied in the time domain. This suggests that the footprint imaging sensor system is suitable for gait analysis applications ranging from healthcare to security.","1530-437X;1530437X","","10.1109/JSEN.2016.2583260","U.K. Engineering and Physical Sciences Research Council through the Knowledge Transfer Scheme; 10.13039/501100007350 - Consejo Nacional de Ciencia y Tecnolog¿a; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496802","Floor sensor;gait analysis;machine learning;pattern recognition;sensor fusion","Floors;Imaging;Intelligent sensors;Legged locomotion;Optical fibers;Sensor systems","biomedical optical imaging;fibre optic sensors;gait analysis;image classification;learning (artificial intelligence);medical image processing","deep learning model;ensemble model;footprint imaging sensor system;gait activities;gait classification experiments;healthcare;machine learning models;model-feature combinations;nonlinear model;plastic optical fiber technology;random forest model;temporal pattern recognition;temporal signals;time domain sensor data;walking","","","","","","","20160621","Dec.15, 15 2016","","IEEE","IEEE Journals & Magazines"
"Customizing CNNs for blood vessel segmentation from fundus images","S. K. Vengalil; N. Sinha; S. S. S. Kruthiventi; R. V. Babu","International Institute of Information Technology, Bangalore, India","2016 International Conference on Signal Processing and Communications (SPCOM)","20161117","2016","","","1","4","For automatic screening of eye diseases, it is very important to segment regions corresponding to the different eye-parts from the fundal images. A challenging task, in this context, is to segment the network of blood vessels. The blood vessel network runs all along the fundal image, varying in density and fineness of structure. Besides, changes in illumination, color and pathology also add to the difficulties in blood vessel segmentation. In this paper, we propose segmentation of blood vessels from fundal images in the deep learning framework, without any pre-processing. A deep convolutional network, consisting of 8 convolutional layers and 3 pooling layers in between, is used to achieve the segmentation. In this work, a Convolutional Neural Network currently in use for semantic image segmentation is customized for blood vessel segmentation by replacing the output layer with a convolutional layer of kernel size 1 × 1 which generates the final segmented image. The output of CNN is a gray scale image and is binarized by thresholding. The proposed method is applied on 2 publicly available databases DRIVE and HRF (capturing diversity in image resolution), consisting of healthy and diseased fundal images boosted by mirror versions of the originals. The method results in an accuracy of 93.94% and yields 0.894 as area under the ROC curve on the test data comprising of randomly selected 23 images from HRF dataset. The promising results illustrate generalizability of the proposed approach.","","Electronic:978-1-5090-1746-1; POD:978-1-5090-1747-8","10.1109/SPCOM.2016.7746702","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746702","","Biomedical imaging;Blood vessels;Image resolution;Image segmentation;Pathology;Testing;Training","blood vessels;convolution;diseases;image colour analysis;image resolution;image segmentation;learning (artificial intelligence);medical image processing;neural nets","CNN;DRIVE database;HRF databases;ROC curve;automatic screening;blood vessel network;blood vessel segmentation;convolutional neural network;deep learning framework;eye diseases;fundal images;fundus images;gray scale image;image resolution;semantic image segmentation","","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Learning relations using semantic-based vector similarity","K. Budai; M. Dînşoreanu; I. Bărbănţan; R. Potolea","Department of Computer Science, Technical University of Cluj-Napoca, Romania","2016 IEEE 12th International Conference on Intelligent Computer Communication and Processing (ICCP)","20161110","2016","","","69","75","The amount of electronic medical documents is growing rapidly every day. While they carry much information, it becomes more and more difficult to manually process it. Our work represents small steps towards automatic knowledge extraction from medical documents using deep learning and similarity based methods. Our goal here is to identify in an unsupervised manner relations between known medical concepts employing a deep learning strategy with Word2Vec. The current solution requires concepts annotations, as it evaluates the similarities between concepts to identify the relationship between them. The experiments suggest that the strategy we considered (to include the POS as part of the information associated to concepts and relation) represents an important step towards a fully unsupervised learning strategy. Although the POS tags alone are not good enough predictors, the addition of other meta-information and sufficient (quantitative and qualitative) training data may enhance the relation identification process, allowing for a meta learning strategy.","","Electronic:978-1-5090-3899-2; POD:978-1-5090-3900-5; USB:978-1-5090-3898-5","10.1109/ICCP.2016.7737125","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737125","data correlation;deep learning;relation extraction","Context;Feature extraction;Machine learning;Medical diagnostic imaging;Neural networks;Semantics;Training","document handling;knowledge acquisition;learning (artificial intelligence);medical administrative data processing","POS tag predictors;Word2Vec;automatic knowledge extraction;concept annotations;deep learning;electronic medical documents;medical concepts;metainformation;metalearning strategy;relation identification process;relation learning;semantic-based vector similarity;training data;unsupervised learning strategy","","","","","","","","8-10 Sept. 2016","","IEEE","IEEE Conference Publications"
"Towards Bayesian Deep Learning: A Framework and Some Existing Methods","H. Wang; D. Y. Yeung","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong","IEEE Transactions on Knowledge and Data Engineering","20161104","2016","28","12","3395","3408","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks.","1041-4347;10414347","","10.1109/TKDE.2016.2606428","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562516","Artificial intelligence;Bayesian networks;data mining;deep learning;machine learning;neural networks","Artificial intelligence;Bayes methods;Learning systems;Machine learning;Medical services;Neural networks;Probabilistic logic;Recommender systems","inference mechanisms;learning (artificial intelligence);probability","Bayesian deep learning;general framework;higher-level inference;human intelligence;perception tasks;planning;principled probabilistic framework;probabilistic graphical models;reasoning","","1","","","","","20160907","Dec. 1 2016","","IEEE","IEEE Journals & Magazines"
"Breast cancer histopathological image classification using Convolutional Neural Networks","F. A. Spanhol; L. S. Oliveira; C. Petitjean; L. Heutte","Federal University of Technology - Parana, Toledo, Brazil","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","2560","2567","The performance of most conventional classification systems relies on appropriate data representation and much of the efforts are dedicated to feature engineering, a difficult and time-consuming process that uses prior expert domain knowledge of the data to create useful features. On the other hand, deep learning can extract and organize the discriminative information from the data, not requiring the design of feature extractors by a domain expert. Convolutional Neural Networks (CNNs) are a particular type of deep, feedforward network that have gained attention from research community and industry, achieving empirical successes in tasks such as speech recognition, signal processing, object recognition, natural language processing and transfer learning. In this paper, we conduct some preliminary experiments using the deep learning approach to classify breast cancer histopathological images from BreaKHis, a publicly dataset available at http://web.inf.ufpr.br/vri/breast-cancer-database. We propose a method based on the extraction of image patches for training the CNN and the combination of these patches for final classification. This method aims to allow using the high-resolution histopathological images from BreaKHis as input to existing CNN, avoiding adaptations of the model that can lead to a more complex and computationally costly architecture. The CNN performance is better when compared to previously reported results obtained by other machine learning models trained with hand-crafted textural descriptors. Finally, we also investigate the combination of different CNNs using simple fusion rules, achieving some improvement in recognition rates.","","","10.1109/IJCNN.2016.7727519","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727519","","Neural networks","cancer;image classification;medical image processing;neural nets","BreaKHis;CNN performance;breast cancer histopathological image classification;breast cancer histopathological images;convolutional neural networks;data representation;deep learning;discriminative information;domain expert;feature extractors;feedforward network;fusion rules;hand-crafted textural descriptors;high-resolution histopathological images;image patches;machine learning models;natural language processing;object recognition;recognition rates;research community;signal processing;speech recognition;time-consuming process;transfer learning","","1","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Segmenting Retinal Blood Vessels With Deep Neural Networks","P. Liskowski; K. Krawiec","Institute of Computing Science, Poznan University of Technology, Poland","IEEE Transactions on Medical Imaging","20161103","2016","35","11","2369","2380","The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 \thinspace000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels ( sensitivity > 0.87), and fares well on pathological cases.","0278-0062;02780062","","10.1109/TMI.2016.2546227","10.13039/501100005632 - Narodowe Centrum Bada? i Rozwoju; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440871","Classification;deep learning;feature learning;fundus;neural networks;retina;retinopathy;structured prediction;vessel segmentation","Biomedical imaging;Blood vessels;Convolution;Databases;Image segmentation;Neural networks;Pathology","blood vessels;eye;image classification;image segmentation;medical image processing;neural nets;sensitivity analysis;unsupervised learning","CHASE databases;DRIVE databases;ROC curve;STARE databases;central vessel reflex;deep neural networks;diagnostic factor;fundus imaging;gamma corrections;geometric transformations;global contrast normalization;hemorrhages;human eye;image classification;microaneurysms;nontrivial task;ophthalmology;retinal blood vessel segmentation;structured prediction;supervised segmentation;vascular network;zero-phase whitening","","7","","","","","20160324","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Classification from generation: Recognizing deep grammatical information during reading from rapid event-related fMRI","T. Bitan; A. Frid; H. Hazan; L. M. Manevitz; H. Shalelashvili; Y. Weiss","Psychology Department,University of Haifa, Israel","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","4637","4642","A novel fMRI classification method designed for rapid event related fMRI experiments is described and applied to the classification of loud reading of isolated words in Hebrew. Three comparisons of different grammatical complexity were performed: (i) words versus asterisks (ii) “with diacritics versus without diacritics” and (iii) “with root versus no root”. We discuss the most difficult task and, for comparison, the easiest one. Earlier work using more standard classification techniques (machine learning and statistical) succeeded fully only in the simplest of these tasks (i), but produced only partial results on (ii) and failed completely, even on the training set on the deepest task (iii). The method performs a “best match” between pre-processed data and computing a full library of artificially generated examples. The method involves a deconvolution of the rapid events on the data and performing a convolution on the generated data. The best-match is performed over all “words” constructed by convolving the response functions of each value of each event performed in a “windowed” sequence. This is accomplished separately for all voxels and then a voting procedure defines the outcome. Using the same feature selection (ANOVA) as in the earlier methods, (i) there is a dramatic increase in the accuracy rate for the third (most difficult task) on the intra-run level (88%) as well as the first task (ii) Unlike the earlier methods training and testing over all runs (within subject) achieves a significant level of classification (64% accuracy) for the training set. This shows the information for this “deeper” cognitive task can in fact be extracted from the fMRI information.","","","10.1109/IJCNN.2016.7727808","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727808","Classification;Cognitive Processing;Functional magnetic resonance imaging (fMRI);Machine Learning;Multivoxel pattern analysis (MVPA);Neural Networks;Pattern Matching","Biological neural networks;Convolution;Dictionaries;Market research;Testing;Training;Training data","biomedical MRI;image classification;medical image processing","ANOVA;deep grammatical information recognition;functional magnetic resonance imaging classification method;loud reading;rapid event-related fMRI;windowed sequence","","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Exploring deep features from brain tumor magnetic resonance images via transfer learning","Renhao Liu; L. O. Hall; D. B. Goldgof; Mu Zhou; R. A. Gatenby; K. B. Ahmed","Department of Computer Science and Engineering, University of South Florida, Tampa, USA","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","235","242","Finding appropriate feature representations from radiological images is a vital task for prediction and diagnosis. Deep convolutional neural networks have recently achieved state-of-the-art performance in classification problems from several different domains. Research has also shown the feasibility of using a pre-trained deep neural network as a feature extractor when only a small dataset is available. This paper proposes a novel image feature extraction method for predicting survival time from brain tumor magnetic resonance images using pretrained deep neural networks. Since all tumors are different sizes, we also explore different image resizing methods in the paper. We demonstrate that deep features can result in better survival time prediction with the highest accuracy of 95.45% versus conventional feature extraction methods from magnetic resonance images of the brain.","","","10.1109/IJCNN.2016.7727204","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727204","","Histograms;Magnetic resonance imaging","biomedical MRI;brain;feature extraction;image representation;learning (artificial intelligence);medical image processing;neural nets;tumours","brain tumor magnetic resonance images;deep features;feature representations;image resizing methods;novel image feature extraction method;pretrained deep convolutional neural networks;survival time prediction;transfer learning","","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Accelerating Convolutional Sparse Coding for Curvilinear Structures Segmentation by Refining SCIRD-TS Filter Banks","R. Annunziata; E. Trucco","School of Science and Engineering (Computing), University of Dundee, Dundee, U.K.","IEEE Transactions on Medical Imaging","20161027","2016","35","11","2381","2392","Deep learning has shown great potential for curvilinear structure (e.g., retinal blood vessels and neurites) segmentation as demonstrated by a recent auto-context regression architecture based on filter banks learned by convolutional sparse coding. However, learning such filter banks is very time-consuming, thus limiting the amount of filters employed and the adaptation to other data sets (i.e., slow re-training). We address this limitation by proposing a novel acceleration strategy to speed-up convolutional sparse coding filter learning for curvilinear structure segmentation. Our approach is based on a novel initialisation strategy (warm start), and therefore it is different from recent methods improving the optimisation itself. Our warm-start strategy is based on carefully designed hand-crafted filters (SCIRD-TS), modelling appearance properties of curvilinear structures which are then refined by convolutional sparse coding. Experiments on four diverse data sets, including retinal blood vessels and neurites, suggest that the proposed method reduces significantly the time taken to learn convolutional filter banks (i.e., up to -82%) compared to conventional initialisation strategies. Remarkably, this speed-up does not worsen performance; in fact, filters learned with the proposed strategy often achieve a much lower reconstruction error and match or exceed the segmentation performance of random and DCT-based initialisation, when used as input to a random forest classifier.","0278-0062;02780062","","10.1109/TMI.2016.2570123","EU Marie Curie ITN REVAMMAD 316990; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470472","Convolutional sparse coding;neurites;retinal blood vessels;segmentation","Acceleration;Biomedical imaging;Blood vessels;Convolutional codes;Detectors;Encoding;Image segmentation","biomedical optical imaging;blood vessels;channel bank filters;convolutional codes;eye;image classification;image coding;image filtering;image reconstruction;image segmentation;medical image processing;neurophysiology;optimisation;regression analysis","DCT-based initialisation;SCIRD-TS filter bank refining;accelerating convolutional sparse coding;autocontext regression architecture;convolutional sparse coding filter learning;curvilinear structure segmentation;data sets;deep learning;handcrafted filters;initialisation strategy;neurites;optimisation;random forest classifier;reconstruction error;retinal blood vessels;warm-start strategy","","","","","","","20160517","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Automatic tissue characterization of air trapping in chest radiographs using deep neural networks","A. Mansoor; G. Perez; G. Nino; M. G. Linguraru","Sheikh Zayed Institute for Pediatric Surgical Innovation, Children's National Health System, Washington DC","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","97","100","Significant progress has been made in recent years for computer-aided diagnosis of abnormal pulmonary textures from computed tomography (CT) images. Similar initiatives in chest radiographs (CXR), the common modality for pulmonary diagnosis, are much less developed. CXR are fast, cost effective and low-radiation solution to diagnosis over CT. However, the subtlety of textures in CXR makes them hard to discern even by trained eye. We explore the performance of deep learning abnormal tissue characterization from CXR. Prior studies have used CT imaging to characterize air trapping in subjects with pulmonary disease; however, the use of CT in children is not recommended mainly due to concerns pertaining to radiation dosage. In this work, we present a stacked autoencoder (SAE) deep learning architecture for automated tissue characterization of air-trapping from CXR. To our best knowledge this is the first study applying deep learning framework for the specific problem on 51 CXRs, an F-score of ≈ 76.5% and a strong correlation with the expert visual scoring (R=0.93, p =<; 0.01) demonstrate the potential of the proposed method to characterization of air trapping.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590649","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590649","","Charge carrier processes;Computed tomography;Lungs;Machine learning;Shape;Training;Visualization","biological tissues;computerised tomography;diseases;learning (artificial intelligence);medical image processing;neural nets;pneumodynamics","CXR textures;abnormal pulmonary textures;air trapping;automatic tissue characterization;chest radiographs;computed tomography images;computer-aided diagnosis;deep neural networks;pulmonary diagnosis;pulmonary disease;radiation dosage;stacked autoencoder deep learning architecture","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Recent machine learning advancements in sensor-based mobility analysis: Deep learning for Parkinson's disease assessment","B. M. Eskofier; S. I. Lee; J. F. Daneault; F. N. Golabchi; G. Ferreira-Carvalho; G. Vergara-Diaz; S. Sapienza; G. Costante; J. Klucken; T. Kautz; P. Bonato","Digital Sports Group, Pattern Recognition Lab, Department of Computer Science, Friedrich-Alexander University Erlangen-N&#x00FC;rnberg (FAU), Erlangen, Germany","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","655","658","The development of wearable sensors has opened the door for long-term assessment of movement disorders. However, there is still a need for developing methods suitable to monitor motor symptoms in and outside the clinic. The purpose of this paper was to investigate deep learning as a method for this monitoring. Deep learning recently broke records in speech and image classification, but it has not been fully investigated as a potential approach to analyze wearable sensor data. We collected data from ten patients with idiopathic Parkinson's disease using inertial measurement units. Several motor tasks were expert-labeled and used for classification. We specifically focused on the detection of bradykinesia. For this, we compared standard machine learning pipelines with deep learning based on convolutional neural networks. Our results showed that deep learning outperformed other state-of-the-art machine learning algorithms by at least 4.6 % in terms of classification rate. We contribute a discussion of the advantages and disadvantages of deep learning for sensor-based movement assessment and conclude that deep learning is a promising method for this field.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590787","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590787","","Biomedical monitoring;Convolution;Feature extraction;Machine learning;Pipelines;Standards;Training","biomechanics;biomedical measurement;body sensor networks;diseases;inertial systems;learning (artificial intelligence)","Parkinson's disease assessment;bradykinesia;deep learning;inertial measurement units;machine learning;sensor-based mobility analysis","","1","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Fetal facial standard plane recognition via very deep convolutional networks","Z. Yu; D. Ni; S. Chen; S. Li; T. Wang; B. Lei","School of Biomedical Engineering, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","627","630","The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3×3) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590780","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590780","","Computer architecture;Convolution;Image recognition;Imaging;Standards;Training;Ultrasonic imaging","biomedical ultrasonics;face recognition;learning (artificial intelligence);medical image processing;neural nets;obstetrics","DCNN;FFSP dataset;US examination;automatic FFSP recognition method;convolution filters;deep learning approach;fetal facial standard plane recognition;hand-crafted visual features;ultrasound images;very deep convolutional networks","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Automatic prostate segmentation on MR images with deep network and graph model","K. Yan; C. Li; X. Wang; A. Li; Y. Yuan; D. Feng; M. Khadra; J. Kim","School of Information Technologies, University of Sydney, Australia","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","635","638","Automated prostate diagnoses and treatments have gained much attention due to the high mortality rate of prostate cancer. In particular, unsupervised (automatic) prostate segmentation is an active and challenging research. Most conventional works usually utilize handcrafted (low-level) features for prostate segmentation; however they often fail to extract the intrinsic structure of the prostate, especially on images with blurred boundaries. In this paper, we propose a novel automated prostate segmentation model with learned features from deep network. Specifically, we first generate a set of prostate proposals in transverse plane via recognizing the position and coarse estimate of the shape of the prostate on the global prostate image and using the deep network to extract highly effective features for the boundary refinement in a finer scale. With consideration of the correlations among different sequential images, we then construct a graph to select the best prostate proposals from proposal set for its use in 3D prostate segmentation. Experimental evaluation demonstrates that our proposed deep network and graph based method is superior to state-of-the-art couterparts, in terms of both dice similarity coefficient and Hausdorff distance, on public dataset.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590782","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590782","","Algorithm design and analysis;Context;Feature extraction;Image segmentation;Proposals;Shape;Three-dimensional displays","biomedical MRI;cancer;graph theory;image segmentation;image sequences;medical image processing;unsupervised learning","3D prostate segmentation;Hausdorff distance;MR images;automated prostate diagnosis;automated prostate segmentation model;automated prostate treatments;boundary refinement;deep network;dice similarity coefficient;global prostate image;graph model;handcrafted features;prostate cancer;prostate intrinsic structure;prostate shape;public dataset;sequential images;transverse plane;unsupervised prostate segmentation","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Vessel extraction in X-ray angiograms using deep learning","E. Nasr-Esfahani; S. Samavi; N. Karimi; S. M. R. Soroushmehr; K. Ward; M. H. Jafari; B. Felfeliyan; B. Nallamothu; K. Najarian","Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","643","646","Coronary artery disease (CAD) is the most common type of heart disease which is the leading cause of death all over the world. X-ray angiography is currently the gold standard imaging technique for CAD diagnosis. These images usually suffer from low quality and presence of noise. Therefore, vessel enhancement and vessel segmentation play important roles in CAD diagnosis. In this paper a deep learning approach using convolutional neural networks (CNN) is proposed for detecting vessel regions in angiography images. Initially, an input angiogram is preprocessed to enhance its contrast. Afterward, the image is evaluated using patches of pixels and the network determines the vessel and background regions. A set of 1,040,000 patches is used in order to train the deep CNN. Experimental results on angiography images of a dataset show that our proposed method has a superior performance in extraction of vessel regions.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590784","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590784","Angiography;convolutional neural networks;deep learning;vessel segmentation","Angiography;Arteries;Feature extraction;Filter banks;Image segmentation;Training;Transforms","blood vessels;diagnostic radiography;diseases;image segmentation;learning (artificial intelligence);medical image processing;neural nets;noise","CAD diagnosis;X-ray angiography;convolutional neural networks;coronary artery disease;death;deep learning;heart disease;noise;vessel enhancement;vessel extraction;vessel segmentation","","1","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Thorax disease diagnosis using deep convolutional neural network","J. Chen; X. Qi; O. Tervonen; O. Silvén; G. Zhao; M. Pietikäinen","University of Oulu, Finland","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2287","2290","Computer aided diagnosis (CAD) is an important issue, which can significantly improve the efficiency of doctors. In this paper, we propose a deep convolutional neural network (CNN) based method for thorax disease diagnosis. We firstly align the images by matching the interest points between the images, and then enlarge the dataset by using Gaussian scale space theory. After that we use the enlarged dataset to train a deep CNN model and apply the obtained model for the diagnosis of new test data. Our experimental results show our method achieves very promising results.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591186","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591186","","Diseases;Filtering theory;Machine learning;Radiography;Solid modeling;Thorax;Training","Gaussian processes;diagnostic radiography;diseases;image matching;medical image processing;neural nets;radiology","CAD;Gaussian scale space theory;computer aided diagnosis;deep CNN model;deep convolutional neural network;image alignment;image matching;radiograph;radiology;thorax disease diagnosis","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Discriminating clinical phases of recovery from major depressive disorder using the dynamics of facial expression","S. Harati; A. Crowell; H. Mayberg; J. Kong; S. Nemati","Department of Biomedical Informatics, Emory University, Atlanta, USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2254","2257","We used several metrics of variability to extract unsupervised features from video recordings of patients before and after deep brain stimulation (DBS) treatment for major depressive disorder (MDD). Our goal was to quantify the treatment effects on facial expressivity. Multiscale entropy (MSE) was used to capture the temporal variability in pixel intensity level at multiple time-scales. A dynamic latent variable model (DLVM) was used to learn a low dimensional (D = 20) set of dynamic factors that explain the observed covariance across the high-dimensional pixels (M = 30 × 30) within each video frame and across time. Our preliminary results indicate that unsupervised features learned from these video recordings can distinguish different phases of depression and recovery. The overarching goal of this research is to develop more refined markers of clinical response to treatment for depression.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591178","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591178","","Entropy;Face;Mathematical model;Observability;Satellite broadcasting;Switches;Video recording","bioelectric potentials;brain;entropy;face recognition;feature extraction;image registration;medical disorders;medical image processing;neurophysiology;patient treatment;video recording","DBS;DLVM;MDD;MSE;clinical recovery phases;clinical response;deep brain stimulation treatment;depression treatment;dynamic latent variable model;facial expression dynamics;facial expressivity;high-dimensional pixels;major depressive disorder;multiscale entropy;pixel intensity level;temporal variability;unsupervised features;video frame;video recording","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Pain detection from facial images using unsupervised feature learning approach","R. Kharghanian; A. Peiravi; F. Moradi","Department of Electrical Engineering, Ferdowsi University of Mashhad, Mashhad, Iran","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","419","422","In this paper a new method for continuous pain detection is proposed. One approach to detect the presence of pain is by processing images taken from the face. It has been reported that expression of pain from the face can be detected utilizing Action Units (AUs). In this manner, each action units must be detected separately and then combined together through a linear expression. Also, pain detection can be directly done from a painful face. There are different methods to extract features of both shape and appearance. Shape and appearance features must be extracted separately, and then used to train a classifier. Here, a hierarchical unsupervised feature learning approach is proposed in order to extract the features needed for pain detection from facial images. In this work, features are extracted using convolutional deep belief network (CDBN). The extracted features include different properties of painful images such as head movements, shape and appearance information. The proposed model was tested on the publicly available UNBC MacMaster Shoulder Pain Archive Database and we achieved near 95% for the area under ROC curve metric that is prominent with respect to the other reported results.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590729","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590729","","Active appearance model;Face;Feature extraction;Pain;Probabilistic logic;Shape;Support vector machines","belief networks;face recognition;feature extraction;image classification;medical image processing;sensitivity analysis;unsupervised learning","ROC curve metrics;UNBC MacMaster Shoulder Pain Archive Database;action units;convolutional deep belief network;facial images;feature extraction;head movements;hierarchical unsupervised feature learning approach;image processing;pain detection","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Melanoma detection by analysis of clinical images using convolutional neural network","E. Nasr-Esfahani; S. Samavi; N. Karimi; S. M. R. Soroushmehr; M. H. Jafari; K. Ward; K. Najarian","Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","1373","1376","Melanoma, most threatening type of skin cancer, is on the rise. In this paper an implementation of a deep-learning system on a computer server, equipped with graphic processing unit (GPU), is proposed for detection of melanoma lesions. Clinical (non-dermoscopic) images are used in the proposed system, which could assist a dermatologist in early diagnosis of this type of skin cancer. In the proposed system, input clinical images, which could contain illumination and noise effects, are preprocessed in order to reduce such artifacts. Afterward, the enhanced images are fed to a pre-trained convolutional neural network (CNN) which is a member of deep learning models. The CNN classifier, which is trained by large number of training samples, distinguishes between melanoma and benign cases. Experimental results show that the proposed method is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590963","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590963","","Computers;Feature extraction;Lesions;Lighting;Machine learning;Malignant tumors;Training","biomedical optical imaging;cancer;graphics processing units;image classification;image denoising;learning (artificial intelligence);medical image processing;neural nets;skin","CNN classifier;artifact reduction;clinical image analysis;convolutional neural network;deep-learning system;diagnostic accuracy;early skin cancer diagnosis;graphic processing unit;illumination effect;input clinical image;melanoma detection;noise effect;nondermoscopic image","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Non-invasive optical imaging techniques for burn-injured tissue detection for debridement surgery","J. Heredia-Juesas; J. E. Thatcher; Y. Lu; J. J. Squiers; D. King; W. Fan; J. M. DiMaio; J. A. Martinez-Lorenzo","Departments of Electrical & Computer and Mechanical & Industrial Engineering, Northeastern University, Boston, MA, USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2893","2896","Burn debridement is a challenging technique that requires significant skill to identify regions requiring excision and appropriate excision depth. A machine learning tool is being developed in order to assist surgeons by providing a quantitative assessment of burn-injured tissue. Three noninvasive optical imaging techniques capable of distinguishing between four kinds of tissue-healthy skin, viable wound bed, deep burn, and shallow burn-during serial burn debridement in a porcine model are presented in this paper. The combination of all three techniques considerably improves the accuracy of tissue classification, from 0.42 to almost 0.77.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591334","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591334","","Heart rate;Imaging;Measurement;Skin;Standards;Surgery;Wounds","biomedical optical imaging;image classification;learning (artificial intelligence);medical image processing;skin;surgery;wounds","burn-injured tissue detection;debridement surgery;deep burn;excision depth;machine learning tool;noninvasive optical imaging techniques;porcine model;serial burn debridement;shallow burn;tissue classification;tissue-healthy skin;viable wound bed","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Semi-advised learning model for skin cancer diagnosis based on histopathalogical images","A. Masood; A. Al-Jumaily","University of Technology Sydney, P.O. Box 123 Broadway, NSW 2007 Australia","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","631","634","Computer aided classification of skin cancer images is an active area of research and different classification methods has been proposed so far. However, the supervised classification models based on insufficient labeled training data can badly influence the diagnosis process. To deal with the problem of limited labeled data availability this paper presents a semi advised learning model for automated recognition of skin cancer using histopathalogical images. Deep belief architecture is constructed using unlabeled data by making efficient use of limited labeled data for fine tuning done the classification model. In parallel an advised SVM algorithm is used to enhance classification results by counteracting the effect of misclassified data using advised weights. To increase generalization capability of the model, advised SVM and Deep belief network are trained in parallel. Then the results are aggregated using least square estimation weighting. The proposed model is tested on a collection of 300 histopathalogical images taken from biopsy samples. The classification performance is compared with some popular methods and the proposed model outperformed most of the popular techniques including KNN, ANN, SVM and semi supervised algorithms like Expectation maximization algorithm and transductive SVM based classification model.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590781","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590781","","Computer architecture;Data models;Machine learning;Skin cancer;Support vector machines;Training;Training data","belief networks;cancer;image classification;learning (artificial intelligence);least squares approximations;medical image processing;skin;support vector machines","advised SVM algorithm;automated recognition;biopsy samples;computer aided classification;deep belief architecture;deep belief network;histopathalogical images;least square estimation weighting;semiadvised learning model;skin cancer diagnosis;skin cancer images","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Automatic Lumbar Vertebrae Detection Based on Feature Fusion Deep Learning for Partial Occluded C-arm X-ray Images","Y. Li; W. Liang; Y. Zhang; H. An; J. Tan","Key Laboratory of Networked Control Systems, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","647","650","Automatic and accurate lumbar vertebrae detection is an essential step of image-guided minimally invasive spine surgery (IG-MISS). However, traditional methods still require human intervention due to the similarity of vertebrae, abnormal pathological conditions and uncertain imaging angle. In this paper, we present a novel convolutional neural network (CNN) model to automatically detect lumbar vertebrae for C-arm X-ray images. Training data is augmented by DRR and automatic segmentation of ROI is able to reduce the computational complexity. Furthermore, a feature fusion deep learning (FFDL) model is introduced to combine two types of features of lumbar vertebrae X-ray images, which uses sobel kernel and Gabor kernel to obtain the contour and texture of lumbar vertebrae, respectively. Comprehensive qualitative and quantitative experiments demonstrate that our proposed model performs more accurate in abnormal cases with pathologies and surgical implants in multi-angle views.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590785","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590785","","Computational modeling;Feature extraction;Kernel;Machine learning;Pathology;Surgery;X-ray imaging","computational complexity;computerised tomography;image fusion;image segmentation;image texture;learning (artificial intelligence);medical image processing;neural nets;prosthetics;surgery","CNN;DRR;FFDL;Gabor kernel;IG-MISS;ROI;abnormal pathological condition;automatic lumbar vertebrae detection;automatic segmentation;computational complexity;convolutional neural network model;feature fusion deep learning model;image-guided minimally invasive spine surgery;imaging angle;lumbar vertebrae X-ray images;lumbar vertebrae contour;lumbar vertebrae texture;partial occluded C-arm X-ray images;sobel kernel;surgical implants;training data","","1","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Integrating holistic and local deep features for glaucoma classification","A. Li; J. Cheng; D. W. K. Wong; J. Liu","Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","1328","1331","Automated glaucoma detection is an important application of retinal image analysis. Compared with segmentation based approaches, image classification based approaches have a potential of better performance. However, it still remains a challenging problem for two reasons. Firstly, due to insufficient sample size, learning effective features is difficult. Secondly, the shape variations of optic disc introduce misalignment. To address these problem, a new classification based approach for glaucoma detection is proposed, in which deep convolutional networks derived from large-scale generic dataset is used to representing the visual appearance and holistic and local features are combined to mitigate the influence of misalignment. The proposed method achieves an area under the receiver operating characteristic curve of 0.8384 on the Origa dataset, which clearly demonstrates its effectiveness.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590952","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590952","","Feature extraction;Optical imaging;Optical receivers;Optical sensors;Retina;Shape;Visualization","diseases;image classification;image segmentation;medical image processing","Origa dataset;automated glaucoma detection;glaucoma classification;image classification;image segmentation;large-scale generic dataset;optic disc;receiver operating characteristic curve;retinal image analysis;sample size;visual appearance","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography","A. Lahiri; A. G. Roy; D. Sheet; P. K. Biswas","Dept. of Electronics and Electrical Communication Engineering, Indian Institute of Technology Kharagpur, India","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","1340","1343","Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590955","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590955","","Biomedical imaging;Feature extraction;Image segmentation;Kernel;Retinal vessels;Standards;Training","biomedical optical imaging;blood vessels;cardiovascular system;diseases;eye;feature extraction;image classification;image coding;image denoising;image segmentation;medical image processing;unsupervised learning","2-level fusion;DRIVE dataset;Kappa agreement coefficient;SoftMax classifier;architectural revision;autoencoders fosters diversity;automated segmentation;bootstrap samples;cardiovascular diseases;classification task;computed aided diagnosis;deep neural ensemble;diabetic retinopathy;first level training;hypertensive disorders;label-free angiography;label-free fundus images;learning dictionary;maximum average accuracy;medical image analysis;network architectures;noisy background;ophthalmic pathologies;physical appearance;retinal blood vessels;retinal vessel segmentation;sparsely trained denoised stacked autoencoder;standard deviation;unsupervised hierarchical feature learning;visual kernels","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Text-mining the neurosynth corpus using deep boltzmann machines","R. Monti; R. Lorenz; R. Leech; C. Anagnostopoulos; G. Montana","Department of Mathematics, Imperial College London","2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI)","20160901","2016","","","1","4","Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.","","Electronic:978-1-4673-6530-7; POD:978-1-4673-6531-4","10.1109/PRNI.2016.7552329","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552329","Deep Boltzmann machines;meta-analysis;text-mining;topic models","Brain modeling;Context;Monte Carlo methods;Neuroimaging;Proposals;Semantics;Vocabulary","Boltzmann machines;biomedical MRI;data mining;learning (artificial intelligence);medical image processing;text analysis","DBM;NeuroSynth database;brain activation coordinate;deep Boltzmann machine;fMRI;functional magnetic resonance imaging;human brain function;machine learning;neuroimaging data metaanalysis;text mining","","2","","","","","","22-24 June 2016","","IEEE","IEEE Conference Publications"
"Evaluation of weight sparsity control during autoencoder training of resting-state fMRI using non-zero ratio and hoyer's sparseness","H. c. Kim; J. h. Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul, Republic of Korea","2016 International Workshop on Pattern Recognition in Neuroimaging (PRNI)","20160901","2016","","","1","4","Recently, an explicit control of weight sparsity level between the layers in the deep neural network has been proposed and gainfully been utilized to resting-state fMRI (rfMRI) data. However, the reliability of the weight sparsity control scheme via the percentage of non-zero weights (PNZ) was not systematically evaluated in term of the convergence property of the sparsity levels across various scenarios of parameter changes (i.e. learning rates and initial weights). Thus, the primary aim of this study is to systematically evaluate the reliability of the PNZ based sparsity control scheme. In addition, the Hoyer's sparseness (HSP) based on the ratio of L1- and L2-norms was adopted as an alternative option to measure the weight sparsity level. To this end, the whole-brain functional connectivity of the rfMRI data from the Human Connectome Project was used as input of the autoencoder (AE) with the sparsity control scheme via either the PNZ or HSP. Then, the convergence to reach a target sparsity level and converged sparsity levels from the PNZ and HSP based schemes were compared. The presented methods and findings will benefit the training of the (stacked-) AE and/or deep neural network with the weight sparsity control scheme to ease the curse-of-dimensionality issue of very highdimensional neuroimaging data with limited available samples.","","Electronic:978-1-4673-6530-7; POD:978-1-4673-6531-4","10.1109/PRNI.2016.7552356","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552356","Deep neural network;Hoyer’s sparseness;Human Connectome Project;functional connectivity;functional magnetic resonance imaging;non-zero ratio;weight sparsity","Convergence;Feature extraction;Indexes;Standards;Training;Visualization;Weight measurement","biomedical MRI;brain;convergence;medical image processing;neural nets;reliability","HSP;Hoyer sparseness;Human Connectome Project;L1-norms;L2-norms;PNZ-based sparsity control;autoencoder training;brain functional connectivity;convergence property;deep neural network;functional magnetic resonance imaging;high-dimensional neuroimaging data;nonzero ratio;percentage-of-nonzero weights;reliability;resting-state fMRI data;target sparsity level;weight sparsity control evaluation","","","","","","","","22-24 June 2016","","IEEE","IEEE Conference Publications"
"A deep learning based approach to classification of CT brain images","X. W. Gao; R. Hui","Department of Computer Science, Middlesex University, London NW4 4BT, UK","2016 SAI Computing Conference (SAI)","20160901","2016","","","28","31","This study explores the applicability of the state of the art of deep learning convolutional neural network (CNN) to the classification of CT brain images, aiming at bring images into clinical applications. Towards this end, three categories are clustered, which contains subjects' data with either Alzheimer's disease (AD) or lesion (e.g. tumour) or normal ageing. Specifically, due to the characteristics of CT brain images with larger thickness along depth (z) direction (~5mm), both 2D and 3D CNN are employed in this research. The fusion is therefore conducted based on both 2D CT images along axial direction and 3D segmented blocks with the accuracy rates are 88.8%, 76.7% and 95% for classes of AD, lesion and normal respectively, leading to an average of 86.8%.","","Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2","10.1109/SAI.2016.7555958","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555958","3D CNN;CT brain images;Classification;convolutional neural network","Alzheimer's disease;Brain;Computed tomography;Kernel;Lesions;Three-dimensional displays;Two dimensional displays","computerised tomography;diseases;image classification;learning (artificial intelligence);medical image processing;neural nets","2D CNN;2D CT images;3D CNN;3D segmented blocks;AD;Alzheimer's disease;CT brain image classification;axial direction;clinical applications;deep learning based approach;deep learning convolutional neural network;lesion;normal ageing","","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"A deep symmetry convnet for stroke lesion segmentation","Y. Wang; A. K. Katsaggelos; X. Wang; T. B. Parrish","Northwestern University, Department of EECS, Evanston, IL, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","111","115","Stroke is one of the leading causes of death and disability. Clinically, to establish stroke patient prognosis, an accurate delineation of brain lesion is essential, which is time consuming and prone to subjective errors. In this paper, we propose a novel method call Deep Lesion Symmetry ConvNet to automatically segment chronic stroke lesions using MRI. An 8-layer 3D convolutional neural network is constructed to handle the MRI voxels. An additional CNN stream using the corresponding symmetric MRI voxels is combined, leading to a significant improvement in system performance. The high average dice coefficient achieved on our dataset based on data collected from three research labs demonstrates the effectiveness of our method.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532329","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532329","Brain Quasi-symmetry;Deep Learning;Image Segmentation;MRI;Stroke","Biological neural networks;Brain modeling;Image segmentation;Lesions;Magnetic resonance imaging;Pipelines;Three-dimensional displays","biomedical MRI;brain;image segmentation;medical image processing;neural nets;patient treatment","3D convolutional neural network;MRI voxels;brain lesion;death;deep symmetry ConvNet;disability;stroke lesion segmentation;stroke patient prognosis","","","","13","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Learning a multiscale patch-based representation for image denoising in X-RAY fluoroscopy","Y. Matviychuk; B. Mailhé; X. Chen; Q. Wang; A. Kiraly; N. Strobel; M. Nadar","Siemens Healthcare, Medical Imaging Technologies, Princeton, NJ, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","2330","2334","Denoising is an indispensable step in processing low-dose X-ray fluoroscopic images that requires development of specialized high-quality algorithms able to operate in near real-time. We address this problem with an efficient deep learning approach based on the process-centric view of traditional iterative thresholding methods. We develop a novel trainable patch-based multiscale framework for sparse image representation. In a computationally efficient way, it allows us to accurately reconstruct important image features on multiple levels of decomposition with patch dictionaries of reduced size and complexity. The flexibility of the chosen machine learning approach allows us to tailor the learned basis for preserving important structural information in the image and noticeably minimize the amount of artifacts. Our denoising results obtained with real clinical data demonstrate significant quality improvement and are computed much faster in comparison with the BM3D algorithm.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532775","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532775","Radiography;iterative algorithms;neural networks","Approximation algorithms;Dictionaries;Image reconstruction;Image representation;Iterative methods;Neural networks;Noise reduction","diagnostic radiography;feature extraction;image denoising;image reconstruction;image representation;image segmentation;iterative methods;learning (artificial intelligence);medical image processing","BM3D algorithm;X-ray fluoroscopy;artifact minimization;clinical data;complexity;deep learning approach;image decomposition;image denoising;image feature reconstruction;iterative thresholding method;low-dose X-ray fluoroscopic image;machine learning;multiscale patch-based representation learning;patch dictionary;sparse image representation;structural information preservation;trainable patch-based multiscale framework","","1","","38","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Alzheimer's disease diagnostics by adaptation of 3D convolutional network","E. Hosseini-Asl; R. Keynton; A. El-Baz","Electrical and Computer Engineering Department, University of Louisville, Louisville, KY, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","126","130","Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532332","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532332","3D convolutional neural network;Alzheimer's disease;autoencoder;brain MRI;deep learning","Convolution;Feature extraction;Hippocampus;Magnetic resonance imaging;Positron emission tomography;Three-dimensional displays;Training","biomedical MRI;brain;diseases;feature extraction;generalisation (artificial intelligence);image classification;learning (artificial intelligence);medical image processing;neural nets;neurophysiology","3D convolutional autoencoder;3D-CNN;AD biomarkers;ADNI dataset;Alzheimer's disease diagnostics;Alzheimer's disease progress prevention;Alzheimer's disease treatment;CADDementia MRI dataset;anatomical brain structures;anatomical shape variation;brain image;brain volume;cortical thickness;deep 3D convolutional neural network;early diagnosis;feature classification;feature extraction;feature generalization;generic feature learning;hippocampus shape;structural brain MRI scan;ventricles size","","","","38","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Cell proposal network for microscopy image analysis","S. U. Akram; J. Kannala; L. Eklund; J. Heikkilä","Center for Machine Vision Research, University of Oulu, Finland","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","3199","3203","Robust cell detection plays a key role in the development of reliable methods for automated analysis of microscopy images. It is a challenging problem due to low contrast, variable fluorescence, weak boundaries, conjoined and overlapping cells, causing most cell detection methods to fail in difficult situations. One approach for overcoming these challenges is to use cell proposals, which enable the use of more advanced features from ambiguous regions and/or information from adjacent frames to make better decisions. However, most current methods rely on simple proposal generation and scoring methods, which limits the performance they can reach. In this paper, we propose a convolutional neural network based method which generates cell proposals to facilitate cell detection, segmentation and tracking. We compare our method against commonly used proposal generation and scoring methods and show that our method generates significantly better proposals, and achieves higher final recall and average precision.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532950","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532950","cell detection;cell proposals;cell tracking;deep learning;fully convolutional network","Feature extraction;Image analysis;Image segmentation;Microscopy;Proposals;Shape;Training","image segmentation;medical image processing;neural nets","cell proposal network;cell segmentation;cell tracking;convolutional neural network based method;microscopy image analysis;robust cell detection","","1","","16","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Improving Tuberculosis Diagnostics Using Deep Learning and Mobile Health Technologies among Resource-Poor and Marginalized Communities","Y. Cao; C. Liu; B. Liu; M. J. Brunette; N. Zhang; T. Sun; P. Zhang; J. Peinado; E. S. Garavito; L. L. Garcia; W. H. Curioso","Dept. of Comput. Sci., Univ. of Massachusetts-Lowell, Lowell, MA, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20160818","2016","","","274","281","Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Perú, a densely occupied urban community and high-burden TB.","","Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2","10.1109/CHASE.2016.18","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545842","Perú;deep convolutional neural networks;deep learning;diagnosis;mHealth;mobile computing;tuberculosis","Diagnostic radiography;Image databases;Mobile communication;Mobile computing;Mobile handsets;X-ray imaging","diagnostic radiography;diseases;epidemics;image classification;learning (artificial intelligence);medical image processing;mobile computing;neural nets;patient diagnosis;visual databases","African;CNN-based models;Deep Learning;Mobile Health Technologies;South-East Asia;TB diagnosis delay;Tuberculosis Diagnostics;Western Pacific;X-ray image database;automated TB screening;chronic infectious disease;deep convolutional neural network-based models;disease transmission;healthcare;image classification;machine learning;mobile computing;tuberculosis epidemic","","","","","","","","27-29 June 2016","","IEEE","IEEE Conference Publications"
"A Deep Learning Method for Microaneurysm Detection in Fundus Images","J. Shan; L. Li","Dept. of Comput. Sci., Pace Univ. New York City, New York, NY, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20160818","2016","","","357","358","Diabetic Retinopathy (DR) is the leading cause of blindness in the working-age population. Microaneurysms (MAs), due to leakage from retina blood vessels, are the early signs of DR. However, automated MA detection is complicated because of the small size of MA lesions and the low contrast between the lesion and its retinal background. Recently deep learning (DL) strategies have been used for automatic feature extraction and classification problems, especially for image analysis. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a DL strategy, is presented for MA detection in fundus images. Small image patches are generated from the original fundus images. The SSAE learns high-level features from pixel intensities alone in order to identify distinguishing features of MA. The high-level features learned by SSAE are fed into a classifier to categorize each image patch as MA or non-MA. The public benchmark DIARETDB is utilized to provide the training/testing data and ground truth. Among the 89 images, totally 2182 image patches with MA lesions, serve as positive data, and another 6230 image patches without MA lesions are generated by a randomly sliding window operation, to serve as negative data. Without any blood vessel removal or complicated preprocessing operations, SSAE learned directly from the raw image patches, and automatically extracted the distinguishing features to classify the patches using Softmax Classifier. By employing the fine-tuning operation, an improved F-measure 91.3% and an average area under the ROC curve (AUC) 96.2% were achieved using 10-fold cross-validation.","","Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2","10.1109/CHASE.2016.12","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545864","automated microaneurysm detection;deep learning;diabetic retinopathy;feature representation;stacked sparse autoencoder","Biomedical image processing;Conferences;Feature extraction;Lesions;Machine learning","blood vessels;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing","10-fold cross-validation;DIARETDB public benchmark;DR;F-measure;SSAE;automated MA detection;automatic feature extraction;blindness;classification problems;deep learning method;diabetic retinopathy;fine-tuning operation;fundus images;image analysis;microaneurysm detection;microaneurysms;randomly sliding window operation;retina blood vessels;softmax classifier;stacked sparse autoencoder;working-age population","","1","","","","","","27-29 June 2016","","IEEE","IEEE Conference Publications"
"Modality Classification for Searching Figures in Biomedical Literature","Z. Xue; M. M. Rahman; S. Antani; L. R. Long; D. Demner-Fushman; G. R. Thoma","Lister Hill Nat. Center for Biomed. Commun., Nat. Libr. of Med., Bethesda, MD, USA","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","152","157","Image modality classification categorizes images according to their type. It is an important module in the Open-iSM multimodal (text+image) search engine that retrieves figures from biomedical articles. It is a hierarchical classification where on the top level the input figures are classified into two general categories: regular images (X-ray, CT, MRI, photographs, etc.) vs. illustration images (cartoon sketch, charts, graphs, etc.). This binary classification task is challenged by the vast diversity of visual material (image type), and the way it is organized (simple or compound figures). We present two methods for this binary classification: (i) Support Vector Machines (SVM) with manually-selected features, including a feature based on semantic concepts, and, (ii) Deep Learning method which avoids the process of feature handcrafting. Both methods were tested and compared on a dataset of 16400 figures. Both methods achieved good performance (above 95% accuracy). The slightly better performance of the feature-based method demonstrates the effectiveness of the features we chose.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.29","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545975","Modality classification;concept feature;convolutional neural networks;deep learning;figure searching;support vector machine","Biomedical imaging;Image color analysis;Image edge detection;Neural networks;Semantics;Support vector machines;Visualization","image classification;image retrieval;learning (artificial intelligence);medical image processing;search engines;support vector machines","Open-iSM multimodal search engine;SVM;binary classification task;biomedical literature;deep learning;figure retrieval;figure searching;hierarchical classification;illustration images;image modality classification;regular images;support vector machines","","","","","","","","20-24 June 2016","","IEEE","IEEE Conference Publications"
"Automatic burn area identification in color images","M. S. Badea; C. Vertan; C. Florea; L. Florea; S. Bădoiu","The Image Processing and Analysis Lab (LAPI), Politehnica University of Bucharest, Romania","2016 International Conference on Communications (COMM)","20160804","2016","","","65","68","This papers presents the use of color imaging as a starting point of burn wound evaluation, by the discrimination between healthy skin and burn wound. The skin/burn area identification is performed pixel-wise, according to the properties of an entire encompassing patch. The classification is learned under a supervised scenario, according to a ground truth defined by specialist surgeons from a large pediatric case database, by a deep learned convolutional neural network. The database is extensive and was recorded over several months in real, hospital conditions The proposed approach achieves an overall performance comparable to the literature-reported average performance of a specialist surgeon.","","DVD:978-1-4673-8196-3; Electronic:978-1-4673-8197-0; POD:978-1-4673-8198-7","10.1109/ICComm.2016.7528325","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528325","burn identification;color imaging;color segmentation;deep learning convolutional networks;diagnosis support","Color;Databases;Image color analysis;Neural networks;Skin;Surgery;Wounds","biomedical optical imaging;convolution;image colour analysis;learning (artificial intelligence);medical image processing;neural nets;paediatrics;skin;wounds","automatic burn area identification;burn wound evaluation;color imaging;deep learned convolutional neural network;ground truth;healthy skin;large pediatric case database;skin-burn area identification","","","","","","","","9-10 June 2016","","IEEE","IEEE Conference Publications"
"Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation","A. Mansoor; J. J. Cerrolaza; R. Idrees; E. Biggs; M. A. Alsharid; R. A. Avery; M. G. Linguraru","Children's National Health System, Washington","IEEE Transactions on Medical Imaging","20160729","2016","35","8","1856","1865","Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.","0278-0062;02780062","","10.1109/TMI.2016.2535222","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7420737","Anterior visual pathway;MRI;intensity normalization;partitioned statistical model;shape model;sparse learning","Biomedical optical imaging;Machine learning;Magnetic resonance imaging;Optical imaging;Pathology;Robustness;Shape","biomedical MRI;curvelet transforms;eye;image segmentation;image sequences;medical image processing;paediatrics;statistical analysis","Dice similarity coefficient;MRI sequences;anatomic structures;anterior visual pathway segmentation;conditional space deep learning;cranial nerve systems;deep learning feature representation;deep learning guided partitioned shape model;joint partitioned statistical shape model;low-grade gliomas;optic nerve;pathologic AVP segmentation;pediatric subjects;robust statistical model;shape localization method;sparse appearance-based learning approach;volumetric multiscale curvelet transform-based intensity normalization method","","3","","","","","20160226","Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Deep random forest-based learning transfer to SVM for brain tumor segmentation","S. Amiri; I. Rekik; M. A. Mahjoub","SAGE laboratory, Higher Institute of Computer Science and Communication Techniques of Hammam sousse, Tunisia","2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)","20160728","2016","","","297","302","Using neuroimaging techniques to diagnose brain tumors and detect both visible and invisible cancer cells infiltration boundaries motivated the emergence of diverse tumor segmentation algorithms. Noting the large variability in both tumor appearance and shape, the task of automatic segmentation becomes more difficult. In this paper, we propose a random-forest (RF) based learning transfer to SVM classifier method for segmenting tumor lesions while capturing their complex characteristics. Our framework is composed of two cascaded stages. In the first stage, we train a random forest to learn the mapping from the image space to the tumor label space. In the testing stage, we use the predicted label output from the random forest and feed it along with the testing intensity image to an SVM classifier to get the refined segmentation. Then we make our RF-SVM cascaded classification steps deep through an iterative process. We tested our method on 20 patients with high-grade gliomas from the Brain Tumor Image Segmentation Challenge (BRATS) dataset. Our proposed framework significantly outperformed SVM-based segmentation and RF-based segmentation-when used solely.","","Electronic:978-1-4673-8526-8; POD:978-1-4673-8527-5","10.1109/ATSIP.2016.7523095","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523095","Brain tumor;MRI;Random Forest;SVM;Segmentation","Image segmentation;Lesions;Magnetic resonance imaging;Radio frequency;Support vector machines;Vegetation","brain;cancer;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neurophysiology;support vector machines;tumours","BRATS;RF-SVM cascaded classification;brain tumor diagnosis;brain tumor image segmentation;cancer cells infiltration boundaries;deep random forest-based learning transfer;iterative process;neuroimaging techniques","","","","","","","","21-23 March 2016","","IEEE","IEEE Conference Publications"
"Clinical decision support for Alzheimer's disease based on deep learning and brain network","C. Hu; R. Ju; Y. Shen; P. Zhou; Q. Li","School of Engineering and Applied Sciences, Harvard University","2016 IEEE International Conference on Communications (ICC)","20160714","2016","","","1","6","Modern e-health systems have undergone rapid development thanks to the advances in communications, computing and machine learning technology. Especially, deep learning has great superiority in image analysis and disease prediction. In this paper, we use Alzheimer's Disease (AD) as an example to show advantages of deep learning in diagnosing brain diseases and providing clinical decision support. Firstly, we convert raw functional magnetic resonance imaging (fMRI) to a matrix to represent activity of 90 brain regions. Secondly, to represent the functional connectivity between different brain regions, a correlation matrix is obtained by calculating the correlation between each pair of brain regions. In the next, a targeted autoencoder network is built to classify the correlation matrix, which is sensitive to AD. Finally, the experiment results show that our proposed method for AD prediction achieves much better effects than traditional means. It finds the correlations between different brain regions efficiently, provides strong reference for AD prediction. Compared to Support Vector Machine (SVM), about 25% improvement is gained in prediction accuracy. The e-health field becomes more complete and effective owing to that. Our work helps predict AD at an early stage and take measures to slow down or even prevent the onset of it.","","Electronic:978-1-4799-6664-6; POD:978-1-4799-6665-3","10.1109/ICC.2016.7510831","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7510831","","Correlation;Cost function;Diseases;Machine learning;Positron emission tomography;Support vector machines;Time series analysis","biomedical MRI;brain;decision support systems;diseases;learning (artificial intelligence);matrix algebra;medical image processing","AD prediction;Alzheimer's Disease;brain diseases diagnosing;brain network;clinical decision support;correlation matrix;deep learning;disease prediction;e-health field;fMRI;functional connectivity;image analysis;modern e-health systems;raw functional magnetic resonance imaging;targeted autoencoder network","","","","","","","","22-27 May 2016","","IEEE","IEEE Conference Publications"
"Deep Independence Network Analysis of Structural Brain Imaging: Application to Schizophrenia","E. Castro; R. D. Hjelm; S. M. Plis; L. Dinh; J. A. Turner; V. D. Calhoun","The Mind Research Network, NM, USA","IEEE Transactions on Medical Imaging","20160701","2016","35","7","1729","1740","Linear independent component analysis (ICA) is a standard signal processing technique that has been extensively used on neuroimaging data to detect brain networks with coherent brain activity (functional MRI) or covarying structural patterns (structural MRI). However, its formulation assumes that the measured brain signals are generated by a linear mixture of the underlying brain networks and this assumption limits its ability to detect the inherent nonlinear nature of brain interactions. In this paper, we introduce nonlinear independent component estimation (NICE) to structural MRI data to detect abnormal patterns of gray matter concentration in schizophrenia patients. For this biomedical application, we further addressed the issue of model regularization of nonlinear ICA by performing dimensionality reduction prior to NICE, together with an appropriate control of the complexity of the model and the usage of a proper approximation of the probability distribution functions of the estimated components. We show that our results are consistent with previous findings in the literature, but we also demonstrate that the incorporation of nonlinear associations in the data enables the detection of spatial patterns that are not identified by linear ICA. Specifically, we show networks including basal ganglia, cerebellum and thalamus that show significant differences in patients versus controls, some of which show distinct nonlinear patterns.","0278-0062;02780062","","10.1109/TMI.2016.2527717","10.13039/100000025 - National Institute of Mental Health; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405347","Deep learning;NICE;nonlinear ICA;schizophrenia;structural MRl","Brain;Couplings;Estimation;Jacobian matrices;Magnetic resonance imaging;Probability density function","","","","","","34","","","20160211","July 2016","","IEEE","IEEE Journals & Magazines"
"Prediction of visual attention with Deep CNN for studies of neurodegenerative diseases","S. Chaabouni; F. Tison; J. Benois-Pineau; C. Ben Amar","LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, 33405 Talence Cedex, France","2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)","20160630","2016","","","1","6","As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video.","","Electronic:978-1-4673-8695-1; POD:978-1-4673-8696-8","10.1109/CBMI.2016.7500243","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500243","","Degradation;Diseases;Measurement;Predictive models;Sociology;Statistics;Visualization","diseases;medical image processing;video signal processing","automatic prediction model;deep CNN;deep learning architecture;dementia;gaze records;neurodegenerative diseases;visual attention prediction","","","","21","","","","15-17 June 2016","","IEEE","IEEE Conference Publications"
"HEp-2 cell classification using a deep neural network trained for natural image classification","B. Benligiray; H. Ç. Akakın","Elektrik-Elektronik M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, Anadolu &#220;niversitesi, Eski&#351;ehir, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","1361","1364","Deep convolutional neural networks is a recently developed method that yields very successful results in image classification. Deep neural networks, which have a high number of parameters, require a large amount of data to avoid overfitting during training. For applications in which the available data is not adequate to train a deep neural network from the scratch, deep neural networks trained for similar objectives can be used as a starting point. In this study, cell images are classified using a deep neural network trained to classify objects in natural images. Even though classification of natural images and cell images are very different objectives, cell images are able to be classified with 74.1% mean class accuracy. The results show that features used for visual classification by deep convolutional neural networks may be more universal than assumed.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7496001","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496001","HEp-2 cells;deep convolutional neural networks;image classification;indirect immunofluorescence","Computer vision;Conferences;Image recognition;Neural networks;Pattern recognition;Training;Visualization","image classification;learning (artificial intelligence);medical image processing;neural nets","HEp-2 cell classification;deep convolutional neural network training;natural image classification;visual classification","","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Detection of articulated instruments in retinal microsurgery","M. Alsheakhali; A. Eslami; N. Navab","Technical University of Munich (TUM), Germany","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","107","110","Instrument detection in retinal microsurgery is still one of the most challenging operations due to illumination changes, fast motion, cluttered background and deformable shape of the instrument. In this work, a new technique is proposed to detect an articulated forceps instrument by modeling it using Conditional Random Field (CRF). The unary potentials of the CRF, which represent the instrument parts, are detected using the deep convolutional neural network, where two probability distribution maps for both the forceps center and its shaft are estimated. The pairwise potentials are modeled using a regression random forest to learn the relation between the instrument parts based on their joint structural features. Sampled combinations from both unary distributions are selected, and each is tested using the regression forest to compute its similarity to the medical instrument structure. The best combination candidate chosen by the CRF predicts the forceps center point (instrument joint point) and the orientation of its shaft. The approach shows high detection accuracy on public datasets and real videos for retinal microsurgery operations.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493222","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493222","Conditional Random Field;Deep Learning;Instrument Detection;Retinal Microsurgery","Feature extraction;Instruments;Microsurgery;Retina;Shafts;Testing","","","","","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Comprehensive autoencoder for prostate recognition on MR images","K. Yan; C. Li; X. Wang; Y. Yuan; A. Li; J. Kim; B. Li; D. Feng","School of Information Technologies, University of Sydney","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1190","1194","Automatic recognition of anatomical structures is an essential prerequisite in computer aided diagnoses (CAD) such as tissue segmentation, physiological signal measurement and disease classification. However, insufficient color and speckle information in medical images pose challenges to the recognition of anatomical structures. Such challenges are evident with prostate recognition on magnetic resonance (MR) images and thus remain an open problem, although prostate cancer is an important problem that are attracting increasing interests in medical imaging. In this study, we propose an automatic approach for prostate recognition on MR images. Firstly, compared to existing works which integrate autoencoder with a specific type of classifier, we let autoencoder itself serve as a classifier and therefore lessening the impact from irregular and complex background found in prostate recognition. Secondly, an image energy minimization scheme with consideration of the coherence information from neighboring pixels is proposed to improve the recognition results with clear boundary appearance. We evaluate our method in comparison with three widely applied classifiers and the phase of atlas-based seeds-selection in prostate segmentation on a public prostate database. Our experiment results demonstrate significant superiority of our method in terms of both precision and F-measure.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493479","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493479","autoencoder;classification;deep learning;prostate recognition","Anatomical structure;Feature extraction;Image recognition;Image reconstruction;Image segmentation;Training","","","","","","29","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Automated mitosis detection with deep regression networks","H. Chen; X. Wang; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1204","1207","Mitosis counting is one of the strongest prognostic markers for invasive breast cancer diagnosis. Clinical visual examination on histology slides by pathologists is tedious, error-prone and time-consuming. Furthermore, with the advent of whole slide imaging for high-throughput digitization, a large quantity of histology images need to be analyzed. Therefore, automated mitosis detection methods are highly demanded in clinical practice. In this paper, we proposed a deep regression network (DRN) to meet these challenges. It consisted of a downsampling path for extracting the high level information and an upsampling path for outputting the score map with original input size, thus it can be trained in an end-to-end way. In addition, we transferred knowledge learned from cross domains to mitigate the issue of insufficient medical training data. Experimental results on the benchmark dataset 2012ICPR Mitosis Detection Challenge demonstrated the efficacy of our approach, which achieved comparable or better performance than the state-of-the-art methods.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493482","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493482","Mitosis detection;convolutional neural network;deep learning;regression","Benchmark testing;Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Pathology;Training","biological tissues;cancer;learning (artificial intelligence);medical image processing;regression analysis","automated mitosis detection;benchmark dataset 2012 ICPR Mitosis Detection Challenge;clinical visual examination;deep regression network;deep regression networks;downsampling path;high level information;high-throughput digitization;histology imaging;histology slides;invasive breast cancer diagnosis;medical training data;prognostic markers;upsampling path;whole slide imaging","","","","20","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Multi-loss convolutional networks for gland analysis in microscopy","A. BenTaieb; J. Kawahara; G. Hamarneh","Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","642","645","Manual tissue diagnosis is the most prevalent approach to cancer diagnosis. However, it mainly relies on a subjective visual quantification of specific morphometric features, which often leads to a relatively limited reproducibility among experts. In most computational techniques proposed to automate the diagnostic procedure, accurate segmentation is paramount as a precursor to the extraction of relevant morphometric features. Since the ultimate goal of segmentation is generally classification, yet a given class imparts an expected tissue appearance beneficial to segmentation, we pose the problem of automatic tissue analysis as the joint task of segmentation and classification. We propose a novel multi-objective learning method that optimizes a single unified deep fully convolutional neural network with two distinct loss functions. We illustrate our reasoning on the task of colon adenocarcinomas diagnosis and show how glands' classification can facilitate their segmentation by adding class-specific spatial priors. The final classification also benefits from this joint learning framework yielding an improvement of 6% over classification-only models.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493349","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493349","Classification;Deep Learning;Histopathology;Segmentation","Cancer;Colon;Feature extraction;Glands;Image segmentation;Training;Tumors","cancer;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;optimisation","automatic tissue analysis;cancer diagnosis;colon adenocarcinoma diagnosis;gland classification;image segmentation;microscopy;morphometric feature extraction;multiloss convolutional networks;multiobjective learning method;optimization","","","","6","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Automatic segmentation of the left ventricle in cardiac CT angiography using convolutional neural networks","M. Zreik; T. Leiner; B. D. de Vos; R. W. van Hamersvelt; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, The Netherlands","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","40","43","Accurate delineation of the left ventricle (LV) is an important step in evaluation of cardiac function. In this paper, we present an automatic method for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation is performed in two stages. First, a bounding box around the LV is detected using a combination of three convolutional neural networks (CNNs). Subsequently, to obtain the segmentation of the LV, voxel classification is performed within the defined bounding box using a CNN. The study included CCTA scans of sixty patients, fifty scans were used to train the CNNs for the LV localization, five scans were used to train LV segmentation and the remaining five scans were used for testing the method. Automatic segmentation resulted in the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1 mm. The results demonstrate that automatic segmentation of the LV in CCTA scans using voxel classification with convolutional neural networks is feasible.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493206","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493206","Cardiac CT Angiography;Classification;Convolutional Neural Network;Deep learning;Left ventricle segmentation","Biomedical imaging;Computed tomography;Heart;Image segmentation;Manuals;Neural networks;Observers","","","","","","16","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation","M. Ghafoorian; N. Karssemeijer; T. Heskes; I. W. M. van Uder; F. E. de Leeuw; E. Marchiori; B. van Ginneken; B. Platel","Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, the Netherlands","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1414","1417","Convolutional neural networks (CNN) have been widely used for visual recognition tasks including semantic segmentation of images. While the existing methods consider uniformly sampled single-or multi-scale patches from the neighborhood of each voxel, this approach might be sub-optimal as it captures and processes unnecessary details far away from the center of the patch. We instead propose to train CNNs with non-uniformly sampled patches that allow a wider extent for the sampled patches. This results in more captured contextual information, which is in particular of interest for biomedical image analysis, where the anatomical location of imaging features are often crucial. We evaluate and compare this strategy for white matter hyperintensity segmentation on a test set of 46 MRI scans. We show that the proposed method not only outperforms identical CNNs with uniform patches of the same size (0.780 Dice coefficient compared to 0.736), but also gets very close to the performance of an independent human expert (0.796 Dice coefficient).","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493532","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493532","convolutional neural network;deep learning;non-uniform patch;white matter hyperintensity","Biological neural networks;Biomedical imaging;Convolution;Image segmentation;Sampling methods;Training;Visualization","biomedical MRI;brain;feature extraction;image sampling;image segmentation;medical image processing;neural nets;neurophysiology","Dice coefficient;MRI scans;anatomical location;biomedical image analysis;captured contextual information;deep convolutional neural networks;identical CNNs;imaging features;independent human expert;nonuniform patch sampling;sampled multiscale patches;sampled single-scale patches;semantic image segmentation;visual recognition tasks;white matter hyperintensity segmentation","","1","","9","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Structure-based assessment of cancerous mitochondria using deep networks","M. Mishra; S. Schmitt; L. Wang; M. K. Strasser; C. Marr; N. Navab; H. Zischka; T. Peng","Computer Aided Medical Procedures (CAMP), Technische Universitaet Muenchen, Germany","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","545","548","Mitochondrial functions are essential for cell survival. Pathologic situations, e.g. cancer, can impair mitochondrial function which is frequently reflected by an altered morphology. So far, feature description of mitochondrial structure in cancer remains largely qualitative. In this study, we propose a learning-based approach to quantitatively assess the structure of mitochondria isolated from liver tumor cell lines using convolutional neural network (CNN). Besides achieving a high classification accuracy on isolated mitochondria from healthy tissue and different tumor cell lines which the CNN model was trained on, CNN is also able to classify unseen tumor cell lines, which suggests its superior capability to capture the intrinsic structural transition from healthy to tumor mitochondria.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493327","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493327","Mitochondria;convolutional neural network;deep learning;electron microscopy","Indexes;Liver;Standards;Support vector machines;Training;Tumors","cancer;cellular biophysics;learning (artificial intelligence);liver;medical image processing;microorganisms;neurophysiology;tumours","CNN model;altered morphology;cancer;cancerous mitochondria;cell survival;convolutional neural network;deep networks;healthy tumor mitochondria;high classification accuracy;learning-based approach;liver tumor cell lines;mitochondria isolated structure;mitochondrial functions;mitochondrial structure;pathologic situations;structural transition;structure-based assessment;tumor cell lines","","","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Describing ultrasound video content using deep convolutional neural networks","Y. Gao; M. A. Maraci; J. A. Noble","Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, UK","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","787","790","We address the task of object recognition in obstetric ultrasound videos using deep Convolutional Neural Networks (CNNs). A transfer learning based design is presented to study the transferability of features learnt from natural images to ultrasound image object recognition which on the surface is a very different problem. Our results demonstrate that CNNs initialised with large-scale pre-trained networks outperform those directly learnt from small-scale ultrasound data (91.5% versus 87.9%), in terms of object identification.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493384","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493384","Classification;Convolutional neural networks;Obstetric ultrasound;Transfer learning","Abdomen;Data visualization;Feature extraction;Heart;Standards;Training;Ultrasonic imaging","biomedical ultrasonics;image recognition;medical image processing;neural nets;object recognition;obstetrics","deep convolutional neural networks;object identification;obstetric ultrasound videos;small-scale ultrasound data;transfer learning-based design;ultrasound image object recognition;ultrasound video content","","1","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"A hybrid learning approach for semantic labeling of cardiac CT slices and recognition of body position","M. Moradi; Y. Gur; H. Wang; P. Prasanna; T. Syeda-Mahmood","IBM Research - Almaden Research Center, San Jose, CA","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1418","1421","We work towards efficient methods of categorizing visual content in medical images as a precursor step to segmentation and anatomy recognition. In this paper, we address the problem of automatic detection of level/position for a given cardiac CT slice. Specifically, we divide the body area depicted in chest CT into nine semantic categories each representing an area most relevant to the study of a disease and/or key anatomic cardiovascular feature. Using a set of handcrafted image features together with features derived form a deep convolutional neural network (CNN), we build a classification scheme to map a given CT slice to the relevant level. Each feature group is used to train a separate support vector machine classifier. The resulting labels are then combined in a linear model, also learned from training data. We report margin zero and margin one accuracy of 91.7% and 98.8% and show that this hybrid approach is a very effective methodology for assigning a given CT image to a relatively narrow anatomic window.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493533","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493533","cardiac CT;image category classification;slice level recognition","Biomedical imaging;Computed tomography;Convolution;Feature extraction;Image recognition;Semantics;Support vector machines","cardiovascular system;computerised tomography;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines","anatomic cardiovascular feature;anatomy recognition;automatic detection;body area;body position recognition;cardiac CT slices;chest CT;classification scheme;deep convolutional neural network;disease;handcrafted image features;hybrid learning approach;medical images;narrow anatomic window;segmentation;semantic categories;semantic labeling;separate support vector machine classifier;training data;visual content","","1","","13","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Sub-cortical brain structure segmentation using F-CNN'S","M. Shakeri; S. Tsogkas; E. Ferrante; S. Lippe; S. Kadoury; N. Paragios; I. Kokkinos","Polytechnique Montreal","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","269","272","In this paper we propose a deep learning approach for segmenting sub-cortical structures of the human brain in Magnetic Resonance (MR) image data. We draw inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN) architecture for semantic segmentation of objects in natural images, and adapt it to our task. Unlike previous CNN-based methods that operate on image patches, our model is applied on a full blown 2D image, without any alignment or registration steps at testing time. We further improve segmentation results by interpreting the CNN output as potentials of a Markov Random Field (MRF), whose topology corresponds to a volumetric grid. Alpha-expansion is used to perform approximate inference imposing spatial volumetric homogeneity to the CNN priors. We compare the performance of the proposed pipeline with a similar system using Random Forest-based priors, as well as state-of-art segmentation algorithms, and show promising results on two different brain MRI datasets.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493261","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493261","Convolutional neural networks;Magnetic Resonance Imaging;Markov Random Fields;semantic segmentation;sub-cortical structures","Brain;Image segmentation;Magnetic resonance imaging;Markov random fields;Semantics;Three-dimensional displays;Training","Markov processes;biomedical MRI;brain;image segmentation;medical image processing;neural nets","F-CNN architecture;Markov random field;brain MRI dataset;deep learning approach;fully-convolutional neural network;human brain;image patch;image registration;image segmentation;magnetic resonance image data;random forest-based prior;semantic segmentation;subcortical brain structure segmentation","","","","12","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Handcrafted features with convolutional neural networks for detection of tumor cells in histology images","M. N. Kashif; S. E. A. Raza; K. Sirinukunwattana; M. Arif; N. Rajpoot","Department of Electrical Engineering, Pakistan Institute of Engineering and Applied Sciences, Islamabad, Pakistan","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1029","1032","Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493441","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493441","Convolutional Neural Network;Digital Pathology;Scattering Transform;Tumor Nuclei Detection","Feature extraction;Image color analysis;Neural networks;Scattering;Standards;Transforms;Tumors","cellular biophysics;feature extraction;medical image processing;neural nets;tumours","cancer histology images;handcrafted features;nonlinear invariant texture features;scattering transform;spatially constrained convolutional neural networks;tumor cell detection","","","","13","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Real-time 2D/3D registration via CNN regression","S. Miao; Z. J. Wang; Y. Zheng; R. Liao","Electrical and Computer Engineering, University of British Columbia, Canada","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1430","1434","In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493536","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493536","2-D/3-D Registration;Convolutional Neural Network;Deep Learning;Image Guided Intervention","Biomedical imaging;Feature extraction;Neural networks;Real-time systems;Solid modeling;Training;X-ray imaging","","","","","","18","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Detection of age-related macular degeneration via deep learning","P. Burlina; D. E. Freund; N. Joshi; Y. Wolfson; N. M. Bressler","Applied Physics Laboratory, Johns Hopkins University","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","184","188","Age-related macular generation (AMD) - when left untreated - is the main cause of blindness for individuals over the age of 50. With the US population now counting over 100 million individuals over 50, it is imperative to develop methods that can effectively determine which individuals with an earlier, often asymptomatic stage, are at risk of developing the advanced stage that can cause severe vision loss. This paper studies the appropriateness of the transfer of image features computed from pre-trained deep neural networks to the problem in AMD detection. Tests using over 5600 images from the NIH AREDS dataset (the largest dataset used thus far for AMD image analysis studies) show good preliminary results (between nearly 92% and 95% accuracy).","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493240","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493240","Age-related macular degeneration;deep learning;pre-trained networks","Blindness;Classification algorithms;Feature extraction;Machine learning;Retina;Sensitivity;Support vector machines","biomedical optical imaging;feature extraction;learning (artificial intelligence);medical image processing;neural nets;vision defects","AMD detection;AMD image analysis;NIH AREDS dataset;age-related macular degeneration;asymptomatic stage;blindness;deep learning;image features;pretrained deep neural networks;severe vision loss","","","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"A new NMF-autoencoder based CAD system for early diagnosis of prostate cancer","I. Reda; A. Shalaby; M. A. El-Ghar; F. Khalifa; M. Elmogy; A. Aboulfotouh; E. Hosseini-Asl; A. El-Baz; R. Keynton","Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1237","1240","In this paper, we propose a novel non-invasive framework for the early diagnosis of prostate cancer from diffusion-weighted magnetic reasoning imaging (DW-MRI). The proposed approach consists of three main steps. In the first step, the prostate is localized and segmented based on a new level-set model. This model is guided by a stochastic speed function that is derived using nonnegative matrix factorization (NMF). The NMF attributes are calculated using information from the MRI intensity, a probabilistic shape model, and the spatial interactions between prostate voxels. In the second step, the apparent diffusion coefficient (ADC) of the segmented prostate volume is mathematically calculated for different b-values. To preserve continuity, the calculated ADC values are normalized and refined using a Generalized Gauss-Markov Random Field (GGMRF) image model. The cumulative distribution function (CDF) of refined ADC for the prostate tissues at different b-values are then constructed. These CDFs are considered as global features which can be used to distinguish between benign and malignant tumors. Finally, a deep learning auto-encoder network, trained by a non-negativity constraint algorithm (NCAE), is used to classify the prostate tumor as benign or malignant based on the CDFs extracted from the previous step. Preliminary experiments on 42 clinical DW-MRI data sets resulted in 97.6% correct classification (sensitivity = 100% and specificity = 95.24%), indicating the high accuracy of the proposed framework.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493490","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493490","CAD;MGRF;NMF;Prostate cancer","Design automation;Image segmentation;Magnetic resonance imaging;Prostate cancer;Shape;Solid modeling","Markov processes;biodiffusion;biomedical MRI;cancer;feature extraction;image classification;image coding;image segmentation;learning (artificial intelligence);matrix decomposition;medical image processing;probability;random processes;set theory;tumours","DW-MRI;MRI intensity;NMF-autoencoder based CAD system;apparent diffusion coefficient;benign tumor;cumulative distribution function;deep learning autoencoder network;diffusion-weighted magnetic reasoning imaging;generalized Gauss-Markov random field image model;level-set model;malignant tumor;noninvasive framework;nonnegative matrix factorization;nonnegativity constraint algorithm;probabilistic shape model;prostate cancer early diagnosis;prostate tissues;prostate tumor classification;prostate voxels;segmented prostate volume;spatial interactions;stochastic speed function","","2","","21","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Segmenting overlapping cervical cell in Pap smear images","Y. Song; J. Z. Cheng; D. Ni; S. Chen; B. Lei; T. Wang","Department of Biomedical Engineering, School of Medicine, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultra-sound Imaging, Shenzhen, China","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1159","1162","Accurate segmentation of cervical cells in Pap smear images is an important task for automatic identification of pre-cancerous changes in the uterine cervix. One of the major segmentation challenges is the overlapping of cytoplasm, which was less addressed by previous studies. In this paper, we propose a learning-based method to tackle the overlapping issue with robust shape priors by segmenting individual cell in Pap smear images. Specifically, we first define the problem as a discrete labeling task for multiple cells with a suitable cost function. We then use the coarse labeling result to initialize our dynamic multiple-template deformation model for further boundary refinement on each cell. Multiple-scale deep convolutional networks are adopted to learn the diverse cell appearance features. Also, we incorporate high level shape information to guide segmentation where the cells boundary is noisy or lost due to touching and overlapping cells. We evaluate the proposed algorithm on two different datasets, and our comparative experiments demonstrate the promising performance of the proposed method in terms of segmentation accuracy.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493472","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493472","Cervical cancer;dynamic multiple-template deformation model;multiple-scale deep convolutional networks;overlapping cells splitting","Biomedical imaging;Blood;Cervical cancer;Image segmentation;Labeling;Level set;Shape","","","","1","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Towards grading gleason score using generically trained deep convolutional neural networks","H. Källén; J. Molin; A. Heyden; C. Lundström; K. Åström","Centre for Mathematical Sciences, Lund University","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1163","1167","We developed an automatic algorithm with the purpose to assist pathologists to report Gleason score on malignant prostatic adenocarcinoma specimen. In order to detect and classify the cancerous tissue, a deep convolutional neural network that had been pre-trained on a large set of photographic images was used. A specific aim was to support intuitive interaction with the result, to let pathologists adjust and correct the output. Therefore, we have designed an algorithm that makes a spatial classification of the whole slide into the same growth patterns as pathologists do. The 22-layer network was cut at an earlier layer and the output from that layer was used to train both a random forest classifier and a support vector machines classifier. At a specific layer a small patch of the image was used to calculate a feature vector and an image is represented by a number of those vectors. We have classified both the individual patches and the entire images. The classification results were compared for different scales of the images and feature vectors from two different layers from the network. Testing was made on a dataset consisting of 213 images, all containing a single class, benign tissue or Gleason score 35. Using 10-fold cross validation the accuracy per patch was 81 %. For whole images, the accuracy was increased to 89 %.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493473","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493473","Convolutional Neural Networks;Deep Learning;Gleason Score;Prostate cancer","Cancer;Feature extraction;Kernel;Neural networks;Radio frequency;Support vector machines;Training","","","","","","15","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning","A. Murali; A. Garg; S. Krishnan; F. T. Pokorny; P. Abbeel; T. Darrell; K. Goldberg","EECS & IEOR, University of California, Berkeley USA","2016 IEEE International Conference on Robotics and Automation (ICRA)","20160609","2016","","","4150","4157","The growth of robot-assisted minimally invasive surgery has led to sizable datasets of fixed-camera video and kinematic recordings of surgical subtasks. Segmentation of these trajectories into locally-similar contiguous sections can facilitate learning from demonstrations, skill assessment, and salvaging good segments from otherwise inconsistent demonstrations. Manual, or supervised, segmentation can be prone to error and impractical for large datasets. We present Transition State Clustering with Deep Learning (TSC-DL), a new unsupervised algorithm that leverages video and kinematic data for task-level segmentation, and finds regions of the visual feature space that correlate with transition events using features constructed from layers of pre-trained image classification Deep Convolutional Neural Networks (CNNs). We report results on three datasets comparing Deep Learning architectures (AlexNet and VGG), choice of convolutional layer, dimensionality reduction techniques, visual encoding, and the use of Scale Invariant Feature Transforms (SIFT). We find that the deep architectures extract features that result in up-to a 30.4% improvement in Silhouette Score (a measure of cluster tightness) over the traditional “shallow” features from SIFT. We also present cases where TSC-DL discovers human annotator omissions. Supplementary material, data and code is available at: http://berkeleyautomation.github.io/tsc-dl/.","","Electronic:978-1-4673-8026-3; POD:978-1-4673-8027-0","10.1109/ICRA.2016.7487607","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487607","","Feature extraction;Hidden Markov models;Kinematics;Machine learning;Motion segmentation;Visualization","control engineering computing;convolution;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;medical robotics;neural nets;pattern clustering;robot vision;surgery;transforms;video signal processing","TSC-DL unsupervised algorithm;dimensionality reduction;feature extraction;fixed-camera video;image classification deep convolutional neural networks;learning from demonstrations;multimodal surgical demonstrations;robot-assisted minimally invasive surgery;scale invariant feature transforms;surgical subtask kinematic recordings;task-level segmentation;transition state clustering with deep learning;unsupervised trajectory segmentation;visual encoding;visual feature space","","1","","29","","","","16-21 May 2016","","IEEE","IEEE Conference Publications"
"A deep convolutional neural network trained on representative samples for circulating tumor cell detection","Y. Mao; Z. Yin; J. Schober","Missouri University of Science and Technology","2016 IEEE Winter Conference on Applications of Computer Vision (WACV)","20160526","2016","","","1","6","The number of Circulating Tumor Cells (CTCs) in blood indicates the tumor response to chemotherapeutic agents and disease progression. In early cancer diagnosis and treatment monitoring routine, detection and enumeration of CTCs in clinical blood samples have significant applications. In this paper, we design a Deep Convolutional Neural Network (DCNN) with automatically learned features for image-based CTC detection. We also present an effective training methodology which finds the most representative training samples to define the classification boundary between positive and negative samples. In the experiment, we compare the performance of auto-learned feature from DCNN and hand-crafted features, in which the DCNN outperforms hand-crafted feature. We also prove that the proposed training methodology is effective in improving the performance of DCNN classifiers.","","Electronic:978-1-5090-0641-0; POD:978-1-5090-0642-7","10.1109/WACV.2016.7477603","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477603","","Blood;Cancer;Cells (biology);Detectors;Feature extraction;Training;Tumors","diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;tumours","CTC detection;DCNN training;chemotherapeutic agent;circulating tumor cell detection;classification boundary;deep convolutional neural network;disease progression;feature autolearning;tumor response","","","","16","","","","7-10 March 2016","","IEEE","IEEE Conference Publications"
"Deep multi-view representation learning for multi-modal features of the schizophrenia and schizo-affective disorder","J. Qi; J. Tejedor","Electrical Engineering, University of Washington, Seattle, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","952","956","This work is originated from the MLSP 2014 Classification Challenge which tries to automatically detect subjects with schizophrenia and schizo-affective disorder by analyzing multi-modal features derived from magnetic resonance imaging (MRI) data. We employ Deep Neural Network (DNN)-based multi-view representation learning for combining multimodal features. The DNN-based multi-view models include deep canonical correlation analysis (DCCA) and deep canonically correlated auto-encoders (DCCAE). In addition, support vector machine with Gaussian kernel is used to conduct classification with the compact bottleneck features learned by the deep multi-view models. Our experiments on the dataset provided by the MLSP Classification Challenge show that bottleneck features learned via deep multi-view models obtain better results than the trimming features used in the baseline system in terms of the receiver operating characteristic (ROC) area under the curve (AUC).","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7471816","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471816","Deep Canonical Correlation Analysis;Deep Canonically Correlated Auto-encoders;MRI;ROC/AUC;Schizophrenia;Support Vector Machine","Correlation;Covariance matrices;Feature extraction;Kernel;Magnetic resonance imaging;Support vector machines;Training","Gaussian processes;biomedical MRI;diseases;feature extraction;image representation;learning (artificial intelligence);medical image processing;support vector machines","AUC;DCCA;Gaussian kernel;MRI data;ROC;area under the curve;deep canonical correlation analysis;deep multiview representation learning;deep neural network;magnetic resonance imaging data;multimodal features;receiver operating characteristic;schizo-affective disorder;schizophrenia disorder;support vector machine","","","","16","","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Gold classification of COPDGene cohort based on deep learning","J. Ying; J. Dutta; N. Guo; L. Xia; A. Sitek; Q. Li; Q. Li","Nuclear Medicine and Molecular Imaging Radiology Department, Massachusetts General Hospital, Boston, MA, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","2474","2478","This study aims to employ deep learning for the development of an automatic classifier for the severity of chronic obstructive pulmonary disease (COPD) in patients. A three-layer deep belief network (DBN) with two hidden layers and one visible layer was employed to generate a model for classification, and the model's robustness against exacerbation was analyzed. Subjects from the COPDGene cohort were staged using the GOLD 2011 guidelines. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 97.2% by using a 10-fold cross validation experiment. The most sensitive features as revealed by the DBN weights were consistent with the clinical consensus as per previous studies and clinical diagnosis rules. In summary, we demonstrate that the DBN is a competitive tool for exacerbation risk assessment for patients suffering from, COPD.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472122","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472122","Chronic Obstructive Pulmonary Disease (COPD);Deep Belief Networks (DBNs);Global Initiative for Chronic Obstructive Lung Disease (GOLD);classification;deep learning","Diseases;Feature extraction;Gold;Lungs;Machine learning;Medical diagnostic imaging;Training","belief networks;biology computing;diseases;feature selection;learning (artificial intelligence);optimisation;pattern classification","COPD gene cohort;DBN weights;automatic gold classification;chronic obstructive pulmonary disease;deep learning;exacerbation risk assessment;feature selection;parameter optimization;three-layer deep belief network","","","","19","","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Latent feature representation with 3-D multi-view deep convolutional neural network for bilateral analysis in digital breast tomosynthesis","D. H. Kim; S. T. Kim; Y. M. Ro","School of Electrical Engineering, KAIST, Daejeon, 305-701, Republic of Korea","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","927","931","In clinical studies of breast cancer, masses appear as asymmetric densities between the left and the right breasts, which show different breast tissue structures. For classifying breast masses, most researchers have developed hand-crafted bilateral features by extracting the asymmetric information in 2-D mammograms. In digital breast tomosynthesis (DBT), which has 3D volume data, effective bilateral features are needed to detect masses. In this paper, we propose latent bilateral feature representation with 3-D multi-view deep convolutional neural network (DCNN) in the DBT reconstructed volume. The proposed DCNN is designed to discover hidden or latent bilateral feature representation of masses in self-taught learning. Experimental results show that the proposed latent bilateral feature representation outperforms conventional hand-crafted features by achieving a high area under the receiver operating characteristic curve.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7471811","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471811","Digital breast tomosynthesis;bilateral analysis;deep learning;false positive reduction;latent features","Biomedical imaging;Breast cancer;Feature extraction;Neural networks;Three-dimensional displays;Training","cancer;learning (artificial intelligence);mammography;medical signal processing","2D mammograms;3D multiview deep convolutional neural network;DBT;DCNN;asymmetric densities;breast cancer;breast tissue structures;digital breast tomosynthesis;hand-crafted bilateral features;latent bilateral feature representation;left breasts;receiver operating characteristic curve;right breasts;self-taught learning","","","","21","","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Discriminative feature extraction from X-ray images using deep convolutional neural networks","M. Srinivas; D. Roy; C. K. Mohan","VIsual learninG and InteLligence (VIGIL) group, Dept. of Computer Science and Engineering, Indian Institute of Technology Hyderabad","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","917","921","Feature extraction is one of the most important phases of medical image classification which requires extensive domain knowledge. Convolutional Neural Networks (CNN) have been successfully used for feature extraction in images from different domains involving a lot of classes. In this paper, CNNs are exploited to extract a hierarchical and discriminative representation of X-ray images. This representation is then used for classification of the X-ray images as various parts of the body. Visualization of the feature maps in the hidden layers show that features learnt by the CNN resemble the essential features which help discern the discrimination among different body parts. A comparison on the standard IRMA X-ray image dataset demonstrates that the CNNs easily outperform classifiers with hand-engineered features.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7471809","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471809","Convolutional Neural Networks (CNN);Feature Extraction;X-ray image","Biomedical imaging;Convolution;Feature extraction;Support vector machines;Training;Visualization;X-ray imaging","X-ray imaging;feature extraction;image classification;image representation;medical image processing;neural nets","CNN;X-ray image classification;deep convolutional neural networks;discriminative feature extraction;discriminative representation;feature map visualization;hierarchical representation;medical image classification;standard IRMA X-ray image dataset","","","","25","","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","20160503","2016","35","5","1170","1181","Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","0278-0062;02780062","","10.1109/TMI.2015.2482920","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156","Computer aided diagnosis;artificial neural networks;computed tomography;deep learning;machine learning;medical diagnostic imaging;multi-layer neural network;object detection","Colonic polyps;Computed tomography;Feature extraction;Lymph nodes;Three-dimensional displays;Training","computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","classification probability;colonic polyp detection;computed tomography;computer-aided detection;deep convolutional neural network classifier training;false positives;lymph node detection;medical imaging;random rotations;random translations;random view aggregation;scale transformations;sclerotic metastasis detection;two-tiered coarse-to-fine cascade framework","","11","","60","","","20150928","May 2016","","IEEE","IEEE Journals & Magazines"
"AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images","S. Albarqouni; C. Baur; F. Achilles; V. Belagiannis; S. Demirci; N. Navab","Chair for Computer Aided Medical Procedure (CAMP), Technische Universit&#x00E4;t M&#x00FC;nchen (TUM), Munich, Germany","IEEE Transactions on Medical Imaging","20160502","2016","35","5","1313","1321","The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.","0278-0062;02780062","","10.1109/TMI.2016.2528120","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405343","Aggregation;crowdsourcing;deep learning;gamification;online learning","Biomedical imaging;Computational modeling;Crowdsourcing;Data models;Machine learning;Noise measurement;Robustness","biological organs;cancer;data aggregation;image classification;image denoising;learning (artificial intelligence);medical image processing","biomedical image database;breast cancer histology imaging;conventional machine-learning methods;convolutional neural network;crowd annotation datasets;crowd flower API;crowd sourcing layer;data aggregation;deep CNN learning;ground-truth data;image annotation tasks;learning annotation models;learning process;mitosis detection;noisy annotations;self-implemented web-platform","","12","","38","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale Feature Integration Applied to Multiple Sclerosis Lesion Segmentation","T. Brosch; L. Y. W. Tang; Y. Yoo; D. K. B. Li; A. Traboulsee; R. Tam","Multiple Sclerosis/Magnetic Resonance Imaging Research Group, Division of Neurology, The University of British Columbia, Vancouver, Canada","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1229","1239","We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.","0278-0062;02780062","","10.1109/TMI.2016.2528821","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404285","Convolutional neural networks;deep learning;machine learning;magnetic resonance imaging (MRI);multiple sclerosis lesions;segmentation","Convolution;Feature extraction;Image segmentation;Imaging;Lesions;Neural networks;Training","biomedical MRI;feature extraction;image segmentation;medical image processing;neural nets","EMS;ISBI 2015 challenges;LST-LGA;LST-LPA;Lesion-TOADS;MICCAI 2008 challenges;MS clinical trial;MS lesion segmentation methods;SLS;automatic feature learning;convolutional pathway;deconvolutional pathway;deep 3D convolutional encoder networks;feature extraction;higher-level image features;interconnected pathways;low-level features;magnetic resonance images;multiple sclerosis lesion segmentation;multiscale feature integration;neural network;prediction pathways;publicly available data sets;segmentation task;shortcut connections;top-ranked state-of-the-art methods;voxel level","","14","","47","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?","N. Tajbakhsh; J. Y. Shin; S. R. Gurudu; R. T. Hurst; C. B. Kendall; M. B. Gotway; J. Liang","Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1299","1312","Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.","0278-0062;02780062","","10.1109/TMI.2016.2535302","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426826","Carotid intima-media thickness;computer-aided detection;convolutional neural networks;deep learning;fine-tuning;medical image analysis;polyp detection;pulmonary embolism detection;video quality assessment","Biomedical imaging;Computed tomography;Feature extraction;Image analysis;Image segmentation;Training;Tuning","biomedical optical imaging;endoscopes;image classification;image segmentation;medical image processing;neural nets","cardiology;classification;deep convolutional neural network;distinct medical imaging applications;gastroenterology;imaging modalities;labeled training data;layer-wise fine-tuning scheme;medical image analysis;radiology;segmentation","","34","","76","","","20160307","May 2016","","IEEE","IEEE Journals & Magazines"
"Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images","S. Pereira; A. Pinto; V. Alves; C. A. Silva","CMEMS-UMinho Research Unit, University of Minho, Guimar&#x00E3;es, Portugal","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1240","1251","Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 ×3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.","0278-0062;02780062","","10.1109/TMI.2016.2538465","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426413","Brain tumor;brain tumor segmentation;convolutional neural networks;deep learning;glioma;magnetic resonance imaging","Brain modeling;Context;Image segmentation;Kernel;Magnetic resonance imaging;Training;Tumors","biomedical MRI;brain;cancer;image segmentation;medical image processing;neurophysiology;tumours","CNN-based segmentation methods;Dice similarity coefficient metrics;MRI images;automatic segmentation;automatic segmentation methods;brain tumor segmentation;clinical practice;convolutional neural networks;data augmentation;gliomas;imaging technique;intensity normalization;kernels;magnetic resonance imaging;manual segmentation;on-site BRATS 2015 Challenge;oncological patients;online evaluation platform;precise quantitative measurements;preprocessing step;quality-of-life;reliable segmentation methods;spatial variability;structural variability","","15","","52","","","20160304","May 2016","","IEEE","IEEE Journals & Magazines"
"Automatic Segmentation of MR Brain Images With a Convolutional Neural Network","P. Moeskops; M. A. Viergever; A. M. Mendrik; L. S. de Vries; M. J. N. L. Benders; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1252","1261","Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T<sub>2</sub>-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T<sub>2</sub>-weighted images of preterm infants acquired at 40 weeks PMA, axial T<sub>1</sub>-weighted images of ageing adults acquired at an average age of 70 years, and T<sub>1</sub>-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.","0278-0062;02780062","","10.1109/TMI.2016.2548501","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444155","Adult brain;MRI;automatic image segmentation;convolutional neural networks;deep learning;preterm neonatal brain","Aging;Biomedical imaging;Brain;Convolution;Image segmentation;Kernel;Pediatrics","biological tissues;biomedical MRI;brain;image segmentation;medical image processing;neural nets;neurophysiology;paediatrics","acquisition protocol;ageing adults;automatic MR brain image segmentation;average Dice coefficients;axial T<sub>2</sub>-weighted images;convolutional neural network;coronal T<sub>2</sub>-weighted images;multiple convolution kernel sizes;multiple patch sizes;multiscale information;postmenstrual age;preterm infants;quantitative analysis;segmented tissue classes;spatial consistency;training data","","11","","46","","","20160330","May 2016","","IEEE","IEEE Journals & Magazines"
"Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring","M. Kallenberg; K. Petersen; M. Nielsen; A. Y. Ng; P. Diao; C. Igel; C. M. Vachon; K. Holland; R. R. Winkel; N. Karssemeijer; M. Lillholm","University of Copenhagen, Copenhagen, Denmark","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1322","1331","Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.","0278-0062;02780062","","10.1109/TMI.2016.2532122","Danish Advanced Technology Foundation; European Seventh Framework Programme FP7; Innovation Fund Denmark; 10.13039/501100000780 - European Commission; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412749","Breast cancer;deep learning;mammograms;prognosis;risk factor;segmentation;unsupervised feature learning","Breast cancer;Computer architecture;Image segmentation;Machine learning;Mammography;Manuals","cancer;image classification;image segmentation;image texture;mammography;medical image processing;risk analysis;tumours;unsupervised learning","breast cancer risk;breast density segmentation;clinical datasets;handcrafted feature extraction;learned features;learned texture scores;mammographic risk scoring;mammographic texture;population sparsity;simple classifier;sparsity regularizer;unlabeled data;unsupervised deep learning","","7","","67","","","20160218","May 2016","","IEEE","IEEE Journals & Magazines"
"q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI Scans","V. Golkov; A. Dosovitskiy; J. I. Sperl; M. I. Menzel; M. Czisch; P. Sämann; T. Brox; D. Cremers","The Department of Computer Science, Technical University of Munich, Garching, Germany","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1344","1351","Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.","0278-0062;02780062","","10.1109/TMI.2016.2551324","Deutsche Telekom Foundation; GE Global Research; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448418","Artificial neural networks;diffusion kurtosis imaging (DKI);diffusion magnetic resonance imaging (diffusion MRI);neurite orientation dispersion and density imaging (NODDI)","Data processing;Diffusion tensor imaging;Fitting;Machine learning;Pipelines;Training","biodiffusion;biomedical MRI;data acquisition;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;optimisation","accurate microstructural characterization;acquisition times;adults;artificial neural networks;children;classical data processing;clinical routine;data points;diffusion MRI data processing;diffusion kurtosis;diffusion magnetic resonance imaging;model-free diffusion MRI scans;neurite orientation dispersion;neuroimaging;noninvasive microstructure assessment method;q-space deep learning;scalar measures;single optimized step;suboptimal data processing pipelines;twelve-fold reduced scan time;twelve-fold shorter diffusion MRI scans","","4","","52","","","20160406","May 2016","","IEEE","IEEE Journals & Magazines"
"Locality Sensitive Deep Learning for Detection and Classification of Nuclei in Routine Colon Cancer Histology Images","K. Sirinukunwattana; S. E. A. Raza; Y. W. Tsang; D. R. J. Snead; I. A. Cree; N. M. Rajpoot","Department of Computer Science, University of Warwick, Coventry, UK","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1196","1206","Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.","0278-0062;02780062","","10.1109/TMI.2016.2525803","10.13039/100008982 - Qatar National Research Fund; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399414","Convolutional neural network;deep learning;histology image analysis;nucleus detection","Cancer;Computer architecture;Feature extraction;Machine learning;Microprocessors;Shape;Tumors","biological organs;biomedical optical imaging;cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;probability;tumours","NEP;SC-CNN;cancerous tissue;cell nuclei classification;cell nuclei detection;cellular heterogeneity;colorectal adenocarcinoma images;dataset;eosin stain;high-probability values;highest average F1 score;histopathology images;joint detection;locality sensitive deep learning;neighboring ensemble predictor;quantitative analysis;routine colon cancer histology images;spatially constrained convolutional neural network;standard hematoxylin;tissue constituents;whole-slide images","","16","","38","","","20160204","May 2016","","IEEE","IEEE Journals & Magazines"
"Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines","G. van Tulder; M. de Bruijne","Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1262","1272","The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.","0278-0062;02780062","","10.1109/TMI.2016.2526687","10.13039/501100003246 - Nederlandse Organisatie voor Wetenschappelijk Onderzoek; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401039","Deep learning;X-ray imaging and computed tomography;lung;machine learning;neural network;pattern recognition and classification;representation learning;restricted Boltzmann machine;segmentation","Computed tomography;Feature extraction;Learning systems;Lungs;Neural networks;Standards;Training data","Boltzmann machines;biological tissues;channel bank filters;computerised tomography;feature extraction;image classification;image filtering;image texture;lung;medical image processing;pneumodynamics","airway detection;convolutional restricted Boltzmann machines;discriminative representation learning;feature description;generative learning objective;generative representation learning;lung CT analysis;lung texture classification;lung tissue classification accuracy;standard predefined filter banks;tissue classification system;training data;unlabeled data representations","","5","","47","","","20160208","May 2016","","IEEE","IEEE Journals & Magazines"
"Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks","Q. Dou; H. Chen; L. Yu; L. Zhao; J. Qin; D. Wang; V. C. Mok; L. Shi; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1182","1195","Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.","0278-0062;02780062","","10.1109/TMI.2016.2528129","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403984","3D convolutional neural networks;biomarker detection;cerebral microbleeds;deep learning;susceptibility-weighted imaging","Biomarkers;Feature extraction;Kernel;MIMICs;Medical diagnostic imaging;Three-dimensional displays","biomedical MRI;blood;blood vessels;brain;cognition;diseases;feature extraction;haemodynamics;medical image processing;neurophysiology;probability","3D FCN strategy;3D convolutional neural networks;3D fully convolutional network strategy;CMB detection;MR volume extraction;MRI;automatic cerebral microbleed detection;blood vessels;cerebrovascular diseases;cognitive dysfunctions;current clinical routine;diagnostic biomarkers;haemorrhages;low-level hand-crafted descriptors;magnetic resonance images;massive redundant computations;probabilities;radiologists;representative high-level features;spatial contextual information;traditional sliding window strategy;well-trained 3D CNN discrimination","","15","","52","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"A CNN Regression Approach for Real-Time 2D/3D Registration","S. Miao; Z. J. Wang; R. Liao","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1352","1363","In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.","0278-0062;02780062","","10.1109/TMI.2016.2521800","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393571","2-D/3-D registration;convolutional neural network;deep learning;image guided intervention","Attenuation;Biomedical imaging;Computed tomography;Feature extraction;Real-time systems;X-ray imaging","diagnostic radiography;feature extraction;image reconstruction;image registration;iterative methods;medical image processing;neural nets;optimisation;regression analysis","3D pose-indexed features;CNN regression approach;CNN regressors;X-ray images;automatic feature extraction step;complex regression task;convolutional neural network regression approach;digitally reconstructed radiograph;formation parameters;intensity-based 2D-3D registration technology;intensity-based methods;iterative optimization;memory footprint;multiple simpler subtasks;optimization-based methods;real-time 2D-3D registration;scalar-valued metric function;transformation parameters","","4","","31","","","20160126","May 2016","","IEEE","IEEE Journals & Magazines"
"Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks","A. A. A. Setio; F. Ciompi; G. Litjens; P. Gerke; C. Jacobs; S. J. van Riel; M. M. W. Wille; M. Naqibullah; C. I. Sánchez; B. van Ginneken","Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1160","1169","We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.","0278-0062;02780062","","10.1109/TMI.2016.2536809","The Netherlands Organization for Scientific Research; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422783","Computed tomography;computer-aided detection;convolutional networks;deep learning;lung cancer;pulmonary nodule","Cancer;Computed tomography;Design automation;Feature extraction;Lesions;Lungs;Solids","cancer;computerised tomography;feature extraction;image classification;image fusion;medical image processing;tumours","2D ConvNets;2D patches;ANODE09 challenge;CAD system;CT images;computer-aided detection system;data augmentation;dedicated fusion method;differently oriented planes;discriminative features;false positive reduction;final classification;multiple streams;multiview ConvNets;multiview convolutional networks;nodule candidates;publicly available LIDC-IDRI dataset;pulmonary nodule detection;training data","","10","1","47","","","20160301","May 2016","","IEEE","IEEE Journals & Magazines"
"Multimodal fusion of brain structural and functional imaging with a deep neural machine translation approach","M. F. Amin; S. M. Plis; E. Damaraju; D. Hjelm; K. Cho; V. D. Calhoun","The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA","2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)","20160428","2016","","","1","4","In this work, we study a novel approach of deep neural machine translation to find linkage between multimodal brain imaging data, such as structural MRI (sMRI) and functional MRI (fMRI). The idea is to consider two different imaging views of the same brain like two different languages conveying some common concepts or facts. An important aspect of the translation model is an attention network module that learns alignment between features from fMRI and sMRI. We use independent component analysis (ICA) based features for the translation model. Our study shows significant group differences between healthy controls and patients with schizophrenia in the learned alignments. Furthermore, this novel approach reveals a group differential relation between a cognitive score (attention and vigilance) and alignments that could not be found when individual modality of data were considered.","","Electronic:978-1-4673-9919-7; POD:978-1-4673-9920-3; USB:978-1-4673-9918-0","10.1109/SSIAI.2016.7459160","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459160","","Brain modeling;Correlation;Feedforward neural networks;Loading;Magnetic resonance imaging","biomedical MRI;image fusion;independent component analysis;language translation;medical image processing;neural nets","ICA based features;attention network module;brain functional imaging;brain structural imaging;cognitive score;data modality;deep neural machine translation approach;fMRI;healthy patients;independent component analysis;magnetic resonance imaging;multimodal fusion;patients with schizophrenia;sMRI","","","","10","","","","6-8 March 2016","","IEEE","IEEE Conference Publications"
"Deep feature learning for pulmonary nodule classification in a lung CT","B. C. Kim; Y. S. Sung; H. I. Suk","Department of Brain and Cognitive Engineering, Korea University, Republic of Korea","2016 4th International Winter Conference on Brain-Computer Interface (BCI)","20160421","2016","","","1","3","In this paper, we propose a novel method of identifying pulmonary nodules in a lung CT. Specifically, we devise a deep neural network by which we extract abstract information inherent in raw hand-crafted imaging features. We then combine the deep learned representations with the original raw imaging features into a long feature vector. By taking the combined feature vectors, we train a classifier, preceded by a feature selection via t-test. To validate the effectiveness of the proposed method, we performed experiments on our in-house dataset of 20 subjects; 3,598 pulmonary nodules (malignant: 178, benign: 3,420), which were manually segmented by a radiologist. In our experiments, we achieved the maximal accuracy of 95.5%, sensitivity of 94.4%, and AUC of 0.987, outperforming the competing method.","","Electronic:978-1-4673-7842-0; POD:978-1-4673-7843-7","10.1109/IWW-BCI.2016.7457462","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457462","Deep learning;Lung cancer;Pulmonary nodule classification;Stacked denoising autoencoder","Cancer;Computed tomography;Feature extraction;Lungs;Noise reduction;Training","computerised tomography;feature extraction;feature selection;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","abstract information extraction;classifier training;deep feature learning;deep learned representations;deep neural network;feature selection;feature vector;hand-crafted imaging features;lung CT;pulmonary nodule classification;pulmonary nodule identification;raw imaging features;t-test","","","","13","","","","22-24 Feb. 2016","","IEEE","IEEE Conference Publications"
"Multi-modal learning-based pre-operative targeting in deep brain stimulation procedures","Y. Liu; B. M. Dawant","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN 37205 USA","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","17","20","Deep brain stimulation, as a primary surgical treatment for various neurological disorders, involves implanting electrodes to stimulate target nuclei within millimeter accuracy. Accurate pre-operative target selection is challenging due to the poor contrast in its surrounding region in MR images. In this paper, we present a learning-based method to automatically and rapidly localize the target using multi-modal images. A learning-based technique is applied first to spatially normalize the images in a common coordinate space. Given a point in this space, we extract a heterogeneous set of features that capture spatial and intensity contextual patterns at different scales in each image modality. Regression forests are used to learn a displacement vector of this point to the target. The target is predicted as a weighted aggregation of votes from various test samples, leading to a robust and accurate solution. We conduct five-fold cross validation using 100 subjects and compare our method to three indirect targeting methods, a state-of-the-art statistical atlas-based approach, and two variations of our method that use only a single modality image. With an overall error of 2.63±1.37mm, our method improves upon the single modality-based variations and statistically significantly outperforms the indirect targeting ones. Our technique matches state-of-the-art registration methods but operates on completely different principles. Both techniques can be used in tandem in processing pipelines operating on large databases or in the clinical flow for automated error detection.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455824","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455824","","Feature extraction;Imaging;Regression tree analysis;Robustness;Surgery;Training;Vegetation","brain;feature extraction;learning (artificial intelligence);medical image processing;patient treatment;regression analysis","deep brain stimulation procedures;feature extraction;multimodal learning-based preoperative targeting;regression forests;single modality image","","","","20","","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Discrimination of ADHD children based on Deep Bayesian Network","A. J. Hao; B. L. He; C. H. Yin","Tongji University, Department of Computer Science and Technology","2015 IET International Conference on Biomedical Image and Signal Processing (ICBISP 2015)","20160409","2015","","","1","6","Attention deficit hyperactivity disorder (ADHD) is a threat for the public health all the time, so the effective discrimination of it is significant and meaningful. In current research, Functional Magnetic Resonance Imaging (fMRI) data has become a popular tool for the analysis of ADHD. In this paper, we introduce the Deep Bayesian Network, a combination of Deep Belief Network and Bayesian Network, to classify the ADHD children from the normal. In Deep Bayesian Network, The Deep Belief Network is applied to normalize and reduce dimension of the fMRI data in every brodmann area. And the Bayesian Network is used to extract the feature of relationships between several well-performed brain areas by structure learning. According to the information of structure and probability in Bayesian Network, we predicted the subjects as control, combined, inattentive or hyperactive using SVM classifier. The final results perform better than using single Deep Belief Network and the best results in ADHD-200 competition.","","Electronic|Paper:978-1-78561-045-5|978-1-78561-044-8","10.1049/cp.2015.0764","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450340","ADHD;Bayesian Network;Deep Learning;SVM","","Bayes methods;belief networks;biomedical MRI;image classification;medical disorders;medical image processing;neurophysiology;paediatrics;support vector machines","ADHD analysis;ADHD children;SVM classifier;attention deficit hyperactivity disorder;brain area;deep Bayesian network;deep belief network;fMRI data;functional MRI;magnetic resonance imaging;structure learning;support vector machine","","","","","","","","19-19 Nov. 2015","","IET","IET Conference Publications"
"Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse Patch Matching","Y. Guo; Y. Gao; D. Shen","Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","20160331","2016","35","4","1077","1089","Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods.","0278-0062;02780062","","10.1109/TMI.2015.2508280","10.13039/100000002 - National Institutes of Health; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353170","Deformable model;MR prostate segmentation;sparse patch matching;stacked sparse auto-encoder (SSAE)","Biomedical imaging;Cancer;Deformable models;Feature extraction;Image segmentation;Machine learning;Shape","biological organs;biomedical MRI;cancer;feature extraction;image matching;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy","T2-wighted prostate MR images;accurate MR prostate localization;clinical applications;dataset;deep feature learning;deformable MR prostate segmentation;feature hierarchy;handcrafted features;inconsistent prostate boundary appearance;inhomogeneous prostate boundary appearance;latent feature representation;prostate cancer radiotherapy;prostate likelihood map;shape variation;sparse patch matching;stacked sparse autoencoder;state-of-the-art segmentation methods","","5","","56","","","20151211","April 2016","","IEEE","IEEE Journals & Magazines"
"Combining deep convolutional networks and SVMs for mass detection on digital mammograms","I. Wichakam; P. Vateekul","Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand","2016 8th International Conference on Knowledge and Smart Technology (KST)","20160324","2016","","","239","244","It is important to detect breast cancers as early as possible, which are commonly diagnosed as a mass region on mammograms. Deep Convolutional networks (ConvNets) have been specially designed for various computer vision tasks. In image classification, it contains many layers to automatically extract image features and employs the softmax function at the last layer to predict a probability. Although it excels in feature extraction, the classification is still limited. In this paper, we propose to apply SVMs into ConvNets to detect a mass on mammograms. To overcome the scarcity of training images, a data augmentation technique is employed to increase the sample data. To further enhance the accuracy, two recent techniques in ConvNets are applied including (i) rectified linear units and (ii) dropout. The experiment was conducted on the INbreast data set. The result showed that the proposed method achieved an accuracy at 98.44%, which is superior to the baseline (ConvNets) for 8%.","","Electronic:978-1-4673-8139-0; POD:978-1-4673-8140-6","10.1109/KST.2016.7440527","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440527","Breast Cancer Detection;Convolutional Networks (ConvNets);Deep Learning;Digital Mammograms;Image Classification;Support Vector Machines (SVMs)","Breast cancer;Decision support systems;Feature extraction;Image classification;Kernel;Mammography;Support vector machines","feature extraction;feedforward neural nets;gynaecology;image classification;mammography;medical image processing;support vector machines","ConvNets;INbreast data set;SVM;breast cancers;computer vision tasks;deep convolutional networks;digital mammograms;image classification;image feature extraction;mass detection;rectified linear units;softmax function","","","","25","","","","3-6 Feb. 2016","","IEEE","IEEE Conference Publications"
"A hybrid convolutional neural networks with extreme learning machine for WCE image classification","J. s. Yu; J. Chen; Z. Q. Xiang; Y. X. Zou","ADSPLAB, School of ECE, Peking University, Shenzhen 518055, China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20160225","2015","","","1822","1827","Wireless Capsule Endoscopy (WCE) is considered as a promising technology for non-invasive gastrointestinal disease examination. This paper studies the classification problem of the digestive organs for wireless capsule endoscopy (WCE) images aiming at saving the review time of doctors. Our previous study has proved the Convolutional Neural Networks (CNN)-based WCE classification system is able to achieve 95% classification accuracy in average, but it is difficult to further improve the classification accuracy owing to the variations of individuals and the complex digestive tract circumstance. Research shows that there are two possible approaches to improve classification accuracy: to extract more discriminative image features and to employ a more powerful classifier. In this paper, we propose to design a WCE classification system by a hybrid CNN with Extreme Learning Machine (ELM). In our approach, we construct the CNN as a data-driven feature extractor and the cascaded ELM as a strong classifier instead of the conventional used full-connection classifier in deep CNN classification system. Moreover, to improve the convergence and classification capability of ELM under supervision manner, a new initialization is employed. Our developed WCE image classification system is named as HCNN-NELM. With about 1 million real WCE images (25 examinations), intensive experiments are conducted to evaluate its performance. Results illustrate its superior performance compared to traditional classification methods and conventional CNN-based method, where about 97.25% classification accuracy can be achieved in average.","","Electronic:978-1-4673-9675-2; USB:978-1-4673-9674-5","10.1109/ROBIO.2015.7419037","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7419037","","Endoscopes;Esophagus;Feature extraction;Image classification;Iron;Testing;Training","diseases;endoscopes;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets","CNN-based WCE classification system;HCNN-NELM;WCE image classification system;cascaded ELM;complex digestive tract;data-driven feature extractor;deep CNN classification system;digestive organs;discriminative image features extraction;extreme learning machine;full-connection classifier;hybrid CNN;hybrid convolutional neural networks;noninvasive gastrointestinal disease examination;wireless capsule endoscopy","","1","","14","","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Deep Feature Learning with Discrimination Mechanism for Brain Tumor Segmentation and Diagnosis","L. Zhao; K. Jia","Multimedia Inf. Process. Group, Beijing Univ. of Technol., Beijing, China","2015 International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP)","20160225","2015","","","306","309","Brain tumor segmentation is one of the main challenging problems in computer vision and its early diagnosis is critical to clinics. Segmentation needs to be accurate, efficient and robust to avoid influences caused by various large and complex biases added to images. This paper proposes a multiple convolutional neural network (CNNs) framework with discrimination mechanism which is effective to achieve these goals. First of all, this paper proposes to construct different triplanar 2D CNNs architecture for 3D voxel classification, greatly reducing segmentation time. Experiment is conducted on images provided by Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized by MICCAI 2013 for both training and testing. As T1, T1-enhanced, T2 and FLAIR MRI images are utilized, multimodal features are combined. As a result, accuracy, sensitivity and specificity are comparable in comparison with manual gold standard images and better than state-of-the-art segmentation methods.","","CD-ROM:978-1-5090-0187-3; Electronic:978-1-5090-0188-0; POD:978-1-5090-0189-7","10.1109/IIH-MSP.2015.41","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415818","CNNs;brain tumor segmentation;voting strategy","Cancer;Computer architecture;Feature extraction;Image segmentation;Magnetic resonance imaging;Three-dimensional displays;Tumors","biomedical MRI;brain;computational geometry;computer vision;feedforward neural nets;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours","3D voxel classification;BRATS;FLAIR MRI image utilization;MICCAI 2013;T1 image utilization;T1-enhanced image utilization;T2 image utilization;brain tumor diagnosis;brain tumor segmentation;computer vision;deep feature learning;discrimination mechanism;multimodal brain tumor image segmentation benchmark;multiple convolutional neural network framework;segmentation time reduction;triplanar 2D CNN architecture","","","","14","","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"Retinal vessel landmark detection using deep learning and hessian matrix","T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang","Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China","2015 8th International Congress on Image and Signal Processing (CISP)","20160218","2015","","","387","392","The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.","","Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2","10.1109/CISP.2015.7407910","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910","Deep learning;Hessian response;Image registration;Landmark detection;The probability map;The retinal image;Vessel Segmentation","Feature extraction;Image color analysis;Image registration;Image segmentation;Machine learning;Neural networks;Retina","Hessian matrices;eye;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery","Hessian matrix;bifurcation points;crossover points;curvature extreme points;deep learning;feature detection;image noise;multimodel retinal image;multiscale Hessian response;ophthalmological surgery;pathology;retinal image probability map;retinal image registration;retinal vessel landmark detection;vessel segmentation","","","","9","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"A supervised method using convolutional neural networks for retinal vessel delineation","Q. Li; L. Xie; Q. Zhang; S. Qi; P. Liang; H. Zhang; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060","2015 8th International Congress on Image and Signal Processing (CISP)","20160218","2015","","","418","422","Retinal vessel delineation is a hot research topic owing to its importance in a lot of clinic application. Several methods have been proposed in the past decades. Here we will present a new supervised method for retinal vessel segmentation. The method is designed to explore the complex relationship between retinal images and their corresponding vessel label maps. Specifically, in order to build a model describing the direct transformation from retinal image to vessel map, we introduce a deep convolutional neural network (abbreviation as CNN), which has strong enough induction ability. For the purpose of constructing the whole vessel probability map, we also design a synthesis method. Our method shows better performance on DRIVE dataset than state-of-the-art of reported approaches in the light of sensitivity (abbreviation as Se), specificity (abbreviation as Sp) and accuracy (abbreviation as Acc). Our proposed method has great potential to be applied in existing computer-assisted diagnostic system of ophthalmologic diseases. Meanwhile, the method may offer a novel, general computing framework for segmentation in other fields.","","Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2","10.1109/CISP.2015.7407916","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407916","CNN;deep learning;retinal image;vessel delineation","Feature extraction;Image segmentation;Measurement;Neural networks;Retinal vessels;Training","blood vessels;eye;feedforward neural nets;image segmentation;medical image processing;patient diagnosis;probability","DRIVE dataset;clinic application;computer-assisted diagnostic system;convolutional neural networks;deep convolutional neural network;general computing framework;ophthalmologic diseases;retinal images;retinal vessel delineation;retinal vessel segmentation;supervised method;synthesis method;vessel label maps;vessel probability map","","","","9","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions","G. Carneiro; T. Peng; C. Bayer; N. Navab","Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia","2015 IEEE International Conference on Computer Vision (ICCV)","20160218","2015","","","648","656","We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.","","Electronic:978-1-4673-8391-2; POD:978-1-4673-8392-9","10.1109/ICCV.2015.81","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410438","","Cancer;Computer vision;Manuals;Microscopy;Support vector machines;Training;Tumors","cancer;graph theory;medical image processing;neural nets;support vector machines;tumours","DCNN model;FLSSVM;cancer treatment;deep convolutional neural network;flexible graph;flexible latent structure support vector machine;high-order loss function;latent graph;malignant tumour;microcirculatory supply units;microscopy imaging;weakly-supervised structured output learning","","","","37","","","","7-13 Dec. 2015","","IEEE","IEEE Conference Publications"
"Convolutional Neural Networks in Automatic Recognition of Trans-differentiated Neural Progenitor Cells under Bright-Field Microscopy","B. Jiang; X. Wang; J. Luo; X. Zhang; Y. Xiong; H. Pang","Guangzhou Inst. of Biomed. & Health, Guangzhou, China","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","20160215","2015","","","122","126","The study of cell morphology changes leads the investigation of the cell fate decision and its function. Bright-field imaging analysis allow us to use a labeling free and non-invasive approach to measure the morphological dynamics during cellular reprogramming, which includes induced pluripotent stem cells (iPSCs), and trans-differentiated neural progenitor cells (NPCs) from somatic cell source. However, the traditional method to study the NPC differentiation and its related function involves staining, and cell lysis, which can not materialized further for the clinical uses. In order to automatically, non-invasively, non-labelled analyze and cultivate cells, a system classifying NPCs under bright-field microscopic imaging is necessary. In this paper, we propose a novel recognition system based on convolutional neural networks, which could pre-process images and classify NPCs and non-NPCs. Experimental results prove that the proposed system provides a new tool for fundamental research in iPSCs and NPCs based generation medicine.","","Electronic:978-1-4673-7723-2; POD:978-1-4673-7724-9","10.1109/IMCCC.2015.33","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405812","bright-field microscopy;convolutional neural networks;deep learning;machine learning;non-invasive;non-labelled;trans-differentiated neural progenitor cells","Biological neural networks;Electronic mail;Feature extraction;Image recognition;Machine learning;Microscopy;Morphology","cellular biophysics;image classification;medical image processing;neural nets;optical microscopy","NPC classification;automatic recognition;bright-field microscopic imaging analysis;convolutional neural networks;image pre-processing;nonNPC classification;trans-differentiated neural progenitor cells","","","","17","","","","18-20 Sept. 2015","","IEEE","IEEE Conference Publications"
"An Automatic Learning-Based Framework for Robust Nucleus Segmentation","F. Xing; Y. Xie; L. Yang","Department of Electrical and Computer Engineering, University of Florida, Gainesville","IEEE Transactions on Medical Imaging","20160202","2016","35","2","550","566","Computer-aided image analysis of histopathology specimens could potentially provide support for early detection and improved characterization of diseases such as brain tumor, pancreatic neuroendocrine tumor (NET), and breast cancer. Automated nucleus segmentation is a prerequisite for various quantitative analyses including automatic morphological feature computation. However, it remains to be a challenging problem due to the complex nature of histopathology images. In this paper, we propose a learning-based framework for robust and automatic nucleus segmentation with shape preservation. Given a nucleus image, it begins with a deep convolutional neural network (CNN) model to generate a probability map, on which an iterative region merging approach is performed for shape initializations. Next, a novel segmentation algorithm is exploited to separate individual nuclei combining a robust selection-based sparse shape model and a local repulsive deformable model. One of the significant benefits of the proposed framework is that it is applicable to different staining histopathology images. Due to the feature learning characteristic of the deep CNN and the high level shape prior modeling, the proposed method is general enough to perform well across multiple scenarios. We have tested the proposed algorithm on three large-scale pathology image datasets using a range of different tissue and stain preparations, and the comparative experiments with recent state of the arts demonstrate the superior performance of the proposed approach.","0278-0062;02780062","","10.1109/TMI.2015.2481436","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274740","Deep convolutional neural network;nucleus segmentation;sparse representation","Breast cancer;Computational modeling;Image color analysis;Image segmentation;Robustness;Shape;Tumors","cancer;diagnostic radiography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;probability;tumours","automated nucleus segmentation;automatic learning-based framework;automatic morphological feature computation;brain tumor;breast cancer;computer-aided image analysis;deep CNN model;deep convolutional neural network model;diseases;early detection;feature learning characteristic;high level shape prior modeling;histopathology imaging;histopathology specimens;iterative region merging approach;large-scale pathology image datasets;local repulsive deformable model;pancreatic neuroendocrine tumor;probability map;quantitative analysis;robust nucleus segmentation;robust selection-based sparse shape model;staining histopathology images;tissue","","7","","83","","","20150923","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Maximum Margin Learning of t-SPNs for Cell Classification With Filtered Input","H. Kang; C. D. Yoo; Y. Na","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong Gu, Daejeon, South Korea","IEEE Journal of Selected Topics in Signal Processing","20160121","2016","10","1","130","139","An algorithm based on a deep probabilistic architecture referred to as tree-structured sum-product network (t-SPN) is considered for cells classification. The t-SPN is a rooted acyclic graph constructed as a tree of several sum-product networks where each network is constructed over a subset of most confusing class features. The constructed t-SPN architecture is learned by maximizing the margin which is defined to be the difference in the conditional probability between the true and the most competitive false labels. To enhance generalization, l<sub>2</sub>-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate compared to other state-of-the-art algorithms that include convolutional neural network (CNN) based algorithms. Ideal high-pass filter was more effective on the HEp-2 dataset which is based on immunofluorescence staining while the LOG was more effective on Feulgen dataset which is based on Feulgen staining.","1932-4553;19324553","","10.1109/JSTSP.2015.2502542","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332738","confusing classes;maximum margin;sub-SPNs;t-SPNs","Computer architecture;Input variables;Microprocessors;Microscopy;Signal processing;Special issues and sections","biomedical optical imaging;cellular biophysics;fluorescence;high-pass filters;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","Feulgen dataset;Feulgen staining;HEp-2 dataset;Laplacian-of-Gaussian filtering;cell classification;cell feature;convolutional neural network;deep probabilistic architecture;filtered input;generic high-pass filter;immunofluorescence staining;l2-regularization;learning process;maximum margin criterion;maximum margin learning;rooted acyclic graph;t-SPN architecture;tree-structured sum-product network","","","","18","","","20151120","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Integrated Optic Disc and Cup Segmentation with Deep Learning","G. Lim; Y. Cheng; W. Hsu; M. L. Lee","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","20160107","2015","","","162","169","Glaucoma is a widespread ocular disorder leading to irreversible loss of vision. Therefore, there is a pressing need for cost-effective screening, such that preventive measures can be taken. This can be achieved with an accurate segmentation of the optic disc and cup from retinal images to obtain the cup-to-disc ratio. We describe a comprehensive solution based on applying convolutional neural networks to feature exaggerated inputs emphasizing disc pallor without blood vessel obstruction, as well as the degree of vessel kinking. The produced raw probability maps then undergo a robust refinement procedure that takes into account prior knowledge about retinal structures. Analysis of these probability maps further allows us to obtain a confidence estimate on the correctness of the segmentation, which can be used to direct the most challenging cases for manual inspection. Tests on two large real-world databases, including the publicly-available MESSIDOR collection, demonstrate the effectiveness of our proposed system.","1082-3409;10823409","Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0","10.1109/ICTAI.2015.36","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372132","Glaucoma screening;optic cup segmentation;optic disc segmentation","Adaptive optics;Image segmentation;Neural networks;Optical computing;Optical imaging;Retina;Training","biomedical optical imaging;eye;feature extraction;image segmentation;learning (artificial intelligence);medical disorders;medical image processing;probability","convolutional neural networks;cost-effective screening;cup segmentation;cup-to-disc ratio;deep learning;disc pallor;feature-exaggerated inputs;glaucoma;integrated optic disc;ocular disorder;publicly-available MESSIDOR collection;raw probability maps;real-world databases;retinal images;retinal structures;robust refinement procedure;vision loss","","","","33","","","","9-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"Automated Mass Detection in Mammograms Using Cascaded Deep Learning and Random Forests","N. Dhungel; G. Carneiro; A. P. Bradley","","2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","20160107","2015","","","1","8","Mass detection from mammograms plays a crucial role as a pre- processing stage for mass segmentation and classification. The detection of masses from mammograms is considered to be a challenging problem due to their large variation in shape, size, boundary and texture and also because of their low signal to noise ratio compared to the surrounding breast tissue. In this paper, we present a novel approach for detecting masses in mammograms using a cascade of deep learning and random forest classifiers. The first stage classifier consists of a multi-scale deep belief network that selects suspicious regions to be further processed by a two-level cascade of deep convolutional neural networks. The regions that survive this deep learning analysis are then processed by a two-level cascade of random forest classifiers that use morphological and texture features extracted from regions selected along the cascade. Finally, regions that survive the cascade of random forest classifiers are combined using connected component analysis to produce state-of-the-art results. We also show that the proposed cascade of deep learning and random forest classifiers are effective in the reduction of false positive regions, while maintaining a high true positive detection rate. We tested our mass detection system on two publicly available datasets: DDSM-BCRP and INbreast. The final mass detection produced by our approach achieves the best results on these publicly available datasets with a true positive rate of 0.96 ± 0.03 at 1.2 false positive per image on INbreast and true positive rate of 0.75 at 4.8 false positive per image on DDSM-BCRP.","","Electronic:978-1-4673-6795-0; POD:978-1-4673-6796-7","10.1109/DICTA.2015.7371234","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371234","","Breast cancer;Feature extraction;Image resolution;Machine learning;Mammography;Support vector machines","belief networks;biological tissues;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);mammography;medical image processing;neural nets;object detection;principal component analysis;trees (mathematics)","DDSM-BCRP;INbreast;automated mass detection;boundary variation;breast tissue;cascaded deep learning;connected component analysis;deep convolutional neural network;false positive region reduction;mammogram;mass classification;mass segmentation;morphological feature extraction;multiscale deep belief network;random forest classifier;shape variation;signal to noise ratio;size variation;texture feature extraction;texture variation","","3","","37","","","","23-25 Nov. 2015","","IEEE","IEEE Conference Publications"
"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images","J. Xu; L. Xiang; Q. Liu; H. Gilmore; J. Wu; J. Tang; A. Madabhushi","Jiangsu Key Laboratory of Big Data Analysis Technique and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Medical Imaging","20160104","2016","35","1","119","130","Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of “Deep Learning” strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 × 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.","0278-0062;02780062","","10.1109/TMI.2015.2458702","; 10.13039/100000062 - National Institute of Diabetes and Digestive and Kidney Diseases; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163353","Automated nuclei detection;breast cancer histopathology;deep learning;digital pathology;feature representation learning;stacked sparse autoencoder","Breast cancer;Decoding;Feature extraction;Image color analysis;Pathology;Training","biological tissues;cancer;image classification;image coding;image representation;image resolution;learning (artificial intelligence);medical image processing","Nottingham histologic score system;automated grading;automated nuclear detection;average area under Precision-Recall curve;breast cancer tissue specimens;computer assisted pathology related image analysis algorithms;deep learning strategy;high resolution digitized pathology images;high-level features;high-resolution breast cancer histopathological images;image classifier;image patch representation;nuclei detection;pixel intensity;sliding window operation;stacked sparse autoencoder","","18","","43","","","20150720","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images","Q. Li; B. Feng; L. Xie; P. Liang; H. Zhang; T. Wang","Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20151229","2016","35","1","109","118","This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.","0278-0062;02780062","","10.1109/TMI.2015.2457891","Project of the National Science Foundation of China; Shenzhen Science Plan of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161344","Cross-modality learning;deep learning;retinal image;vessel segmentation","Accuracy;Deformable models;Feature extraction;Image segmentation;Neural networks;Retina;Training","eye;image segmentation;medical image processing;neural nets","center pixel;computing framework;cross-modality learning approach;cross-training evaluation;image diagnosis;image segmentation;induction ability;neural network;ophthalmologic diseases;retinal images;vessel map;vessel segmentation","","14","","35","","","20150717","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"A novel method with a deep network and directional edges for automatic detection of a fetal head","S. Nie; J. Yu; P. Chen; J. Zhang; Y. Wang","Department of Electronic Engineering, Fudan University, Shanghai, China","2015 23rd European Signal Processing Conference (EUSIPCO)","20151228","2015","","","654","658","In this paper, we propose a novel method for the automatic detection of fetal head in 2D ultrasound images. Fetal head detection has been a challenging task, as the ultrasound images usually have poor quality, the structures contained in the images are complex, and the gray scale distribution is highly variable. Our approach is based on a deep belief network and a modified circle detection method. The whole process can be divided into two steps: first, a deep learning architecture is applied to search the whole image and determine the result patch that contains the entire fetal head; second, a modified circle detection method is used along with Hough transform to detect the position and size of the fetal head. In order to validate our method, experiments are performed on both synthetic data and clinic ultrasound data. A good performance of the proposed method is shown in the paper.","","Electronic:978-0-9928-6263-3; POD:978-1-4799-8851-8","10.1109/EUSIPCO.2015.7362464","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362464","Fetal head;circle detection;deep learning","Europe;Head;Image edge detection;Magnetic heads;Signal processing;Training;Ultrasonic imaging","Hough transforms;belief networks;medical signal detection;ultrasonic imaging","2D ultrasound images;Hough transform;automatic detection;clinic ultrasound data;deep belief network;deep learning architecture;deep network;directional edges;fetal head detection;gray scale distribution;modified circle detection;synthetic data","","1","","19","","","","Aug. 31 2015-Sept. 4 2015","","IEEE","IEEE Conference Publications"
"Parallel convolutional-linear neural network for motor imagery classification","S. Sakhavi; C. Guan; S. Yan","A&#x2217;STAR Institute for Infocomm Research (IR) Brain-Computer Interface Lab Singapore","2015 23rd European Signal Processing Conference (EUSIPCO)","20151228","2015","","","2736","2740","Deep learning, recently, has been successfully applied to image classification, object recognition and speech recognition. However, the benefits of deep learning and accompanying architectures have been largely unknown for BCI applications. In motor imagery-based BCI, an energy-based feature, typically after spatial filtering, is commonly used for classification. Although this feature corresponds to the estimate of event-related synchronization/desynchronization in the brain, it neglects energy dynamics which may contain valuable discriminative information. Because traditional classiication methods, such as SVM, cannot handle this dynamical property, we proposed an architecture that inputs a dynamic energy representation of EEG data and utilizes convolutional neural networks for classification. By combining this network with a static energy network, we saw a significant increase in performance. We evaluated the proposed method and compared with SVM on a multi-class motor imagery dataset (BCI competition dataset IV-2a). Our method outperforms SVM with static energy features significantly (p <; 0.01).","","Electronic:978-0-9928-6263-3; POD:978-1-4799-8851-8","10.1109/EUSIPCO.2015.7362882","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362882","Brain-Computer Interface;Convolutional Neural Network;Deep Learning;EEG;Motor Imagery","Computer architecture;Convolution;Electroencephalography;Europe;Feature extraction;Support vector machines","brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing;neural nets;signal classification;synchronisation","BCI competition dataset IV-2a;EEG data;dynamic energy representation;energy dynamics;energy-based feature;event-related synchronization-desynchronization;motor imagery classification;motor imagery-based BCI;multiclass motor imagery dataset;parallel convolutional-linear neural network;static energy network;support vector machines","","","","20","","","","Aug. 31 2015-Sept. 4 2015","","IEEE","IEEE Conference Publications"
"Improving EEG feature learning via synchronized facial video","X. Li; X. Jia; G. Xun; A. Zhang","Dept. of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY, U.S.A.","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","843","848","Morden physiological analysis begins to involve more and more types of information. Electroencephalogram (EEG) signals as a typical example is starting to be analyzed with facial expressions videos to detect emotions. Emotions play an important role in the daily life of human beings, the need and importance of automatic emotion recognition has grown with increasing role of human computer interface applications. In this paper, we concentrate on recognition of the emotions jointly from ""inner"" and ""outer"" reactions, which are electroencephalogram (EEG) signals and facial expression video. Due to the streaming nature of this problem, the data volume and velocity is very challenging. We address these challenges from the theoretic perspective and propose a real time algorithm based on EEG signals and synchronized facial video to learn feature vector jointly. Our algorithm consists of an unsupervisedly EEG dictionary component based on deep learning theorem, and a probability pooling component transforms a continuous sequential signal into an EEG ""sentence"" which consists of a sequence of EEG words. The EEG sentence is then jointly learned with video features into a new fixed length feature representation for emotion classification. We overcome several computational challenges on the data based on the idea of convolution and pooling, and we conduct extensive evaluation for each component of our model. We also demonstrate the state-of-the-art classification result on real-world dataset. The superior performances on the emotion recognition task indicates that 1) the natural language scenario can be applied in EEG sequences and 2) borrowing video modality can increase the overall performance.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363831","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363831","Classification;Data stream;EEG;Feature Learning","Dictionaries;Electroencephalography;Emotion recognition;Physiology;Streaming media;Synchronization;Transforms","electroencephalography;emotion recognition;face recognition;medical image processing;probability;video signal processing","EEG dictionary component;EEG feature learning;automatic emotion recognition;deep learning theorem;electroencephalogram signal;emotion classification;emotion detection;facial expression video;feature vector;human computer interface;natural language;physiological analysis;probability pooling component;synchronized facial video;video modality","","","","10","","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Deep learning of tissue fate features in acute ischemic stroke","N. Stier; N. Vincent; D. Liebeskind; F. Scalzo","Neurovascular Imaging Research Core, Department of Neurology, Univerisity of California, Los Angeles (UCLA), USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1316","1321","In acute ischemic stroke treatment, prediction of tissue survival outcome plays a fundamental role in the clinical decision-making process, as it can be used to assess the balance of risk vs. possible benefit when considering endovascular clot-retrieval intervention. For the first time, we construct a deep learning model of tissue fate based on randomly sampled local patches from the hypoperfusion (Tmax) feature observed in MRI immediately after symptom onset. We evaluate the model with respect to the ground truth established by an expert neurologist four days after intervention. Experiments on 19 acute stroke patients evaluated the accuracy of the model in predicting tissue fate. Results show the superiority of the proposed regional learning framework versus a single-voxel-based regression model.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359869","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359869","","Biomedical imaging;Convolution;Predictive models","biological tissues;biomedical MRI;decision making;image fusion;learning (artificial intelligence);medical image processing;prediction theory","MRI;acute ischemic stroke treatment;clinical decision-making process;deep learning;endovascular clot-retrieval intervention;hypoperfusion feature;regional learning framework;tissue fate features;tissue fate prediction;tissue survival outcome prediction","","3","","32","","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Temporal Pattern and Association Discovery of Diagnosis Codes Using Deep Learning","S. Mehrabi; S. Sohn; D. Li; J. J. Pankratz; T. Therneau; J. L. S. Sauver; H. Liu; M. Palakal","Dept. of Health Sci. Res., Mayo Clinic, Rochester, MN, USA","2015 International Conference on Healthcare Informatics","20151210","2015","","","408","416","Longitudinal health records contain data on patients' visits, condition, treatment, and test results representing progression of their health status over time. In poorly understood patient populations, such data are particularly helpful in characterizing disease progression and early detection. In this work we developed a deep learning algorithm for temporal pattern discovery over Rochester Epidemiology Project data. We modeled each patient's records as a matrix of temporal clinical events with ICD9 and HCUP CSS diagnosis codes as rows and years of diagnosis as columns. Patients aged 18 or younger at the time of diagnosis were selected. A deep Boltzmann machine network with three hidden layers was constructed with each patient's diagnosis matrix values as visible nodes. The final weights of the network model were analyzed as the common features among patients' records.","","Electronic:978-1-4673-9548-9; POD:978-1-4673-9549-6","10.1109/ICHI.2015.58","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349719","Deep Learning;Rochester Epidemiology Project;Temporal Pattern Discovery","Cascading style sheets;Diseases;Machine learning;Medical diagnostic imaging;Sociology;Statistics","Boltzmann machines;data mining;electronic health records;learning (artificial intelligence);matrix algebra;patient diagnosis","HCUP CSS diagnosis;ICD9 diagnosis code;Rochester Epidemiology Project data;deep Boltzmann machine network;deep learning;deep learning algorithm;disease progression;longitudinal health record;patient diagnosis matrix value;temporal association discovery;temporal pattern discovery","","4","","45","","","","21-23 Oct. 2015","","IEEE","IEEE Conference Publications"
"Clinical deep brain stimulation region prediction using regression forests from high-field MRI","J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel","Department of Electrical and Computer Engineering, Duke University, Durham, USA","2015 IEEE International Conference on Image Processing (ICIP)","20151210","2015","","","2480","2484","This paper presents a prediction framework of brain subcortical structures which are invisible on clinical low-field MRI, learning detailed information from ultrahigh-field MR training data. Volumetric segmentation of Deep Brain Stimulation (DBS) structures within the Basal ganglia is a prerequisite process for reliable DBS surgery. While ultrahigh-field MR imaging (7 Tesla) allows direct visualization of DBS targeting structures, such ultrahigh-fields are not always clinically available, and therefore the relevant structures need to be predicted from the clinical data. We address the shape prediction problem with a regression forest, non-linearly mapping predictors to target structures with high confidence, exploiting ultrahigh-field MR training data. We consider an application for the subthalamic nucleus (STN) prediction as a crucial DBS target. Experimental results on Parkinson's patients validate that the proposed approach enables reliable estimation of the STN from clinical 1.5T MRI.","","Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7","10.1109/ICIP.2015.7351248","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351248","Deep brain stimulation;regression forests;statistical shape models;ultrahigh-field MRI","Magnetic resonance imaging;Predictive models;Satellite broadcasting;Shape;Training;Training data;Vegetation","biomedical MRI;brain;image segmentation;medical image processing;neuromuscular stimulation;regression analysis;surgery","Basal ganglia;DBS surgery;Parkinson patients;STN prediction;brain subcortical structure prediction framework;clinical deep brain stimulation region prediction;clinical low-field MRI;deep brain stimulation structures;direct DBS targeting structure visualization;high-field MRI;nonlinearly mapping predictors;regression forests;shape prediction problem;subthalamic nucleus prediction;ultrahigh-field MR training data;ultrahighfield MR imaging;volumetric segmentation","","","1","21","","","","27-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"Lung segmentation in chest radiographs using distance regularized level set and deep-structured learning and inference","T. A. Ngo; G. Carneiro","Australia Centre for Visual Technologies, The University of Adelaide, Australia","2015 IEEE International Conference on Image Processing (ICIP)","20151210","2015","","","2140","2143","Computer-aided diagnosis of digital chest X-ray (CXR) images critically depends on the automated segmentation of the lungs, which is a challenging problem due to the presence of strong edges at the rib cage and clavicle, the lack of a consistent lung shape among different individuals, and the appearance of the lung apex. From recently published results in this area, hybrid methodologies based on a combination of different techniques (e.g., pixel classification and deformable models) are producing the most accurate lung segmentation results. In this paper, we propose a new methodology for lung segmentation in CXR using a hybrid method based on a combination of distance regularized level set and deep structured inference. This combination brings together the advantages of deep learning methods (robust training with few annotated samples and top-down segmentation with structured inference and learning) and level set methods (use of shape and appearance priors and efficient optimization techniques). Using the publicly available Japanese Society of Radiological Technology (JSRT) dataset, we show that our approach produces the most accurate lung segmentation results in the field. In particular, depending on the initialization used, our methodology produces an average accuracy on JSTR that varies from 94.8% to 98.5%.","","Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7","10.1109/ICIP.2015.7351179","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351179","Deep learning;Level set methods;Lung segmentation","Databases;Image segmentation;Level set;Lungs;Optimization;Shape;Training","diagnostic radiography;image segmentation;inference mechanisms;learning (artificial intelligence);lung;medical image processing;set theory","CXR images;JSRT dataset;Japanese Society of Radiological Technology dataset;automated lung segmentation;chest radiographs;clavicle;computer-aided diagnosis;deep learning methods;deep-structured inference;deep-structured learning;deformable models;digital chest X-ray images;distance regularized level set method;lung apex appearance;lung shape;optimization techniques;pixel classification;rib cage;top-down segmentation","","3","","14","","","","27-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"Dimensionality reduction by supervised locality analysis","L. Zhang; P. Peng; X. Xiang; X. Zhen","College of Information and Communication Engineering, Harbin Engineering University, Harbin, China","2015 IEEE International Conference on Image Processing (ICIP)","20151210","2015","","","1488","1492","High-dimensional feature representations have recently been widely used for image classification, which not only induce large storage requirement and high computational complexity, but also tend to be lack of discrimination due to redundant and noisy features. In this paper, we propose a novel algorithm named supervised locality analysis (SLA) for dimensionality reduction. In contrast to conventional dimensionality reduction methods, the proposed SLA incorporates supervision into locality analysis by fully exploring multi-class distributions, which can handle the non-linear data structure while preserving intrinsic discriminative information. The obtained compact and highly discriminative features by the SLA is enables more accurate and efficient classification. Moreover, the SLA can be used for supervised dimensionality reduction of both handcrafted and deep learning based features. We have conduced experiments to evaluate the proposed SLA on three datasets for image classification. The SLA has produced state-of-the-art performance and largely outperformed widely-used dimensionality reduction methods.","","Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7","10.1109/ICIP.2015.7351048","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351048","Dimensionality reduction;image classification;locality analysis;manifold learning","Algorithm design and analysis;Linear programming;Manifolds;Optimization;Principal component analysis;Silicon;Yttrium","image classification;image representation;learning (artificial intelligence)","deep learning based features;high-dimensional feature representations;image classification;multiclass distributions;supervised dimensionality reduction;supervised locality analysis","","","","21","","","","27-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"Deep structured learning for mass segmentation from mammograms","N. Dhungel; G. Carneiro; A. P. Bradley","School of Information Technology and Electrical Engineering, The University of Queensland","2015 IEEE International Conference on Image Processing (ICIP)","20151210","2015","","","2950","2954","In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to assess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computation of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time.","","Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7","10.1109/ICIP.2015.7351343","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351343","Mammograms;mass segmentation;structured inference;structured learning","Breast cancer;Image segmentation;Indexes;Mammography;Training","image segmentation;learning (artificial intelligence);mammography;medical image processing;support vector machines","DDSM-BCRP;Dice index;INbreast;SSVM;breast mass segmentation;deep structured learning;image regions;mammograms;structured support vector machine","","1","","19","","","","27-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"FingerNet: Deep learning-based robust finger joint detection from radiographs","S. Lee; M. Choi; H. s. Choi; M. S. Park; S. Yoon","EECS, Seoul National University, Seoul, 151-744, Korea","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","20151207","2015","","","1","4","Radiographic image assessment is the most common method used to measure physical maturity and diagnose growth disorders, hereditary diseases and rheumatoid arthritis, with hand radiography being one of the most frequently used techniques due to its simplicity and minimal exposure to radiation. Finger joints are considered as especially important factors in hand skeleton examination. Although several automation methods for finger joint detection have been proposed, low accuracy and reliability are hindering full-scale adoption into clinical fields. In this paper, we propose FingerNet, a novel approach for the detection of all finger joints from hand radiograph images based on convolutional neural networks, which requires little user intervention. The system achieved 98.02% average detection accuracy for 130 test data sets containing over 1,950 joints. Further analysis was performed to verify the system robustness against factors such as epiphysis and metaphysis in different age groups.","","Electronic:978-1-4799-7234-0; POD:978-1-4799-7235-7; USB:978-1-4799-7233-3","10.1109/BioCAS.2015.7348440","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348440","","Bones;Convolution;Joints;Merging;Radiography;Thumb","diagnostic radiography;learning (artificial intelligence);medical image processing;neural nets","FingerNet;convolutional neural networks;deep learning-based robust finger joint detection;epiphysis;metaphysis;radiograph images","","","","18","","","","22-24 Oct. 2015","","IEEE","IEEE Conference Publications"
"A deep architecture for visually analyze Pap cells","O. Chang; P. Constante; A. Gordon; M. Singania; F. Acuna","Universidad de las Fuerzas Armadas. ESPE Extension Latacunga, Ecuador 050150","2015 IEEE 2nd Colombian Conference on Automatic Control (CCAC)","20151203","2015","","","1","6","This work proposes a deep ANN architecture which accomplishes the reliable visual classification of abnormal Pap smear cell. The system is driven by independent agents where the first agent consists of a three layer ANN pretrained to closely track a reticle pattern. This net participates in a local close loop that oscillates and produces unique time-space versions of the visual data. This information is stabilized and sparsed in order to obtain compact data representations, with implicit space time content. The obtained representations are delivered to second level agents, formed by independent three layers ANNs dedicated to learning and recognition activities. To train the system a noise-balanced algorithm is employed, where the training set is composed by pap cells and white noise. This combination operating on finite databases and in a self controlled learning loop, auto develops enough cell recognition knowledge as to classify whole classes of Pap smear cells. The system has been tested in real time utilizing documented data bases.","","Electronic:978-1-4673-9305-8; POD:978-1-4673-9306-5; USB:978-1-4673-9304-1","10.1109/CCAC.2015.7345210","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345210","Pap cell recognition;deep architectures;deep learning;noise balanced learning","Artificial neural networks;Backpropagation;Computer architecture;Databases;Neurons;Tracking;Training","image classification;medical image processing;neural net architecture","abnormal Pap smear cell;deep ANN architecture;finite databases;noise-balanced algorithm;reticle pattern;visual classification","","","","16","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"Deep independence network analysis of structural brain imaging: A simulation study","E. Castro; D. Hjelm; S. Plis; L. Dinh; J. Turner; V. Calhoun","The Mind Research Network, NM, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","20151112","2015","","","1","6","The objective of this paper is to further validate theoretically and empirically a nonlinear independent component analysis (ICA) algorithm implemented with a deep learning architecture. We first revisited its formulation to verify its consistency with the criterion of minimization of mutual information. Then, we applied the nonlinear independent component estimation algorithm (NICE) to synthetic 2D images that resemble structural magnetic resonance imaging (sMRI) data. This data was generated by mixing spatial components that represent axial slices of sMRI tissue concentration images. Next, we generated the images under linear and mildly nonlinear mixtures, being able to show that NICE matches ICA when the data is generated by using the conventional linear mixture and outperforms ICA for the nonlinear mixture of components. The obtained results are promising and suggest that NICE has potential to find richer brain networks if applied to real sMRI data, provided that small conditioning adjustments are performed along with this approach.","1551-2541;15512541","Electronic:978-1-4673-7454-5; POD:978-1-4673-7455-2","10.1109/MLSP.2015.7324318","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324318","NICE;Nonlinear ICA;deep learning;simulation;structural MRI","Brain;Couplings;Imaging;Independent component analysis;Jacobian matrices;Machine learning;Mutual information","biological tissues;biomedical MRI;brain;independent component analysis;learning (artificial intelligence);medical image processing;minimisation;network analysis;neurophysiology","ICA;MRI data;MRI tissue concentration imaging;NICE matches;axial slices;brain networks;conventional linear mixture;deep independence network analysis;deep learning architecture;mildly nonlinear mixtures;minimization criterion;nonlinear independent component analysis;spatial components;structural brain imaging;structural magnetic resonance imaging data;synthetic 2D imaging","","1","","18","","","","17-20 Sept. 2015","","IEEE","IEEE Conference Publications"
"Synthetic structural magnetic resonance image generator improves deep learning prediction of schizophrenia","A. Ulloa; S. Plis; E. Erhardt; V. Calhoun","Dept. of Electrical and Computer Engineering, University of New Mexico, Albuquerque, NM, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","20151112","2015","","","1","6","Despite the rapidly growing interest, progress in the study of relations between physiological abnormalities and mental disorders is hampered by complexity of the human brain and high costs of data collection. The complexity can be captured by deep learning approaches, but they still may require significant amounts of data. In this paper, we seek to mitigate the latter challenge by developing a generator for synthetic realistic training data. Our method greatly improves generalization in classification of schizophrenia patients and healthy controls from their structural magnetic resonance images. A feed forward neural network trained exclusively on continuously generated synthetic data produces the best area under the curve compared to classifiers trained on real data alone.","1551-2541;15512541","Electronic:978-1-4673-7454-5; POD:978-1-4673-7455-2","10.1109/MLSP.2015.7324379","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324379","","Biological neural networks;Generators;Machine learning;Magnetic resonance imaging;Neuroimaging;Probability density function;Training","biomedical MRI;brain;medical disorders","data collection;deep learning approaches;human brain;mental disorders;physiological abnormalities;schizophrenia patients;synthetic realistic training data;synthetic structural magnetic resonance image generator","","","","19","","","","17-20 Sept. 2015","","IEEE","IEEE Conference Publications"
"A comparative study for chest radiograph image retrieval using binary texture and deep learning classification","Y. Anavi; I. Kogan; E. Gelbart; O. Geva; H. Greenspan","Medical Image Processing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel Aviv University, Israel","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","2940","2943","In this work various approaches are investigated for X-ray image retrieval and specifically chest pathology retrieval. Given a query image taken from a data set of 443 images, the objective is to rank images according to similarity. Different features, including binary features, texture features, and deep learning (CNN) features are examined. In addition, two approaches are investigated for the retrieval task. One approach is based on the distance of image descriptors using the above features (hereon termed the “descriptor”-based approach); the second approach (“classification”-based approach) is based on a probability descriptor, generated by a pair-wise classification of each two classes (pathologies) and their decision values using an SVM classifier. Best results are achieved using deep learning features in a classification scheme.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319008","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319008","","Biomedical imaging;Feature extraction;Heart;Machine learning;Measurement;Pathology;Support vector machines","diagnostic radiography;image classification;image retrieval;image texture;learning (artificial intelligence);medical image processing;probability;support vector machines","CNN;SVM classifier;X-ray image retrieval;binary features;binary texture;chest pathology retrieval;chest radiograph image retrieval;classification-based approach;decision values;deep learning classification;deep learning features;descriptor-based approach;image descriptors;pair-wise classification;probability descriptor;query image;texture features","","3","","12","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Automatic localization of the left ventricle in cardiac MRI images using deep learning","O. Emad; I. A. Yassine; A. S. Fahmy","Center for Informatics Science, Nile University, Giza, Egypt","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","683","686","Automatic localization of the left ventricle (LV) in cardiac MRI images is an essential step for automatic segmentation, functional analysis, and content based retrieval of cardiac images. In this paper, we introduce a new approach based on deep Convolutional Neural Network (CNN) to localize the LV in cardiac MRI in short axis views. A six-layer CNN with different kernel sizes was employed for feature extraction, followed by Softmax fully connected layer for classification. The pyramids of scales analysis was introduced in order to take account of the different sizes of the heart. A publically-available database of 33 patients was used for learning and testing. The proposed method was able it localize the LV with 98.66%, 83.91% and 99.07% for accuracy, sensitivity and specificity respectively.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318454","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318454","","Biomedical imaging;Convolution;Feature extraction;Heart;Image segmentation;Magnetic resonance imaging;Sensitivity","biomedical MRI;cardiology;feature extraction;image classification;image segmentation;medical image processing;neural nets","Softmax;automatic segmentation;cardiac MRI images;deep convolutional neural network;deep learning;feature extraction;image classification;left ventricle automatic localization","","2","","21","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Investigating deep learning for fNIRS based BCI","J. Hennrich; C. Herff; D. Heger; T. Schultz","Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","2844","2847","Functional Near infrared Spectroscopy (fNIRS) is a relatively young modality for measuring brain activity which has recently shown promising results for building Brain Computer Interfaces (BCI). Due to its infancy, there are still no standard approaches for meaningful features and classifiers for single trial analysis of fNIRS. Most studies are limited to established classifiers from EEG-based BCIs and very simple features. The feasibility of more complex and powerful classification approaches like Deep Neural Networks has, to the best of our knowledge, not been investigated for fNIRS based BCI. These networks have recently become increasingly popular, as they outperformed conventional machine learning methods for a variety of tasks, due in part to advances in training methods for neural networks. In this paper, we show how Deep Neural Networks can be used to classify brain activation patterns measured by fNIRS and compare them with previously used methods.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318984","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318984","","Accuracy;Biological neural networks;Feature extraction;Standards;Training;Yttrium","biomedical optical imaging;brain-computer interfaces;electroencephalography;feature extraction;infrared spectroscopy;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;signal classification","EEG-based BCIs;brain activation pattern classification;brain activity measurement;brain-computer interfaces;conventional machine learning methods;deep learning;deep neural networks;fNIRS based BCI;functional near infrared spectroscopy;single trial analysis classifiers;single trial analysis features;young modality","","3","","23","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Glaucoma detection based on deep convolutional neural network","X. Chen; Y. Xu; D. W. Kee Wong; T. Y. Wong; J. Liu","Institute for Infocomm Research, Agency for Science, Technology and Research, 138632, Singapore","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","715","718","Glaucoma is a chronic and irreversible eye disease, which leads to deterioration in vision and quality of life. In this paper, we develop a deep learning (DL) architecture with convolutional neural network for automated glaucoma diagnosis. Deep learning systems, such as convolutional neural networks (CNNs), can infer a hierarchical representation of images to discriminate between glaucoma and non-glaucoma patterns for diagnostic decisions. The proposed DL architecture contains six learned layers: four convolutional layers and two fully-connected layers. Dropout and data augmentation strategies are adopted to further boost the performance of glaucoma diagnosis. Extensive experiments are performed on the ORIGA and SCES datasets. The results show area under curve (AUC) of the receiver operating characteristic curve in glaucoma detection at 0.831 and 0.887 in the two databases, much better than state-of-the-art algorithms. The method could be used for glaucoma detection.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318462","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318462","","Biomedical optical imaging;Diseases;Machine learning;Neural networks;Optical imaging;Prediction algorithms;Training","diseases;image representation;learning (artificial intelligence);medical image processing;neural nets;object detection","AUC;CNNs;DL architecture;ORIGA datasets;SCES datasets;area under curve;automated glaucoma diagnosis;data augmentation strategy;deep convolutional neural network;deep learning architecture;dropout augmentation strategy;glaucoma detection;glaucoma patterns;hierarchical image representation;irreversible eye disease;nonglaucoma patterns;receiver operating characteristic curve","","","","18","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks","C. Wang; X. Yan; M. Smith; K. Kochhar; M. Rubin; S. M. Warren; J. Wrobel; H. Lee","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","2415","2418","Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore, the quality and quantity of the tissue in the wound bed also offer important prognostic information. Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to automatically segment wound regions and analyze wound conditions in wound images. Different from previous segmentation techniques which rely on handcrafted features or unsupervised approaches, our proposed deep learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned features are applied to further analysis of wounds in two ways: infection detection and healing progress prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate the effectiveness and reliability of the proposed system.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318881","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318881","","Feature extraction;Image segmentation;Neural networks;Support vector machines;Training;Visualization;Wounds","biological tissues;feature extraction;image segmentation;medical image processing;neural nets;unsupervised learning","deep convolutional neural networks;deep learning method;handcrafted features;healing progress prediction;infection detection;large-scale wound database;long-term predictions;prognostic information;task-relevant visual features;tissue quality;tissue quantity;unified automatic wound segmentation framework;unsupervised approach;wound healing process;wound images;wound surface area measurements;wound treatment","","","","24","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Brain tumor grading based on Neural Networks and Convolutional Neural Networks","Y. Pan; W. Huang; Z. Lin; W. Zhu; J. Zhou; J. Wong; Z. Ding","School of EEE, Nanyang Technological University, Singapore","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","699","702","This paper studies brain tumor grading using multiphase MRI images and compares the results with various configurations of deep learning structure and baseline Neural Networks. The MRI images are used directly into the learning machine, with some combination operations between multiphase MRIs. Compared to other researches, which involve additional effort to design and choose feature sets, the approach used in this paper leverages the learning capability of deep learning machine. We present the grading performance on the testing data measured by the sensitivity and specificity. The results show a maximum improvement of 18% on grading performance of Convolutional Neural Networks based on sensitivity and specificity compared to Neural Networks. We also visualize the kernels trained in different layers and display some self-learned features obtained from Convolutional Neural Networks.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318458","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318458","","Artificial neural networks;Biological neural networks;Image segmentation;Kernel;Sensitivity and specificity;Training;Tumors","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;tumours","baseline neural networks;brain tumor grading;convolutional neural networks;deep learning machine;deep learning structure;grading performance;multiphase MRI imaging;self-learned features;testing data","","","","8","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Deep neural network and random forest hybrid architecture for learning to detect retinal vessels in fundus images","D. Maji; A. Santara; S. Ghosh; D. Sheet; P. Mitra","Indian Institute of Technology Kharagpur, WB 721302, India","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","3029","3032","Vision impairment due to pathological damage of the retina can largely be prevented through periodic screening using fundus color imaging. However the challenge with large-scale screening is the inability to exhaustively detect fine blood vessels crucial to disease diagnosis. In this work we present a computational imaging framework using deep and ensemble learning based hybrid architecture for reliable detection of blood vessels in fundus color images. A deep neural network (DNN) is used for unsupervised learning of vesselness dictionaries using sparse trained denoising auto-encoders (DAE), followed by supervised learning of the DNN response using a random forest for detecting vessels in color fundus images. In experimental evaluation with the DRIVE database, we achieve the objective of vessel detection with max. avg. accuracy of 0.9327 and area under ROC curve of 0.9195.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319030","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319030","Computational imaging;deep learning;denoising auto-encoder;random forests;vessel detection","Biomedical imaging;Image analysis;Radio frequency;Retinal vessels;Vegetation","biomedical optical imaging;blood vessels;diseases;eye;image denoising;learning (artificial intelligence);medical image processing;vision defects","DNN response;DRIVE database;ROC curve;blood vessels;computational imaging framework;deep network;deep neural network;disease diagnosis;fundus color imaging;large-scale screening;learning based hybrid architecture;pathological damage;periodic screening;random forest hybrid architecture;retinal vessel detection;sparse trained denoising autoencoders;unsupervised learning;vision impairment","","1","","16","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Detection of exudates in fundus photographs using convolutional neural networks","P. Prentašić; S. Lončarić","University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Image Processing Group 10000, Croatia","2015 9th International Symposium on Image and Signal Processing and Analysis (ISPA)","20151026","2015","","","188","192","Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.","1845-5921;18455921","Electronic:978-1-4673-8032-4; POD:978-1-4673-8033-1","10.1109/ISPA.2015.7306056","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306056","","Convolution;Diabetes;Neural networks;Optical imaging;Optical sensors;Retina;Retinopathy","biomedical optical imaging;colour photography;diseases;eye;image colour analysis;medical image processing;neural nets;object detection","color fundus photographs;convolutional neural networks;deep neural networks;diabetic retinopathy;early diagnosis;exudate detection;machine learning technique;preventable blindness","","","","31","","","","7-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"Automatic Feature Learning to Grade Nuclear Cataracts Based on Deep Learning","X. Gao; S. Lin; T. Y. Wong","Institute for Infocomm Research, Agency for Science, Technology and Research, Singapore","IEEE Transactions on Biomedical Engineering","20151026","2015","62","11","2693","2701","Goal: Cataracts are a clouding of the lens and the leading cause of blindness worldwide. Assessing the presence and severity of cataracts is essential for diagnosis and progression monitoring, as well as to facilitate clinical research and management of the disease. Methods: Existing automatic methods for cataract grading utilize a predefined set of image features that may provide an incomplete, redundant, or even noisy representation. In this study, we propose a system to automatically learn features for grading the severity of nuclear cataracts from slit-lamp images. Local filters are first acquired through clustering of image patches from lenses within the same grading class. The learned filters are fed into a convolutional neural network, followed by a set of recursive neural networks, to further extract higher order features. With these features, support vector regression is applied to determine the cataract grade. Results: The proposed system is validated on a large population-based dataset of 5378 images, where it outperforms the state of the art by yielding with respect to clinical grading a mean absolute error (ε) of 0.304, a 70.7% exact integral agreement ratio (R<sub>0</sub>), an 88.4% decimal grading error ≤0.5 (R<sub>e0.5</sub>), and a 99.0% decimal grading error ≤1.0 (R<sub>e1.0</sub>). Significance: The proposed method is useful for assisting and improving clinical management of the disease in the context of large-population screening and has the potential to be applied to other eye diseases.","0018-9294;00189294","","10.1109/TBME.2015.2444389","10.13039/501100001348 - Agency for Science, Technology and Research, Singapore; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122265","Automatic Feature Learning;Automatic feature learning;Cataract Grading;Deep Learning;cataract grading;deep learning","Feature extraction;Image color analysis;Lenses;Neural networks;Standards;Training;Visualization","biomedical optical imaging;diseases;eye;image denoising;image representation;learning (artificial intelligence);medical image processing;regression analysis;support vector machines;vision defects","automatic feature learning;clinical grading;clinical management;clouding;decimal grading error;deep learning;diagnosis monitoring;eye diseases;image patch clustering;integral agreement ratio;large-population screening;lens;local filters;mean absolute error;noisy representation;nuclear cataracts;population-based dataset;progression monitoring;slit-lamp imaging","Adult;Algorithms;Cataract;Diagnostic Techniques, Ophthalmological;Humans;Lens, Crystalline;Neural Networks (Computer)","3","","38","","","20150611","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"A Configurable Deep Network for high-dimensional clinical trial data","J. O' Donoghue; M. Roantree; M. Van Boxtel","Insight Centre for Data Analytics, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland","2015 International Joint Conference on Neural Networks (IJCNN)","20151001","2015","","","1","8","Clinical studies provide interesting case studies for data mining researchers, given the often high degree of dimensionality and long term nature of these studies. In areas such as dementia, accurate predictions from data scientists provide vital input into the understanding of how certain features (representing lifestyle) can predict outcomes such as dementia. Most research involved has used traditional or shallow data mining approaches which have been shown to offer varying degrees of accuracy in datasets with high dimensionality. In this research, we explore the use of deep learning architectures, as they have been shown to have high predictive capabilities in image and audio datasets. The purpose of our research is to build a framework which allows easy reconfiguration for the performance of experiments across a number of deep learning approaches. In this paper, we present our framework for a configurable deep learning machine and our evaluation and analysis of two shallow approaches: regression and multi-layer perceptron, as a platform to a deep belief network, and using a dataset created over the course of 12 years by researchers in the area of dementia.","2161-4393;21614393","Electronic:978-1-4799-1960-4; POD:978-1-4799-1961-1","10.1109/IJCNN.2015.7280841","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280841","","Dementia","belief networks;data mining;learning (artificial intelligence);medical disorders;medical information systems;multilayer perceptrons","audio dataset;clinical study;configurable deep learning machine;configurable deep network;data mining researcher;data scientist;deep belief network;deep learning architecture;dementia;easy reconfiguration;high-dimensional clinical trial data;image dataset;multilayer perceptron;predictive capability;regression perception","","1","","27","","","","12-17 July 2015","","IEEE","IEEE Conference Publications"
"Accurate Segmentation of Cervical Cytoplasm and Nuclei Based on Multiscale Convolutional Network and Graph Partitioning","Y. Song; L. Zhang; S. Chen; D. Ni; B. Lei; T. Wang","Shenzhen University","IEEE Transactions on Biomedical Engineering","20150916","2015","62","10","2421","2433","In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.","0018-9294;00189294","","10.1109/TBME.2015.2430895","48th Scientific Research Foundation for the Returned Overseas Chinese Scholars; Shenzhen Key Basic Research; Shenzhen-Hong Kong Innovation Circle Funding; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004608 - National Natural Science Foundation of Guangdong Province; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103332","Cervical segmentation;coarse to fine;graph partitioning;graph-partitioning;multi-scale convolutional network;multiscale convolutional network (MSCN);touching-cell splitting","Computer architecture;Feature extraction;Image color analysis;Image edge detection;Image segmentation;Microprocessors;Shape","biomedical optical imaging;cancer;cellular biophysics;computational complexity;feature extraction;image segmentation;image texture;learning (artificial intelligence);medical image processing","MSCN;automated graph partitioning method;cervical cytoplasm segmentation;cervical nucleus cell segmentation;coarse segmentation;coarse-to-fine nucleus segmentation framework;computational complexity;contextual information;deep learning;multiscale convolutional network;pretrained feature;raw pixels;scale invariant feature extraction;shape information;superpixel;target objects;texture information","","12","","51","","","20150507","Oct. 2015","","IEEE","IEEE Journals & Magazines"
"Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network","Y. Zou; L. Li; Y. Wang; J. Yu; Y. Li; W. J. Deng","ADSPLAB/ELIP, School of ECE, Peking University, Shenzhen 518055, China","2015 IEEE International Conference on Digital Signal Processing (DSP)","20150910","2015","","","1274","1278","This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.","1546-1874;15461874","Electronic:978-1-4799-8058-1; POD:978-1-4799-8059-8; USB:978-1-4799-8057-4","10.1109/ICDSP.2015.7252086","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7252086","deep convolutional neural network;digestive organs classification;parameter selection;wireless capsule endoscopy","Accuracy;Convolution;Endoscopes;Feature extraction;Intestines;Training;Wireless communication","biological organs;biomedical optical imaging;brightness;endoscopes;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;object recognition","DCNN-WCE-CS;complex digestive tract circumstance;deep CNN-based WCE classification system;deep convolutional neural network;digestive organ classification problem;human biological visual systems;layer-wise hierarchy models;luminance change;semantic image feature recognition;training data;wireless capsule endoscopy images","","2","","14","","","","21-24 July 2015","","IEEE","IEEE Conference Publications"
"Multi-Layer and Recursive Neural Networks for Metagenomic Classification","G. Ditzler; R. Polikar; G. Rosen","Department of Electrical & Computer Engineering, Drexel University, Philadelphia","IEEE Transactions on NanoBioscience","20150828","2015","14","6","608","616","Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.","1536-1241;15361241","","10.1109/TNB.2015.2461219","Department of Energy; 10.13039/100000001 - National Science Foundation; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432","Comparative metagenomics;metagenomics;microbiome;neural networks","Feature extraction;Machine learning;Nanobioscience;Neural networks;Organisms;Training;Vegetation","DNA;genomics;image classification;image representation;learning (artificial intelligence);medical image processing;molecular biophysics;multilayer perceptrons;natural language processing;neurophysiology","classification accuracy;data structure;deep belief network;deep learning methods;generalization performance;hierarchical representations;image classification;language modeling;machine learning;machine learning community;metagenomic classification;metagenomic data analysis;metagenomic literature;multilayer perceptron;multilayer-recursive neural networks;natural language processing;neural networks;nonlinear feature representations;prediction algorithm;predictive metagenomic analysis;unsupervised fashion","Algorithms;Metagenomics;Microbiota;Neural Networks (Computer)","4","","44","","","20150824","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Deep convolutional activation features for large scale Brain Tumor histopathology image classification and segmentation","Y. Xu; Z. Jia; Y. Ai; F. Zhang; M. Lai; E. I. C. Chang","Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beihang University, China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","947","951","We propose a simple, efficient and effective method using deep convolutional activation features (CNNs) to achieve stat- of-the-art classification and segmentation for the MICCAI 2014 Brain Tumor Digital Pathology Challenge. Common traits of such medical image challenges are characterized by large image dimensions (up to the gigabyte size of an image), a limited amount of training data, and significant clinical feature representations. To tackle these challenges, we transfer the features extracted from CNNs trained with a very large general image database to the medical image challenge. In this paper, we used CNN activations trained by ImageNet to extract features (4096 neurons, 13.3% active). In addition, feature selection, feature pooling, and data augmentation are used in our work. Our system obtained 97.5% accuracy on classification and 84% accuracy on segmentation, demonstrating a significant performance gain over other participating teams.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178109","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178109","classification;deep convolutional activation features;deep learning;feature learning;segmentation","Biomedical imaging;Feature extraction;Image segmentation;Support vector machines;Training;Training data;Tumors","brain;feature extraction;image classification;image segmentation;medical image processing;tumours","CNN activations;ImageNet;MICCAI 2014 Brain Tumor Digital Pathology Challenge;brain tumor histopathology;deep convolutional activation features;features extraction;image classification;image dimensions;image segmentation","","3","","23","","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"Cloud based virtualization for a calorie measurement e-health mobile application","S. V. B. Peddi; A. Yassine; S. Shirmohammadi","Distributed and Collaborative Virtual Environments Research Laboratory, University of Ottawa, Canada","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20150730","2015","","","1","6","Smartphones have transformed people approach towards technology. It not only empowers them to use it for communication but also for tracking and maintaining health-fitness via mobile applications. With the increasing number of complex mobile applications, the mobile, with its limited resources (in terms of the computational power and storage capacity) cannot efficiently run these applications autonomously. Similarly our e-health mobile application (Eat Healthy Stay Healthy) requires a platform to run highly computational intensive algorithms like the deep learning (for recognizing food images) and calorie measurement would need higher processing power to perform optimally. We propose a cloud based virtualization model that provides our e-health application with the required computational power that it needs to perform efficiently and at the same time would also give it the flexibility to make use of the various cloud resources. Our model comprises of concepts like virtual swap between various mobile sessions that assist the system for faster processing and intelligent decision mechanism for distributing the task of image processing to cloud servers. By implementing intelligent decision mechanism, the final calorie computation significantly improved by 20.5% while implementing deep learning in cloud.","","Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3","10.1109/ICMEW.2015.7169853","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169853","Cloud based Virtualization model;Intelligent Decision Mechanism in Cloud;Mobile Cloud Computing;Mobile e-Health Application;Virtual Swap","Androids;Humanoid robots;Servers;Time measurement","cloud computing;image recognition;learning (artificial intelligence);medical information systems;mobile computing;smart phones;virtualisation","Eat Healthy Stay Healthy;calorie measurement e-health mobile application;cloud based virtualization;cloud resources;cloud servers;complex mobile applications;computational intensive algorithms;computational power;deep learning;food image recognition;health fitness;image processing;intelligent decision mechanism;mobile sessions;smart phones;storage capacity;task distribution;virtual swap","","1","","12","","","","June 29 2015-July 3 2015","","IEEE","IEEE Conference Publications"
"Deep learninig of EEG signals for emotion recognition","Y. Gao; H. J. Lee; R. M. Mehmood","Division of Computer Science and Engineering, Chonbuk National University, Jeonju 561-756, Korea","2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20150730","2015","","","1","5","Emotion recognition is an important task for computer to understand the human status in brain computer interface (BCI) systems. It is difficult to perceive the emotion of some disabled people through their facial expression, such as functional autism patient. EEG signal provides us a non-invasive way to recognize the emotion of these disable people through EEG headset electrodes placed on their scalp. In this paper, we propose a deep learning algorithm to simultaneously learn the features and classify the emotions of EEG signals. It differs from the conventional methods as we apply deep learning on the raw signal without explicit hand-crafted feature extraction. Because the EEG signal has subject dependency, it is better to train the emotion model subject-wise, while there is not much epochs available for each subject. Deep learning algorithm provides a solution with a pre-training way using three layers of restricted Boltzmann machines (RBMs). Thus, we can use epochs of all subjects to pre-training the deep network, and use back-propagation to fine tuning the network subject by subject. Experiment results show that our proposed framework achieves better recognition accuracy than conventional algorithms.","","Electronic:978-1-4799-7079-7; POD:978-1-4799-7080-3","10.1109/ICMEW.2015.7169796","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169796","Deep learning;EEG;Emotion Recognition;RBM","Artificial neural networks;Biomedical imaging;Brain modeling;Electroencephalography;Feature extraction;Image recognition;Support vector machines","Boltzmann machines;behavioural sciences computing;electrodes;electroencephalography;emotion recognition;handicapped aids;learning (artificial intelligence);medical signal processing","BCI;EEG headset electrodes;EEG signals;RBM;backpropagation;brain computer interface systems;deep learning algorithm;deep network;disabled people;emotion model;emotion recognition;facial expression;functional autism patient;human status;restricted Boltzmann machines;subject dependency","","1","","21","","","","June 29 2015-July 3 2015","","IEEE","IEEE Conference Publications"
"Clinical subthalamic nucleus prediction from high-field brain MRI","J. Kim; Y. Duchin; G. Sapiro; J. Vitek; N. Harel","Department of Electrical and Computer Engineering, Duke University, Durham, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1264","1267","The subthalamic nucleus (STN) within the sub-cortical region of the Basal ganglia is a crucial targeting structure for Parkinson's Deep brain stimulation (DBS) surgery. Volumetric segmentation of such small and complex structure, which is elusive in clinical MRI protocols, is thereby a pre-requisite process for reliable DBS direct targeting. While direct visualization of the STN is facilitated with advanced ultrahigh-field MR imaging (7 Tesla), such high fields are not always clinically available. In this paper, we aim at the automatic prediction of the STN region on clinical low-field MRI, exploiting dependencies between the STN and its adjacent structures, learned from ultrahigh-field MRI. We present a framework based on a statistical shape model to learn such shape relationship on high quality MR data sets. This allows for an accurate prediction and visualization of the STN structure, given detectable predictors on the low-field MRI. Experimental results on Parkinson's patients demonstrate that the proposed approach enables accurate estimation of the STN on clinical 1.5T MRI.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164104","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164104","Deep brain stimulation;high-field MRI;statistical shape models;subthalamic nucleus","Image segmentation;Magnetic resonance imaging;Predictive models;Satellite broadcasting;Shape;Training","biomedical MRI;brain;diseases;image segmentation;medical image processing;physiological models;statistical analysis;surgery","DBS direct targeting;Parkinson's Deep brain stimulation surgery;STN structure prediction;STN structure visualization;adjacent structures;basal ganglia;clinical MRI protocols;clinical low-field MRI;clinical subthalamic nucleus prediction;direct visualization;high quality MR data;high-field brain MRI;magnetic flux density 1.5 T;magnetic flux density 7 T;shape relationship;statistical shape model;subcortical region;ultrahigh-field MR imaging;ultrahigh-field MRI;volumetric segmentation","","0","","20","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Tree RE-weighted belief propagation using deep learning potentials for mass segmentation from mammograms","N. Dhungel; G. Carneiro; A. P. Bradley","ACVT, School of Computer Science, The University of Adelaide","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","760","763","In this paper, we propose a new method for the segmentation of breast masses from mammograms using a conditional random field (CRF) model that combines several types of potential functions, including one that classifies image regions using deep learning. The inference method used in this model is the tree re-weighted (TRW) belief propagation, which allows a learning mechanism that directly minimizes the mass segmentation error and an inference approach that produces an optimal result under the approximations of the TRW formulation. We show that the use of these inference and learning mechanisms and the deep learning potential functions provides gains in terms of accuracy and efficiency in comparison with the current state of the art using the publicly available datasets INbreast and DDSM-BCRP.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163983","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163983","Deep learning;Gaussian Mixture model;Mammograms;mass segmentation;tree re-weighted belief propagation","Belief propagation;Image segmentation;Machine learning;Mammography;Shape;Solid modeling;Training","belief networks;cancer;image segmentation;inference mechanisms;learning (artificial intelligence);mammography;medical image processing;minimisation;tumours","CRF model;DDSM-BCRP dataset;INbreast dataset;TRW formulation;breast masses;conditional random field;deep learning potentials;error minimization;inference approach;mammograms;mass segmentation;tree re-weighted belief propagation","","1","","20","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Generation of synthetic structural magnetic resonance images for deep learning pre-training","E. Castro; A. Ulloa; S. M. Plis; J. A. Turner; V. D. Calhoun","The Mind Research Network, Albuquerque, NM","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1057","1060","Deep learning methods have significantly improved classification accuracy in different areas such as speech, object and text recognition. However, this field has only began to be explored in the brain imaging field, which differs from other fields in terms of the amount of data available, its data dimensionality and other factors. This paper proposes a methodology to generate an extensive synthetic structural magnetic resonance imaging (sMRI) dataset to be used at the pre-training stage of a shallow network model to address the issue of having a limited amount of data available. Our results show that by extending our dataset using 5,000 synthetic sMRI volumes for pretraining, which accounts to approximately 10 times the size of the original dataset, we can obtain a 5% average improvement on classification results compared to the regular approach on a schizophrenia dataset. While the use of synthetic sMRI data for pre-training has only been tested on a shallow network, this can be readily applied to deeper networks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164053","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164053","deep learning;pretraining;schizophrenia;simulation;structural MRI","Data models;Machine learning;Magnetic resonance imaging;Neuroimaging;Probability density function;Support vector machines;Training","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing","brain imaging field;classification accuracy;deep learning pretraining method;deeper networks;magnetic resonance imaging;object recognition;schizophrenia dataset;shallow network model;speech recognition;structural MRI;synthetic sMR image;synthetic sMRI volume;text recognition","","0","","15","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Automated anatomical landmark detection ondistal femur surface using convolutional neural network","D. Yang; S. Zhang; Z. Yan; C. Tan; K. Li; D. Metaxas","CBIM, Rutgers University, Piscataway, NJ, US","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","17","21","Accurate localization of the anatomical landmarks on distal femur bone in the 3D medical images is very important for knee surgery planning and biomechanics analysis. However, the landmark identification process is often conducted manually or by using the inserted auxiliaries, which is time-consuming and lacks of accuracy. In this paper, an automatic localization method is proposed to determine positions of initial geometric landmarks on femur surface in the 3D MR images. Based on the results from the convolutional neural network (CNN) classifiers and shape statistics, we use the narrow-band graph cut optimization to achieve the 3D segmentation of femur surface. Finally, the anatomical landmarks are located on the femur according to the geometric cues of surface mesh. Experiments demonstrate that the proposed method is effective, efficient, and reliable to segment femur and locate the anatomical landmarks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163806","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163806","Deep learning;anatomical landmark detection;convolutional neural network;graph cut;mesh curvature","Biomedical imaging;Bones;Image segmentation;Neural networks;Shape;Three-dimensional displays;Training","biomechanics;biomedical MRI;bone;graph theory;image classification;medical image processing;neural nets;optimisation;surgery","3D MR images;3D medical images;3D segmentation;CNN classifiers;automated anatomical landmark detection;automatic localization method;biomechanics analysis;convolutional neural network;distal femur bone;distal femur surface;knee surgery planning;landmark identification process;magnetic resonance images;narrow-band graph cut optimization;shape statistics;surface mesh","","0","","20","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Automatic detection of cerebral microbleeds via deep learning based 3D feature representation","H. Chen; L. Yu; Q. Dou; L. Shi; V. C. T. Mok; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","764","767","Clinical identification and rating of the cerebral microbleeds (CMBs) are important in vascular diseases and dementia diagnosis. However, manual labeling is time-consuming with low reproducibility. In this paper, we present an automatic method via deep learning based 3D feature representation, which solves this detection problem with three steps: candidates localization with high sensitivity, feature representation, and precise classification for reducing false positives. Different from previous methods by exploiting low-level features, e.g., shape features and intensity values, we utilize the deep learning based high-level feature representation. Experimental results validate the efficacy of our approach, which outperforms other methods by a large margin with a high sensitivity while significantly reducing false positives per subject.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163984","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163984","cerebral microbleeds;deep learning;feature representation;object detection","Biomedical imaging;Feature extraction;Machine learning;Radio frequency;Sensitivity;Three-dimensional displays;Training","biomedical MRI;blood vessels;brain;diseases;image representation;learning (artificial intelligence);medical image processing;object detection","CMB;automatic detection;candidate localization;cerebral microbleed rating;clinical identification;deep learning based 3D feature representation;deep learning based high-level feature representation;dementia diagnosis;detection problem;false positive reduction;intensity values;low-level features;manual labeling;precise classification;shape features;vascular diseases","","2","","13","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Using deep learning for robustness to parapapillary atrophy in optic disc segmentation","R. Srivastava; J. Cheng; D. W. K. Wong; J. Liu","Institute for Infocomm Research, Singapore 138632","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","768","771","Optic Disc (OD) segmentation from retinal fundus images is important for many applications such as detecting other optic structures and early detection of glaucoma. One of the major problems in segmenting OD is the presence of Para-papillary Atrophy (PPA) which in many cases looks similar to the OD. Researchers have used many different features to distinguish between PPA and OD, however each of these features has some limitation or the other. In this paper, we propose to use a deep neural network for OD segmentation which can learn features to distinguish PPA from OD. Using simple image intensity based features, the proposed method has the least mean overlapping error (9.7%) among the state-of-the-art works for OD segmentation in fundus images with PPA.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163985","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163985","Optic Disc segmentation;deep learning;parapapillary atrophy;retinal fundus images","Adaptive optics;Atrophy;Feature extraction;Image segmentation;Optical imaging;Retina;Training","eye;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;neurophysiology","deep learning;deep neural network;glaucoma detection;image intensity based features;least mean overlapping error;optic disc segmentation;optic structures;parapapillary atrophy;retinal fundus images","","1","","10","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Deep learning for automatic cell detection in wide-field microscopy zebrafish images","B. Dong; L. Shao; M. Da Costa; O. Bandmann; A. F. Frangi","Centre of Computational Imaging & Simulation Technologies in Biomedicine (CISTIB)","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","772","776","The zebrafish has become a popular experimental model organism for biomedical research. In this paper, a unique framework is proposed for automatically detecting Tyrosine Hydroxylase-containing (TH-labeled) cells in larval zebrafish brain z-stack images recorded through the wide-field microscope. In this framework, a supervised max-pooling Convolutional Neural Network (CNN) is trained to detect cell pixels in regions that are preselected by a Support Vector Machine (SVM) classifier. The results show that the proposed deep-learned method outperforms hand-crafted techniques and demonstrate its potential for automatic cell detection in wide-field microscopy z-stack zebrafish images.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163986","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163986","","Computer architecture;Histograms;Microprocessors;Microscopy;Neurons;Three-dimensional displays;Training","biomedical optical imaging;brain;cellular biophysics;convolution;enzymes;feature extraction;image classification;learning (artificial intelligence);medical image processing;molecular biophysics;neural nets;neurophysiology;optical microscopy;support vector machines","SVM classifier;automatic TH-labeled cell detection;automatic tyrosine hydroxylase-containing cell detection;biomedical research;cell pixel detection;convolutional neural network;deep learning;experimental model organism;hand-crafted technique;larval zebrafish brain z-stack image recording;region preselection;supervised max-pooling CNN training;support vector machine;wide-field microscopy","","3","","26","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology","D. Sheet; S. P. K. Karri; A. Katouzian; N. Navab; A. K. Ray; J. Chatterjee","Department of Electrical Engineering, Indian Institute of Technology Kharagpur, India","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","777","780","Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163987","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163987","Representation learning;cutaneous wounds;denoising autoencoders;in situ histology;optical coherence tomography;tissue characterization","Adaptive optics;Biomedical optical imaging;Machine learning;Optical imaging;Skin;Speckle;Wounds","biomedical optical imaging;image coding;image denoising;medical image processing;neural nets;optical tomography;skin;speckle;tissue engineering;unsupervised learning;wounds","OCT;choroid;clinical reporting variability;coherent sensing;conventional histology;coronary vessels;critical organs;deep learning;deep neural network-based architecture;denoising autoencoders;healing wounds;histologically characterizing healthy skin;in situ histology;interobserver reporting variability challenges;intraobserver reporting variability challenges;laser source incident;leveraged rapid clinical translation;live tissues;noninvasive high-resolution in situ imaging;optical coherence tomography;photon diffraction;retina;speckle image formation;speckle representations;stochastic nature;swept-source OCT;tissue specific speckle appearance;tissue specific speckle representations;unsupervised learning;wounded mice skin","","0","","11","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Disease Inference from Health-Related Questions via Sparse Deep Learning","L. Nie; M. Wang; L. Zhang; S. Yan; B. Zhang; T. S. Chua","School of Computing, National University of Singapore, Singapore","IEEE Transactions on Knowledge and Data Engineering","20150706","2015","27","8","2107","2119","Automatic disease inference is of importance to bridge the gap between what online health seekers with unusual symptoms need and what busy human doctors with biased expertise can offer. However, accurately and efficiently inferring diseases is non-trivial, especially for community-based health services due to the vocabulary gap, incomplete information, correlated medical concepts, and limited high quality training samples. In this paper, we first report a user study on the information needs of health seekers in terms of questions and then select those that ask for possible diseases of their manifested symptoms for further analytic. We next propose a novel deep learning scheme to infer the possible diseases given the questions of health seekers. The proposed scheme is comprised of two key components. The first globally mines the discriminant medical signatures from raw features. The second deems the raw features and their signatures as input nodes in one layer and hidden nodes in the subsequent layer, respectively. Meanwhile, it learns the inter-relations between these two layers via pre-training with pseudo-labeled data. Following that, the hidden nodes serve as raw features for the more abstract signature mining. With incremental and alternative repeating of these two components, our scheme builds a sparsely connected deep architecture with three hidden layers. Overall, it well fits specific tasks with fine-tuning. Extensive experiments on a real-world dataset labeled by online doctors show the significant performance gains of our scheme.","1041-4347;10414347","","10.1109/TKDE.2015.2399298","NUS-Tsinghua Extreme Search Project; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029673","Community-based Health Services;Community-based health services;Deep Learning;Disease Inference;Question Answering;deep learning;disease inference;question answering","Cancer;Diseases;Educational institutions;Medical diagnostic imaging;Training","data mining;diseases;health care;inference mechanisms;information needs;learning (artificial intelligence);medical information systems","abstract signature mining;automatic disease inference;community-based health services;discriminant medical signatures;health-related questions;information needs;online health seekers;pseudolabeled data;sparse deep learning;sparsely connected deep architecture","","23","","51","","","20150203","Aug. 1 2015","","IEEE","IEEE Journals & Magazines"
"Self-supervised learning model for skin cancer diagnosis","A. Masood; A. Al- Jumaily; K. Anam","University of Technology Sydney, P.O. Box 123 Broadway, NSW 2007 Australia","2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)","20150702","2015","","","1012","1015","Automated diagnosis of skin cancer is an active area of research with different classification methods proposed so far. However, classification models based on insufficient labeled training data can badly influence the diagnosis process if there is no self-advising and semi supervising capability in the model. This paper presents a semi supervised, self-advised learning model for automated recognition of melanoma using dermoscopic images. Deep belief architecture is constructed using labeled data together with unlabeled data, and fine tuning done by an exponential loss function in order to maximize separation of labeled data. In parallel a self-advised SVM algorithm is used to enhance classification results by counteracting the effect of misclassified data. To increase generalization capability and redundancy of the model, polynomial and radial basis function based SA-SVMs and Deep network are trained using training samples randomly chosen via a bootstrap technique. Then the results are aggregated using least square estimation weighting. The proposed model is tested on a collection of 100 dermoscopic images. The variation in classification error is analyzed with respect to the ratio of labeled and unlabeled data used in the training phase. The classification performance is compared with some popular classification methods and the proposed model using the deep neural processing outperforms most of the popular techniques including KNN, ANN, SVM and semi supervised algorithms like Expectation maximization and transductive SVM.","1948-3546;19483546","Electronic:978-1-4673-6389-1; POD:978-1-4673-6387-7","10.1109/NER.2015.7146798","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146798","","Lesions;Malignant tumors;Skin cancer;Support vector machines;Training;Training data","cancer;image classification;image segmentation;learning (artificial intelligence);least squares approximations;medical image processing;skin;support vector machines","ANN;KNN;SA-SVM;automated recognition;bootstrap technique;classification error;classification models;classification performance;deep belief architecture;deep neural processing outperforms;dermoscopic imaging;expectation maximization;exponential loss function;least square estimation weighting;melanoma;misclassified data effect;polynomial basis function;radial basis function;self-advised SVM algorithm;self-supervised learning model;semisupervised algorithms;skin cancer diagnosis;training data;transductive SVM","","1","","29","","","","22-24 April 2015","","IEEE","IEEE Conference Publications"
"Classification on ADHD with Deep Learning","D. Kuang; L. He","Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China","2014 International Conference on Cloud Computing and Big Data","20150319","2014","","","27","32","Effective discrimination of attention deficit hyperactivity disorder (ADHD) using imaging and functional biomarkers would have fundamental influence on public health. In usual, the discrimination is based on the standards of American Psychiatric Association. In this paper, we modified one of the deep learning method on structure and parameters according to the properties of ADHD data, to discriminate ADHD on the unique public dataset of ADHD-200. We predicted the subjects as control, combined, inattentive or hyperactive through their frequency features. The results achieved improvement greatly compared to the performance released by the competition. Besides, the imbalance in datasets of deep learning model influenced the results of classification. As far as we know, it is the first time that the deep learning method has been used for the discrimination of ADHD with fMRI data.","","Electronic:978-1-4799-6621-9; POD:978-1-4799-6622-6","10.1109/CCBD.2014.42","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062868","ADHD;Deep Belief Network;Deep Learning;fMRI","Accuracy;Brain modeling;Data models;Feature extraction;Learning systems;Magnetic resonance;Training","learning (artificial intelligence);medical computing;medical disorders;pattern classification","ADHD discrimination;ADHD-200;American Psychiatric Association;attention deficit hyperactivity disorder;classification;datasets imbalance;deep learning method;fMRI data;frequency features;functional biomarkers;imaging;public health","","1","","19","","","","12-14 Nov. 2014","","IEEE","IEEE Conference Publications"
"Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease","S. Liu; S. Liu; W. Cai; H. Che; S. Pujol; R. Kikinis; D. Feng; M. J. Fulham; ADNI","Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney, Sydney, N.S.W., Australia","IEEE Transactions on Biomedical Engineering","20150318","2015","62","4","1132","1140","The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.","0018-9294;00189294","","10.1109/TBME.2014.2372011","AADRF; ARC; NA-MIC; NAC; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963480","Alzheimer’s Disease;Alzheimer's disease (AD);Classification;Deep Learning;MRI;Neuroimaging;PET;classification;deep Learning;neuroimaging;positron emission tomography (PET)","Biomarkers;Diseases;Feature extraction;Neuroimaging;Neurons;Positron emission tomography;Training","biomedical MRI;diseases;learning (artificial intelligence);neurophysiology;patient care;patient diagnosis;positron emission tomography;sensor fusion","Alzheimer disease binary classification;Alzheimer disease computer-aided diagnosis;Alzheimer disease multiclass classification;Alzheimer disease multiclass diagnosis;data fusion;machine learning method;multimodal neuroimaging feature learning;neuroimaging biomarker;patient care;zero-masking strategy","Alzheimer Disease;Brain;Humans;Image Interpretation, Computer-Assisted;Multimodal Imaging;Neuroimaging;Support Vector Machine","27","","65","","","20141120","April 2015","","IEEE","IEEE Journals & Magazines"
"Mobile cloud computing for medical applications","Vishwa Kiran S; R. Prasad; Thriveni J; Venugopal K R; L. M. Patnaik","Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore University, 560 001, India","2014 Annual IEEE India Conference (INDICON)","20150205","2014","","","1","6","Mobile Devices like Smartphones and Tablets are getting ever increasing processing power. This makes them powerful enough to do heavy duty realtime 3D video processing. Tablets capable of recording 3D video using stereo camera can have applications in various fields. In the medical field these tablets can be used for micromanipulations such as microsurgery. We describe the design of a tablet capable of 3D vision and a glasses free stereoscopic display, which can be used to enhance the working of the technicians performing micromanipulations. We also illustrate how by providing a deep integration with the cloud, we can reduce the processing overheads and enable valuable new services. For example, in case of microsurgeries, the process is very specialized and is done in a few specialized medical centers only. When these powerful mobile devices are coupled with the cloud, it adds whole new dimension in the services which can be offered. With our deep cloud integration, the surgery being performed can be streamed in realtime from the Tablet itself. This live feed can be viewed by a remote expert to provide guidance to the surgeon. The students at remote locations can also view this live feed. This would provide them a unique learning opportunity. Moreover, the recordings of these surgeries can be backed up on cloud and provided for on demand viewing for learning purposes.","2325-940X;2325940X","Electronic:978-1-4799-5364-6; POD:978-1-4799-5365-3","10.1109/INDICON.2014.7030597","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030597","3D Video;Android;Mobile Cloud Computing;Multicore Processor;Open Source;System Design;Tablet;sterescopic display","Androids;Bandwidth;Cameras;Humanoid robots;Streaming media;Surgery;Three-dimensional displays","cloud computing;medical computing;mobile computing;stereo image processing;surgery;video signal processing","3D video processing;3D video recording;3D vision;deep cloud integration;glass free stereoscopic display;learning opportunity;medical applications;medical centers;micromanipulations;microsurgery;mobile cloud computing;mobile devices;smartphones;stereo camera;tablets","","0","","9","","","","11-13 Dec. 2014","","IEEE","IEEE Conference Publications"
"Deep learning for brain decoding","O. Firat; L. Oztekin; F. T. Y. Vural","Department of Computer Engineering, Middle East Technical University, Ankara, Turkey","2014 IEEE International Conference on Image Processing (ICIP)","20150129","2014","","","2784","2788","Learning low dimensional embedding spaces (manifolds) for efficient feature representation is crucial for complex and high dimensional input spaces. Functional magnetic resonance imaging (fMRI) produces high dimensional input data and with a less then ideal number of labeled samples for a classification task. In this study, we explore deep learning methods for fMRI classification tasks in order to reduce dimensions of feature space, along with improving classification performance for brain decoding. We employ sparse autoencoders for unsupervised feature learning, leveraging unlabeled fMRI data to learn efficient, non-linear representations as the building blocks of a deep learning architecture by stacking them. Proposed method is tested on a memory encoding/retrieval experiment with ten classes. The results support the efficiency compared to the baseline multi-voxel pattern analysis techniques.","1522-4880;15224880","Electronic:978-1-4799-5751-4; POD:978-1-4799-5752-1","10.1109/ICIP.2014.7025563","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025563","Deep Learning;MVPA;Stacked Autoencoders;brain state decoding;fMRI","Computer architecture;Decoding;Encoding;Feature extraction;Magnetic resonance imaging;Manifolds;Pattern analysis","biomedical MRI;brain;decoding;feature extraction;image classification;medical image processing;neurophysiology;unsupervised learning","baseline multi-voxel pattern analysis techniques;brain decoding;complex input spaces;deep learning architecture;deep learning methods;efficient feature representation;fMRI classification tasks;feature space dimension;functional magnetic resonance imaging;high dimensional input data;high dimensional input spaces;low dimensional embedding spaces;manifolds;memory encoding;memory retrieval;nonlinear representations;sample classification;sparse autoencoders;unlabeled fMRI data;unsupervised feature learning","","0","","26","","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Experimental Study of Unsupervised Feature Learning for HEp-2 Cell Images Clustering","Y. Zhao; Z. Gao; L. Wang; L. Zhou","Univ. of Wollongong, Wollongong, NSW, Australia","2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA)","20150115","2014","","","1","8","Automatic identification of HEp-2 cell images has received an increasing research attention. Feature representations play a critical role in achieving good identification performance. Much recent work has focused on supervised feature learning. Typical methods consist of BoW model (based on hand-crafted features) and deep learning model (learning hierarchical features). However, these labels used in supervised feature learning are very labour-intensive and time-consuming. They are commonly manually annotated by specialists and very expensive to obtain. In this paper, we follow this fact and focus on unsupervised feature learning. We have verified and compared the features of these two typical models by clustering. Experimental results show the BoW model generally perform better than deep learning models. Also, we illustrate BoW model and deep learning models have complementarity properties.","","Electronic:978-1-4799-5409-4; POD:978-1-4799-5410-0","10.1109/DICTA.2014.7008108","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008108","","Decoding;Feature extraction;Image coding;Neural networks;Noise reduction;Training;Vectors","feature extraction;image representation;medical image processing;unsupervised learning","BoW model;HEp-2 cell image clustering;deep learning model;feature representation;unsupervised feature learning","","0","","18","","","","25-27 Nov. 2014","","IEEE","IEEE Conference Publications"
"Deep learning for healthcare decision making with EMRs","Z. Liang; G. Zhang; J. X. Huang; Q. V. Hu","School of Information Technology, York University, Toronto, ON, M3J1P3, Canada","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20150115","2014","","","556","559","Computer aid technology is widely applied in decision-making and outcome assessment of healthcare delivery, in which modeling knowledge and expert experience is technically important. However, the conventional rule-based models are incapable of capturing the underlying knowledge because they are incapable of simulating the complexity of human brains and highly rely on feature representation of problem domains. Thus we attempt to apply a deep model to overcome this weakness. The deep model can simulate the thinking procedure of human and combine feature representation and learning in a unified model. A modified version of convolutional deep belief networks is used as an effective training method for large-scale data sets. Then it is tested by two instances: a dataset on hypertension retrieved from a HIS system, and a dataset on Chinese medical diagnosis and treatment prescription from a manual converted electronic medical record (EMR) database. The experimental results indicate that the proposed deep model is able to reveal previously unknown concepts and performs much better than the conventional shallow models.","","Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8","10.1109/BIBM.2014.6999219","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999219","deep belief network;deep learning;restricted Boltzmann machine;syndrome classification;unsupervised feature learning","Brain modeling;Data models;Hypertension;Medical diagnostic imaging;Support vector machines;Training","belief networks;brain;brain-computer interfaces;decision making;electronic health records;health care;patient diagnosis;patient treatment;unsupervised learning","Chinese medical diagnosis;Chinese medical treatment prescription;EMR database;HIS system;belief networks;computer aid technology;decision making;deep learning;electronic medical record database;feature representation;healthcare;human brains;shallow models;training method","","3","","18","","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Auto-encoder using the bi-firing activation function","Zihong Cao; Guangjun Zeng; W. W. Y. Ng; Jincheng Le","Machine Learning and Cybernetics Research Center, School of Computing Science and Engineering, South China University of Technology, Guangzhou, China, 510006","2014 International Conference on Machine Learning and Cybernetics","20150115","2014","1","","271","277","Training the whole deep neural network together is restricted by the gradient diffusion problem. Greedy layer-wise training of an auto-encoder has achieved promising results in deep neural networks. However, it can not learn useful input representation from the original input directly. In this work, we propose to use the bi-firing activation function for auto-encoder with an end-to-end training scheme. It not only improves the training efficiency but also learns better features than the traditional stacked auto-encoder. Experimental results show that it extracts more representative features and also outperforms the stacked auto-encoder in supervised classification task.","2160-133X;2160133X","CD-ROM:978-1-4799-4217-6; Electronic:978-1-4799-4215-2; POD:978-1-4799-4214-5","10.1109/ICMLC.2014.7009128","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009128","Auto-encoder;Bi-firing function;Deep Learning;Layer-wise scheme","Abstracts;Accuracy;Lead;Training;Tuning","feature extraction;gradient methods;image classification;image representation;neural nets","auto-encoder;bi-firing activation function;end-to-end training scheme;feature extraction;gradient diffusion problem;greedy layer-wise training;supervised classification task;whole deep neural network","","1","","18","","","","13-16 July 2014","","IEEE","IEEE Conference Publications"
"The community FabLab platform: Applications and implications in biomedical engineering","M. K. Stephenson; D. E. Dow","Wentworth Institute of Technology, Department of Biomedical Engineering, 550 Huntington Ave., Boston, MA, 02115","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","1821","1825","Skill development in science, technology, engineering and math (STEM) education present one of the most formidable challenges of modern society. The Community FabLab platform presents a viable solution. Each FabLab contains a suite of modern computer numerical control (CNC) equipment, electronics and computing hardware and design, programming, computer aided design (CAD) and computer aided machining (CAM) software. FabLabs are community and educational resources and open to the public. Development of STEM based workforce skills such as digital fabrication and advanced manufacturing can be enhanced using this platform. Particularly notable is the potential of the FabLab platform in STEM education. The active learning environment engages and supports a diversity of learners, while the iterative learning that is supported by the FabLab rapid prototyping platform facilitates depth of understanding, creativity, innovation and mastery. The product and project based learning that occurs in FabLabs develops in the student a personal sense of accomplishment, self-awareness, command of the material and technology. This helps build the interest and confidence necessary to excel in STEM and throughout life. Finally the introduction and use of relevant technologies at every stage of the education process ensures technical familiarity and a broad knowledge base needed for work in STEM based fields. Biomedical engineering education strives to cultivate broad technical adeptness, creativity, interdisciplinary thought, and an ability to form deep conceptual understanding of complex systems. The FabLab platform is well designed to enhance biomedical engineering education.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6943963","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6943963","","Biomedical imaging;Communities;Computer numerical control;Education;Fabrication;Three-dimensional displays","CAD;biomedical education;biomedical equipment;computer aided instruction;computer aided manufacturing;computerised numerical control;continuing education;educational aids;engineering education;innovation management;mathematics;medical computing;physics education;rapid prototyping (industrial);societies;student experiments;teaching","CAD software;CAM software;CNC equipment;FabLab platform design;FabLab rapid prototyping platform;STEM based fields;STEM based workforce skill development;STEM education;active learning environment;advanced manufacturing;biomedical engineering education;community FabLab platform application;community resources;complex system conceptual understanding;computer aided design;computer aided machining;computing design;computing hardware;digital fabrication;educational resources;electronics;interdisciplinary thought;iterative learning;material familiarity;modern computer numerical control equipment;product based learning;programming;project based learning;science, technology, engineering and math education;technical adeptness;technical creativity;technical familiarity;technical innovation;technical mastery","","2","","7","","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"A deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei","Y. Song; L. Zhang; S. Chen; D. Ni; B. Li; Y. Zhou; B. Lei; T. Wang","Department of Biomedical Engineering, School of Medicine, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","2903","2906","In this paper, a superpixel and convolution neural network (CNN) based segmentation method is proposed for cervical cancer cell segmentation. Since the background and cytoplasm contrast is not relatively obvious, cytoplasm segmentation is first performed. Deep learning based on CNN is explored for region of interest detection. A coarse-to-fine nucleus segmentation for cervical cancer cell segmentation and further refinement is also developed. Experimental results show that an accuracy of 94.50% is achieved for nucleus region detection and a precision of 0.9143±0.0202 and a recall of 0.8726±0.0008 are achieved for nucleus cell segmentation. Furthermore, our comparative analysis also shows that the proposed method outperforms the related methods.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6944230","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944230","","Accuracy;Cervical cancer;Image color analysis;Image segmentation;Neural networks;Training","biological organs;cancer;cellular biophysics;image segmentation;medical image processing;neural nets","CNN based segmentation method;cervical cancer cell segmentation;cervical cytoplasm;coarse-to-fine nucleus segmentation;convolution neural network;cytoplasm segmentation;deep learning based framework;nucleus cell segmentation;nucleus region detection","","1","","9","","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"Distributed cerebellar plasticity implements multiple-scale memory components of Vestibulo-Ocular Reflex in real-robots","C. Casellato; A. Antonietti; J. A. Garrido; A. Pedrocchi; E. D'Angelo","Neuro Engineering And medical Robotics Laboratory, Dept. Electronics, Information and Bioengineering, Politecnico di Milano, P.zza L. Da Vinci 32, 20133, Milano, Italy","5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics","20141002","2014","","","813","818","The cerebellum plays a crucial role in motor learning and it acts as a predictive controller. A biological inspired cerebellar model with distributed plasticity has been embedded into a real-time controller of a neurorobot. A cerebellum-driven task has been designed: the Vestibulo-Ocular Reflex (VOR), which produces eye movements stabilizing images on the retina during head movement. The cerebellar controller drives eye compensation, by providing joint torque based on network output activity. We compared a cerebellar controller with only the cortical plasticity and a cerebellar controller with also the plasticity mechanisms at deep nuclei, in VOR multiple sessions. The results were interpreted using a two state multi-rate model integrating two learning processes with different sensitivities to error and different retention strengths. The cerebellar model showed effective learning along task repetitions, allowing a fine timing and gain adaptation based on the head stimulus. The multisite plasticity proved superior to single-site plasticity in generating human-like VOR during acquisition, extinction and consolidation.","2155-1774;21551774","Electronic:978-1-4799-3128-6; POD:978-1-4799-3129-3; USB:978-1-4799-3127-9","10.1109/BIOROB.2014.6913879","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913879","","Adaptation models;Biological system modeling;Brain modeling;Computational modeling;Head;Robot sensing systems","medical robotics;motion control;neurocontrollers;predictive control;torque control","biological inspired cerebellar model;cerebellar controller;cerebellum-driven task;cortical plasticity;distributed cerebellar plasticity;eye compensation;head stimulus;joint torque;motor learning;multiple-scale memory component;multisite plasticity;neurorobot;predictive controller;real-time controller;vestibulo-ocular reflex","","2","","23","","","","12-15 Aug. 2014","","IEEE","IEEE Conference Publications"
"Fully Automated Non-rigid Segmentation with Distance Regularized Level Set Evolution Initialized and Constrained by Deep-Structured Inference","T. A. Ngo; G. Carneiro","Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia","2014 IEEE Conference on Computer Vision and Pattern Recognition","20140925","2014","","","3118","3125","We propose a new fully automated non-rigid segmentation approach based on the distance regularized level set method that is initialized and constrained by the results of a structured inference using deep belief networks. This recently proposed level-set formulation achieves reasonably accurate results in several segmentation problems, and has the advantage of eliminating periodic re-initializations during the optimization process, and as a result it avoids numerical errors. Nevertheless, when applied to challenging problems, such as the left ventricle segmentation from short axis cine magnetic ressonance (MR) images, the accuracy obtained by this distance regularized level set is lower than the state of the art. The main reasons behind this lower accuracy are the dependence on good initial guess for the level set optimization and on reliable appearance models. We address these two issues with an innovative structured inference using deep belief networks that produces reliable initial guess and appearance model. The effectiveness of our method is demonstrated on the MICCAI 2009 left ventricle segmentation challenge, where we show that our approach achieves one of the most competitive results (in terms of segmentation accuracy) in the field.","1063-6919;10636919","Electronic:978-1-4799-5118-5; POD:978-1-4799-5119-2","10.1109/CVPR.2014.399","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909795","Deep inference;Deep learning;Level sets method;Non-rigid segmentation","Accuracy;Active contours;Heart;Image segmentation;Level set;Shape;Training","belief networks;biomedical MRI;image segmentation;medical image processing;optimisation","MICCAI 2009;MR images;deep belief networks;deep-structured inference;distance regularized level set evolution;fully automated non-rigid segmentation;innovative structured inference;left ventricle segmentation;magnetic resonance images;optimization process;periodic reinitialization elimination","","4","","30","","","","23-28 June 2014","","IEEE","IEEE Conference Publications"
"Non-rigid Segmentation Using Sparse Low Dimensional Manifolds and Deep Belief Networks","J. C. Nascimento; G. Carneiro","Inst. de Sist. e Robot., Inst. Super. Tecnico, Lisbon, Portugal","2014 IEEE Conference on Computer Vision and Pattern Recognition","20140925","2014","","","288","295","In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the search procedure is onducted directly on a sparse low-dimensional manifold, guided by the classification results computed from a deep belief network. Our main contribution is the fact that we do not rely on the typical sub-division of segmentation tasks into rigid detection and non-rigid delineation. Instead, the non-rigid segmentation is performed directly, where points in the sparse low-dimensional can be mapped to an explicit contour representation in image space. Our proposal shows significantly smaller search and training complexities given that the dimensionality of the manifold is much smaller than the dimensionality of the search spaces for rigid detection and non-rigid delineation aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database. Our experiments show that the use of sparse low dimensional manifolds reduces the search and training complexities of current segmentation approaches without a significant impact on the segmentation accuracy shown by state-of-the-art approaches.","1063-6919;10636919","Electronic:978-1-4799-5118-5; POD:978-1-4799-5119-2","10.1109/CVPR.2014.44","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909438","Deep Belief Nets;Non-rigid segmentation;Sparse manifolds","Complexity theory;Image segmentation;Manifolds;Search problems;Shape;Training;Visualization","belief networks;biomedical ultrasonics;cardiology;image classification;image representation;image segmentation;learning (artificial intelligence);medical image processing;visual databases","contour representation;deep belief networks;extended Cohn-Kanade database;image classification;left ventricle endocardial segmentation;lip segmentation;nonrigid visual object segmentation;sparse low dimensional manifolds;ultrasound images","","3","","24","","","","23-28 June 2014","","IEEE","IEEE Conference Publications"
"EEG-based emotion classification using deep belief networks","W. L. Zheng; J. Y. Zhu; Y. Peng; B. L. Lu","Department of Computer Science and Engineering, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng., Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai, China","2014 IEEE International Conference on Multimedia and Expo (ICME)","20140908","2014","","","1","6","In recent years, there are many great successes in using deep architectures for unsupervised feature learning from data, especially for images and speech. In this paper, we introduce recent advanced deep learning models to classify two emotional categories (positive and negative) from EEG data. We train a deep belief network (DBN) with differential entropy features extracted from multichannel EEG as input. A hidden markov model (HMM) is integrated to accurately capture a more reliable emotional stage switching. We also compare the performance of the deep models to KNN, SVM and Graph regularized Extreme Learning Machine (GELM). The average accuracies of DBN-HMM, DBN, GELM, SVM, and KNN in our experiments are 87.62%, 86.91%, 85.67%, 84.08%, and 69.66%, respectively. Our experimental results show that the DBN and DBN-HMM models improve the accuracy of EEG-based emotion classification in comparison with the state-of-the-art methods.","1945-7871;19457871","Electronic:978-1-4799-4761-4; POD:978-1-4799-4760-7","10.1109/ICME.2014.6890166","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890166","Affective Computing;Deep Belief Network;EEG;Emotion Classification","Accuracy;Brain models;Electroencephalography;Feature extraction;Hidden Markov models;Support vector machines","electroencephalography;emotion recognition;entropy;feature extraction;hidden Markov models;learning (artificial intelligence);medical signal processing;support vector machines","DBN-HMM model;EEG data;EEG-based emotion classification;GELM;Graph regularized Extreme Learning Machine;KNN;SVM;advanced deep learning model;deep architectures;deep belief networks;deep model;differential entropy feature extraction;emotional categories;emotional stage switching;hidden Markov model;image;multichannel EEG;speech;unsupervised feature learning","","12","","9","","","","14-18 July 2014","","IEEE","IEEE Conference Publications"
"Convolutional deep belief networks for feature extraction of EEG signal","Y. Ren; Y. Wu","Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China","2014 International Joint Conference on Neural Networks (IJCNN)","20140904","2014","","","2850","2853","In recent years, deep learning approaches have been successfully used to learn hierarchical representations of image data, audio data etc. However, to our knowledge, these deep learning approaches have not been extensively studied for electroencephalographic (EEG) data. Considering the properties of EEG data, high-dimensional and multichannel, we applied convolutional deep belief networks to the feature learning of EEG data and evaluated it on the datasets from previous BCI competitions. Compared with other state-of-the-art feature extraction methods, the learned features using convolutional deep belief network have better performance.","2161-4393;21614393","CD-ROM:978-1-4799-6627-1; Electronic:978-1-4799-1484-5; POD:978-1-4799-1482-1","10.1109/IJCNN.2014.6889383","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6889383","EEG;convolutional deep belief networks;deep learning;feature learning","Accuracy;Convolution;Convolutional codes;Electroencephalography;Feature extraction;Probabilistic logic;Training","belief networks;convolution;data analysis;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing","BCI competitions;EEG signal;audio data;convolutional deep belief networks;datasets;deep learning approaches;electroencephalographic data;feature extraction methods;hierarchical representations;high-dimensional data;image data;multichannel data","","7","","14","","","","6-11 July 2014","","IEEE","IEEE Conference Publications"
"Early diagnosis of Alzheimer's disease with deep learning","S. Liu; S. Liu; W. Cai; S. Pujol; R. Kikinis; D. Feng","BMIT Research Group, School of IT, University of Sydney, Australia","2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)","20140731","2014","","","1015","1018","The accurate diagnosis of Alzheimer's disease (AD) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped. Although many studies have applied machine learning methods for computer-aided-diagnosis (CAD) of AD recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models. In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of AD and its prodromal stage, Mild Cognitive Impairment (MCI). Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge. A significant performance gain on classification of all diagnosis groups was achieved in our experiments.","1945-7928;19457928","Electronic:978-1-4673-1961-4; POD:978-1-4673-1960-7; USB:978-1-4673-1959-1","10.1109/ISBI.2014.6868045","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868045","Alzheimer's disease;classification;neuroimaging","Alzheimer's disease;Feature extraction;Magnetic resonance imaging;Neurons;Support vector machines;Training","biomedical MRI;brain;cognition;diseases;health care;image classification;image coding;learning (artificial intelligence);medical disorders;medical image processing","Alzheimer disease diagnosis;computer-aided-diagnosis;irreversible brain damages;machine learning methods;magnetic resonance imaging;mild cognitive impairment;minimal domain prior knowledge;patient care;stacked auto-encoders","","13","","30","","","","April 29 2014-May 2 2014","","IEEE","IEEE Conference Publications"
"Stacked Sparse Autoencoder (SSAE) based framework for nuclei patch classification on breast cancer histopathology","J. Xu; L. Xiang; R. Hang; J. Wu","Nanjing University of Information Science and Technology, Nanjing 210044, China","2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)","20140731","2014","","","999","1002","In this paper, a Stacked Sparse Autoencoder (SSAE) based framework is presented for nuclei classification on breast cancer histopathology. SSAE works very well in learning useful high-level feature for better representation of input raw data. To show the effectiveness of proposed framework, SSAE+Softmax is compared with conventional Softmax classifier, PCA+Softmax, and single layer Sparse Autoencoder (SAE)+Softmax in classifying the nuclei and non-nuclei patches extracted from breast cancer histopathology. The SSAE+Softmax for nuclei patch classification yields an accuracy of 83.7%, F1 score of 82%, and AUC of 0.8992, which outperform Softmax classifier, PCA+Softmax, and SAE+Softmax.","1945-7928;19457928","Electronic:978-1-4673-1961-4; POD:978-1-4673-1960-7; USB:978-1-4673-1959-1","10.1109/ISBI.2014.6868041","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868041","Breast Cancer Histopathology;Deep learning;Sparse Autoencoder","Breast cancer;Decoding;Neural networks;Principal component analysis;Testing;Training","biological organs;cancer;cellular biophysics;feature extraction;image classification;image coding;medical image processing","SSAE based framework;breast cancer histopathology;high-level feature extraction;nuclei patch classification;stacked sparse autoencoder","","4","","6","","","","April 29 2014-May 2 2014","","IEEE","IEEE Conference Publications"
"Left ventricle segmentation from cardiac MRI combining level set methods with deep belief networks","T. A. Ngo; G. Carneiro","Australian Centre for Visual Technol., Univ. of Adelaide, Adelaide, SA, Australia","2013 IEEE International Conference on Image Processing","20140213","2013","","","695","699","This paper introduces a new semi-automated methodology combining a level set method with a top-down segmentation produced by a deep belief network for the problem of left ventricle segmentation from cardiac magnetic resonance images (MRI). Our approach combines the level set advantages that uses several a priori facts about the object to be segmented (e.g., smooth contour, strong edges, etc.) with the knowledge automatically learned from a manually annotated database (e.g., shape and appearance of the object to be segmented). The use of deep belief networks is justified because of its ability to learn robust models with few annotated images and its flexibility that allowed us to adapt it to a top-down segmentation problem. We demonstrate that our method produces competitive results using the database of the MICCAI grand challenge on left ventricle segmentation from cardiac MRI images, where our methodology produces results on par with the best in the field in each one of the measures used in that challenge (perpendicular distance, Dice metric, and percentage of good detections). Therefore, we conclude that our proposed methodology is one of the most competitive approaches in the field.","1522-4880;15224880","Electronic:978-1-4799-2341-0; POD:978-1-4799-2342-7; USB:978-1-4799-2340-3","10.1109/ICIP.2013.6738143","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738143","","Active contours;Databases;Image segmentation;Level set;Magnetic resonance imaging;Shape;Training","biomedical MRI;cardiology;image segmentation;medical image processing","cardiac MRI;cardiac magnetic resonance images;deep belief networks;left ventricle segmentation;level set methods;semi-automated methodology","","3","","24","","","","15-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Deep Learning method for classification of images RSVP events with EEG data","S. Ahmed; L. Mauricio Merino; Z. Mao; J. Meng; K. Robbins; Y. Huang","Dept. of Electr. & Comput. Eng., Univ. of Texas at San Antonio, San Antonio, TX, USA","2013 IEEE Global Conference on Signal and Information Processing","20140213","2013","","","33","36","In this paper, we investigated Deep Learning (DL) for characterizing and detecting target images in an image rapid serial visual presentation (RSVP) task based on EEG data. We exploited DL technique with input feature clusters to handle high dimensional features related to time - frequency events. The method was applied to EEG recordings of a RSVP experiment with multiple sessions and subjects. For classification of target and non-target images, a deep belief net (DBN) classifier was based on the uncorrelated features, which was constructed from original correlated features using clustering method. The performance of the proposed DBN was tested for different combinations of hidden units and hidden layers on multiple subjects. The results of DBN were compared with cluster Linear Discriminant Analysis (cLDA) and Support vector machine (SVM) and DBN demonstrated better performance in all tested cases. There was an improvement of 10 - 25% for certain cases. We also demonstrated how DBN is used to characterize brain activities.","","Electronic:978-1-4799-0248-4; POD:978-1-4799-0246-0","10.1109/GlobalSIP.2013.6736804","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6736804","DBN;Deep learning;RSVP;SVM;cLDA;feature clustering","Brain;Electroencephalography;Support vector machine classification;Time-frequency analysis;Training;Visualization","electroencephalography;image classification;learning (artificial intelligence);medical image processing","DBN classifier;EEG data;RSVP event;brain activity;clustering method;deep belief net classifier;deep learning method;feature cluster;high dimensional feature;image classification;rapid serial visual presentation;time-frequency event","","5","","6","","","","3-5 Dec. 2013","","IEEE","IEEE Conference Publications"
"Automation for individualization of Kinect-based quantitative progressive exercise regimen","S. k. Jun; S. Kumar; X. Zhou; D. K. Ramsey; V. N. Krovi","MAE Dept., SUNY Buffalo, Buffalo, NY, USA","2013 IEEE International Conference on Automation Science and Engineering (CASE)","20131107","2013","","","243","248","The Smart Health paradigm has opened up immense possibilities for designing cyber-physical systems with integrated sensing and analysis for data-driven healthcare decision-making. Clinical motor-rehabilitation has traditionally tended to entail labor-intensive approaches with limited quantitative methods and numerous logistics deployment challenges. We believe such labor-intensive rehabilitation procedures offer a fertile application field for robotics and automation technologies. We seek to concretize this Smart Health paradigm in the context of alleviating knee osteoarthritis (OA). Our long-term goal is the creation, analysis and validation of a low-cost cyber-physical framework for individualized but quantitative motor-rehabilitation. We seek build upon parameterized exercise-protocols, low-cost data-acquisition capabilities of the Kinect sensor and appropriate statistical data-processing to aid individualized-assessment and close the quantitative feedback-loop. Specifically, in this paper, we focus our attention on quantitative evaluation of a clinically-relevant deep-squatting exercise. Data for multiple trials with multiple of squatting motions were captured by Kinect system and examined to aid our individualization goals. Principal Component Analysis (PCA) approaches facilitated both dimension-reduction and filtering of the noisy-data while the K-Nearest Neighbors (K-NN) method was adapted for subject classification. Our preliminary deployment of this approach with 5 subjects achieved 95.6% classification accuracy.","2161-8070;21618070","Electronic:978-1-4799-1515-6; POD:978-1-4799-1513-2; USB:978-1-4799-1514-9","10.1109/CoASE.2013.6654038","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654038","Human identification;Kinect;Nearest neighbors;PCA","Accuracy;Hip;Joints;Knee;Medical treatment;Monitoring;Principal component analysis","image sensors;learning (artificial intelligence);medical computing;patient rehabilitation;pattern classification;principal component analysis","K-NN method;Kinect sensor;Kinect-based quantitative progressive exercise regimen;PCA;automation technologies;classification accuracy;clinical motor-rehabilitation;clinically-relevant deep-squatting exercise;cyber-physical systems design;data-driven health care decision-making;dimension reduction;k-nearest neighbor method;knee osteoarthritis;labor-intensive rehabilitation procedures;low-cost cyber-physical framework;noisy-data filtering;principal component analysis;quantitative feedback-loop;quantitative motor-rehabilitation;robotics;smart health paradigm;squatting motions;statistical data-processing;subject classification","","2","","25","","","","17-20 Aug. 2013","","IEEE","IEEE Conference Publications"
"Cancer patient blogs: How patients, clinicians, and researchers learn from rich narratives of illness","L. Gualtieri; F. Y. Akhtar","Sch. of Med., Dept. of Public Health & Community Med., Tufts Univ., Boston, MA, USA","Proceedings of the ITI 2013 35th International Conference on Information Technology Interfaces","20131031","2013","","","3","8","Blogs written by cancer patients can offer deep insights to other patients about what to expect in the course of illness and treatment, can provide information to oncologists and other clinicians about patient experiences outside of appointments, and can increase researcher awareness of treatment effects and alternatives. While many forms of social media and online communities are used by patients, blogs are unique in that they provide a narrative of many aspects of disease and treatment, offering a comprehensive view of the disease experience delivered in installments, often from diagnosis through life as a survivor. However, the impact of patient blogs has been modest thus far because patient blogs are spread across the internet and, with no central repository of patient blogs, opportunities for analysis are limited. Given the life-altering and potentially devastating impact of cancer on people's lives, we seek to develop a tool that will analyze tens of thousands of patient-authored blogs to improve cancer patient care.","1334-2762;13342762","Electronic:978-953-7138-32-5; POD:978-1-4799-0407-5","10.2498/iti.2013.0586","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648988","Blogs;clinicians;narratives;patients;reserachers;social media","Blogs;Cancer;Communities;Diseases;Internet;Media;Medical diagnostic imaging","Internet;Web sites;cancer;medical diagnostic computing;patient treatment","Internet;cancer patient blogs;cancer patient care;disease;online community;patient diagnosis;patient treatment;patient-authored blogs;social media;treatment effects","","0","","26","","","","24-27 June 2013","","IEEE","IEEE Conference Publications"
"Visual Feature Extraction From Voxel-Weighted Averaging of Stimulus Images in 2 fMRI Studies","C. B. Hart; W. J. Rose","Advanced Technology and Innovations , Lockheed Martin, King of Prussia, PA, USA","IEEE Transactions on Biomedical Engineering","20131016","2013","60","11","3124","3130","Multiple studies have provided evidence for distributed object representation in the brain, with several recent experiments leveraging basis function estimates for partial image reconstruction from fMRI data. Using a novel combination of statistical decomposition, generalized linear models, and stimulus averaging on previously examined image sets and Bayesian regression of recorded fMRI activity during presentation of these data sets, we identify a subset of relevant voxels that appear to code for covarying object features. Using a technique we term “voxel-weighted averaging,” we isolate image filters that these voxels appear to implement. The results, though very cursory, appear to have significant implications for hierarchical and deep-learning-type approaches toward the understanding of neural coding and representation.","0018-9294;00189294","","10.1109/TBME.2013.2268538","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531633","Bayesian estimation;component analysis;fMRI;generalized linear models;imaging;voxel","Bayes methods;Decoding;Feature extraction;Image reconstruction;Principal component analysis;Visualization","Bayes methods;biomedical MRI;brain;feature extraction;image coding;image reconstruction;image representation;medical image processing;neurophysiology;regression analysis;vision","Bayesian regression models;brain;deep-learning-type approaches;fMRI data;generalized linear models;hierarchical approaches;image filters;neural coding;partial image reconstruction;recorded fMRI activity;statistical decomposition;stimulus averaging;stimulus images;visual feature extraction;voxel-weighted averaging","Algorithms;Bayes Theorem;Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Photic Stimulation;Principal Component Analysis","0","","22","","","20130613","Nov. 2013","","IEEE","IEEE Journals & Magazines"
"Combining Multiple Dynamic Models and Deep Learning Architectures for Tracking the Left Ventricle Endocardium in Ultrasound Data","G. Carneiro; J. C. Nascimento","University of Adelaide, Adelaide","IEEE Transactions on Pattern Analysis and Machine Intelligence","20130917","2013","35","11","2592","2607","We present a new statistical pattern recognition approach for the problem of left ventricle endocardium tracking in ultrasound data. The problem is formulated as a sequential importance resampling algorithm such that the expected segmentation of the current time step is estimated based on the appearance, shape, and motion models that take into account all previous and current images and previous segmentation contours produced by the method. The new appearance and shape models decouple the affine and nonrigid segmentations of the left ventricle to reduce the running time complexity. The proposed motion model combines the systole and diastole motion patterns and an observation distribution built by a deep neural network. The functionality of our approach is evaluated using a dataset of diseased cases containing 16 sequences and another dataset of normal cases comprised of four sequences, where both sets present long axis views of the left ventricle. Using a training set comprised of diseased and healthy cases, we show that our approach produces more accurate results than current state-of-the-art endocardium tracking methods in two test sequences from healthy subjects. Using three test sequences containing different types of cardiopathies, we show that our method correlates well with interuser statistics produced by four cardiologists.","0162-8828;01628828","","10.1109/TPAMI.2013.96","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517436","Left ventricle segmentation;deep belief networks;discriminative classifiers;dynamical model;particle filters","Computational modeling;Image segmentation;Imaging;Motion segmentation;Shape;Tracking;Training","biomedical ultrasonics;brain;cardiology;computational complexity;image motion analysis;image recognition;medical image processing","appearance model;cardiopathies;deep learning architecture;diastole motion pattern;dynamic model;interuser statistics;left ventricle endocardium tracking;motion model;neural network;sequential importance resampling algorithm;shape model;statistical pattern recognition approach;systole motion pattern;time complexity reduction;ultrasound data","Algorithms;Artificial Intelligence;Computer Simulation;Echocardiography;Endocardium;Heart Ventricles;Humans;Image Interpretation, Computer-Assisted;Models, Cardiovascular;Pattern Recognition, Automated;Ventricular Dysfunction, Left","22","","76","","","20130520","Nov. 2013","","IEEE","IEEE Journals & Magazines"
"Deep learning for acupuncture point selection patterns based on veteran doctor experience of Chinese medicine","Z. Liang; G. Zhang; Z. Li; J. Yin; W. Fu","The 2nd Affiliated Hospital, Guangzhou University of Chinese Medicine, Guangzhou, China, 510120","2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops","20130225","2012","","","396","401","The inheritance of clinical experience of veteran doctors of Chinese medicine (CM) plays a key role in the development and effectiveness enhancement of Chinese medicine in the history. The clinical experience are classified as the patterns of disease diagnosis and Chinese medical Zheng diagnosis, the identification of core elements of Zheng, the treatment experience and relation of herbal medicine formulae, Zheng and disease, and the common law of diagnosis and treatment in real practice. The source of the experience mainly originates from literature and manuscripts of CM masters, which are being electronically recorded during the last two decades. As a result, it makes feasible to apply data mining to the knowledge discovery through the experience of veteran CM doctors. However, the current focus on this field is limited to the published literature such as journal papers, conference proceedings and textbooks, but the paper based manuscripts personally written by the veteran doctors are usually neglected. In this paper, we established a database for Dr Situ Ling, who is a deceased famous CM acupuncture master in southern China. The study objective is to discover the acupuncture point selection patterns which require profession knowledge and experience from senior CM doctors. It is believed these patterns are deposited as underlying knowledge with various middle level concepts that can be analyzed and discover by a serial of algorithms. Thus in this work, we formularized the patterns of acupuncture point selection as a learning task with deep architecture, which attempts to capture either existent or underlying concepts so as to simulate the planning process of the combined diagnosis of western medicine and Chinese medicine. The Restricted Boltzmann Machines (RBM) was used as the main model for deep learning to process to medical record data with international standard diagnosis (ICD-10) previously made by trained doctors. Then the ICD-10 based diagnosis dataset was- introduced into our framework to enhance the concepts diversity. After applying this model, the learning accuracy based on the medical record database of Dr Situ Ling was raised up to 75%. Thus this model can serve as a solution to discover the acupuncture point selection patterns of CM acupuncture veteran doctors. Furthermore, the data mining study model linked by international diagnosis standard (i.e. ICD-10), point selection patterns, and clinical symptoms will provide useful cues to reveal the essence of Zheng diagnosis through experience of CM veteran doctors.","","Electronic:978-1-4673-2747-3; POD:978-1-4673-2746-6","10.1109/BIBMW.2012.6470346","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470346","ICD-10;acupuncture point patterns;deep learning;knowledge discover;veteran doctor of Chinese medicine","Data models;Diseases;Medical diagnostic imaging;Pain;Standards;Training","Boltzmann machines;data mining;medical computing;medical information systems;patient diagnosis","CM acupuncture master;CM master literature;CM master manuscripts;Chinese medical Zheng diagnosis;Chinese medicine;ICD-10 based diagnosis dataset;RBM;Zheng core element identification;acupuncture point selection patterns;clinical symptoms;conference proceedings;data mining;disease diagnosis patterns;herbal medicine formulae;international standard diagnosis;journal papers;medical record database;planning process simulation;restricted Boltzmann machines;serial algorithm;southern China;textbooks;veteran CM doctor experience;western medicine","","1","","16","","","","4-7 Oct. 2012","","IEEE","IEEE Conference Publications"
"Computer vision-based breast self-examination stroke position and palpation pressure level classification using artificial neural networks and wavelet transforms","M. K. Cabatuan; E. P. Dadios; R. N. G. Naguib; A. Oikonomou","De La Salle University, Manila, Philippines","2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20121110","2012","","","6259","6262","This paper focuses on breast self-examination (BSE) stroke position and palpation level classification for the development of a computer vision-based BSE training and guidance system. In this study, image frames are extracted from a BSE video and processed considering the color information, shape, and texture by wavelet transform and first order color moment. The new approach using artificial neural network and wavelet transform can identify BSE stroke positions and palpation levels, i.e. light, medium, and deep, at 97.8 % and 87.5 % accuracy respectively.","1094-687X;1094687X","Electronic:978-1-4577-1787-1; POD:978-1-4244-4119-8; USB:978-1-4244-4120-4","10.1109/EMBC.2012.6347425","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6347425","","Artificial neural networks;Breast cancer;Image color analysis;Training;Wavelet transforms","brain;cancer;computer vision;diagnostic radiography;feature extraction;image classification;image texture;learning (artificial intelligence);mammography;medical image processing;neural nets;neurophysiology;wavelet transforms","artificial neural networks;computer vision-based BSE guidance system;computer vision-based BSE training system;computer vision-based breast self-examination stroke position;image extraction;image frames;image processing;palpation pressure level classification;wavelet transforms","Artificial Intelligence;Breast Self-Examination;Female;Humans;Neural Networks (Computer);Patient Education as Topic","4","","17","","","","Aug. 28 2012-Sept. 1 2012","","IEEE","IEEE Conference Publications"
"The use of on-line co-training to reduce the training set size in pattern recognition methods: Application to left ventricle segmentation in ultrasound","G. Carneiro; J. C. Nascimento","Australian Centre for Visual Technologies, The University of Adelaide, Australia","2012 IEEE Conference on Computer Vision and Pattern Recognition","20120726","2012","","","948","955","The use of statistical pattern recognition models to segment the left ventricle of the heart in ultrasound images has gained substantial attention over the last few years. The main obstacle for the wider exploration of this methodology lies in the need for large annotated training sets, which are used for the estimation of the statistical model parameters. In this paper, we present a new on-line co-training methodologythat reduces the need for large training sets for such parameter estimation. Our approach learns the initial parameters of two different models using a small manually annotated training set. Then, given each frame of a test sequence, the methodology not only produces the segmentation of the current frame, but it also uses the results of both classifiers to retrain each other incrementally. This on-line aspect of our approach has the advantages of producing segmentation results and retraining the classifiers on the fly as frames of a test sequence are presented, but it introduces a harder learning setting compared to the usual off-line co-training, where the algorithm has access to the whole set of un-annotated training samples from the beginning. Moreover, we introduce the use of the following new types of classifiers in the co-training framework: deep belief network and multiple model probabilistic data association. We show that our method leads to a fully automatic left ventricle segmentation system that achieves state-of-the-art accuracy on a public database with training sets containing at least twenty annotated images.","1063-6919;10636919","Electronic:978-1-4673-1228-8; POD:978-1-4673-1226-4; USB:978-1-4673-1227-1","10.1109/CVPR.2012.6247770","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6247770","","Data models;Image segmentation;Pattern recognition;Probabilistic logic;Training;Ultrasonic imaging;Vectors","image segmentation;medical image processing;pattern recognition","annotated image;annotated training set;automatic left ventricle segmentation system;belief network;multiple model probabilistic data association;offline co-training;online co-training methodology;parameter estimation;pattern recognition method;public database;statistical model parameter;statistical pattern recognition model;test sequence;training set size;training sets;ultrasound image;un-annotated training sample","","1","","22","","","","16-21 June 2012","","IEEE","IEEE Conference Publications"
"Deep learning architecture for data mining from surgical data","I. Nikolova","Technical University of Sofia, Bulgaria","2012 Proceedings of the 35th International Convention MIPRO","20120716","2012","","","998","1002","The paper addresses issues about data mining from surgical data. Surgical data has several distinguishing features - it is voluminous, heterogeneous, noise-prone, has low level of formalization due to lack of standardization in domain and highly correlated features. Architecture for data mining from surgical data, which deals with described features, is proposed. It is related to deep learning architectures as it consists of several hierarchical levels. Context-wise input layer modules process heterogeneous data. Results from these modules are calibrated before being passed to the next layer. At the last layer inference model for classification and prediction is derived.","","DVD:978-953-233-072-4; Electronic:978-953-233-068-7; POD:978-1-4673-2577-6","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240790","","Computer architecture;Data mining;Feature extraction;Medical diagnostic imaging;Noise;Surgery","data mining;inference mechanisms;learning (artificial intelligence);medical computing;medical information systems;pattern classification;surgery","classification;context-wise input layer module;data mining;deep learning architecture;heterogeneous data;highly correlated features;last layer inference model;noise-prone data;prediction;surgical data;voluminous data","","0","","14","","","","21-25 May 2012","","IEEE","IEEE Conference Publications"
"The Segmentation of the Left Ventricle of the Heart From Ultrasound Data Using Deep Learning Architectures and Derivative-Based Search Methods","G. Carneiro; J. C. Nascimento; A. Freitas","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, Australia","IEEE Transactions on Image Processing","20120216","2012","21","3","968","982","We present a new supervised learning model designed for the automatic segmentation of the left ventricle (LV) of the heart in ultrasound images. We address the following problems inherent to supervised learning models: 1) the need of a large set of training images; 2) robustness to imaging conditions not present in the training data; and 3) complex search process. The innovations of our approach reside in a formulation that decouples the rigid and nonrigid detections, deep learning methods that model the appearance of the LV, and efficient derivative-based search algorithms. The functionality of our approach is evaluated using a data set of diseased cases containing 400 annotated images (from 12 sequences) and another data set of normal cases comprising 80 annotated images (from two sequences), where both sets present long axis views of the LV. Using several error measures to compute the degree of similarity between the manual and automatic segmentations, we show that our method not only has high sensitivity and specificity but also presents variations with respect to a gold standard (computed from the manual annotations of two experts) within interuser variability on a subset of the diseased cases. We also compare the segmentations produced by our approach and by two state-of-the-art LV segmentation models on the data set of normal cases, and the results show that our approach produces segmentations that are comparable to these two approaches using only 20 training images and increasing the training set to 400 images causes our approach to be generally more accurate. Finally, we show that efficient search methods reduce up to tenfold the complexity of the method while still producing competitive segmentations. In the future, we plan to include a dynamical model to improve the performance of the algorithm, to use semisupervised learning methods to reduce even more the dependence on rich and large training sets, and to design a shape model less dependent on the trai- ing set.","1057-7149;10577149","","10.1109/TIP.2011.2169273","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026237","","Complexity theory;Image segmentation;Imaging;Robustness;Shape;Supervised learning;Training","biomedical ultrasonics;cardiology;image segmentation;image sequences;learning (artificial intelligence);medical image processing;search problems;ultrasonic imaging","automatic segmentation;complex search process;deep learning methods;derivative based search algorithm;diseased;heart;image annotation;left ventricle;semisupervised learning method;supervised learning model;training image;training set;ultrasound images","Algorithms;Artificial Intelligence;Heart Ventricles;Humans;Hypertrophy, Left Ventricular;Image Interpretation, Computer-Assisted;ROC Curve;Ventricular Dysfunction, Left","24","","59","","","20110923","March 2012","","IEEE","IEEE Journals & Magazines"
"Incremental on-line semi-supervised learning for segmenting the left ventricle of the heart from ultrasound data","G. Carneiro; J. C. Nascimento","Australian Centre for Visual Technologies, The University of Adelaide, Australia","2011 International Conference on Computer Vision","20120112","2011","","","1700","1707","Recently, there has been an increasing interest in the investigation of statistical pattern recognition models for the fully automatic segmentation of the left ventricle (LV) of the heart from ultrasound data. The main vulnerability of these models resides in the need of large manually annotated training sets for the parameter estimation procedure. The issue is that these training sets need to be annotated by clinicians, which makes this training set acquisition process quite expensive. Therefore, reducing the dependence on large training sets is important for a more extensive exploration of statistical models in the LV segmentation problem. In this paper, we present a novel incremental on-line semi-supervised learning model that reduces the need of large training sets for estimating the parameters of statistical models. Compared to other semi-supervised techniques, our method yields an on-line incremental re-training and segmentation instead of the off-line incremental re-training and segmentation more commonly found in the literature. Another innovation of our approach is that we use a statistical model based on deep learning architectures, which are easily adapted to this on-line incremental learning framework. We show that our fully automatic LV segmentation method achieves state-of-the-art accuracy with training sets containing less than twenty annotated images.","1550-5499;15505499","Electronic:978-1-4577-1102-2; POD:978-1-4577-1101-5","10.1109/ICCV.2011.6126433","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126433","","Image segmentation","biomedical ultrasonics;cardiology;image segmentation;learning (artificial intelligence);medical image processing;parameter estimation;statistical analysis","deep learning architectures;heart left ventricle segmentation;incremental online semisupervised learning model;large training sets;online incremental retraining;parameter estimation procedure;statistical pattern recognition models;ultrasound data","","3","","20","","","","6-13 Nov. 2011","","IEEE","IEEE Conference Publications"
"Modeling two-photon calcium fluorescence of episodic V1 recordings using multifrequency analysis","H. W. Zheng; W. Q. Malik; C. A. Runyan; M. Sur; E. N. Brown","Yale University, New Haven, CT 06520","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20111201","2011","","","3016","3019","The use of two-photon microscopy allows for imaging of deep neural tissue in vivo. This paper examines frequency-based analysis to two-photon calcium fluorescence images with the goal of deriving smooth tuning curves. We present a multifrequency analysis approach for improved extraction of calcium responses in episodic stimulation experiments, that is, when the stimulus is applied for a number of frames, then turned off for the next few frames, and so on. Episodic orientation stimulus was applied while recording from the primary visual cortex of an anesthetized mouse. The multifrequency model demonstrated improved tuning curve descriptions of the neurons. It also offers perspective regarding the characteristics of calcium fluorescence imaging of the brain.","1094-687X;1094687X","Electronic:978-1-4577-1589-1; POD:978-1-4244-4121-1; USB:978-1-4244-4122-8","10.1109/IEMBS.2011.6090826","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6090826","Two-photon microscopy;calcium imaging;multifrequency analysis;tuning curve;visual cortex","Brain modeling;Calcium;Fluorescence;Microscopy;Neurons;Tuning","biological tissues;biomedical optical imaging;brain;calcium;fluorescence;medical image processing;neurophysiology;optical microscopy;physiological models;two-photon spectra","anesthetized mouse;brain;deep neural tissue imaging;episodic V1 recordings;episodic orientation stimulus;frequency-based analysis;multifrequency analysis;neurons;primary visual cortex;smooth tuning curves;two-photon calcium fluorescence images;two-photon microscopy","Animals;Calcium;Fluorescence;Models, Theoretical;Photons","0","","9","","","","Aug. 30 2011-Sept. 3 2011","","IEEE","IEEE Conference Publications"
"Semi-supervised self-trainingmodel for the segmentationof the left ventricle of the heart from ultrasound data","G. Carneiro; J. Nascimento; A. Freitas","Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Av. Rovisco Pais, 1049-001 Lisbon, Portugal","2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro","20110609","2011","","","1295","1301","The design and use of statistical pattern recognition models can be regarded as one of the core research topics in the segmentation of the left ventricle of the heart from ultrasound data. These models trade a strong prior model of the shape and appearance of the left ventricle for a statistical model whose parameters can be learned using a manually segmented data set (this set is commonly known as the training set). The trouble is that such statistical model is usually quite complex, requiring a large number of parameters that can be robustly learned only if the training set is sufficiently large. The difficulty in obtaining large training sets is currently a major roadblock for the further exploration of statistical models in medical image analysis problems, such as the automatic left ventricle segmentation. In this paper, we present a novel semi-supervised self-training model that reduces the need of large training sets for estimating the parameters of statistical models. This model is initially trained with a small set of manually segmented images, and for each new test sequence, the system reestimates the model parameters incrementally without any further manual intervention. We show that state-of-the-art segmentation results can be achieved with training sets containing 50 annotated examples for the problem of left ventricle segmentation from ultrasound data.","1945-7928;19457928","Electronic:978-1-4244-4128-0; POD:978-1-4244-4127-3","10.1109/ISBI.2011.5872638","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872638","Segmentation of the left ventricle of the heart;deep neural networks;optimization algorithms;self-training;semi-supervised training","Data models;Heart;Image segmentation;Manuals;Shape;Training;Ultrasonic imaging","data analysis;echocardiography;image segmentation;image sequences;learning (artificial intelligence);medical image processing;parameter estimation;pattern recognition;statistical analysis","heart ultrasound dataset;image segmentation;image sequence;left ventricle;parameter estimation;semisupervised self-training model;statistical pattern recognition model","","0","","20","","","","March 30 2011-April 2 2011","","IEEE","IEEE Conference Publications"
"Towards a deep learning approach to brain parcellation","N. Lee; A. F. Laine; A. Klein","Columbia University, Department of Biomedical Engineering, 351 Engineering Terrace MC-8904, 1210 Amsterdam Avenue, New York, 10027, USA","2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro","20110609","2011","","","321","324","Establishing correspondences across structural and functional brain images via labeling, or parcellation, is an important and challenging task for clinical neuroscience and cognitive psychology. A limitation with existing approaches is that they i) possess shallow architectures, ii) are based on heuristic manual feature engineering, and iii) assume the validity of the designed feature model. In contrast, we advocate a deep learning approach to automate brain parcellation. We present a novel application of convolutional networks to build discriminative features for brain parcellation, which are automatically learned from labels provided by human experts. Initial validation experiments show promising results for automatic brain parcellation, suggesting that the proposed approach has potential to be an alternative to template or atlas-based parcellation approaches.","1945-7928;19457928","Electronic:978-1-4244-4128-0; POD:978-1-4244-4127-3","10.1109/ISBI.2011.5872414","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872414","Brain Parcellation;Convolutional Networks;Deep Learning;Feature Learning","Biological system modeling;Brain modeling;Computational modeling;Computer architecture;Humans;Training","brain;cognition;feature extraction;heuristic programming;medical expert systems;medical image processing;neurophysiology","atlas-based parcellation approaches;brain parcellation;clinical neuroscience;cognitive psychology;convolutional networks;deep learning approach;discriminative features;functional brain image;heuristic manual feature engineering;human experts;image labeling;structural brain image","","4","","5","","","","March 30 2011-April 2 2011","","IEEE","IEEE Conference Publications"
"Hemodynamic correlates of visuomotor motor adaptation by functional Near Infrared Spectroscopy","R. J. Gentili; C. Hadavi; H. Ayaz; P. A. Shewokis; J. L. Contreras-Vidal","Department of Kinesiology and Graduate Program in Neuroscience and Cognitive Science, University of Maryland, College Park, 20742 USA","2010 Annual International Conference of the IEEE Engineering in Medicine and Biology","20101111","2010","","","2918","2921","The development of rehabilitation engineering technologies such as the design of smart prosthetics necessitates a deep understanding of brain mechanisms engaged in ecological situations when human interact with new tools and/or environments. Thus, we aimed to investigate potential hemodynamic signatures reflecting the level of cognitive-motor performance and/or the internal or mental states of individuals when learning a novel tool with unknown properties. These markers were derived from functional Near Infrared Spectroscopy (fNIR) signals. Our results indicate an increased level of oxy-hemoglobin in prefrontal sensors associated with enhanced kinematics during early compared with late learning. This is consistent with previous neuroimaging studies that revealed a higher contribution of prefrontal areas during early compare to late adaptation learning. These non-invasive functional hemodynamic markers may play a role in bioengineering applications such as smart neuroprosthesis and brain monitoring where adaptive behavior is important.","1094-687X;1094687X","Electronic:978-1-4244-4124-2; POD:978-1-4244-4123-5","10.1109/IEMBS.2010.5626284","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5626284","","Biomedical engineering;Decoding;Educational institutions;Electroencephalography;Hemodynamics;Induction motors;Kinematics","bio-optics;biomechanics;brain;cognition;haemodynamics;infrared spectra;infrared spectroscopy;medical signal processing;molecular biophysics;optical sensors;proteins;visual perception","brain mechanisms;cognitive-motor performance;enhanced kinematics;functional near infrared spectroscopy;internal states;mental states;noninvasive functional hemodynamic markers;oxyhemoglobin;prefrontal sensors;rehabilitation engineering;visuomotor motor adaptation","Algorithms;Biomechanics;Brain Mapping;Cognition;Diagnostic Imaging;Hemodynamics;Humans;Models, Statistical;Monte Carlo Method;Motor Skills;Oxyhemoglobins;Signal Processing, Computer-Assisted;Spectroscopy, Near-Infrared;Vision, Ocular","1","","23","","","","Aug. 31 2010-Sept. 4 2010","","IEEE","IEEE Conference Publications"
"Application of Granger causality analysis to effective connectivity of the default-mode network","Xiaoyan Miao; Kewei Chen; Rui Li; Xiaotong Wen; Li Yao; Xia Wu","State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, 100875, China","IEEE/ICME International Conference on Complex Medical Engineering","20100826","2010","","","156","160","The default-mode network (DMN), which is suggested to have important functions related to internal modes of cognition and increasingly implicated in brain disorders, has attracted much attention in the past few years. Effective connectivity, defined as the influence one neuronal system exerts over another, can provide deep understanding of directed influence between brain regions in the network from the view of functional integration. Granger causality analysis is one of the conventional approaches to explore the effective connectivity in brain imaging researches. In this study, we applied Granger causality analysis to resting-state functional Magnetic Resonance Imaging (fMRI) data from 12 young subjects to explore the effective connectivity pattern of the DMN. The results demonstrated that posterior cingulate cortex (PCC), medial prefrontal cortex (MPFC) and inferior parietal cortex (IPC) were the only three regions had significant causal relationship with all other regions in more than 50% subjects and PCC was the only brain area influenced by all others while had no directed influence to others. The strong effective connectivity pattern demonstrated that PCC, MPFC and IPC were the three key regions and PCC was the convergence hub in the network. These results provide further understanding of physiological mechanism of DMN underlying internal modes of cognition.","","Electronic:978-1-4244-6843-0; POD:978-1-4244-6841-6","10.1109/ICCME.2010.5558851","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558851","","Algorithm design and analysis;Brain models;Laboratories;Large Hadron Collider;Magnetic analysis;Magnetic resonance imaging","biomedical MRI;brain;medical computing;neurophysiology","Granger causality analysis application;brain area;cognition internal modes;default-mode network;effective connectivity pattern;inferior parietal cortex;medial prefrontal cortex;physiological mechanism;posterior cingulate cortex;resting-state functional magnetic resonance imaging data","","0","","34","","","","13-15 July 2010","","IEEE","IEEE Conference Publications"
"Multiple dynamic models for tracking the left ventricle of the heart from ultrasound data using particle filters and deep learning architectures","G. Carneiro; J. C. Nascimento","Instituto de Sistemas e Rob&#x00F3;tica Instituto Superior T&#x00E9;cnico, Lisbon, Portugal","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","20100805","2010","","","2815","2822","The problem of automatic tracking and segmentation of the left ventricle (LV) of the heart from ultrasound images can be formulated with an algorithm that computes the expected segmentation value in the current time step given all previous and current observations using a filtering distribution. This filtering distribution depends on the observation and transition models, and since it is hard to compute the expected value using the whole parameter space of segmentations, one has to resort to Monte Carlo sampling techniques to compute the expected segmentation parameters. Generally, it is straightforward to compute probability values using the filtering distribution, but it is hard to sample from it, which indicates the need to use a proposal distribution to provide an easier sampling method. In order to be useful, this proposal distribution must be carefully designed to represent a reasonable approximation for the filtering distribution. In this paper, we introduce a new LV tracking and segmentation algorithm based on the method described above, where our contributions are focused on a new transition and observation models, and a new proposal distribution. Our tracking and segmentation algorithm achieves better overall results on a previously tested dataset used as a benchmark by the current state-of-the-art tracking algorithms of the left ventricle of the heart from ultrasound images.","1063-6919;10636919","Electronic:978-1-4244-6985-7; POD:978-1-4244-6984-0","10.1109/CVPR.2010.5540013","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540013","","Computer architecture;Distributed computing;Filtering algorithms;Heart;Image segmentation;Monte Carlo methods;Particle filters;Particle tracking;Proposals;Ultrasonic imaging","Monte Carlo methods;cardiology;medical image processing;particle filtering (numerical methods)","Monte Carlo sampling;deep learning architectures;filtering distribution;heart;left ventricle;multiple dynamic models;particle filters;probability;tracking;ultrasound data;ultrasound images","","15","1","21","","","","13-18 June 2010","","IEEE","IEEE Conference Publications"
"Applying deep-layered clustering to mammography image analytics","D. C. Rose; I. Arel; T. P. Karnowski; V. C. Paquit","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN 37996, USA","2010 Biomedical Sciences and Engineering Conference","20100715","2010","","","1","4","This paper details a methodology and preliminary results for applying a hierarchy of clustering units to mammographic image data. The identification of patients with breast cancer through the detection of microcalcifications and masses is a demanding classification problem; minimal false negatives are desired while simultaneously avoiding false positives that cause unnecessary cost to patients and health institutions. This research examines a segmented look at mammograms for computer aided detection with the goal of reliably labeling regions of interest requiring the attention of a radiologist. Classification is achieved by employing the building blocks, namely unsupervised clustering, of a deep learning architecture in tandem with a standard feed-forward neural network. Early results show promise for creating a classification engine that handles high-dimensional data with minimum engineering of image features, with a per-image patch sensitivity of 0.96 and specificity of 0.99.","","Electronic:978-1-4244-6714-3; POD:978-1-4244-6713-6","10.1109/BSEC.2010.5510827","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5510827","","Breast cancer;Cancer detection;Computer architecture;Computer network reliability;Costs;Feedforward systems;Image analysis;Image segmentation;Labeling;Mammography","biological organs;cancer;feature extraction;feedforward neural nets;image classification;image segmentation;mammography;medical image processing;pattern clustering;unsupervised learning","breast cancer;computer aided detection;deep-layered clustering;feed-forward neural network;image classification;image features;image segmentation;mammography;masses;microcalcifications;per-image patch sensitivity;specificity;unsupervised clustering","","1","1","10","","","","25-26 May 2010","","IEEE","IEEE Conference Publications"
"Multiscale 3-D Shape Representation and Segmentation Using Spherical Wavelets","D. Nain; S. Haker; A. Bobick; A. Tannenbaum","Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA","IEEE Transactions on Medical Imaging","20070402","2007","26","4","598","618","This paper presents a novel multiscale shape representation and segmentation algorithm based on the spherical wavelet transform. This work is motivated by the need to compactly and accurately encode variations at multiple scales in the shape representation in order to drive the segmentation and shape analysis of deep brain structures, such as the caudate nucleus or the hippocampus. Our proposed shape representation can be optimized to compactly encode shape variations in a population at the needed scale and spatial locations, enabling the construction of more descriptive, nonglobal, nonuniform shape probability priors to be included in the segmentation and shape analysis framework. In particular, this representation addresses the shortcomings of techniques that learn a global shape prior at a single scale of analysis and cannot represent fine, local variations in a population of shapes in the presence of a limited dataset. Specifically, our technique defines a multiscale parametric model of surfaces belonging to the same population using a compact set of spherical wavelets targeted to that population. We further refine the shape representation by separating into groups wavelet coefficients that describe independent global and/or local biological variations in the population, using spectral graph partitioning. We then learn a prior probability distribution induced over each group to explicitly encode these variations at different scales and spatial locations. Based on this representation, we derive a parametric active surface evolution using the multiscale prior coefficients as parameters for our optimization procedure to naturally include the prior for segmentation. Additionally, the optimization method can be applied in a coarse-to-fine manner. We apply our algorithm to two different brain structures, the caudate nucleus and the hippocampus, of interest in the study of schizophrenia. We show: 1) a reconstruction task of a test set to validate the expressiveness of - - our multiscale prior and 2) a segmentation task. In the reconstruction task, our results show that for a given training set size, our algorithm significantly improves the approximation of shapes in a testing set over the Point Distribution Model, which tends to oversmooth data. In the segmentation task, our validation shows our algorithm is computationally efficient and outperforms the Active Shape Model algorithm, by capturing finer shape details","0278-0062;02780062","","10.1109/TMI.2007.893284","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141198","Brain structures;schizophrenia;segmentation;shape representation;wavelets","Active shape model;Biological information theory;Brain;Hippocampus;Parametric statistics;Probability distribution;Surface waves;Testing;Wavelet coefficients;Wavelet transforms","biomedical MRI;brain;image coding;image reconstruction;image representation;medical image processing;optimisation;probability;wavelet transforms","MRI;caudate nucleus;deep brain structures;hippocampus;image reconstruction;image segmentation;multiscale 3-D shape representation;multiscale parametric model;multiscale prior coefficients;optimization;parametric active surface evolution;point distribution model;probability distribution;schizophrenia;shape analysis;shape probability;shape variation encoding;spectral graph partitioning;spherical wavelet transform;wavelet coefficients","Algorithms;Artificial Intelligence;Brain;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity","37","","50","","","","April 2007","","IEEE","IEEE Journals & Magazines"
"Automatic target localization using microelectrode recordings","R. A. Santiago; J. McNames; H. Falkenberg; K. Burchiel","Northwest Computational Intelligence Lab., Portland State Univ., OR, USA","Proceedings of the Second Joint 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society] [Engineering in Medicine and Biology","20030106","2002","1","","42","43 vol.1","We describe an algorithm that objectively and automatically identifies target regions in the brain for ablation or stimulation during neurosurgery for Parkinson's disease and other movement disorders. The algorithm uses microelectrode recordings to distinguish between the target and adjacent anatomic structures during stereotactic neurosurgery. This algorithm uses a novel method of signal feature extraction that enables standard classification algorithms such as support vector machines to perform well. The algorithm was validated on microelectrode recordings acquired near the globus pallidus internus and labeled by the neurosurgeon.","1094-687X;1094687X","POD:0-7803-7612-9","10.1109/IEMBS.2002.1134378","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1134378","","Laboratories;Magnetic resonance imaging;Microelectrodes;Neurons;Neurosurgery;Parkinson's disease;Signal processing algorithms;Support vector machine classification;Support vector machines;Surgery","biomedical electrodes;diseases;feature extraction;learning automata;medical signal processing;microelectrodes;surgery","Parkinson's disease;adjacent anatomic structures;algorithm validation;deep brain stimulation;globus pallidus internus;microelectrode recording;movement disorders;neurosurgeon;signal feature extraction;spike source identification;standard classification algorithms;stereotactic neurosurgery;support vector machines","","0","","6","","","","2002","","IEEE","IEEE Conference Publications"
"The computer tool of the mitral valve prolapse determination based on automatic learning","J. Zavrsnik; K. Kancler; P. Kokol; M. Mernik; I. Malcic","House of Health, Maribor, Slovenia","Proceedings of IEEE Symposium on Computer-Based Medical Systems (CBMS)","20020806","1994","","","170","175","Prolapse is defined as the displacement of a bodily part from its normal position. The term mitral valve prolapse (PMV), therefore, implies that the mitral leaflets are displaced relative to some structure, generally taken to be the mitral annulus. The implications of the PMV are following: disturbed normal laminar blood flow, turbulence of the blood flow, injury of the chordae tendinae, the possibility of thrombus's composition, bacterial endocarditis and finally hemodynamic changes defined as mitral insufficiency and mitral regurgitation. Uncertainty persists about how it should be diagnosed and about its clinical importance. It is our deep belief that the echocardiography enables proper trained experts armed with proper criteria to evaluate PMV almost 100%. Unfortunately there are some problems concerned with the use of echocardiography. We have decided to start a research project aimed to find new criteria and enable general practitioners to evaluate the PMV using conventional methods and to select potential patients from the general population. To empower one to perform needed activities we developed a computer tool called ROSE (Computerised Prolaps Syndrome Determination) based on algorithms of automatic learning. This tool supports the definition of new criteria and the selection of potential PMV-patients","","POD:0-8186-6256-5","10.1109/CBMS.1994.316006","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=316006","","Blood flow;Connective tissue;Echocardiography;Hemodynamics;Injuries;Lesions;Medical diagnostic imaging;Microorganisms;Uncertainty;Valves","biomedical ultrasonics;cardiology;haemodynamics;learning (artificial intelligence);medical diagnostic computing;medical expert systems","Computerised Prolaps Syndrome Determination;ROSE;automatic learning;bacterial endocarditis;blood flow turbulence;chordae tendinae;disturbed normal laminar blood flow;echocardiography;hemodynamic change;mitral annulus;mitral insufficiency;mitral regurgitation;mitral valve prolapse;patient selection;thrombus composition","","0","","7","","","","10-12 Jun 1994","10 Jun 1994-12 Jun 1994","IEEE","IEEE Conference Publications"
"Detecting activity from deep brain areas with magnetoencephalographic arrays","C. D. Tesche","Low Temp. Lab., Helsinki Univ. of Technol., Espoo, Finland","Proceedings of the 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. Vol.20 Biomedical Engineering Towards the Year 2000 and Beyond (Cat. No.98CH36286)","20020806","1998","4","","2201","2204 vol.4","Magnetoencephalographic (MEG) sensors detect with millisecond resolution current flow in the brain which is generated by tens of thousands of simultaneously active neurons. Arrays of sensors covering the entire scalp are now used to capture both temporal and topographic features of activity generated in multiple brain areas. The temporal information available with MEG compliments the excellent spatial resolution obtainable with other brain imaging technologies such as PET and fMRI. The majority of recent MEG studies have concentrated on the characterization of activity generated in relatively superficial fissural cortex. These brain areas are crucial for the initial processing of information from the senses and for the generation of movement. However, recent developments in MEG signal analysis now make possible the identification of responses from deep brain areas, such as hippocampus, thalamus and cerebellum. Many of these structures participate in a variety of higher cognitive functions, including learning and memory. The rational for and application of these new methods is demonstrated with whole-scalp MEG data recorded during the performance of a short-term memory task and tasks which interrogates the brain's ability to detect changes in the temporal pattern of stimulus presentation","1094-687X;1094687X","POD:0-7803-5164-9","10.1109/IEMBS.1998.747048","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=747048","","Brain;Character generation;Magnetic sensors;Neurons;Positron emission tomography;Scalp;Sensor arrays;Sensor phenomena and characterization;Signal analysis;Spatial resolution","arrays;biomedical equipment;magnetoencephalography;medical signal detection","brain current flow;cerebellum;deep brain areas activity detection;functional brain imaging;higher cognitive functions;hippocampus;initial information processing;learning;magnetoencephalographic arrays;memory;millisecond resolution;multiple brain areas;scalp;signal-space projection;spatial resolution;stimulus presentation temporal pattern;superficial fissural cortex;thalamus;topographic features","","1","","11","","","","29 Oct-1 Nov 1998","29 Oct 1998-01 Nov 1998","IEEE","IEEE Conference Publications"
"[Front cover]","","","2016 International Conference on Computer, Control, Informatics and its Applications (IC3INA)","20170228","2016","","","c1","c1","The following topics are dealt with: Wi-Fi based temperature monitoring system; Kalman filter implementation; multiple robots visual SLAM; speed detection; image processing; vehicle classification; lane categorization; Stratix V DE5-Net FPGA board; high performance computing; discrete-time model-based controller; Bayesian Twitter-based prediction;timing estimation; normalized 4-th order moment; OFDM-based cognitive radio systems; software size measurement; knowledge management portal; scene text detection; IEEE 802.11n; IEEE 802.11ac; distributed order-up-to inventory management; uncertain demand-system modelling; predictor-based dynamic soft VSC; time-delay systems; magnitude-constrained input signal; heart rate prediction; cycling cadence; feedforward neural network; computer vision; autonomous UAV; spatial co-location pattern discovery; multiple neighborhood relationship function; Kansei based interface design analysis; open source e-learning system; high education; asset management system functionality; bitcoin platform; multi-label classification; deep belief networks; virtual screening; multi-target drug; cancer subtype identification; deep learning approach; fault-tolerant control; nonlinear systems; projection optimization; compressive sensing framework; industrial control system security-malware botnet detection; AUV high-precision path following control system; PD-controller; an- maly detection; computational optimization; violent scenes detection; educational institution DNS network traffic; circle detection; Hough transform; Mexican hat filter; multilink manipulators; super-symmetric particle classication; noise labelling; role-based programming; adaptive IoT applications; smart dog feeder design; DSS01 COBIT5; part-of-speech tagging; Bahasa Indonesia; 2D spatial interpolation; water quality parameter distribution; XQuery evaluation; SLIM+; advanced-simple lightweight and intuitive multicast protocol and MANET.","","Electronic:978-1-5090-2323-3; POD:978-1-5090-2324-0; USB:978-1-5090-2322-6","10.1109/IC3INA.2016.7862986","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862986","","","Hough transforms;Internet of Things;Kalman filters;OFDM modulation;PD control;SLAM (robots);autonomous aerial vehicles;autonomous underwater vehicles;belief networks;cognitive radio;compressed sensing;computer aided instruction;computer vision;delays;discrete time systems;fault tolerant control;field programmable gate arrays;filtering theory;further education;image classification;information retrieval;interpolation;invasive software;inventory management;knowledge management;learning (artificial intelligence);manipulators;medical signal processing;mobile ad hoc networks;multi-robot systems;natural language processing;nonlinear control systems;optimisation;parallel processing;portals;position control;recurrent neural nets;routing protocols;shape recognition;social networking (online);temperature;text detection;water quality;wireless LAN","2D spatial interpolation;AUV high-precision path following control system;Bahasa Indonesia;Bayesian Twitter-based prediction;DSS01 COBIT5;Hough transform;IEEE 802.11ac;IEEE 802.11n;Kalman filter implementation;Kansei based interface design analysis;MANET;Mexican hat filter;OFDM-based cognitive radio systems;PD-controller;SLIM+;Stratix V DE5-Net FPGA board;Wi-Fi based temperature monitoring system;XQuery evaluation;adaptive IoT applications;advanced-simple lightweight and intuitive multicast protocol;anomaly detection;asset management system functionality;autonomous UAV;bitcoin platform;cancer subtype identification;circle detection;compressive sensing framework;computational optimization;computer vision;cycling cadence;deep belief networks;deep learning approach;discrete-time model-based controller;distributed order-up-to inventory management;educational institution DNS network traffic;fault-tolerant control;feedforward neural network;heart rate prediction;high education;high performance computing;image processing;industrial control system security-malware botnet detection;knowledge management portal;lane categorization;magnitude-constrained input signal;multi-label classification;multilink manipulators;multiple neighborhood relationship function;multiple robots visual SLAM;noise labelling;nonlinear systems;normalized 4-th order moment;open source e-learning system;part-of-speech tagging;predictor-based dynamic soft VSC;projection optimization;role-based programming;scene text detection;smart dog feeder design;software size measurement;spatial co-location pattern discovery;speed detection;super-symmetric particle classication;time-delay systems;timing estimation;uncertain demand-system modelling;vehicle classification;violent scenes detection;virtual screening;water quality parameter distribution","","","","","","","","3-5 Oct. 2016","","IEEE","IEEE Conference Publications"
"[Title page i]","","","2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","20170116","2016","","","i","i","The following topics are dealt with: animation; simulation; biomedical imaging; biomedical visualization; computational geometry; computer vision; robotics; deep learning; feature extraction; feature matching; solid modeling; geometric modeling; graphics hardware; video analysis; image analysis; video registration; image registration; video retrieval; image retrieval; video segmentation; image segmentation; information visualization; mathematical morphology; pattern recognition; remote sensing; geophysical imaging; rendering; shape representation; shape matching; and video surveillance.","","Electronic:978-1-5090-3568-7; POD:978-1-5090-3569-4","10.1109/SIBGRAPI.2016.001","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813001","","","computer graphics;geophysics computing;image processing;learning (artificial intelligence);mathematics;medical computing;pattern recognition;remote sensing;robots;video surveillance","animation;biomedical imaging;biomedical visualization;computational geometry;computer vision;deep learning;feature extraction;feature matching;geometric modeling;geophysical imaging;graphics hardware;image analysis;image registration;image retrieval;image segmentation;information visualization;mathematical morphology;pattern recognition;remote sensing;rendering;robotics;shape matching;shape representation;simulation;solid modeling;video analysis;video registration;video retrieval;video segmentation;video surveillance","","","","","","","","4-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"[Title page i]","","","2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","20161219","2016","","","i","i","The following topics are dealt with: computer vision; vehicle technology; biometrics; egocentric vision; DeepVision; deep learning; biomedical image registration; large scale 3D data; satellite visual analysis; street imagery; face analysis workshop; embedded vision; computational cameras; differential geometry; machine learning; tracking performance evaluation; microscopy image analysis; moving cameras; video surveillance; body-borne cameras; context-based affect recognition; affective face in-the-wild and automatic traffic surveillance.","","Electronic:978-1-5090-1437-8; POD:978-1-5090-1438-5","10.1109/CVPRW.2016.1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789491","","","cameras;computer vision;differential geometry;face recognition;geophysical image processing;image registration;learning (artificial intelligence);medical image processing;object tracking;traffic engineering computing;vehicles;video surveillance","DeepVision;affective face in-the-wild;automatic traffic surveillance;biomedical image registration;biometrics;body-borne cameras;computational cameras;computer vision;context-based affect recognition;deep learning;differential geometry;egocentric vision;embedded vision;face analysis workshop;large scale 3D data;machine learning;microscopy image analysis;moving cameras;satellite visual analysis;street imagery;tracking performance evaluation;vehicle technology;video surveillance","","","","","","","","June 26 2016-July 1 2016","","IEEE","IEEE Conference Publications"
"[Copyright notice]","","","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1","1","The following topics are dealt with: brain CAD; cardiac imaging; fMRI analysis; image reconstruction; interventional imaging; microscopy imaging and reconstruction; musculo-skeletal imaging; retinal imaging; segmentation methods for microscopy images; modeling and simulation; brain segmentation; motion tracking; neuron image analysis; optical imaging; segmentation and quantification of biological images; tissue quantification; ultrasound; visualization; fast MR acquisition and reconstruction; structural brain connectivity; CT reconstruction; image analysis of neurons; perspectives on deep learning for biomedical and biological imaging and image analysis; shape analysis; imaging cellular processes; breast imaging; EEG; foetal imaging; histological image analysis; imaging genetics; and machine learning.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493196","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493196","","","biomedical MRI;biomedical optical imaging;biomedical ultrasonics;brain;cardiology;cellular biophysics;computerised tomography;eye;genetics;image reconstruction;image segmentation;learning (artificial intelligence);mammography;medical image processing;neurophysiology;optical microscopy","CT reconstruction;EEG;biological images;brain CAD;brain segmentation;breast imaging;cardiac imaging;cellular processes;deep learning;fMRI analysis;fast MR acquisition;fetal imaging;histology image analysis;image reconstruction;imaging genetics;interventional imaging;machine learning;microscopy imaging;microscopy reconstruction;motion tracking;musculo-skeletal imaging;neuron image analysis;optical imaging;retinal imaging;segmentation methods;shape analysis;structural brain connectivity;tissue quantification;ultrasound;visualization","","","","","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"[Front cover]","","","2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES)","20151123","2015","","","c1","c1","The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.","","Electronic:978-1-4673-7939-7; POD:978-1-4673-7940-3; USB:978-1-4673-7938-0","10.1109/INES.2015.7329762","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329762","","","Internet;MIMO systems;augmented reality;collaborative filtering;content management;data mining;decision support systems;discrete time systems;fuzzy control;fuzzy reasoning;genetic algorithms;heat exchangers;human computer interaction;humidity control;image coding;image reconstruction;learning (artificial intelligence);medical robotics;mobile robots;neural nets;nonlinear control systems;predictive control;public domain software;recommender systems;stability;surgery;temperature control;vehicular ad hoc networks;water supply;wireless sensor networks","Androids;DCI approach;IP Core;MIMO system;VANET;Web;artificial neural network;augmented reality;authorized V2V communication;collaborative filtering;content management system;data mining technique;decision support system;deep neural network architecture;discrete-time SISO system;fuzzy controller design;fuzzy reasoning;genetic algorithm;heat exchanger;human computer interaction;human machine interface design;humidity control;image coding;image reconstruction;learning;linked open data source;mobile device;mobile robot;nonlinear MPC;ontologies;process mining;real-time sensor network;recommendation;social human-robot interaction;stability;surgical robotics;temperature control;urban water-supply system;visual analysis;wireless sensor networks","","","","","","","","3-5 Sept. 2015","","IEEE","IEEE Conference Publications"
"Special section on deep learning in medical applications","","","IEEE Transactions on Medical Imaging","20150828","2015","34","9","1990","1990","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0278-0062;02780062","","10.1109/TMI.2015.2474097","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229396","","","","","","0","","","","","","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"[Copyright notice]","","","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","20150820","2015","","","1","1","The following topics are dealt with: Big Data; deep learning; space robotics; computer-aided patient evaluation; human-robot interaction; cryptography; inverted pendulum control; bidirectional flyback inverter; Web-based real-time collaboration; mobile robots; wireless sensor network; content-based image retrieval; PID controller; robot arms; PD control; digital signal processing; fuzzy automata; adaptive control; multiview computer vision; university education; cloud computing; mobile network; business process similarity; social network; distribution network; PageRank-based recommender system; intrusion detection; surgical robots; and robot car model.","","Electronic:978-1-4799-9911-8; POD:978-1-4799-9912-5; USB:978-1-4799-9910-1","10.1109/SACI.2015.7208269","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208269","","","Big Data;PD control;adaptive control;aerospace robotics;automata theory;business data processing;cloud computing;computer vision;content-based retrieval;cryptography;distribution networks;engineering education;human-robot interaction;image retrieval;invertors;learning (artificial intelligence);manipulators;medical computing;medical robotics;mobile radio;mobile robots;nonlinear control systems;recommender systems;security of data;signal processing;social networking (online);three-term control;wireless sensor networks","Big Data;PD control;PID controller;PageRank-based recommender system;Web-based real-time collaboration;adaptive control;bidirectional flyback inverter;business process similarity;cloud computing;computer-aided patient evaluation;content-based image retrieval;cryptography;deep learning;digital signal processing;distribution network;fuzzy automata;human-robot interaction;intrusion detection;inverted pendulum control;mobile network;mobile robots;multiview computer vision;robot arms;robot car model;social network;space robotics;surgical robots;university education;wireless sensor network","","0","","","","","","21-23 May 2015","","IEEE","IEEE Conference Publications"
"Special section on deep learning in medical applications","","","IEEE Transactions on Medical Imaging","20150729","2015","34","8","1769","1769","Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.","0278-0062;02780062","","10.1109/TMI.2015.2460431","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172577","","","","","","0","","","","","","Aug. 2015","","IEEE","IEEE Journals & Magazines"
