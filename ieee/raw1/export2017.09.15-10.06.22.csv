"http://ieeexplore.ieee.org/search/searchresult.jsp?bulkSetSize=2000&rowsPerPage%3D75%26queryText%3Dultrasound+deep+learning",2017/09/15 10:06:22
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Classification of thyroid nodules in ultrasound images using deep model based transfer learning and hybrid features","T. Liu; S. Xie; J. Yu; L. Niu; W. Sun","Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","919","923","Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful semantic features to the classification. Firstly, a CNN model trained with a massive natural dataset is transferred to the ultrasound image domain, to generate semantic deep features and handle the small sample problem. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Local Binary Patterns (LBP) together, to form a hybrid feature space. Finally, a positive-sample-first majority voting and a feature-selected based strategy are employed for the hybrid classification. Experimental results on 1037 images show that the accuracy of our proposed method is 0.931, which outperformed other relative methods by over 10%.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952290","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952290","classification;deep learning;feature fusion;transfer learning;ultrasound image","Biomedical imaging;Cancer;Feature extraction;Indexes;Machine learning;Semantics;Ultrasonic imaging","biomedical ultrasonics;feature extraction;feedforward neural nets;image classification;learning (artificial intelligence);medical image processing;ultrasonic imaging","CNN model trained;HOG;LBP;benign nodules;convolution neural networks;deep model based transfer learning;feature extraction method;feature-selected based strategy;histogram of oriented gradient;hybrid feature space;local binary patterns;malignant nodules;positive-sample-first majority voting;semantic deep features;thyroid nodule classification;thyroid nodules;ultrasonography;ultrasound images","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Automatic 3D ultrasound segmentation of the first trimester placenta using deep learning","P. Looney; G. N. Stevenson; K. H. Nicolaides; W. Plasencia; M. Molloholli; S. Natsis; S. L. Collins","Nuffield Department of Obstetrics and Gynaecology, University of Oxford, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","279","282","Placental volume measured with 3D ultrasound in the first trimester has been shown to be correlated to adverse pregnancy outcomes. This could potentially be used as a screening test to predict the “at risk” pregnancy. However, manual segmentation whilst previously shown to be accurate and repeatable is very time consuming and semi-automated methods still require operator input. To generate a screening tool, fully automated placental segmentation is required. In this work, a deep convolutional neural network (cNN), DeepMedic, was trained using the output of the semi-automated Random Walker method as ground truth. 300 3D ultrasound scans of first trimester placentas were used to train, validate and test the cNN. Compared against the semi-automated segmentation, resultant median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Dice Similarity Coefficient was 0.73 (0.66, 0.76). The median (1<sup>st</sup> Quartile, 3<sup>rd</sup> Quartile) Hausdorff distance was 27 mm (18 mm, 36 mm). We present the first attempt at using a deep cNN for segmentation of 3D ultrasound of the placenta. This work shows that feasible results compared to ground truth were obtained that could form the basis of a fully automatic segmentation method.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950519","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950519","3D;automatic segmentation;deep learning;neural network;placenta;random walker;ultrasound","Biological neural networks;Image segmentation;Magnetic resonance imaging;Pregnancy;Three-dimensional displays;Ultrasonic imaging","biomedical ultrasonics;image segmentation;learning (artificial intelligence);medical image processing;neural nets;obstetrics","DeepMedic;automatic 3D ultrasound segmentation;deep convolutional neural network;deep learning;first trimester placenta","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Hybrid approach for automatic segmentation of fetal abdomen from ultrasound images using deep learning","H. Ravishankar; S. M. Prabhu; V. Vaidya; N. Singhal","GE Global Research, Bangalore, India","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","779","782","In this paper, we propose a hybrid approach combining traditional texture analysis methods with deep learning for the automatic detection and measurement of abdominal contour from 2-D fetal ultrasound images. Following a learning-based procedure for region of interest (ROI) localization to segment the abdominal boundary, we show that convolutional neural networks (CNNs) outperform other state-of-the-art texture features and conventional classifiers, in addressing the binary classification problem of distinguishing between abdomen versus non-abdomen regions. However, we obtain significantly better segmentation results in identifying the best ROI containing fetal abdomen, when the predictions from CNN are combined with those from gradient boosting machine (GBM) using histogram of oriented gradient (HOG) features. We trained our method on a set of 70 images and tested them on another distinct set of 70 images. We obtained a mean DICE similarity coefficient of 0.90, which shows excellent overlap with the ground truth. We report that the mean computed gestational age difference between our segmentation results and the ground truth, is within two weeks for 90% (and within one week for 70%) of the testing cases.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493382","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493382","","Abdomen;Feature extraction;Image segmentation;Machine learning;Training;Ultrasonic imaging;Ultrasonic variables measurement","","","","","","14","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Combining Multiple Dynamic Models and Deep Learning Architectures for Tracking the Left Ventricle Endocardium in Ultrasound Data","G. Carneiro; J. C. Nascimento","University of Adelaide, Adelaide","IEEE Transactions on Pattern Analysis and Machine Intelligence","20130917","2013","35","11","2592","2607","We present a new statistical pattern recognition approach for the problem of left ventricle endocardium tracking in ultrasound data. The problem is formulated as a sequential importance resampling algorithm such that the expected segmentation of the current time step is estimated based on the appearance, shape, and motion models that take into account all previous and current images and previous segmentation contours produced by the method. The new appearance and shape models decouple the affine and nonrigid segmentations of the left ventricle to reduce the running time complexity. The proposed motion model combines the systole and diastole motion patterns and an observation distribution built by a deep neural network. The functionality of our approach is evaluated using a dataset of diseased cases containing 16 sequences and another dataset of normal cases comprised of four sequences, where both sets present long axis views of the left ventricle. Using a training set comprised of diseased and healthy cases, we show that our approach produces more accurate results than current state-of-the-art endocardium tracking methods in two test sequences from healthy subjects. Using three test sequences containing different types of cardiopathies, we show that our method correlates well with interuser statistics produced by four cardiologists.","0162-8828;01628828","","10.1109/TPAMI.2013.96","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517436","Left ventricle segmentation;deep belief networks;discriminative classifiers;dynamical model;particle filters","Computational modeling;Image segmentation;Imaging;Motion segmentation;Shape;Tracking;Training","biomedical ultrasonics;brain;cardiology;computational complexity;image motion analysis;image recognition;medical image processing","appearance model;cardiopathies;deep learning architecture;diastole motion pattern;dynamic model;interuser statistics;left ventricle endocardium tracking;motion model;neural network;sequential importance resampling algorithm;shape model;statistical pattern recognition approach;systole motion pattern;time complexity reduction;ultrasound data","Algorithms;Artificial Intelligence;Computer Simulation;Echocardiography;Endocardium;Heart Ventricles;Humans;Image Interpretation, Computer-Assisted;Models, Cardiovascular;Pattern Recognition, Automated;Ventricular Dysfunction, Left","22","","76","","","20130520","Nov. 2013","","IEEE","IEEE Journals & Magazines"
"The Segmentation of the Left Ventricle of the Heart From Ultrasound Data Using Deep Learning Architectures and Derivative-Based Search Methods","G. Carneiro; J. C. Nascimento; A. Freitas","Australian Centre for Visual Technologies, University of Adelaide, Adelaide, Australia","IEEE Transactions on Image Processing","20120216","2012","21","3","968","982","We present a new supervised learning model designed for the automatic segmentation of the left ventricle (LV) of the heart in ultrasound images. We address the following problems inherent to supervised learning models: 1) the need of a large set of training images; 2) robustness to imaging conditions not present in the training data; and 3) complex search process. The innovations of our approach reside in a formulation that decouples the rigid and nonrigid detections, deep learning methods that model the appearance of the LV, and efficient derivative-based search algorithms. The functionality of our approach is evaluated using a data set of diseased cases containing 400 annotated images (from 12 sequences) and another data set of normal cases comprising 80 annotated images (from two sequences), where both sets present long axis views of the LV. Using several error measures to compute the degree of similarity between the manual and automatic segmentations, we show that our method not only has high sensitivity and specificity but also presents variations with respect to a gold standard (computed from the manual annotations of two experts) within interuser variability on a subset of the diseased cases. We also compare the segmentations produced by our approach and by two state-of-the-art LV segmentation models on the data set of normal cases, and the results show that our approach produces segmentations that are comparable to these two approaches using only 20 training images and increasing the training set to 400 images causes our approach to be generally more accurate. Finally, we show that efficient search methods reduce up to tenfold the complexity of the method while still producing competitive segmentations. In the future, we plan to include a dynamical model to improve the performance of the algorithm, to use semisupervised learning methods to reduce even more the dependence on rich and large training sets, and to design a shape model less dependent on the trai- ing set.","1057-7149;10577149","","10.1109/TIP.2011.2169273","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026237","","Complexity theory;Image segmentation;Imaging;Robustness;Shape;Supervised learning;Training","biomedical ultrasonics;cardiology;image segmentation;image sequences;learning (artificial intelligence);medical image processing;search problems;ultrasonic imaging","automatic segmentation;complex search process;deep learning methods;derivative based search algorithm;diseased;heart;image annotation;left ventricle;semisupervised learning method;supervised learning model;training image;training set;ultrasound images","Algorithms;Artificial Intelligence;Heart Ventricles;Humans;Hypertrophy, Left Ventricular;Image Interpretation, Computer-Assisted;ROC Curve;Ventricular Dysfunction, Left","24","","59","","","20110923","March 2012","","IEEE","IEEE Journals & Magazines"
"Multiple dynamic models for tracking the left ventricle of the heart from ultrasound data using particle filters and deep learning architectures","G. Carneiro; J. C. Nascimento","Instituto de Sistemas e Rob&#x00F3;tica Instituto Superior T&#x00E9;cnico, Lisbon, Portugal","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","20100805","2010","","","2815","2822","The problem of automatic tracking and segmentation of the left ventricle (LV) of the heart from ultrasound images can be formulated with an algorithm that computes the expected segmentation value in the current time step given all previous and current observations using a filtering distribution. This filtering distribution depends on the observation and transition models, and since it is hard to compute the expected value using the whole parameter space of segmentations, one has to resort to Monte Carlo sampling techniques to compute the expected segmentation parameters. Generally, it is straightforward to compute probability values using the filtering distribution, but it is hard to sample from it, which indicates the need to use a proposal distribution to provide an easier sampling method. In order to be useful, this proposal distribution must be carefully designed to represent a reasonable approximation for the filtering distribution. In this paper, we introduce a new LV tracking and segmentation algorithm based on the method described above, where our contributions are focused on a new transition and observation models, and a new proposal distribution. Our tracking and segmentation algorithm achieves better overall results on a previously tested dataset used as a benchmark by the current state-of-the-art tracking algorithms of the left ventricle of the heart from ultrasound images.","1063-6919;10636919","Electronic:978-1-4244-6985-7; POD:978-1-4244-6984-0","10.1109/CVPR.2010.5540013","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5540013","","Computer architecture;Distributed computing;Filtering algorithms;Heart;Image segmentation;Monte Carlo methods;Particle filters;Particle tracking;Proposals;Ultrasonic imaging","Monte Carlo methods;cardiology;medical image processing;particle filtering (numerical methods)","Monte Carlo sampling;deep learning architectures;filtering distribution;heart;left ventricle;multiple dynamic models;particle filters;probability;tracking;ultrasound data;ultrasound images","","15","1","21","","","","13-18 June 2010","","IEEE","IEEE Conference Publications"
"Automatic fetal body and amniotic fluid segmentation from fetal ultrasound images by encoder-decoder network with inner layers","Y. Li; R. Xu; J. Ohya; H. Iwata","Faculty of Science and Engineering, Waseda University, Tokyo, Japan","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","1485","1488","This paper explores the effectiveness of applying a deep learning based method to segment the amniotic fluid and fetal tissues in fetal ultrasound (US) images. The deeply learned model firstly encodes the input image into down scaled feature maps by convolution and pooling structures, then up-scale the feature maps to confidence maps by corresponded un-pooling and convolution layers. Additional convolution layers with 1×1 sized kernels are adopted to enhance the feature representations, which could be used to further improve the discriminative learning of our model. We effectively update the weights of the network by fine-tuning on part of the layers from a pre-trained model. By conducting experiments using clinical data, the feasibility of our proposed approach is compared and discussed. The result proves that this work achieves satisfied results for segmentation of specific anatomical structures from US images.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8037116","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037116","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Automated embolic signal detection using Deep Convolutional Neural Network","P. Sombune; P. Phienphanich; S. Phuechpanpaisal; S. Muengtaweepongsa; A. Ruamthanthong; C. Tantibundhit","Faculty of Engineering, Thammasat University, Thailand","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","3365","3368","This work investigated the potential of Deep Neural Network in detection of cerebral embolic signal (ES) from transcranial Doppler ultrasound (TCD). The resulting system is aimed to couple with TCD devices in diagnosing a risk of stroke in real-time with high accuracy. The Adaptive Gain Control (AGC) approach developed in our previous study is employed to capture suspected ESs in real-time. By using spectrograms of the same TCD signal dataset as that of our previous work as inputs and the same experimental setup, Deep Convolutional Neural Network (CNN), which can learn features while training, was investigated for its ability to bypass the traditional handcrafted feature extraction and selection process. Extracted feature vectors from the suspected ESs are later determined whether they are of an ES, artifact (AF) or normal (NR) interval. The effectiveness of the developed system was evaluated over 19 subjects going under procedures generating emboli. The CNN-based system could achieve in average of 83.0% sensitivity, 80.1% specificity, and 81.4% accuracy, with considerably much less time consumption in development. The certainly growing set of training samples and computational resources will contribute to high performance. Besides having potential use in various clinical ES monitoring settings, continuation of this promising study will benefit developments of wearable applications by leveraging learnable features to serve demographic differentials.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8037577","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037577","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"CEUS-based classification of liver tumors with deep canonical correlation analysis and multi-kernel learning","L. Guo; D. Wang; H. Xu; Y. Qian; C. Wang; X. Zheng; Q. Zhang; J. Shi","Department of Medical Ultrasound, Shanghai Tenth People's Hospital, Ultrasound Research and Education Institute, School of Medicine, Tongji University, China","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20170914","2017","","","1748","1751","The contrast-enhanced ultrasound (CEUS) has been a widely accepted imaging modality for diagnosis of liver cancers. In clinical practice, several typical images selected from enhancement patterns of the arterial, portal venous and late phases can provide reliable information basis for diagnosis. In this work, we propose to develop a CEUS-based computer-aided diagnosis (CAD) for liver cancers with only three typical CEUS images selected from three phases, which simulates the clinical diagnosis mode of radiologists. In the proposed CAD, the deep canonical correlation analysis (DCCA) is first performed on three CEUS pairs between arterial and portal venous phases, arterial and late phases, respectively, due to the effectiveness of multi-view fusion of DCCA. The generated six-view features are then fed to a multiple kernel learning (MKL) classifier to further promote the predictive diagnosis result. The experimental results indicate that the proposed DCCA-MKL algorithm achieves best performance for discriminating benign liver tumors from malignant liver cancers.","","Electronic:978-1-5090-2809-2; POD:978-1-5090-2810-8","10.1109/EMBC.2017.8037181","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8037181","","","","","","","","","","","","11-15 July 2017","","IEEE","IEEE Conference Publications"
"Automatic localization of the needle target for ultrasound-guided epidural injections","M. Pesteie; V. Lessoway; P. Abolmaesumi; R. N. Rohling","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, B.C., Canada.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Accurate identification of the needle target is crucial for effective epidural anesthesia. Currently, epidural needle placement is administered by a manual technique, relying on the sense of feel, which has a significant failure rate. Moreover, misleading the needle may lead to inadequate anesthesia, post dural puncture headaches and other potential complications. Ultrasound offers guidance to the physician for identification of the needle target, but accurate interpretation and localization remain challenges. A hybrid machine learning system is proposed to automatically localize the needle target for epidural needle placement in ultrasound images of the spine. In particular, a deep network architecture along with a feature augmentation technique is proposed for automatic identification of the anatomical landmarks of the epidural space in ultrasound images. Experimental results of the target localization on planes of 3D as well as 2D images have been compared agianst an expert sonographer. When compared with the expert annotations, the average lateral and vertical error on planes of 3D test data was 1 mm and 0.4 mm, respectively. On 2D test data set, an average lateral error of 1.7 mm and vertical error of 0.8 mm were acquired.","0278-0062;02780062","","10.1109/TMI.2017.2739110","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008783","3D ultrasound;Epidural injection;deep learning;prepuncture scan;target localization","Anesthesia;Needles;Real-time systems;Spine;Three-dimensional displays;Ultrasonic imaging","","","","","","","","","20170811","","","IEEE","IEEE Early Access Articles"
"Automated Breast Ultrasound Lesions Detection using Convolutional Neural Networks","M. H. Yap; G. Pons; J. Martí; S. Ganau; M. Sentís; R. Zwiggelaar; A. K. Davison; R. Martí","School of Computing, Mathematics, and Digital Technology, Manchester Metropolitan University, Manchester, Lancashire United Kingdom of Great Britain and Northern Ireland M1 5GD (e-mail: M.Yap@mmu.ac.uk)","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Breast lesion detection using ultrasound imaging is considered an important step of Computer-Aided Diagnosis systems. Over the past decade, researchers have demonstrated the possibilities to automate the initial lesion detection. However, the lack of a common dataset impedes research when comparing the performance of such algorithms. This paper proposes the use of deep learning approaches for breast ultrasound lesion detection and investigates three different methods: a Patch-based LeNet, a U-Net, and a transfer learning approach with a pretrained FCN-AlexNet. Their performance is compared against four state-of-the-art lesion detection algorithms (i.e. Radial Gradient Index, Multifractal Filtering, Rule-based Region Ranking and Deformable Part Models). In addition, this paper compares and contrasts two conventional ultrasound image datasets acquired from two different ultrasound systems. Dataset A comprises 306 (60 malignant and 246 benign) images and Dataset B comprises 163 (53 malignant and 110 benign) images. To overcome the lack of public datasets in this domain, Dataset B will be made available for research purposes. The results demonstrate an overall improvement by the deep learning approaches when assessed on both datasets in terms of True Positive Fraction, False Positives per image, and F-measure.","2168-2194;21682194","","10.1109/JBHI.2017.2731873","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003418","Lesion detection;breast cancer;convolutional neural networks;transfer learning;ultrasound imaging","Breast cancer;Filtering;Fractals;Imaging;Lesions;Ultrasonic imaging","","","","","","","","","20170807","","","IEEE","IEEE Early Access Articles"
"Deep Learning on Sparse Manifolds for Faster Object Segmentation","J. C. Nascimento; G. Carneiro","Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Lisboa, Portugal","IEEE Transactions on Image Processing","20170804","2017","26","10","4978","4990","We propose a new combination of deep belief networks and sparse manifold learning strategies for the 2D segmentation of non-rigid visual objects. With this novel combination, we aim to reduce the training and inference complexities while maintaining the accuracy of machine learning-based non-rigid segmentation methodologies. Typical non-rigid object segmentation methodologies divide the problem into a <italic>rigid detection</italic> followed by a <italic>non-rigid segmentation</italic>, where the low dimensionality of the rigid detection allows for a robust training (i.e., a training that does not require a vast amount of annotated images to estimate robust appearance and shape models) and a fast search process during inference. Therefore, it is desirable that the dimensionality of this rigid transformation space is as small as possible in order to enhance the advantages brought by the aforementioned division of the problem. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection space. Furthermore, we propose the use of deep belief networks to allow for a training process that can produce robust appearance models without the need of large annotated training sets. We test our approach in the segmentation of the left ventricle of the heart from ultrasound images and lips from frontal face images. Our experiments show that the use of sparse manifolds and deep belief networks for the rigid detection stage leads to segmentation results that are as accurate as the current state of the art, but with lower search complexity and training processes that require a small amount of annotated training data.","1057-7149;10577149","","10.1109/TIP.2017.2725582","FCT; 10.13039/501100000923 - Australian Research Council¿¿¿s Discovery Projects funding scheme; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973159","Deep belief netwolks;defonnable objects;non-rigid segmentation;sparse manifold","Image segmentation;Manifolds;Robustness;Search problems;Shape;Training;Visualization","","","","","","","","","20170711","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"Constrained Deep Weak Supervision for Histopathology Image Segmentation","Z. Jia; X. Huang; E. I. C. Chang; Y. Xu","Microsoft Research, Beijing 100080, China and Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing 100084, China.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-toimage weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.","0278-0062;02780062","","10.1109/TMI.2017.2724070","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7971941","Convolutional neural networks;fully convolutional networks;histopathology image segmentation;multiple instance learning;weakly supervised learning","Biomedical imaging;Cancer;Image segmentation;Neural networks;Prediction algorithms;Supervised learning;Training","","","","","","","","","20170707","","","IEEE","IEEE Early Access Articles"
"Automated assessment of endometrium from transvaginal ultrasound using Deep Learned Snake","N. Singhal; S. Mukherjee; C. Perrey","GE Global Research, Bangalore, India","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","283","286","Endometrium assessment via thickness measurement is commonly performed in routine gynecological ultrasound examination for assessing the reproductive health of patients undergoing fertility related treatments and endometrium cancer screening in women with post-menopausal bleeding. This paper introduces a fully automated technique for endometrium thickness measurement from three-dimensional transvaginal ultrasound (TVUS) images. The algorithm combines the robustness of deep neural networks with the more interpretable level set method for segmentation. We propose a hybrid variational curve propagation model which embeds a deep-learned endometrium probability map in the segmentation energy functional. This solution provides approximately 30% performance improvement over a contemporary supervised learning method on a database of 59 TVUS images and the thickness measurement is found to be within ±2mm of the manual measurement in 87% of the cases.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950520","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950520","Endometrium;deep learning;level set;segmentation;ultrasound;uterus","Image segmentation;Level set;Shape;Thickness measurement;Three-dimensional displays;Training;Ultrasonic imaging","biomedical measurement;biomedical ultrasonics;cancer;image segmentation;learning (artificial intelligence);medical image processing;neural nets;support vector machines;thickness measurement","3D TVUS images;3D transvaginal ultrasound images;automated endometrium assessment;deep learned snake;deep neural networks;deep-learned endometrium probability map;endometrium cancer screening;endometrium thickness measurement;gynecological ultrasound examination;image segmentation;post-menopausal bleeding;supervised learning method;variational curve propagation model","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Toward automatic diagnosis of hip dysplasia from 2D ultrasound","A. R. Hareendranathan; D. Zonoobi; M. Mabee; D. Cobzas; K. Punithakumar; M. Noga; J. L. Jaremko","Servier Virtual Cardiac Centre, Mazankowski Alberta Heart Institute, Canada","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","982","985","Developmental dysplasia of the hip (DDH) is a congenital deformity occurring in ~3% of infants. If diagnosed early most cases of DDH can be effectively treated using a Pavlik harness. However, current diagnosis of DDH using 2D ultrasound is and can have high inter-operator variability. In this paper we propose a method to automatically segment the acetabulum bone and derive geometric indices of hip dysplasia from this model. In the proposed method, using multi-scale superpixels, we incorporate global and local image features into a Deep Learning framework to obtain a probability map of the bone to be segmented and then use this map in probabilistic graph search to guide the segmentation. Clinically relevant geometric measures of hip dysplasia, including a new index of acetabular rounding, are then automatically calculated from the segmented acetabulum contour. We tested this method on 2D ultrasound of 50 infant hips and the contours generated matched closely with manual segmentations at root mean square error 1.8±0.7 mm and Hausdorff distance 2.1±0.9 mm. In this pilot data, the measured indices of dysplasia give an area under the curve of 86.2% for classifying normal vs dysplastic hips. The proposed approach could be used clinically for accurate and automatic diagnosis of hip dysplasia in infants.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950680","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950680","deep learning;hip dysplasia;segmentation;superpixels;ultrasound","Bones;Feature extraction;Hip;Image segmentation;Machine learning;Pediatrics;Ultrasonic imaging","biomedical ultrasonics;bone;diseases;image classification;image segmentation;learning (artificial intelligence);medical image processing;paediatrics","2D ultrasound;DDH diagnosis;Pavlik harness;acetabulum bone;acetabulum contour;automatic hip dysplasia diagnosis;congenital deformity;deep learning framework;developmental hip dysplasia;image feature;manual segmentation;multiscale superpixel","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Liver Fibrosis Classification Based on Transfer Learning and FCNet for Ultrasound Images","D. Meng; L. Zhang; G. Cao; W. Cao; G. Zhang; B. Hu","MOE Research Center for Software/Hardware Co-Design Engineering, East China Normal University, Shanghai, China","IEEE Access","20170520","2017","5","","5804","5810","Diagnostic ultrasound offers great improvements in diagnostic accuracy and robustness. However, it is difficult to make subjective and uniform diagnoses, because the quality of ultrasound images can be easily influenced by machine settings, the characteristics of ultrasonic waves, the interactions between ultrasound and body tissues, and other uncontrollable factors. In this paper, we propose a novel liver fibrosis classification method based on transfer learning (TL) using VGGNet and a deep classifier called fully connected network (FCNet). In case of insufficient samples, deep features extracted using TL strategy can provide sufficient classification information. These deep features are then sent to FCNet for the classification of different liver fibrosis statuses. With this framework, tests show that our deep features combined with the FCNet can provide suitable information to enable the construction of the most accurate prediction model when compared with other methods.","2169-3536;21693536","","10.1109/ACCESS.2017.2689058","NSFC-Zhejiang Joint Fund for the Integration of Industrialization and Informatization; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890483","Deep neural networks;fully connected layers;liver fibrosis;transfer learning","Feature extraction;Heating systems;Liver;Medical services;Neural networks;Training;Ultrasonic imaging","","","","","","","","","20170330","2017","","IEEE","IEEE Journals & Magazines"
"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel","Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","48","55","Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","2168-2194;21682194","","10.1109/JBHI.2016.2631401","European Regions Development; FIS; Marie-Curie Actions Program of the European Union; 10.13039/100000002 - NIH; 10.13039/100007065 - NVIDIA; 10.13039/501100000783 - REA; 10.13039/501100003741 - ICREA; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798","Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound","Atherosclerosis;Feature extraction;Imaging;Lipidomics;Machine learning;Neural networks;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","calcified tissue;cardiovascular events;carotid plaque composition;carotid ultrasound;cerebrovascular events;convolutional neural network;deep learning framework;fibrous cap;fibrous tissue;image noise;imaging features;lipid core;plaque constituents;thresholding values","","","","","","","20161122","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks","H. Chen; D. Ni; J. Qin; S. Li; X. Yang; T. Wang; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Journal of Biomedical and Health Informatics","20170520","2015","19","5","1627","1636","Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning-based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.","2168-2194;21682194","","10.1109/JBHI.2015.2425041","Hong Kong Innovation and Technology Fund; Research Grants Council of Hong Kong; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7090943","Convolutional neural network (CNN);Ultrasound;convolutional neural network;deep learning;domain transfer;knowledge transfer;standard plane;ultrasound (US)","Biomedical imaging;Dictionaries;Feature extraction;Informatics;Standards;Training;Videos","biomedical ultrasonics;image classification;learning (artificial intelligence);medical image processing;neural nets;object detection;obstetrics","FASP localization;US videos;automatic standard plane localization;classification performance;domain transferred deep convolutional neural network;fetal abdominal standard plane;fetal ultrasound;high-level features;learning-based approach;low-level features;medical image computing problems;natural images;overfitting problem;task-specific CNN;transfer learning strategy;ultrasound videos","0;Abdomen;Female;Fetus;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Pregnancy;ROC Curve;Ultrasonography, Prenatal","32","","37","","","20150421","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"A Deep Convolutional Neural Network Based Framework for Automatic Fetal Facial Standard Plane Recognition","Z. Yu; E. L. Tan; D. Ni; J. Qin; S. Chen; S. Li; B. Lei; T. Wang","","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Ultrasound imaging has become a prevalent examination method in prenatal diagnosis. Accurate acquisition of fetal facial standard plane (FFSP) is the most important precondition for subsequent diagnosis and measurement. In the past few years, considerable effort has been devoted to FFSP recognition using various hand-crafted features, but the recognition performance is still unsatisfactory due to the high intra-class variation of FFSPs and the high degree of visual similarity between FFSPs and other non-FFSPs. To improve the recognition performance, we propose a method to automatically recognize FFSP via a deep convolutional neural network (DCNN) architecture. The proposed DCNN consists of 16 convolutional layers with small 3×3 size kernels and three fully connected layers. A global average pooling (GAP) is adopted in the last pooling layer to significantly reduce network parameters, which alleviates the overfitting problems and improves the performance under limited training data. Both the transfer learning strategy and a data augmentation technique tailored for FFSP are implemented to further boost the recognition performance. Extensive experiments demonstrate the advantage of our proposed method over traditional approaches and the effectiveness of DCNN to recognize FFSP for clinical diagnosis.","2168-2194;21682194","","10.1109/JBHI.2017.2705031","10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930382","Deep convolutional neural network;Standard plane recognition;Transfer learning;Ultrasound image","Biomedical imaging;Biomedical measurement;Feature extraction;Image recognition;Neural networks;Standards;Ultrasonic imaging","","","","","","","","","20170517","","","IEEE","IEEE Early Access Articles"
"Deep learning of submerged body images from 2D sonar sensor based on convolutional neural network","S. Lee","Division of Mechanical and Automotive Engineering, Kongju University, Cheonan, 31080, Korea","2017 IEEE Underwater Technology (UT)","20170403","2017","","","1","3","Given the harsh working conditions such as high-speed flow rate, turbid watch, and steep terrain, it is a very challenging task to find submerged bodies in disaster site occurred at sea or river or for the military purpose. Therefore, if it is possible to utilize the unmanned robot, such as the USV(Unmanned Surface Vehicle) and UUV (Unmanned Underwater Vehicle) for the navigational operation of these special purpose, it has a great effect. Underwater ultrasound image information is pretty difficult to make the geometric modeling of submerged body due to heavy noise on its characteristics. This study presents the robust method of submerged body recognition based on the CNN(Convolutional Neural Network), which is one of the deep learning approach.","","Electronic:978-1-5090-5266-0; POD:978-1-5090-5267-7","10.1109/UT.2017.7890309","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890309","Convolutional Neural Network;Submerged Body Recognition;Underwater Sonar Image;Unmanned Surface Vehicle;Unmanned Underwater Vehicle","Kernel;Machine learning;Neural networks;Robot sensing systems;Sea surface;Sonar","image recognition;image sensors;learning (artificial intelligence);neural nets;sonar imaging","2D sonar sensor;CNN;USV;UUV;convolutional neural network;deep learning;disaster;geometric modeling;high-speed flow rate;military purpose;navigational operation;steep terrain;submerged body image recognition;turbid watch;underwater ultrasound image information;unmanned robot;unmanned surface vehicle;unmanned underwater vehicle","","","","","","","","21-24 Feb. 2017","","IEEE","IEEE Conference Publications"
"Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images","Y. Zhang; M. T. C. Ying; L. Yang; A. T. Ahuja; D. Z. Chen","Department of Computer Science and Engineering, University of Notre Dame, IN 46556, USA","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","443","448","Ultrasound as a well-established imaging modality is widely used in imaging lymph nodes for clinical diagnosis and disease analysis. Quantitative analysis of lymph node features, morphology, and relations can provide valuable information for diagnosis and immune system studies. For such analysis, it is necessary to first accurately segment the lymph node areas in ultrasound images. In this paper, we develop a new deep learning method, called Coarse-to-Fine Stacked Fully Convolutional Nets (CFS-FCN), for automatically segmenting lymph nodes in ultrasound images. Our method consists of multiple stages of FCN modules. We train the CFS-FCN model to learn the segmentation knowledge from a coarse-to-fine, simple-to-complex manner. A data set of 80 ultrasound images containing both normal and diseased lymph nodes is used in our experiments, which show that our method considerably outperforms the state-of-the-art deep learning methods for lymph node segmentation.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822557","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822557","","Biological system modeling;Biomedical imaging;Image segmentation;Lymph nodes;Machine learning;Training;Ultrasonic imaging","biomedical ultrasonics;diseases;image segmentation;learning (artificial intelligence);medical image processing","CFS-FCN;FCN module;clinical diagnosis;coarse-to-fine stacked fully convolutional net;deep learning method;disease analysis;lymph node imaging;lymph node segmentation;ultrasound images;ultrasound imaging modality","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Neurodynamic non-invasive fetal electrocardiogram extraction","D. Devyatykh; O. Gerget","Applied Mathematics department, National Research Tomsk Polytechnic University, Tomsk, Russia","2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)","20161219","2016","","","1","6","Fetal electrocardiography in contrary to adult is not that well represented in publications, yet circulatory system of the fetus is probably the most valuable and crucial biological infrastructure. Fetal heart ratio, form of QRS-wave and dynamics of cardiovascular system activity allow estimating fetus state, maturity, possibilities of heart abnormality occasion. This information can be received with guaranteed accuracy through Doppler-ultrasound procedure, however duration of such kind of monitoring is limited. Fetal electrocardiogram is an obvious source of information about fetal heart activity. However, because of low signal-to-noise ratio and prevailing of maternal component, non-invasive ways of acquiring this signal do not guarantee absolute accuracy. Problems of non-invasive electrocardiography demand complex mathematical approaches because maternal and fetal R-peaks overlap in time and frequency domains and have similar morphological structure of heart waves. In this paper we propose approach for extracting fetal electrocardiography from abdominal signal, which is based on dynamic neural network. The common problem for both dynamic and deep learning is caused by linearity of backpropagation and thus vanishing or exploding of gradients occurs. We proposed resilient propagation through time approach that unites training based on sign of derivative and parallel unfolding. We compared developed algorithm with blind source separation through independent component analysis and noted several important advantages that our model delivers - accuracy does not depend on: length of signal; amount of independent channels.","","Electronic:978-1-5090-3429-1; POD:978-1-5090-3430-7","10.1109/IISA.2016.7785333","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785333","blind source separation;dynamic neural network;fetal electrocardiogram;resilient propagation;vanishing gradient","Blind source separation;Electrocardiography;Fetal heart;Fetus;Neural networks;Neurons;Training","backpropagation;electrocardiography;independent component analysis;medical signal processing;neural nets","Doppler-ultrasound procedure;abdominal signal;backpropagation;blind source separation;complex mathematical approaches;deep learning;dynamic neural network;fetal heart activity;fetal heart ratio;independent component analysis;neurodynamic noninvasive fetal electrocardiogram extraction","","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"Fetal facial standard plane recognition via very deep convolutional networks","Z. Yu; D. Ni; S. Chen; S. Li; T. Wang; B. Lei","School of Biomedical Engineering, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","627","630","The accurate recognition of fetal facial standard plane (FFSP) (i.e., axial, coronal and sagittal plane) from ultrasound (US) images is quite essential for routine US examination. Since the labor-intensive and subjective measurement is too time-consuming and unreliable, the development of the automatic FFSP recognition method is highly desirable. Different from the previous methods, we leverage a general framework to recognize the FFSP from US images automatically. Specifically, instead of using the previous hand-crafted visual features, we utilize the recent developed deep learning approach via very deep convolutional networks (DCNN) architecture to represent fine-grained details of US image. Also, very small (3×3) convolution filters are adopted to improve the performance. The evaluation of our FFSP dataset shows the superiority of our method over the previous studies and achieves the state-of-the-art FFSP recognition results.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7590780","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590780","","Computer architecture;Convolution;Image recognition;Imaging;Standards;Training;Ultrasonic imaging","biomedical ultrasonics;face recognition;learning (artificial intelligence);medical image processing;neural nets;obstetrics","DCNN;FFSP dataset;US examination;automatic FFSP recognition method;convolution filters;deep learning approach;fetal facial standard plane recognition;hand-crafted visual features;ultrasound images;very deep convolutional networks","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Multi-atlas segmentation using manifold learning with deep belief networks","J. C. Nascimento; G. Carneiro","Instituto de Sistemas e Rob&#243;tica, Instituto Superior T&#233;cnico, 1049-001 Lisboa, Portugal","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","867","871","This paper proposes a novel combination of manifold learning with deep belief networks for the detection and segmentation of left ventricle (LV) in 2D — ultrasound (US) images. The main goal is to reduce both training and inference complexities while maintaining the segmentation accuracy of machine learning based methods for non-rigid segmentation methodologies. The manifold learning approach used can be viewed as an atlas-based segmentation. It partitions the data into several patches. Each patch proposes a segmentation of the LV that somehow must be fused. This is accomplished by a deep belief network (DBN) multi-classifier that assigns a weight for each patch LV segmentation. The approach is thus threefold: (i) it does not rely on a single segmentation, (ii) it provides a great reduction in the rigid detection phase that is performed at lower dimensional space comparing with the initial contour space, and (iii) DBN's allows for a training process that can produce robust appearance models without the need of large annotated training sets.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493403","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493403","","Complexity theory;Context;Image segmentation;Manifolds;Principal component analysis;Training;Visualization","","","","","","17","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Describing ultrasound video content using deep convolutional neural networks","Y. Gao; M. A. Maraci; J. A. Noble","Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, UK","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","787","790","We address the task of object recognition in obstetric ultrasound videos using deep Convolutional Neural Networks (CNNs). A transfer learning based design is presented to study the transferability of features learnt from natural images to ultrasound image object recognition which on the surface is a very different problem. Our results demonstrate that CNNs initialised with large-scale pre-trained networks outperform those directly learnt from small-scale ultrasound data (91.5% versus 87.9%), in terms of object identification.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493384","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493384","Classification;Convolutional neural networks;Obstetric ultrasound;Transfer learning","Abdomen;Data visualization;Feature extraction;Heart;Standards;Training;Ultrasonic imaging","biomedical ultrasonics;image recognition;medical image processing;neural nets;object recognition;obstetrics","deep convolutional neural networks;object identification;obstetric ultrasound videos;small-scale ultrasound data;transfer learning-based design;ultrasound image object recognition;ultrasound video content","","1","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing","F. C. Ghesu; E. Krubasik; B. Georgescu; V. Singh; Y. Zheng; J. Hornegger; D. Comaniciu","Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1217","1228","Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% o- er the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","0278-0062;02780062","","10.1109/TMI.2016.2538802","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426845","Deep learning;image parsing;marginal space learning;sparse representations;three-dimensional (3D) object detection and segmentation","Context;Feature extraction;Image segmentation;Machine learning;Robustness;Shape;Three-dimensional displays","biomedical ultrasonics;feature extraction;image classification;image sampling;image segmentation;learning (artificial intelligence);medical image processing;pattern clustering;probability;ultrasonic imaging","3D context;DL-based active shape model;anatomical object detection;anatomical pose estimation;annotated image databases;aortic valve;automated feature design;boundary delineation;clinical workflow;clustered high-probability regions;computational efficiency;deep learning network architectures;deep learning systems;diagnosis;extensive dataset;full 3D data detection;full 3D data segmentation;hierarchical marginal spaces;learning classifiers;machine learning methods;marginal space deep learning;nonrigid object boundary;object localization;object parametrization;parametrized representations;patient stratification;representative image features;restricted affine transformation;run-time performance;scanning high-dimensional parametric spaces;scanning hypotheses;segmentation support;sparse adaptive data sampling patterns;therapy planning;two-step learning problem;ultrasound;volumetric medical image data parsing","","9","","46","","","20160307","May 2016","","IEEE","IEEE Journals & Magazines"
"A novel method with a deep network and directional edges for automatic detection of a fetal head","S. Nie; J. Yu; P. Chen; J. Zhang; Y. Wang","Department of Electronic Engineering, Fudan University, Shanghai, China","2015 23rd European Signal Processing Conference (EUSIPCO)","20151228","2015","","","654","658","In this paper, we propose a novel method for the automatic detection of fetal head in 2D ultrasound images. Fetal head detection has been a challenging task, as the ultrasound images usually have poor quality, the structures contained in the images are complex, and the gray scale distribution is highly variable. Our approach is based on a deep belief network and a modified circle detection method. The whole process can be divided into two steps: first, a deep learning architecture is applied to search the whole image and determine the result patch that contains the entire fetal head; second, a modified circle detection method is used along with Hough transform to detect the position and size of the fetal head. In order to validate our method, experiments are performed on both synthetic data and clinic ultrasound data. A good performance of the proposed method is shown in the paper.","","Electronic:978-0-9928-6263-3; POD:978-1-4799-8851-8","10.1109/EUSIPCO.2015.7362464","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362464","Fetal head;circle detection;deep learning","Europe;Head;Image edge detection;Magnetic heads;Signal processing;Training;Ultrasonic imaging","Hough transforms;belief networks;medical signal detection;ultrasonic imaging","2D ultrasound images;Hough transform;automatic detection;clinic ultrasound data;deep belief network;deep learning architecture;deep network;directional edges;fetal head detection;gray scale distribution;modified circle detection;synthetic data","","1","","19","","","","Aug. 31 2015-Sept. 4 2015","","IEEE","IEEE Conference Publications"
"Mapping between ultrasound and vowel speech using DNN framework","X. Zheng; J. Wei; W. Lu; Q. Fang; J. Dang","School of Computer Science and Technology, Tianjin University, China","The 9th International Symposium on Chinese Spoken Language Processing","20141027","2014","","","372","376","Building up the mapping between articulatory movements and corresponding speech could great facility the speech training and speech aid for voiceless patients. In this paper, we propose a deep learning framework for building up a mapping between articulatory information and corresponding speech, which were recorded by ultrasound system. The dataset includes six Chinese vowels. We use Bimodal Deep Autoencoder algorithm based on RBM to learn the relationship between speech and articulation, the weights matrix of representation of them. Speech and ultrasound images have been reconstructed using the extracted features. The reconstruction error of articulation by our method is less than that of PCA based approach. The reconstructed speech is similar to the original one. We propose a mapping from ultrasound tongue image to acoustic signal with a revised Denoising Autoencoder, the results show that it is a promising approach. In contrast, another experiment is conducted to synthesize the ultrasound tongue image from the speech, but the result should be improved.","","Electronic:978-1-4799-4219-0; POD:978-1-4799-4218-3; USB:978-1-4799-4220-6","10.1109/ISCSLP.2014.6936700","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936700","DBN;Denoising Autoencoder;articulatory-acoustic mapping","Acoustics;Feature extraction;Image reconstruction;Speech;Synchronization;Tongue;Ultrasonic imaging","acoustic signal processing;feature extraction;handicapped aids;image reconstruction;natural language processing;neural nets;speech processing;ultrasonic imaging","Chinese vowels;DNN framework;RBM;acoustic signal;articulatory movements;bimodal deep autoencoder algorithm;deep neural network framework;denoising autoencoder;feature extraction;speech aid;speech reconstruction;speech training;ultrasound image reconstruction;ultrasound system;ultrasound tongue image;voiceless patients;vowel speech","","1","","9","","","","12-14 Sept. 2014","","IEEE","IEEE Conference Publications"
"Cognitive Radar for Microwave Breast Cancer Detection ?? A Simulation Study","P. Arnold; J. Moll; V. Krozer","","GeMiC 2014; German Microwave Conference","20140324","2014","","","1","4","Microwave imaging is a promising technology for breast cancer detection since it provides independent information about malignant tissue compared to ultrasound or X-ray mammography. Since the tumour can occur close to the surface or deep within the tissue it can be advantageous to alter the excitation signal adaptively for higher detection probability. The contribution of this paper is to investigate on a simulation basis how the adaptation of the excitation waveform in a cognitive radar (CR) framework can improve the detection of breast cancer in the sense of an automatic self-learning frequency adjustment.","","Paper:978-3-8007-3585-3","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6775109","","","","","","0","","","","","","10-12 March 2014","","VDE","VDE Conference Publications"
"Manifold Learning for Object Tracking With Multiple Nonlinear Models","J. C. Nascimento; J. G. Silva; J. S. Marques; J. M. Lemos","Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Lisboa, Portugal","IEEE Transactions on Image Processing","20140224","2014","23","4","1593","1605","This paper presents a novel manifold learning algorithm for high-dimensional data sets. The scope of the application focuses on the problem of motion tracking in video sequences. The framework presented is twofold. First, it is assumed that the samples are time ordered, providing valuable information that is not presented in the current methodologies. Second, the manifold topology comprises multiple charts, which contrasts to the most current methods that assume one single chart, being overly restrictive. The proposed algorithm, Gaussian process multiple local models (GP-MLM), can deal with arbitrary manifold topology by decomposing the manifold into multiple local models that are probabilistic combined using Gaussian process regression. In addition, the paper presents a multiple filter architecture where standard filtering techniques are integrated within the GP-MLM. The proposed approach exhibits comparable performance of state-of-the-art trackers, namely multiple model data association and deep belief networks, and compares favorably with Gaussian process latent variable models. Extensive experiments are presented using real video data, including a publicly available database of lip sequences and left ventricle ultrasound images, in which the GP-MLM achieves state of the art results.","1057-7149;10577149","","10.1109/TIP.2014.2303652","; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728687","Manifold learning;multiple dynamics;tangent bundle;tracking","Estimation;Gaussian processes;Kernel;Manifolds;Principal component analysis;Trajectory;Vectors","Gaussian processes;object tracking;regression analysis;video signal processing","GP-MLM;Gaussian process latent variable models;Gaussian process multiple local models;Gaussian process regression;deep belief networks;high dimensional data sets;left ventricle ultrasound images;lip sequences;manifold learning algorithm;manifold topology;motion tracking;multiple charts;multiple filter architecture;multiple model data association;multiple nonlinear models;object tracking;publicly available database;real video data;standard filtering;video sequences","Algorithms;Artificial Intelligence;Databases, Factual;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Nonlinear Dynamics;Speech","3","","53","","","20140129","April 2014","","IEEE","IEEE Journals & Magazines"
"The use of on-line co-training to reduce the training set size in pattern recognition methods: Application to left ventricle segmentation in ultrasound","G. Carneiro; J. C. Nascimento","Australian Centre for Visual Technologies, The University of Adelaide, Australia","2012 IEEE Conference on Computer Vision and Pattern Recognition","20120726","2012","","","948","955","The use of statistical pattern recognition models to segment the left ventricle of the heart in ultrasound images has gained substantial attention over the last few years. The main obstacle for the wider exploration of this methodology lies in the need for large annotated training sets, which are used for the estimation of the statistical model parameters. In this paper, we present a new on-line co-training methodologythat reduces the need for large training sets for such parameter estimation. Our approach learns the initial parameters of two different models using a small manually annotated training set. Then, given each frame of a test sequence, the methodology not only produces the segmentation of the current frame, but it also uses the results of both classifiers to retrain each other incrementally. This on-line aspect of our approach has the advantages of producing segmentation results and retraining the classifiers on the fly as frames of a test sequence are presented, but it introduces a harder learning setting compared to the usual off-line co-training, where the algorithm has access to the whole set of un-annotated training samples from the beginning. Moreover, we introduce the use of the following new types of classifiers in the co-training framework: deep belief network and multiple model probabilistic data association. We show that our method leads to a fully automatic left ventricle segmentation system that achieves state-of-the-art accuracy on a public database with training sets containing at least twenty annotated images.","1063-6919;10636919","Electronic:978-1-4673-1228-8; POD:978-1-4673-1226-4; USB:978-1-4673-1227-1","10.1109/CVPR.2012.6247770","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6247770","","Data models;Image segmentation;Pattern recognition;Probabilistic logic;Training;Ultrasonic imaging;Vectors","image segmentation;medical image processing;pattern recognition","annotated image;annotated training set;automatic left ventricle segmentation system;belief network;multiple model probabilistic data association;offline co-training;online co-training methodology;parameter estimation;pattern recognition method;public database;statistical model parameter;statistical pattern recognition model;test sequence;training set size;training sets;ultrasound image;un-annotated training sample","","1","","22","","","","16-21 June 2012","","IEEE","IEEE Conference Publications"
"Incremental on-line semi-supervised learning for segmenting the left ventricle of the heart from ultrasound data","G. Carneiro; J. C. Nascimento","Australian Centre for Visual Technologies, The University of Adelaide, Australia","2011 International Conference on Computer Vision","20120112","2011","","","1700","1707","Recently, there has been an increasing interest in the investigation of statistical pattern recognition models for the fully automatic segmentation of the left ventricle (LV) of the heart from ultrasound data. The main vulnerability of these models resides in the need of large manually annotated training sets for the parameter estimation procedure. The issue is that these training sets need to be annotated by clinicians, which makes this training set acquisition process quite expensive. Therefore, reducing the dependence on large training sets is important for a more extensive exploration of statistical models in the LV segmentation problem. In this paper, we present a novel incremental on-line semi-supervised learning model that reduces the need of large training sets for estimating the parameters of statistical models. Compared to other semi-supervised techniques, our method yields an on-line incremental re-training and segmentation instead of the off-line incremental re-training and segmentation more commonly found in the literature. Another innovation of our approach is that we use a statistical model based on deep learning architectures, which are easily adapted to this on-line incremental learning framework. We show that our fully automatic LV segmentation method achieves state-of-the-art accuracy with training sets containing less than twenty annotated images.","1550-5499;15505499","Electronic:978-1-4577-1102-2; POD:978-1-4577-1101-5","10.1109/ICCV.2011.6126433","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126433","","Image segmentation","biomedical ultrasonics;cardiology;image segmentation;learning (artificial intelligence);medical image processing;parameter estimation;statistical analysis","deep learning architectures;heart left ventricle segmentation;incremental online semisupervised learning model;large training sets;online incremental retraining;parameter estimation procedure;statistical pattern recognition models;ultrasound data","","3","","20","","","","6-13 Nov. 2011","","IEEE","IEEE Conference Publications"
"Deep Belief Networks for Real-Time Extraction of Tongue Contours from Ultrasound During Speech","I. Fasel; J. Berry","Univ. of Arizona, Tucson, AZ, USA","2010 20th International Conference on Pattern Recognition","20101007","2010","","","1493","1496","Ultrasound has become a useful tool for speech scientists studying mechanisms of language sound production. State-of-the-art methods for extracting tongue contours from ultrasound images of the mouth, typically based on active contour snakes, require considerable manual interaction by an expert linguist. In this paper we describe a novel method for fully automatic extraction of tongue contours based on a hierarchy of restricted Boltzmann machines (RBMs), i.e. deep belief networks (DBNs). Usually, DBNs are first trained generatively on sensor data, then discriminatively to predict human-provided labels of the data. In this paper we introduce the translational RBM (tRBM), which allows the DBN to make use of both human labels and raw sensor data at all stages of learning. This method yields performance in contour extraction comparable to human labelers, without any temporal smoothing or human intervention, and runs in real-time.","1051-4651;10514651","Electronic:978-1-4244-7541-4; POD:978-1-4244-7542-1","10.1109/ICPR.2010.369","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597284","Computer aided detection and diagnosis;Pattern recognition systems and applications;Signal/image representation","Decoding;Humans;Image reconstruction;Speech;Tongue;Training;Ultrasonic imaging","Boltzmann machines;belief networks;feature extraction;linguistics;ultrasonic imaging","contour extraction;deep belief networks;language sound production;real time extraction;restricted Boltzmann machines;speech scientists;tongue contours;translational RBM;ultrasound images","","10","","6","","","","23-26 Aug. 2010","","IEEE","IEEE Conference Publications"
"Automatic installation of underwater elastic structures under unknown currents","H. Suzuki; Q. Tao; K. Yoshida","Dept. of Naval Archit. & Ocean Eng., Tokyo Univ., Japan","Proceedings of 1998 International Symposium on Underwater Technology","20020806","1998","","","274","281","This paper deals with the new technique of installation and construction of underwater flexible structures by using learning tracking controller (LTC). The LTC has such potentials on the installation or construction of underwater structures, in deep water under unknown currents, that the structure can be installed precisely on the desired point on seabed. By this method automated construction of very large structure, assembling the onshore fabricated partitioned structures on site is possible. In this paper, the LTC is extended to use for the flexible structural control to avoid spillovers under unknown disturbances. The mathematical convergence condition and design process for the real system is presented. In order to confirm the capability and effectiveness of LTC, basin tests were executed using two types of neutrally buoyant flexible models with ultrasound ranging system and thrusters under unknown currents","","POD:0-7803-4273-9","10.1109/UT.1998.670108","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670108","","Automatic control;Convergence;Cranes;Differential equations;Flexible structures;Motion control;Oceans;Testing;Underwater structures;Underwater tracking","assembling;feedback;feedforward;flexible structures;installation;learning systems;marine systems;tracking","assembling;automatic installation;feedback;feedforward;flexible structures;learning tracking controller;mathematical convergence;underwater elastic structures","","2","","6","","","","15-17 Apr 1998","15 Apr 1998-17 Apr 1998","IEEE","IEEE Conference Publications"
"[Copyright notice]","","","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1","1","The following topics are dealt with: brain CAD; cardiac imaging; fMRI analysis; image reconstruction; interventional imaging; microscopy imaging and reconstruction; musculo-skeletal imaging; retinal imaging; segmentation methods for microscopy images; modeling and simulation; brain segmentation; motion tracking; neuron image analysis; optical imaging; segmentation and quantification of biological images; tissue quantification; ultrasound; visualization; fast MR acquisition and reconstruction; structural brain connectivity; CT reconstruction; image analysis of neurons; perspectives on deep learning for biomedical and biological imaging and image analysis; shape analysis; imaging cellular processes; breast imaging; EEG; foetal imaging; histological image analysis; imaging genetics; and machine learning.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493196","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493196","","","biomedical MRI;biomedical optical imaging;biomedical ultrasonics;brain;cardiology;cellular biophysics;computerised tomography;eye;genetics;image reconstruction;image segmentation;learning (artificial intelligence);mammography;medical image processing;neurophysiology;optical microscopy","CT reconstruction;EEG;biological images;brain CAD;brain segmentation;breast imaging;cardiac imaging;cellular processes;deep learning;fMRI analysis;fast MR acquisition;fetal imaging;histology image analysis;image reconstruction;imaging genetics;interventional imaging;machine learning;microscopy imaging;microscopy reconstruction;motion tracking;musculo-skeletal imaging;neuron image analysis;optical imaging;retinal imaging;segmentation methods;shape analysis;structural brain connectivity;tissue quantification;ultrasound;visualization","","","","","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Ultrasound aided vertebral level localization for lumbar surgery","N. Baka; S. Leenstra; T. v. Walsum","Biomedical Imaging Group Rotterdam, Departments of Radiology &#x0026; Nuclear Medicine and Medical Informatics, Erasmus MC, University Medical Center Rotterdam, The Netherlands.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Localization of the correct vertebral level for surgical entry during lumbar hernia surgery is not straightforward. In this paper we develop and evaluate a solution using free-hand 2D ultrasound (US) imaging in the operation room (OR). Our system exploits the difference in spinous process shapes of the vertebrae. The spinous processes are pre-operatively outlined and labeled in a lateral lumbar X-ray of the patient. Then, in the OR the spinous processes are imaged with 2D sagittal US, and are automatically segmented and registered with the X-ray shapes. After a small number of scanned vertebrae, the system robustly matches the shapes, and propagates the X-ray label to the US images. The main contributions of our work are: We propose a deep convolutional neural network based bone segmentation algorithm from US imaging, that outperforms state-of-the-art methods in both performance and speed. We present a matching strategy that determines the levels of the spinal processes being imaged. And lastly, we evaluate the complete procedure on 19 clinical datasets from two hospitals, and two observers. The final labeling was correct in 92% of the cases, demonstrating the feasibility of US based surgical entry point detection for spinal surgeries.","0278-0062;02780062","","10.1109/TMI.2017.2738612","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007292","bone segmentation;computer aided surgery;deep learning;lumbar X-ray;machine learning;spine;surgical guidance","Bones;Image segmentation;Shape;Surgery;Two dimensional displays;Ultrasonic imaging;X-ray imaging","","","","","","","","","20170810","","","IEEE","IEEE Early Access Articles"
"Small Sample Deep Learning for Newborn Gestational Age Estimation","M. T. Torres; M. F. Valstar; C. Henry; C. Ward; D. Sharkey","Sch. of Comput. Sci., Univ. of Nottingham, Nottingham, UK","2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","20170629","2017","","","79","86","A baby's gestational age determines whether or not they are preterm, which helps clinicians decide on suitable post-natal treatment. The most accurate dating methods use Ultrasound Scan (USS) machines, but these machines are expensive, require trained personnel and cannot always be deployed to remote areas. In the absence of USS, the Ballard Score can be used, which is a manual postnatal dating method. However, this method is highly subjective and results can vary widely depending on the experience of the rater. In this paper, we present an automatic system for postnatal gestational age estimation aimed to be deployed on mobile phones, using small sets of images of a newborn's face, foot and ear. We present a novel two-stage approach that makes the most out of Convolutional Neural Networks trained on small sets of images to predict broad classes of gestational age, and then fuse the outputs of these discrete classes with a baby's weight to make fine-grained predictions of gestational age. On a purpose=collected dataset of 88 babies, experiments show that our approach attains an expected error of 6 days and is three times more accurate than the manual postnatal method (Ballard). Making use of images improves predictions by 30% compared to using weight only. This indicates that even with a very small set of data, our method is a viable candidate for postnatal gestational age estimation in areas were USS is not available.","","Electronic:978-1-5090-4023-0; POD:978-1-5090-4024-7","10.1109/FG.2017.19","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961726","","Biomedical imaging;Ear;Estimation;Face;Image segmentation;Machine learning;Pediatrics","biomedical ultrasonics;convolution;image classification;image segmentation;learning (artificial intelligence);medical image processing;mobile computing;paediatrics","Ballard score;USS machines;convolutional neural network training;deep learning;image segmentation;mobile phones;newborn gestational age estimation;post-natal treatment;postnatal dating method;postnatal gestational age estimation;ultrasound scan machines","","","","","","","","May 30 2017-June 3 2017","","IEEE","IEEE Conference Publications"
"Automated characterization of the fetal heart in ultrasound images using fully convolutional neural networks","V. Sundaresan; C. P. Bridge; C. Ioannou; J. A. Noble","Institute of Biomedical Engineering, University of Oxford, UK","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","671","674","Automatic analysis of fetal echocardiography screening images could aid in the identification of congenital heart diseases. The first step towards automatic fetal echocardiography analysis is locating the fetal heart in an image and identifying the viewing (imaging) plane. This is highly challenging since the fetal heart is small with relatively indistinct anatomical structural appearance. This is further compounded by the presence of artefacts in ultrasound images. Herein we provide a state-of-art solution for detecting the fetal heart and classifying each individual frame as belonging to one of the standard viewing planes using fully convolutional neural networks (FCNs). Our FCN model achieves a classification error rate of 23.48% on real-world clinical ultrasound data. We also present comparative performance for analysis of different FCN architectures.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950609","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950609","Fetal ultrasound images;deep learning;fetal echocardiography;fully convolutional neural networks","Echocardiography;Fetal heart;Neural networks;Standards;Training;Ultrasonic imaging","echocardiography;image classification;medical image processing;neural nets","FCN model;classification error rate;congenital heart diseases;fetal echocardiography screening images;fetal heart;fully convolutional neural networks;real-world clinical ultrasound data;ultrasound images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Feature selection and thyroid nodule classification using transfer learning","T. Liu; S. Xie; Y. Zhang; J. Yu; L. Niu; W. Sun","Dept. of Electronic Engineering, Tsinghua University, Beijing 100084, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1096","1099","Ultrasonography is a valuable diagnosis method for thyroid nodules. Automatically discriminating benign and malignant nodules in the ultrasound images can provide aided diagnosis suggestions, or increase the diagnosis accuracy when lack of experts. The core problem in this issue is how to capture appropriate features for this specific task. Here, we propose a feature extraction method for ultrasound images based on the convolution neural networks (CNNs), try to introduce more meaningful and specific features to the classification. A CNN model trained with ImageNet data is transferred to the ultrasound image domain, to generate semantic deep features under small sample condition. Then, we combine those deep features with conventional features such as Histogram of Oriented Gradient (HOG) and Scale Invariant Feature Transform (SIFT) together to form a hybrid feature space. Furthermore, to make the general deep features more pertinent to our problem, a feature subset selection process is employed for the hybrid nodule classification, followed by a detailed discussion on the influence of feature number and feature composition method. Experimental results on 1037 images show that the accuracy of our proposed method is 0.929, which outperforms other relative methods by over 10%.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950707","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950707","feature subset selection;thyroid nodules classification;transfer learning;ultrasound image","Biomedical imaging;Cancer;Convolution;Feature extraction;Indexes;Training;Ultrasonic imaging","biomedical ultrasonics;feature extraction;feature selection;image classification;learning (artificial intelligence);medical image processing;neural nets;programming language semantics","ImageNet data;benign nodules;convolution neural networks;diagnosis accuracy;feature extraction method;feature selection;histogram-of-oriented-gradient;malignant nodules;scale-invariant-feature-transform;semantic deep features;thyroid nodule classification;transfer learning;ultrasonography;ultrasound images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Classification of breast lesions using cross-modal deep learning","O. Hadad; R. Bakalo; R. Ben-Ari; S. Hashoul; G. Amit","IBM Research, Haifa, Israel","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","109","112","Automatic detection and classification of lesions in medical images is a desirable goal, with numerous clinical applications. In breast imaging, multiple modalities such as X-ray, ultrasound and MRI are often used in the diagnostic workflow. Training robust classifiers for each modality is challenging due to the typically small size of the available datasets. We propose to use cross-modal transfer learning to improve the robustness of the classifiers. We demonstrate the potential of this approach on a problem of identifying masses in breast MRI images, using a network that was trained on mammography images. Comparison between cross-modal and cross-domain transfer learning showed that the former improved the classification performance, with overall accuracy of 0.93 versus 0.90, while the accuracy of de-novo training was 0.94. Using transfer learning within the medical imaging domain may help to produce standard pre-trained shared models, which can be utilized to solve a variety of specific clinical problems.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950480","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950480","breast imaging;computer-aided diagnosis;deep learning;multimodal analysis;transfer learning","Biomedical imaging;Breast;Data models;Lesions;Magnetic resonance imaging;Training","biomedical MRI;image classification;learning (artificial intelligence);mammography;medical image processing","breast MRI images;breast imaging;breast lesion classification;cross-domain transfer learning;cross-modal deep learning;mammography images","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Handcrafted features vs ConvNets in 2D echocardiographic images","C. Raynaud; H. Langet; M. S. Amzulescu; E. Saloux; H. Bertrand; P. Allain; P. Piro","Philips Research Medisys, Paris, France","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1116","1119","In this paper, we address the problem of automated pose classification and segmentation of the left ventricle (LV) in 2D echocardiographic images. For this purpose, we compare two complementary approaches. The first one is based on engineering ad-hoc features according to the traditional machine learning paradigm. Namely, we extract phase features to build an unsupervised LV pose estimator, as well as a global image descriptor for view type classification. We also apply the Supervised Descent Method (SDM) to iteratively refine the LV contour. The second approach follows the deep learning framework, where a Convolutional Network (ConvNet) learns the visual features automatically. Our experiments on a large database of apical sequences show that the two approaches yield comparable results on view classification, but SDM outperforms ConvNet on LV segmentation at a significantly lower training computational cost.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950712","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950712","2D ultrasound;ConvNets;Supervised Descent Method;left ventricle segmentation;phase features","Databases;Image segmentation;Pose estimation;Robustness;Shape;Training;Two dimensional displays","echocardiography;feature extraction;image classification;image segmentation;image sequences;iterative methods;learning (artificial intelligence);medical image processing","2D echocardiographic images;ConvNet;LV contour;LV segmentation;SDM;ad-hoc features;apical sequences;automated pose classification;convolutional network;deep learning framework;handcrafted features;image classification;iterative method;left ventricle segmentation;machine learning paradigm;phase feature extraction;supervised descent method;unsupervised LV pose estimator;visual features","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Detection of lumen and media-adventitia borders in IVUS images using sparse auto-encoder neural network","S. Su; Z. Gao; H. Zhang; Q. Lin; W. K. Hau; S. Li","College of Sciences, Zhejiang University of Technology, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1120","1124","This paper describes an artificial neural network (ANN) method that employs a feature-learning algorithm to detect the lumen and MA borders in intravascular ultrasound (IVUS) images. Three types of imaging features including spatial, neighboring, and gradient features were used as the input features to the neural network, and then the different vascular layers were distinguished using two sparse autoencoders and one softmax classifier. To smooth the lumen and MA borders detected by the ANN method, we used the active contour model. The performance of our approach was compared with the manual drawing method and another existing method on 538 IVUS images from six subjects. Results showed that our approach had a high correlation (r = 0.9284 ~ 0.9875 for all measurements) and good agreement (bias = 0.0148 ~ 0.4209 mm) with the manual drawing method, and small detection error (lumen border: 0.0928±0.0935 mm, MA border: 0.1056±0.1088 mm). The average time to process each image was 14±4.6 seconds. The obtained results indicate that our proposed approach can be used to efficiently and accurately detect the lumen and MA borders in IVUS images.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950713","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950713","Deep neural network;Image segmentation;Intravascular image;Sparse autoencoder","Active contours;Artificial neural networks;Biomedical imaging;Feature extraction;Manuals;Training","biomedical ultrasonics;blood vessels;feature extraction;image classification;medical image processing;neural nets","ANN method;IVUS image;artificial neural network;feature-learning algorithm;imaging feature;intravascular ultrasound;lumen detection;media-adventitia border detection;softmax classifier;sparse autoencoder neural network;vascular layer","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Feature extraction using multimodal convolutional neural networks for visual speech recognition","E. Tatulli; T. Hueber","CNRS / Univ. Grenoble-Alpes / GIPSA-lab, France","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20170619","2017","","","2971","2975","This article addresses the problem of continuous speech recognition from visual information only, without exploiting any audio signal. Our approach combines a video camera and an ultrasound imaging system for monitoring simultaneously the speaker's lips and the movement of the tongue. We investigate the use of convolutional neural networks (CNN) to extract visual features directly from the raw ultrasound and video images. We propose different architectures among which a multimodal CNN processing jointly the two visual modalities. Combined with an HMM-GMM decoder, the CNN-based approach outperforms our previous baseline based on Principal Component Analysis. Importantly, the recognition accuracy is only 4% lower than the one obtained when decoding the audio signal, which makes it a good candidate for a practical visual speech recognition system.","","Electronic:978-1-5090-4117-6; POD:978-1-5090-4118-3; USB:978-1-5090-4116-9","10.1109/ICASSP.2017.7952701","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952701","Convolutional Neural Networks;Deep Learning;Visual Speech Recognition","Decoding;Feature extraction;Hidden Markov models;Speech;Speech recognition;Ultrasonic imaging;Visualization","Gaussian processes;convolution;feature extraction;hidden Markov models;mixture models;neural nets;principal component analysis;speech recognition;ultrasonic imaging;video cameras;video signal processing","Gaussian mixture model;HMM-GMM decoder;continuous speech recognition;feature extraction;hidden Markov model;multimodal CNN processing;multimodal convolutional neural networks;principal component analysis;speaker lips monitoring;tongue movement;ultrasound images;ultrasound imaging system;video camera;video images;visual modalities;visual speech recognition","","","","","","","","5-9 March 2017","","IEEE","IEEE Conference Publications"
"Coronary luminal and wall mask prediction using convolutional neural network","Y. Hong; Y. M. Hong; Y. Jang; S. Kim; B. Jeon; S. Jung; S. Ha; D. Han; H. Shim; H. J. Chang","Brain Korea 21 PLUS Project for Medical Science, Yonsei University, South Korea","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","1049","1052","A significant amount of research has been done on the segmentation of coronary arteries. However, the resulting automated boundary delineation is still not suitable for clinical utilization. The convolutional neural network was driving advances in the medical image processing. We propose the brief convolutional network (BCN) that automatically produces the labeled mask with the luminal and wall boundaries of the coronary artery. We utilized 50 patients of CCTA - intravascular ultrasound matched image data sets. Training and testing were performed on 40 and 10 patient data sets, respectively. The prediction of luminal and wall mask was performed using stacked BCN on the each image view: axial, coronal, and sagittal of straightened curved planar reformation. We defined the vector that includes probability from BCN result on each image view and proposed amplified probability. We used an Adaptive Boost regressor with an extremely randomized tree regressor to determine the label for unknown probability vector.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950696","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950696","Classification;Convolutional neural network;Coronary artery;Deep learning;Plaque quantification","Arteries;Computer architecture;Feature extraction;Image segmentation;Neural networks;Training;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiology;image segmentation;learning (artificial intelligence);medical image processing;neural nets;probability;regression analysis","CCTA-intravascular ultrasound image;adaptive boost regressor;brief convolutional network;convolutional neural network;coronary artery segmentation;coronary luminal;coronary wall mask prediction;extremely randomized tree regressor;image view;medical image processing","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Ultrasound Standard Plane Detection Using a Composite Neural Network Framework","H. Chen; L. Wu; Q. Dou; J. Qin; S. Li; J. Z. Cheng; D. Ni; P. A. Heng","Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Cybernetics","20170520","2017","47","6","1576","1586","Ultrasound (US) imaging is a widely used screening tool for obstetric examination and diagnosis. Accurate acquisition of fetal standard planes with key anatomical structures is very crucial for substantial biometric measurement and diagnosis. However, the standard plane acquisition is a labor-intensive task and requires operator equipped with a thorough knowledge of fetal anatomy. Therefore, automatic approaches are highly demanded in clinical practice to alleviate the workload and boost the examination efficiency. The automatic detection of standard planes from US videos remains a challenging problem due to the high intraclass and low interclass variations of standard planes, and the relatively low image quality. Unlike previous studies which were specifically designed for individual anatomical standard planes, respectively, we present a general framework for the automatic identification of different standard planes from US videos. Distinct from conventional way that devises hand-crafted visual features for detection, our framework explores in- and between-plane feature learning with a novel composite framework of the convolutional and recurrent neural networks. To further address the issue of limited training data, a multitask learning framework is implemented to exploit common knowledge across detection tasks of distinctive standard planes for the augmentation of feature learning. Extensive experiments have been conducted on hundreds of US fetus videos to corroborate the better efficacy of the proposed framework on the difficult standard plane detection problem.","2168-2267;21682267","","10.1109/TCYB.2017.2685080","National Basic Research Program of China, 973 Program; National Natural Science Foundation of China; Research Grants Council of Hong Kong Special Administrative Region; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890445","Convolutional neural network (CNN);deep learning;knowledge transfer;recurrent neural network (RNN);standard plane;ultrasound (US)","Biomedical imaging;Feature extraction;Fetus;Machine learning;Standards;Training data;Videos","","","","","","","","","20170330","June 2017","","IEEE","IEEE Journals & Magazines"
"Automatic Scoring of Multiple Semantic Attributes With Multi-Task Feature Leverage: A Study on Pulmonary Nodules in CT Images","S. Chen; J. Qin; X. Ji; B. Lei; T. Wang; D. Ni; J. Z. Cheng","Department of Biomedical Engineering, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Medicine, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20170301","2017","36","3","802","814","The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of “spiculation”, “texture”, “margin”, etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.","0278-0062;02780062","","10.1109/TMI.2016.2629462","(Key) Project of Department of Education of Guangdong Province; Natural Science Foundation of SZU; Shenzhen Key Basic Research Project; Shenzhen-Hong Kong Innovation Circle Funding Program; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745891","Computer-aided diagnosis (CAD);computed tomography (CT);deep learning;feature learning;lung nodule;multi-task learning","Computational modeling;Computed tomography;Lungs;Machine learning;Medical diagnostic imaging;Semantics","computerised tomography;feature extraction;lung;medical image processing;pneumodynamics;regression analysis","CT images;HoG features;Lung Image Database Consortium data;clinical semantic terms;computer-aided diagnosis performance;convolutional neural network;deep learning models;elastic net regression methods;heterogeneous computational features;lung nodules;medical image;medical search engine;multitask learning schemes;pulmonary nodules;semantic features;single-task LASSO;stacked denoising autoencoder","","1","","","","","20161116","March 2017","","IEEE","IEEE Journals & Magazines"
"Accurate Cervical Cell Segmentation from Overlapping Clumps in Pap Smear Images","Y. Song; E. L. Tan; X. Jiang; J. Z. Cheng; D. Ni; S. Chen; B. Lei; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20161230","2017","36","1","288","300","Accurate segmentation of cervical cells in Pap smear images is an important step in automatic pre-cancer identification in the uterine cervix. One of the major segmentation challenges is overlapping of cytoplasm, which has not been well-addressed in previous studies. To tackle the overlapping issue, this paper proposes a learning-based method with robust shape priors to segment individual cell in Pap smear images to support automatic monitoring of changes in cells, which is a vital prerequisite of early detection of cervical cancer. We define this splitting problem as a discrete labeling task for multiple cells with a suitable cost function. The labeling results are then fed into our dynamic multi-template deformation model for further boundary refinement. Multi-scale deep convolutional networks are adopted to learn the diverse cell appearance features. We also incorporated high-level shape information to guide segmentation where cell boundary might be weak or lost due to cell overlapping. An evaluation carried out using two different datasets demonstrates the superiority of our proposed method over the state-of-the-art methods in terms of segmentation accuracy.","0278-0062;02780062","","10.1109/TMI.2016.2606380","(Key) Project of Department of Education of Guangdong Province; National Key Research and Development Project; Shenzhen Key Basic Research Project; 10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7562400","Cervical cancer;Pap smear screening;dynamic multi-template deformation model;multi-scale convolutional networks;overlapping cells splitting","Cervical cancer;Deformable models;Image color analysis;Image edge detection;Image segmentation;Labeling;Shape","biomedical optical imaging;cancer;cellular biophysics;feature extraction;gynaecology;image segmentation;medical image processing;tumours","Pap smear images;accurate cervical cell segmentation;automatic monitoring;automatic precancer identification;boundary refinement;cell boundary;cell overlapping;cervical cancer detection;cervical cells;cytoplasm;datasets;discrete labeling task;diverse cell appearance features;dynamic multitemplate deformation model;high-level shape information;learning-based method;multiple cells;multiscale deep convolutional networks;overlapping clumps;uterine cervix","","1","","","","","20160907","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Segmenting overlapping cervical cell in Pap smear images","Y. Song; J. Z. Cheng; D. Ni; S. Chen; B. Lei; T. Wang","Department of Biomedical Engineering, School of Medicine, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultra-sound Imaging, Shenzhen, China","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1159","1162","Accurate segmentation of cervical cells in Pap smear images is an important task for automatic identification of pre-cancerous changes in the uterine cervix. One of the major segmentation challenges is the overlapping of cytoplasm, which was less addressed by previous studies. In this paper, we propose a learning-based method to tackle the overlapping issue with robust shape priors by segmenting individual cell in Pap smear images. Specifically, we first define the problem as a discrete labeling task for multiple cells with a suitable cost function. We then use the coarse labeling result to initialize our dynamic multiple-template deformation model for further boundary refinement on each cell. Multiple-scale deep convolutional networks are adopted to learn the diverse cell appearance features. Also, we incorporate high level shape information to guide segmentation where the cells boundary is noisy or lost due to touching and overlapping cells. We evaluate the proposed algorithm on two different datasets, and our comparative experiments demonstrate the promising performance of the proposed method in terms of segmentation accuracy.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493472","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493472","Cervical cancer;dynamic multiple-template deformation model;multiple-scale deep convolutional networks;overlapping cells splitting","Biomedical imaging;Blood;Cervical cancer;Image segmentation;Labeling;Level set;Shape","","","","1","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks","Q. Dou; H. Chen; L. Yu; L. Zhao; J. Qin; D. Wang; V. C. Mok; L. Shi; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1182","1195","Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.","0278-0062;02780062","","10.1109/TMI.2016.2528129","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403984","3D convolutional neural networks;biomarker detection;cerebral microbleeds;deep learning;susceptibility-weighted imaging","Biomarkers;Feature extraction;Kernel;MIMICs;Medical diagnostic imaging;Three-dimensional displays","biomedical MRI;blood;blood vessels;brain;cognition;diseases;feature extraction;haemodynamics;medical image processing;neurophysiology;probability","3D FCN strategy;3D convolutional neural networks;3D fully convolutional network strategy;CMB detection;MR volume extraction;MRI;automatic cerebral microbleed detection;blood vessels;cerebrovascular diseases;cognitive dysfunctions;current clinical routine;diagnostic biomarkers;haemorrhages;low-level hand-crafted descriptors;magnetic resonance images;massive redundant computations;probabilities;radiologists;representative high-level features;spatial contextual information;traditional sliding window strategy;well-trained 3D CNN discrimination","","15","","52","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"Retinal vessel landmark detection using deep learning and hessian matrix","T. Fang; R. Su; L. Xie; Q. Gu; Q. Li; P. Liang; T. Wang","Department of Ophthalmology, Affiliated Nanshan people's Hospital of Shenzhen University, Shenzhen University, Shenzhen, China","2015 8th International Congress on Image and Signal Processing (CISP)","20160218","2015","","","387","392","The purpose of retinal image registration is to establish the coherent correspondences between the multi-model retinal image for applying into the ophthalmological surgery. Vessel landmarks detection in retinal image is the vital step in the retinal image registration. In this paper, a novel approach is proposed, firstly, a deep learning technology is used to vessel segmentation to generate the probability map of the retinal image, which is more reliable for optimizing the feature detection in retinal image. Secondly, we detect the landmarks using the multi-scale Hessian response on the probability map of the retinal image. Compared to the traditional methods, the results show that our method enable a majority of the bifurcation points, crossover points and curvature extreme points to be detected out simultaneously. Moreover, the impact of image noise and pathology can be reduced significantly.","","Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2","10.1109/CISP.2015.7407910","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407910","Deep learning;Hessian response;Image registration;Landmark detection;The probability map;The retinal image;Vessel Segmentation","Feature extraction;Image color analysis;Image registration;Image segmentation;Machine learning;Neural networks;Retina","Hessian matrices;eye;feature extraction;image registration;image segmentation;learning (artificial intelligence);medical image processing;object detection;probability;surgery","Hessian matrix;bifurcation points;crossover points;curvature extreme points;deep learning;feature detection;image noise;multimodel retinal image;multiscale Hessian response;ophthalmological surgery;pathology;retinal image probability map;retinal image registration;retinal vessel landmark detection;vessel segmentation","","","","9","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"A supervised method using convolutional neural networks for retinal vessel delineation","Q. Li; L. Xie; Q. Zhang; S. Qi; P. Liang; H. Zhang; T. Wang","National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, School of Medicine, Shenzhen University, Shenzhen 518060","2015 8th International Congress on Image and Signal Processing (CISP)","20160218","2015","","","418","422","Retinal vessel delineation is a hot research topic owing to its importance in a lot of clinic application. Several methods have been proposed in the past decades. Here we will present a new supervised method for retinal vessel segmentation. The method is designed to explore the complex relationship between retinal images and their corresponding vessel label maps. Specifically, in order to build a model describing the direct transformation from retinal image to vessel map, we introduce a deep convolutional neural network (abbreviation as CNN), which has strong enough induction ability. For the purpose of constructing the whole vessel probability map, we also design a synthesis method. Our method shows better performance on DRIVE dataset than state-of-the-art of reported approaches in the light of sensitivity (abbreviation as Se), specificity (abbreviation as Sp) and accuracy (abbreviation as Acc). Our proposed method has great potential to be applied in existing computer-assisted diagnostic system of ophthalmologic diseases. Meanwhile, the method may offer a novel, general computing framework for segmentation in other fields.","","Electronic:978-1-4673-9098-9; POD:978-1-4673-9099-6; USB:978-1-4673-9097-2","10.1109/CISP.2015.7407916","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407916","CNN;deep learning;retinal image;vessel delineation","Feature extraction;Image segmentation;Measurement;Neural networks;Retinal vessels;Training","blood vessels;eye;feedforward neural nets;image segmentation;medical image processing;patient diagnosis;probability","DRIVE dataset;clinic application;computer-assisted diagnostic system;convolutional neural networks;deep convolutional neural network;general computing framework;ophthalmologic diseases;retinal images;retinal vessel delineation;retinal vessel segmentation;supervised method;synthesis method;vessel label maps;vessel probability map","","","","9","","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"A Cross-Modality Learning Approach for Vessel Segmentation in Retinal Images","Q. Li; B. Feng; L. Xie; P. Liang; H. Zhang; T. Wang","Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Department of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Medical Imaging","20151229","2016","35","1","109","118","This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.","0278-0062;02780062","","10.1109/TMI.2015.2457891","Project of the National Science Foundation of China; Shenzhen Science Plan of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161344","Cross-modality learning;deep learning;retinal image;vessel segmentation","Accuracy;Deformable models;Feature extraction;Image segmentation;Neural networks;Retina;Training","eye;image segmentation;medical image processing;neural nets","center pixel;computing framework;cross-modality learning approach;cross-training evaluation;image diagnosis;image segmentation;induction ability;neural network;ophthalmologic diseases;retinal images;vessel map;vessel segmentation","","14","","35","","","20150717","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Accurate Segmentation of Cervical Cytoplasm and Nuclei Based on Multiscale Convolutional Network and Graph Partitioning","Y. Song; L. Zhang; S. Chen; D. Ni; B. Lei; T. Wang","Shenzhen University","IEEE Transactions on Biomedical Engineering","20150916","2015","62","10","2421","2433","In this paper, a multiscale convolutional network (MSCN) and graph-partitioning-based method is proposed for accurate segmentation of cervical cytoplasm and nuclei. Specifically, deep learning via the MSCN is explored to extract scale invariant features, and then, segment regions centered at each pixel. The coarse segmentation is refined by an automated graph partitioning method based on the pretrained feature. The texture, shape, and contextual information of the target objects are learned to localize the appearance of distinctive boundary, which is also explored to generate markers to split the touching nuclei. For further refinement of the segmentation, a coarse-to-fine nucleus segmentation framework is developed. The computational complexity of the segmentation is reduced by using superpixel instead of raw pixels. Extensive experimental results demonstrate that the proposed cervical nucleus cell segmentation delivers promising results and outperforms existing methods.","0018-9294;00189294","","10.1109/TBME.2015.2430895","48th Scientific Research Foundation for the Returned Overseas Chinese Scholars; Shenzhen Key Basic Research; Shenzhen-Hong Kong Innovation Circle Funding; 10.13039/501100001809 - National Natural Science Foundation of China; 10.13039/501100004608 - National Natural Science Foundation of Guangdong Province; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103332","Cervical segmentation;coarse to fine;graph partitioning;graph-partitioning;multi-scale convolutional network;multiscale convolutional network (MSCN);touching-cell splitting","Computer architecture;Feature extraction;Image color analysis;Image edge detection;Image segmentation;Microprocessors;Shape","biomedical optical imaging;cancer;cellular biophysics;computational complexity;feature extraction;image segmentation;image texture;learning (artificial intelligence);medical image processing","MSCN;automated graph partitioning method;cervical cytoplasm segmentation;cervical nucleus cell segmentation;coarse segmentation;coarse-to-fine nucleus segmentation framework;computational complexity;contextual information;deep learning;multiscale convolutional network;pretrained feature;raw pixels;scale invariant feature extraction;shape information;superpixel;target objects;texture information","","12","","51","","","20150507","Oct. 2015","","IEEE","IEEE Journals & Magazines"
"A deep learning based framework for accurate segmentation of cervical cytoplasm and nuclei","Y. Song; L. Zhang; S. Chen; D. Ni; B. Li; Y. Zhou; B. Lei; T. Wang","Department of Biomedical Engineering, School of Medicine, Shenzhen University, National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, Shenzhen, China","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20141106","2014","","","2903","2906","In this paper, a superpixel and convolution neural network (CNN) based segmentation method is proposed for cervical cancer cell segmentation. Since the background and cytoplasm contrast is not relatively obvious, cytoplasm segmentation is first performed. Deep learning based on CNN is explored for region of interest detection. A coarse-to-fine nucleus segmentation for cervical cancer cell segmentation and further refinement is also developed. Experimental results show that an accuracy of 94.50% is achieved for nucleus region detection and a precision of 0.9143±0.0202 and a recall of 0.8726±0.0008 are achieved for nucleus cell segmentation. Furthermore, our comparative analysis also shows that the proposed method outperforms the related methods.","1094-687X;1094687X","Electronic:978-1-4244-7929-0; POD:978-1-4244-7927-6","10.1109/EMBC.2014.6944230","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944230","","Accuracy;Cervical cancer;Image color analysis;Image segmentation;Neural networks;Training","biological organs;cancer;cellular biophysics;image segmentation;medical image processing;neural nets","CNN based segmentation method;cervical cancer cell segmentation;cervical cytoplasm;coarse-to-fine nucleus segmentation;convolution neural network;cytoplasm segmentation;deep learning based framework;nucleus cell segmentation;nucleus region detection","","1","","9","","","","26-30 Aug. 2014","","IEEE","IEEE Conference Publications"
"Non-rigid Segmentation Using Sparse Low Dimensional Manifolds and Deep Belief Networks","J. C. Nascimento; G. Carneiro","Inst. de Sist. e Robot., Inst. Super. Tecnico, Lisbon, Portugal","2014 IEEE Conference on Computer Vision and Pattern Recognition","20140925","2014","","","288","295","In this paper, we propose a new methodology for segmenting non-rigid visual objects, where the search procedure is onducted directly on a sparse low-dimensional manifold, guided by the classification results computed from a deep belief network. Our main contribution is the fact that we do not rely on the typical sub-division of segmentation tasks into rigid detection and non-rigid delineation. Instead, the non-rigid segmentation is performed directly, where points in the sparse low-dimensional can be mapped to an explicit contour representation in image space. Our proposal shows significantly smaller search and training complexities given that the dimensionality of the manifold is much smaller than the dimensionality of the search spaces for rigid detection and non-rigid delineation aforementioned, and that we no longer require a two-stage segmentation process. We focus on the problem of left ventricle endocardial segmentation from ultrasound images, and lip segmentation from frontal facial images using the extended Cohn-Kanade (CK+) database. Our experiments show that the use of sparse low dimensional manifolds reduces the search and training complexities of current segmentation approaches without a significant impact on the segmentation accuracy shown by state-of-the-art approaches.","1063-6919;10636919","Electronic:978-1-4799-5118-5; POD:978-1-4799-5119-2","10.1109/CVPR.2014.44","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909438","Deep Belief Nets;Non-rigid segmentation;Sparse manifolds","Complexity theory;Image segmentation;Manifolds;Search problems;Shape;Training;Visualization","belief networks;biomedical ultrasonics;cardiology;image classification;image representation;image segmentation;learning (artificial intelligence);medical image processing;visual databases","contour representation;deep belief networks;extended Cohn-Kanade database;image classification;left ventricle endocardial segmentation;lip segmentation;nonrigid visual object segmentation;sparse low dimensional manifolds;ultrasound images","","3","","24","","","","23-28 June 2014","","IEEE","IEEE Conference Publications"
"Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds","J. C. Nascimento; G. Carneiro","Inst. de Sist. e Robot., Inst. Super. Tecnico, Lisbon, Portugal","2013 IEEE Conference on Computer Vision and Pattern Recognition","20131003","2013","","","1963","1970","The solution for the top-down segmentation of non rigid visual objects using machine learning techniques is generally regarded as too complex to be solved in its full generality given the large dimensionality of the search space of the explicit representation of the segmentation contour. In order to reduce this complexity, the problem is usually divided into two stages: rigid detection and non-rigid segmentation. The rationale is based on the fact that the rigid detection can be run in a lower dimensionality space (i.e., less complex and faster) than the original contour space, and its result is then used to constrain the non-rigid segmentation. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection search space of current state-of-the-art top-down segmentation methodologies. The main goals targeted by this smaller dimensionality search space are the decrease of the search running time complexity and the reduction of the training complexity of the rigid detector. These goals are attainable given that both the search and training complexities are function of the dimensionality of the rigid search space. We test our approach in the segmentation of the left ventricle from ultrasound images and lips from frontal face images. Compared to the performance of state-of-the-art non-rigid segmentation system, our experiments show that the use of sparse manifolds for the rigid detection leads to the two goals mentioned above.","1063-6919;10636919","Electronic:978-0-7695-4989-7; POD:978-1-4673-6410-2","10.1109/CVPR.2013.256","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619100","Non-rigid top-down segmentation;deep belief network;manifold learning","Complexity theory;Image segmentation;Manifolds;Search problems;Training;Vectors;Visualization","computational complexity;image representation;image segmentation;learning (artificial intelligence)","contour space;derivative-based search;dimensionality search space;dimensionality space;explicit representation;frontal face images;machine learning techniques;non rigid visual objects;non-rigid visual objects;rigid detection search space;rigid detector;running time complexity;search complexity;segmentation contour;segmentation methodology;sparse manifolds;state-of-the-art nonrigid segmentation system;top-down segmentation;training complexity;ultrasound images","","1","","31","","","","23-28 June 2013","","IEEE","IEEE Conference Publications"
"Semi-supervised self-trainingmodel for the segmentationof the left ventricle of the heart from ultrasound data","G. Carneiro; J. Nascimento; A. Freitas","Instituto de Sistemas e Rob&#x00F3;tica, Instituto Superior T&#x00E9;cnico, Av. Rovisco Pais, 1049-001 Lisbon, Portugal","2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro","20110609","2011","","","1295","1301","The design and use of statistical pattern recognition models can be regarded as one of the core research topics in the segmentation of the left ventricle of the heart from ultrasound data. These models trade a strong prior model of the shape and appearance of the left ventricle for a statistical model whose parameters can be learned using a manually segmented data set (this set is commonly known as the training set). The trouble is that such statistical model is usually quite complex, requiring a large number of parameters that can be robustly learned only if the training set is sufficiently large. The difficulty in obtaining large training sets is currently a major roadblock for the further exploration of statistical models in medical image analysis problems, such as the automatic left ventricle segmentation. In this paper, we present a novel semi-supervised self-training model that reduces the need of large training sets for estimating the parameters of statistical models. This model is initially trained with a small set of manually segmented images, and for each new test sequence, the system reestimates the model parameters incrementally without any further manual intervention. We show that state-of-the-art segmentation results can be achieved with training sets containing 50 annotated examples for the problem of left ventricle segmentation from ultrasound data.","1945-7928;19457928","Electronic:978-1-4244-4128-0; POD:978-1-4244-4127-3","10.1109/ISBI.2011.5872638","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872638","Segmentation of the left ventricle of the heart;deep neural networks;optimization algorithms;self-training;semi-supervised training","Data models;Heart;Image segmentation;Manuals;Shape;Training;Ultrasonic imaging","data analysis;echocardiography;image segmentation;image sequences;learning (artificial intelligence);medical image processing;parameter estimation;pattern recognition;statistical analysis","heart ultrasound dataset;image segmentation;image sequence;left ventricle;parameter estimation;semisupervised self-training model;statistical pattern recognition model","","0","","20","","","","March 30 2011-April 2 2011","","IEEE","IEEE Conference Publications"
