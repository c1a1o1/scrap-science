"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7966408,7897281,7780643,7298712,8019872,8007292,7961205,7862905,7960357,7950602,7950548,7950492,7950585,7947200,7776792,7101222,7752798,7932065,7906022,7822557,7792251,7783289,7727966,7727204,7591186,7532329,7523095,7314894,7493520,7493206,7493497,7493490,7472122,7279156,7426826,7422082,7403984,7422783,7401052,7457462,7353170,7359868,7318458,7163826,6963480,6998135,7463094",2017/09/15 09:51:57
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Ultrasound aided vertebral level localization for lumbar surgery","N. Baka; S. Leenstra; T. v. Walsum","Biomedical Imaging Group Rotterdam, Departments of Radiology &#x0026; Nuclear Medicine and Medical Informatics, Erasmus MC, University Medical Center Rotterdam, The Netherlands.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Localization of the correct vertebral level for surgical entry during lumbar hernia surgery is not straightforward. In this paper we develop and evaluate a solution using free-hand 2D ultrasound (US) imaging in the operation room (OR). Our system exploits the difference in spinous process shapes of the vertebrae. The spinous processes are pre-operatively outlined and labeled in a lateral lumbar X-ray of the patient. Then, in the OR the spinous processes are imaged with 2D sagittal US, and are automatically segmented and registered with the X-ray shapes. After a small number of scanned vertebrae, the system robustly matches the shapes, and propagates the X-ray label to the US images. The main contributions of our work are: We propose a deep convolutional neural network based bone segmentation algorithm from US imaging, that outperforms state-of-the-art methods in both performance and speed. We present a matching strategy that determines the levels of the spinal processes being imaged. And lastly, we evaluate the complete procedure on 19 clinical datasets from two hospitals, and two observers. The final labeling was correct in 92% of the cases, demonstrating the feasibility of US based surgical entry point detection for spinal surgeries.","0278-0062;02780062","","10.1109/TMI.2017.2738612","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8007292","bone segmentation;computer aided surgery;deep learning;lumbar X-ray;machine learning;spine;surgical guidance","Bones;Image segmentation;Shape;Surgery;Two dimensional displays;Ultrasonic imaging;X-ray imaging","","","","","","","","","20170810","","","IEEE","IEEE Early Access Articles"
"Probabilistic visual search for masses within mammography images using deep learning","M. G. Ertosun; D. L. Rubin","Department of Radiology, Stanford School of Medicine, CA USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1310","1315","We developed a deep learning-based visual search system for the task of automated search and localization of masses in whole mammography images. The system consists of two modules: a classification engine and a localization engine. It first classifies mammograms as containing a mass or no mass using a deep learning classifier, and then localizes the mass(es) within the image using a regional probabilistic approach based on a deep learning network. We obtained 85% accuracy for the task of identifying images that contain a mass, and we were able to localize 85% of the masses at an average of 0.9 false positives per image. Our system has the advantages of being able to work with an entire mammography image as input without the need for image segmentation or other pre-processing steps, such as cropping or tiling the image, and it is based on deep learning with unsupervised feature discovery, so it does not require pre-defined and hand-crafted image features.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359868","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359868","Breast Cancer;CAD;Classification;Deep Learning;Detection;Mammography;Visual Search","Breast;Engines;Informatics;Mammography;Visualization","biomedical engineering;learning (artificial intelligence);mammography","automated search;classification engine;deep learning classifier;deep learning network;deep learning-based visual search system;localization engine;mammography images;mass localization","","","","21","","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Deep vessel tracking: A generalized probabilistic approach via deep learning","A. Wu; Z. Xu; M. Gao; M. Buty; D. J. Mollura","Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1363","1367","Analysis of vascular geometry is important in many medical imaging applications, such as retinal, pulmonary, and cardiac investigations. In order to make reliable judgments for clinical usage, accurate and robust segmentation methods are needed. Due to the high complexity of biological vasculature trees, manual identification is often too time-consuming and tedious to be used in practice. To design an automated and computerized method, a major challenge is that the appearance of vasculatures in medical images has great variance across modalities and subjects. Therefore, most existing approaches are specially designed for a particular task, lacking the flexibility to be adapted to other circumstances. In this paper, we present a generic approach for vascular structure identification from medical images, which can be used for multiple purposes robustly. The proposed method uses the state-of-the-art deep convolutional neural network (CNN) to learn the appearance features of the target. A Principal Component Analysis (PCA)-based nearest neighbor search is then utilized to estimate the local structure distribution, which is further incorporated within the generalized probabilistic tracking framework to extract the entire connected tree. Qualitative and quantitative results over retinal fundus data demonstrate that the proposed framework achieves comparable accuracy as compared with state-of-the-art methods, while efficiently producing more information regarding the candidate tree structure.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493520","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493520","Deep Learning;Generalized Probabilistic Tracking;Nearest Neighbor Search;Principal Component Analysis;Vascular Structure","Biomedical imaging;Dictionaries;Image segmentation;Machine learning;Probabilistic logic;Robustness","","","","1","","10","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","20160503","2016","35","5","1170","1181","Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","0278-0062;02780062","","10.1109/TMI.2015.2482920","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156","Computer aided diagnosis;artificial neural networks;computed tomography;deep learning;machine learning;medical diagnostic imaging;multi-layer neural network;object detection","Colonic polyps;Computed tomography;Feature extraction;Lymph nodes;Three-dimensional displays;Training","computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","classification probability;colonic polyp detection;computed tomography;computer-aided detection;deep convolutional neural network classifier training;false positives;lymph node detection;medical imaging;random rotations;random translations;random view aggregation;scale transformations;sclerotic metastasis detection;two-tiered coarse-to-fine cascade framework","","11","","60","","","20150928","May 2016","","IEEE","IEEE Journals & Magazines"
"Coarse-to-Fine Stacked Fully Convolutional Nets for lymph node segmentation in ultrasound images","Y. Zhang; M. T. C. Ying; L. Yang; A. T. Ahuja; D. Z. Chen","Department of Computer Science and Engineering, University of Notre Dame, IN 46556, USA","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","443","448","Ultrasound as a well-established imaging modality is widely used in imaging lymph nodes for clinical diagnosis and disease analysis. Quantitative analysis of lymph node features, morphology, and relations can provide valuable information for diagnosis and immune system studies. For such analysis, it is necessary to first accurately segment the lymph node areas in ultrasound images. In this paper, we develop a new deep learning method, called Coarse-to-Fine Stacked Fully Convolutional Nets (CFS-FCN), for automatically segmenting lymph nodes in ultrasound images. Our method consists of multiple stages of FCN modules. We train the CFS-FCN model to learn the segmentation knowledge from a coarse-to-fine, simple-to-complex manner. A data set of 80 ultrasound images containing both normal and diseased lymph nodes is used in our experiments, which show that our method considerably outperforms the state-of-the-art deep learning methods for lymph node segmentation.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822557","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822557","","Biological system modeling;Biomedical imaging;Image segmentation;Lymph nodes;Machine learning;Training;Ultrasonic imaging","biomedical ultrasonics;diseases;image segmentation;learning (artificial intelligence);medical image processing","CFS-FCN;FCN module;clinical diagnosis;coarse-to-fine stacked fully convolutional net;deep learning method;disease analysis;lymph node imaging;lymph node segmentation;ultrasound images;ultrasound imaging modality","","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks","N. Pezzotti; T. Höllt; J. v. Gemert; B. P. F. Lelieveldt; E. Eisemann; A. Vilanova","Intelligent Systems department, Delft University of Technology, Delft, the Netherlands","IEEE Transactions on Visualization and Computer Graphics","","2017","PP","99","1","1","Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.","1077-2626;10772626","","10.1109/TVCG.2017.2744358","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019872","Progressive visual analytics;deep neural networks;machine learning","Kernel;Layout;Neural networks;Neurons;Three-dimensional displays;Training;Visual analytics","","","","","","","","","20170829","","","IEEE","IEEE Early Access Articles"
"Anatomy-specific classification of medical images using deep convolutional nets","H. R. Roth; C. T. Lee; H. C. Shin; A. Seff; L. Kim; J. Yao; L. Lu; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892, USA","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","101","104","Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. “Deep learning” methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163826","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163826","Computed tomography (CT);Convolutional Networks;Deep Learning;Image Classification","Computed tomography;Convolution;Lungs;Medical diagnostic imaging;Neural networks;Training","PACS;biological organs;computerised tomography;image classification;medical image processing","ConvNets;anatomy variability;anatomy-specific classification;anatomy-specific classification error;area-under-the-curve;automated classification;axial 2D key-images;body part-specific anatomical classification;computed tomography;computer-aided diagnosis systems;convolutional networks;data augmentation;data augmentation approach;deep convolutional nets;deep learning methods;hospital PACS archive;human anatomy;image classification;medical images;organ-specific anatomical classification;spatial complexity","","5","","16","","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Efficient Training of Convolutional Deep Belief Networks in the Frequency Domain for Application to High-Resolution 2D and 3D Images","T. Brosch; R. Tam","MS/MRI Research Group, Vancouver, BC V6T 2B5, Canada, and Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada brosch.tom@gmail.com","Neural Computation","20141224","2015","27","1","211","227","<para>Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8 times on 2D images and up to 200 times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128 × 128 × 128 voxels practical, which opens new directions for using deep learning for medical image analysis.</para>","0899-7667;08997667","","10.1162/NECO_a_00682","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998135","","","","","","3","","","","","","Jan. 2015","","MIT Press","MIT Press Journals"
"Biopsy-guided learning with deep convolutional neural networks for Prostate Cancer detection on multiparametric MRI","Y. Tsehay; N. Lay; X. Wang; J. T. Kwak; B. Turkbey; P. Choyke; P. Pinto; B. Wood; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Department of Radiology and Imaging Science, National Institute of Health, Clinical Center, Bethesda, MD 20892, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","642","645","Prostate Cancer (PCa) is highly prevalent and is the second most common cause of cancer-related deaths in men. Multiparametric MRI (mpMRI) is robust in detecting PCa. We developed a weakly supervised computer-aided detection (CAD) system that uses biopsy points to learn to identify PCa on mpMRI. Our CAD system, which is based on a deep convolutional neural network architecture, yielded an area under the curve (AUC) of 0.903±0.009 on a receiver operation characteristic (ROC) curve computed on 10 different models in a 10 fold cross-validation. 9 of the 10 ROCs were statistically significantly different from a competing support vector machine based CAD, which yielded a 0.86 AUC when tested on the same dataset (α = 0.05). Furthermore, our CAD system proved to be more robust in detecting high-grade transition zone lesions.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950602","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950602","Biopsy Database;Computer-Aided Detection;Holistically-nested Edge Detection;Prostate;Prostate-CAD;Radiology","Biopsy;Databases;Lesions;Principal component analysis;Solid modeling;Training","biomedical MRI;cancer;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis;support vector machines","biopsy-guided learning;computer-aided detection;deep convolutional neural network;multiparametric MRI;prostate cancer detection;receiver operation characteristic curve;support vector machine","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"A deep symmetry convnet for stroke lesion segmentation","Y. Wang; A. K. Katsaggelos; X. Wang; T. B. Parrish","Northwestern University, Department of EECS, Evanston, IL, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","111","115","Stroke is one of the leading causes of death and disability. Clinically, to establish stroke patient prognosis, an accurate delineation of brain lesion is essential, which is time consuming and prone to subjective errors. In this paper, we propose a novel method call Deep Lesion Symmetry ConvNet to automatically segment chronic stroke lesions using MRI. An 8-layer 3D convolutional neural network is constructed to handle the MRI voxels. An additional CNN stream using the corresponding symmetric MRI voxels is combined, leading to a significant improvement in system performance. The high average dice coefficient achieved on our dataset based on data collected from three research labs demonstrates the effectiveness of our method.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7532329","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532329","Brain Quasi-symmetry;Deep Learning;Image Segmentation;MRI;Stroke","Biological neural networks;Brain modeling;Image segmentation;Lesions;Magnetic resonance imaging;Pipelines;Three-dimensional displays","biomedical MRI;brain;image segmentation;medical image processing;neural nets;patient treatment","3D convolutional neural network;MRI voxels;brain lesion;death;deep symmetry ConvNet;disability;stroke lesion segmentation;stroke patient prognosis","","","","13","","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse Patch Matching","Y. Guo; Y. Gao; D. Shen","Department of Radiology and BRIC, University of North Carolina, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","20160331","2016","35","4","1077","1089","Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods.","0278-0062;02780062","","10.1109/TMI.2015.2508280","10.13039/100000002 - National Institutes of Health; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353170","Deformable model;MR prostate segmentation;sparse patch matching;stacked sparse auto-encoder (SSAE)","Biomedical imaging;Cancer;Deformable models;Feature extraction;Image segmentation;Machine learning;Shape","biological organs;biomedical MRI;cancer;feature extraction;image matching;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy","T2-wighted prostate MR images;accurate MR prostate localization;clinical applications;dataset;deep feature learning;deformable MR prostate segmentation;feature hierarchy;handcrafted features;inconsistent prostate boundary appearance;inhomogeneous prostate boundary appearance;latent feature representation;prostate cancer radiotherapy;prostate likelihood map;shape variation;sparse patch matching;stacked sparse autoencoder;state-of-the-art segmentation methods","","5","","56","","","20151211","April 2016","","IEEE","IEEE Journals & Magazines"
"Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?","N. Tajbakhsh; J. Y. Shin; S. R. Gurudu; R. T. Hurst; C. B. Kendall; M. B. Gotway; J. Liang","Department of Biomedical Informatics, Arizona State University, Scottsdale, AZ, USA","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1299","1312","Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.","0278-0062;02780062","","10.1109/TMI.2016.2535302","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426826","Carotid intima-media thickness;computer-aided detection;convolutional neural networks;deep learning;fine-tuning;medical image analysis;polyp detection;pulmonary embolism detection;video quality assessment","Biomedical imaging;Computed tomography;Feature extraction;Image analysis;Image segmentation;Training;Tuning","biomedical optical imaging;endoscopes;image classification;image segmentation;medical image processing;neural nets","cardiology;classification;deep convolutional neural network;distinct medical imaging applications;gastroenterology;imaging modalities;labeled training data;layer-wise fine-tuning scheme;medical image analysis;radiology;segmentation","","34","","76","","","20160307","May 2016","","IEEE","IEEE Journals & Magazines"
"Combining fully convolutional networks and graph-based approach for automated segmentation of cervical cell nuclei","L. Zhang; M. Sonka; L. Lu; R. M. Summers; J. Yao","Radiology and Imaging Sciences Department, National Institutes of Health (NIH), Bethesda MD, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","406","409","Cervical nuclei carry substantial diagnostic information for cervical cancer. Therefore, in automation-assisted reading of cervical cytology, automated and accurate segmentation of nuclei is essential. This paper proposes a novel approach for segmentation of cervical nuclei that combines fully convolutional networks (FCN) and graph-based approach (FCNG). FCN is trained to learn the nucleus high-level features to generate a nucleus label mask and a nucleus probabilistic map. The mask is used to construct a graph by image transforming. The map is formulated into the graph cost function in addition to the properties of the nucleus border and nucleus region. The prior constraints regarding the context of nucleus-cytoplasm position are also utilized to modify the local cost functions. The globally optimal path in the constructed graph is identified by dynamic programming. Validation of our method was performed on cell nuclei from Herlev Pap smear dataset. Our method shows a Zijdenbos similarity index (ZSI) of 0.92 ± 0.09, compared to the best state-of-the-art approach of 0.89 ± 0.15. The nucleus areas measured by our method correlated strongly with the independent standard (r<sup>2</sup> = 0.91).","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950548","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950548","Deep learning;FCN;Pap smear;graph-based segmentation","Computer architecture;Context;Cost function;Image segmentation;Imaging;Microprocessors;Shape","cancer;cellular biophysics;dynamic programming;image segmentation;medical image processing","Herlev Pap smear dataset;Zijdenbos similarity index;automated segmentation;automation-assisted reading;cervical cancer;cervical cell nuclei;cervical cytology;diagnostic information;dynamic programming;fully convolutional networks;graph cost function;graph-based approach;image transforming;nucleus high-level features;nucleus label mask;nucleus probabilistic map;nucleus-cytoplasm position","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease","S. Liu; S. Liu; W. Cai; H. Che; S. Pujol; R. Kikinis; D. Feng; M. J. Fulham; ADNI","Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, University of Sydney, Sydney, N.S.W., Australia","IEEE Transactions on Biomedical Engineering","20150318","2015","62","4","1132","1140","The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.","0018-9294;00189294","","10.1109/TBME.2014.2372011","AADRF; ARC; NA-MIC; NAC; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963480","Alzheimer’s Disease;Alzheimer's disease (AD);Classification;Deep Learning;MRI;Neuroimaging;PET;classification;deep Learning;neuroimaging;positron emission tomography (PET)","Biomarkers;Diseases;Feature extraction;Neuroimaging;Neurons;Positron emission tomography;Training","biomedical MRI;diseases;learning (artificial intelligence);neurophysiology;patient care;patient diagnosis;positron emission tomography;sensor fusion","Alzheimer disease binary classification;Alzheimer disease computer-aided diagnosis;Alzheimer disease multiclass classification;Alzheimer disease multiclass diagnosis;data fusion;machine learning method;multimodal neuroimaging feature learning;neuroimaging biomarker;patient care;zero-masking strategy","Alzheimer Disease;Brain;Humans;Image Interpretation, Computer-Assisted;Multimodal Imaging;Neuroimaging;Support Vector Machine","27","","65","","","20141120","April 2015","","IEEE","IEEE Journals & Magazines"
"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation","H. C. Shin; K. Roberts; L. Lu; D. Demner-Fushman; J. Yao; R. M. Summers","Imaging Biomarkers & Comput. Aided Diagnosis Lab., Nat. Inst. of Health, Bethesda, MD, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2497","2506","Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normalvs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.274","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780643","","Biomedical imaging;Context;Diseases;Radiology;Recurrent neural networks;Training;X-rays","X-rays;convolution;diagnostic radiography;diseases;image annotation;medical image processing;radiology;recurrent neural nets","CNNs;automated image annotation;chest X-rays;convolutional neural networks;deep learning model;image caption datasets;natural images;normalvs-diseased cases bias;radiology dataset;recurrent neural cascade model","","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN)","H. Chen; Y. Zhang; M. K. Kalra; F. Lin; Y. Chen; P. Liao; J. Zhou; G. Wang","College of Computer Science, Sichuan University, Chengdu 610065, China.","IEEE Transactions on Medical Imaging","","2017","PP","99","1","1","Given the potential risk of X-ray radiation to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. Currently, the main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction algorithms, but they need to access raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation, and lesion detection.","0278-0062;02780062","","10.1109/TMI.2017.2715284","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947200","Low-dose CT;auto-encoder;convolutional;deconvolutional;deep learning;residual neural network","Computed tomography;Convolution;Decoding;Feature extraction;Image reconstruction;X-ray imaging","","","","","","","","","20170613","","","IEEE","IEEE Early Access Articles"
"Deep learning analytics for diagnostic support of breast cancer disease management","T. He; M. Puppala; R. Ogunti; J. J. Mancuso; X. Yu; S. Chen; J. C. Chang; T. A. Patel; S. T. C. Wong","Systems Medicine and Bioengineering Department of Houston Methodist Research Institute and Informatics Development Department of Houston Methodist Hospital, Houston, TX 77030 USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","365","368","Breast cancer continues to be one of the leading causes of cancer death among women. Mammogram is the standard of care for screening and diagnosis of breast cancer. The American College of Radiology developed the Breast Imaging Reporting and Data System (BI-RADS) lexicon to standardize mammographic reporting to assess cancer risk and facilitate biopsy decision-making. However, because substantial inter-observer variability remains in the application of the BI-RADS lexicon, including inappropriate term usage and missing data, current biopsy decision-making accuracy using the unstructured free text or semi-structured reports varies greatly. Hence, incorporating novel and accurate technique into breast cancer decision-making data is critical. Here, we combined natural language processing and deep learning methods to develop an analytic model that targets well-characterized and defined specific breast suspicious patient subgroups rather than a broad heterogeneous group for diagnostic support of breast cancer management.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897281","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897281","","Biological system modeling;Biopsy;Breast cancer;Feature extraction;Maximum likelihood detection;Nonlinear filters","cancer;decision making;learning (artificial intelligence);mammography;medical computing;natural language processing","BI-RADS lexicon;Breast Imaging Reporting and Data System;breast cancer decision-making data;breast cancer diagnosis;breast cancer disease management;breast cancer screening;cancer risk;decision-making accuracy;deep learning analytics;deep learning method;diagnostic support;natural language processing","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Automatic segmentation of the left ventricle in cardiac CT angiography using convolutional neural networks","M. Zreik; T. Leiner; B. D. de Vos; R. W. van Hamersvelt; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, The Netherlands","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","40","43","Accurate delineation of the left ventricle (LV) is an important step in evaluation of cardiac function. In this paper, we present an automatic method for segmentation of the LV in cardiac CT angiography (CCTA) scans. Segmentation is performed in two stages. First, a bounding box around the LV is detected using a combination of three convolutional neural networks (CNNs). Subsequently, to obtain the segmentation of the LV, voxel classification is performed within the defined bounding box using a CNN. The study included CCTA scans of sixty patients, fifty scans were used to train the CNNs for the LV localization, five scans were used to train LV segmentation and the remaining five scans were used for testing the method. Automatic segmentation resulted in the average Dice coefficient of 0.85 and mean absolute surface distance of 1.1 mm. The results demonstrate that automatic segmentation of the LV in CCTA scans using voxel classification with convolutional neural networks is feasible.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493206","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493206","Cardiac CT Angiography;Classification;Convolutional Neural Network;Deep learning;Left ventricle segmentation","Biomedical imaging;Computed tomography;Heart;Image segmentation;Manuals;Neural networks;Observers","","","","","","16","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Classification of radiology reports using neural attention models","B. Shin; F. H. Chokshi; T. Lee; J. D. Choi","Mathematics and Computer Science, Emory University, Atlanta, GA 30322","2017 International Joint Conference on Neural Networks (IJCNN)","20170703","2017","","","4363","4370","The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure “black-box” models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.","","Electronic:978-1-5090-6182-2; POD:978-1-5090-6183-9","10.1109/IJCNN.2017.7966408","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966408","","Computational modeling;Convolution;Machine learning;Neural networks;Radiology;Sentiment analysis;Support vector machines","computerised tomography;convolution;data analysis;electronic health records;neural nets;pattern classification;radiology","CNN;EHR;attention analysis;convolutional neural networks;double-annotated dataset;electronic health record;neural attention mechanism;neural attention models;radiology head computed tomography reports classification","","","","","","","","14-19 May 2017","","IEEE","IEEE Conference Publications"
"Gold classification of COPDGene cohort based on deep learning","J. Ying; J. Dutta; N. Guo; L. Xia; A. Sitek; Q. Li; Q. Li","Nuclear Medicine and Molecular Imaging Radiology Department, Massachusetts General Hospital, Boston, MA, USA","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","2474","2478","This study aims to employ deep learning for the development of an automatic classifier for the severity of chronic obstructive pulmonary disease (COPD) in patients. A three-layer deep belief network (DBN) with two hidden layers and one visible layer was employed to generate a model for classification, and the model's robustness against exacerbation was analyzed. Subjects from the COPDGene cohort were staged using the GOLD 2011 guidelines. 10,300 subjects with 361 features each were included in the analysis. After feature selection and parameter optimization, the proposed classification method achieved an accuracy of 97.2% by using a 10-fold cross validation experiment. The most sensitive features as revealed by the DBN weights were consistent with the clinical consensus as per previous studies and clinical diagnosis rules. In summary, we demonstrate that the DBN is a competitive tool for exacerbation risk assessment for patients suffering from, COPD.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472122","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472122","Chronic Obstructive Pulmonary Disease (COPD);Deep Belief Networks (DBNs);Global Initiative for Chronic Obstructive Lung Disease (GOLD);classification;deep learning","Diseases;Feature extraction;Gold;Lungs;Machine learning;Medical diagnostic imaging;Training","belief networks;biology computing;diseases;feature selection;learning (artificial intelligence);optimisation;pattern classification","COPD gene cohort;DBN weights;automatic gold classification;chronic obstructive pulmonary disease;deep learning;exacerbation risk assessment;feature selection;parameter optimization;three-layer deep belief network","","","","19","","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Interleaved text/image Deep Mining on a large-scale radiology database","H. C. Shin; Le Lu; L. Kim; A. Seff; J. Yao; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","1090","1099","Despite tremendous progress in computer vision, effective learning on very large-scale (> 100K patients) medical image databases has been vastly hindered. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's picture archiving and communication system. Instead of using full 3D medical volumes, we focus on a collection of representative ~216K 2D key images/slices (selected by clinicians for diagnostic reference) with text-driven scalar and vector labels. Our system interleaves between unsupervised learning (e.g., latent Dirichlet allocation, recurrent neural net language models) on document- and sentence-level texts to generate semantic labels and supervised learning via deep convolutional neural networks (CNNs) to map from images to label spaces. Disease-related key words can be predicted for radiology images in a retrieval manner. We have demonstrated promising quantitative and qualitative results. The large-scale datasets of extracted key images and their categorization, embedded vector labels and sentence descriptions can be harnessed to alleviate the deep learning “data-hungry” obstacle in the medical domain.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7298712","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298712","","Machine learning;Medical diagnostic imaging;Radiology;Semantics;Visualization","PACS;computer vision;data mining;image retrieval;learning (artificial intelligence);medical image processing;radiology;recurrent neural nets;text analysis","3D medical volume;CNN;computer vision;data-hungry obstacle;deep convolutional neural network;document-level text;embedded vector label;extracted key image;interleaved text/image deep learning system;interleaved text/image deep mining;large-scale radiology database;latent Dirichlet allocation;national research hospital;picture archiving and communication system;radiology image;recurrent neural net language model;retrieval manner;semantic interaction;semantic label;sentence description;sentence-level text;unsupervised learning;very large-scale medical image database","","3","","47","","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique","H. Greenspan; B. van Ginneken; R. M. Summers","Biomedical Image Computing Lab, Department of Biomedical Engineering, Faculty of Engineering, Tel-Aviv University, Tel-Aviv, Israel","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1153","1159","The papers in this special section focus on the technology and applications supported by deep learning. Deep learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications.","0278-0062;02780062","","10.1109/TMI.2016.2553401","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463094","","Artificial neural networks;Biomedical image processing;Computer vision;Data analysis;Machine learning;Special issues and sections","","","","22","","38","","","","May 2016","","IEEE","IEEE Journals & Magazines"
"Exploring deep features from brain tumor magnetic resonance images via transfer learning","Renhao Liu; L. O. Hall; D. B. Goldgof; Mu Zhou; R. A. Gatenby; K. B. Ahmed","Department of Computer Science and Engineering, University of South Florida, Tampa, USA","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","235","242","Finding appropriate feature representations from radiological images is a vital task for prediction and diagnosis. Deep convolutional neural networks have recently achieved state-of-the-art performance in classification problems from several different domains. Research has also shown the feasibility of using a pre-trained deep neural network as a feature extractor when only a small dataset is available. This paper proposes a novel image feature extraction method for predicting survival time from brain tumor magnetic resonance images using pretrained deep neural networks. Since all tumors are different sizes, we also explore different image resizing methods in the paper. We demonstrate that deep features can result in better survival time prediction with the highest accuracy of 95.45% versus conventional feature extraction methods from magnetic resonance images of the brain.","","","10.1109/IJCNN.2016.7727204","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727204","","Histograms;Magnetic resonance imaging","biomedical MRI;brain;feature extraction;image representation;learning (artificial intelligence);medical image processing;neural nets;tumours","brain tumor magnetic resonance images;deep features;feature representations;image resizing methods;novel image feature extraction method;pretrained deep convolutional neural networks;survival time prediction;transfer learning","","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Thorax disease diagnosis using deep convolutional neural network","J. Chen; X. Qi; O. Tervonen; O. Silvén; G. Zhao; M. Pietikäinen","University of Oulu, Finland","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20161018","2016","","","2287","2290","Computer aided diagnosis (CAD) is an important issue, which can significantly improve the efficiency of doctors. In this paper, we propose a deep convolutional neural network (CNN) based method for thorax disease diagnosis. We firstly align the images by matching the interest points between the images, and then enlarge the dataset by using Gaussian scale space theory. After that we use the enlarged dataset to train a deep CNN model and apply the obtained model for the diagnosis of new test data. Our experimental results show our method achieves very promising results.","1557-170X;1557170X","Electronic:978-1-4577-0220-4; POD:978-1-4577-0219-8","10.1109/EMBC.2016.7591186","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591186","","Diseases;Filtering theory;Machine learning;Radiography;Solid modeling;Thorax;Training","Gaussian processes;diagnostic radiography;diseases;image matching;medical image processing;neural nets;radiology","CAD;Gaussian scale space theory;computer aided diagnosis;deep CNN model;deep convolutional neural network;image alignment;image matching;radiograph;radiology;thorax disease diagnosis","","","","","","","","16-20 Aug. 2016","","IEEE","IEEE Conference Publications"
"Correction to “Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning” [Jul 16 1505-1516]","G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen","Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Biomedical Engineering","20161220","2017","64","1","250","250","Presents corrections to the paper, ""Scalable high performance image registration framework by unsupervised deep feature representations"", (Wu, G. et al.), IEEE Trans. Biomed. Eng., vol. 63, no. 7, pp. 1505–1516, Jul. 2016.","0018-9294;00189294","","10.1109/TBME.2016.2633139","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792251","","Image registration;Machine learning;Scalability;Unsupervised learning","","","","","","","","","","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"ConvNet-Based Localization of Anatomical Structures in 3-D Medical Images","B. D. de Vos; J. M. Wolterink; P. A. de Jong; T. Leiner; M. A. Viergever; I. Išgum","Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands","IEEE Transactions on Medical Imaging","20170630","2017","36","7","1470","1481","Localization of anatomical structures is a prerequisite for many tasks in a medical image analysis. We propose a method for automatic localization of one or more anatomical structures in 3-D medical images through detection of their presence in 2-D image slices using a convolutional neural network (ConvNet). A single ConvNet is trained to detect the presence of the anatomical structure of interest in axial, coronal, and sagittal slices extracted from a 3-D image. To allow the ConvNet to analyze slices of different sizes, spatial pyramid pooling is applied. After detection, 3-D bounding boxes are created by combining the output of the ConvNet in all slices. In the experiments, 200 chest CT, 100 cardiac CT angiography (CTA), and 100 abdomen CT scans were used. The heart, ascending aorta, aortic arch, and descending aorta were localized in chest CT scans, the left cardiac ventricle in cardiac CTA scans, and the liver in abdomen CT scans. Localization was evaluated using the distances between automatically and manually defined reference bounding box centroids and walls. The best results were achieved in the localization of structures with clearly defined boundaries (e.g., aortic arch) and the worst when the structure boundary was not clearly visible (e.g., liver). The method was more robust and accurate in localization multiple structures.","0278-0062;02780062","","10.1109/TMI.2017.2673121","10.13039/100007065 - NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research; 10.13039/501100003958 - Netherlands Organization for Scientific Research Foundation for Technology Sciences Project 12726; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862905","CT;Localization;convolutional neural networks;deep learning;detection","Abdomen;Anatomical structure;Computed tomography;Heart;Three-dimensional displays;Two dimensional displays","angiocardiography;computerised tomography;feature extraction;liver;medical image processing;neural nets;stereo image processing","3D medical images;ConvNet-based localization;abdomen CT scans;anatomical structure;aortic arch;ascending aorta;cardiac CT angiography;chest CT;convolutional neural network;descending aorta;heart;left cardiac ventricle;liver","","","","","","","20170223","July 2017","","IEEE","IEEE Journals & Magazines"
"A comparison of deep learning and hand crafted features in medical image modality classification","S. Khan; S. P. Yong","Computer and Information Sciences Department, Universiti Teknologi PETRONAS, Malaysia","2016 3rd International Conference on Computer and Information Sciences (ICCOINS)","20161215","2016","","","633","638","Modality corresponding to medical images is a vital filter in medical image retrieval systems, as radiologists or physicians are interested in only one of radiology images e.g CT scan, MRI, X-ray. Various handcrafted feature schemes have been proposed for medical image modality classification. On the other hand not enough attempts have been made for deep learned feature extraction. A comparative evaluation of both handcrafted and deep learned features for medical image modality classification is presented in this paper. The experiments are performed on IMAGECLEF 2012 data. After carrying out the experiments it is shown that the handcrafted features outperforms the deep learned features and shows the potential of handcrafted feature extraction models in the medical image field.","","Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1","10.1109/ICCOINS.2016.7783289","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783289","Feature representations;deep learned features;handcrafted features;modality classification","Biomedical imaging;Computer architecture;Computers;Feature extraction;Machine learning;Visualization","feature extraction;image classification;image retrieval;learning (artificial intelligence);medical image processing;radiology","IMAGECLEF 2012 data;deep learned feature extraction;hand crafted features;handcrafted feature extraction models;medical image modality classification;medical image retrieval systems;radiology images","","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Cloud-based deep learning of big EEG data for epileptic seizure prediction","M. P. Hosseini; H. Soltanian-Zadeh; K. Elisevich; D. Pompili","Dept. of Electrical and Computer Engineering, Rutgers University-New Brunswick, NJ, USA","2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20170424","2016","","","1151","1155","Developing a Brain-Computer Interface (BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients.","","Electronic:978-1-5090-4545-7; POD:978-1-5090-4546-4; USB:978-1-5090-4544-0","10.1109/GlobalSIP.2016.7906022","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906022","Big Data;Brain-Computer Interface;Cloud Computing;Deep Learning;EEG;Epilepsy;Seizure Prediction","Big Data;Cloud computing;Electrodes;Electroencephalography;Epilepsy;Feature extraction;Real-time systems","Big Data;biomedical electrodes;brain-computer interfaces;cloud computing;data analysis;data reduction;electroencephalography;feature extraction;medical signal processing;signal classification;unsupervised learning","Big EEG Data analysis;benchmark clinical dataset;brain data recording;brain-computer interface;classification accuracy;cloud-based BCI system;cloud-based deep learning approach;cloud-computing;communication bandwidth;dimensionality-reduction technique;epileptic patients;epileptic seizure prediction;implanted electrodes;manual feature extraction;nonstationary EEG signals;stacked autoencoder;unsupervised feature extraction","","","","","","","","7-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Scalable High-Performance Image Registration Framework by Unsupervised Deep Feature Representations Learning","G. Wu; M. Kim; Q. Wang; B. C. Munsell; D. Shen","Department of Radiology and BRIC, The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Biomedical Engineering","20160621","2016","63","7","1505","1516","Feature selection is a critical step in deformable image registration. In particular, selecting the most discriminative features that accurately and concisely describe complex morphological patterns in image patches improves correspondence detection, which in turn improves image registration accuracy. Furthermore, since more and more imaging modalities are being invented to better identify morphological changes in medical imaging data, the development of deformable image registration method that scales well to new image modalities or new image applications with little to no human intervention would have a significant impact on the medical image analysis community. To address these concerns, a learning-based image registration framework is proposed that uses deep learning to discover compact and highly discriminative features upon observed imaging data. Specifically, the proposed feature selection method uses a convolutional stacked autoencoder to identify intrinsic deep feature representations in image patches. Since deep learning is an unsupervised learning method, no ground truth label knowledge is required. This makes the proposed feature selection method more flexible to new imaging modalities since feature representations can be directly learned from the observed imaging data in a very short amount of time. Using the LONI and ADNI imaging datasets, image registration performance was compared to two existing state-of-the-art deformable image registration methods that use handcrafted features. To demonstrate the scalability of the proposed image registration framework, image registration experiments were conducted on 7.0-T brain MR images. In all experiments, the results showed that the new image registration framework consistently demonstrated more accurate registration results when compared to state of the art.","0018-9294;00189294","","10.1109/TBME.2015.2496253","10.13039/100000071 - National Institute of Child Health and Human Development; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314894","Deep learning;Deformable image registration;deep learning;deformable image registration;hierarchical feature representation","Biomedical imaging;Feature extraction;Image registration;Machine learning;Three-dimensional displays;Unsupervised learning","","","","3","","70","","","20151102","July 2016","","IEEE","IEEE Journals & Magazines"
"The importance of stain normalization in colorectal tissue classification with convolutional networks","F. Ciompi; O. Geessink; B. E. Bejnordi; G. S. de Souza; A. Baidoshvili; G. Litjens; B. van Ginneken; I. Nagtegaal; J. van der Laak","Dept. of Pathology, Radboud University Medical Center, Nijmegen, Netherlands","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","160","163","The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950492","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950492","Colorectal Cancer;Deep learning;Digital pathology","Algorithm design and analysis;Biomarkers;Blood;Cancer;Image color analysis;Training;Tumors","biological tissues;cancer;image classification;medical image processing;neural nets","ConvNets;colorectal tissue classification;convolutional networks;hematoxylin-eosin stained histopathology images;imaging biomarkers;stain normalization","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network","M. Anthimopoulos; S. Christodoulidis; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1207","1216","Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 × 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance ( ~ 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.","0278-0062;02780062","","10.1109/TMI.2016.2535865","Bern University hospital Inselspital; Swiss National Science Foundation SNSF; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422082","Convolutional neural networks;interstitial lung diseases;texture classification","Computed tomography;Convolution;Design automation;Diseases;Feature extraction;Lungs;Neural networks","biological tissues;computerised tomography;convolution;diseases;feature extraction;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","CT volume scans;ILD pattern classification;automated tissue characterization;computer aided diagnosis system;computer vision problems;consolidation;deep convolutional neural network;deep learning techniques;feature maps;ground glass opacity;honeycombing;interstitial lung diseases;lung pattern classification;medical image analysis;micronodules;reticulation","","17","","42","","","20160229","May 2016","","IEEE","IEEE Journals & Magazines"
"Investigation of transfer learning on pulmonary nodule characteristics","A. Kaya; A. S. Keçeli; A. B. Can","Hacettepe &#x00DC;niversitesi, Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Ankara, T&#x00FC;rkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","20170629","2017","","","1","4","Studies on the classification of small pulmonary nodules generally focus on the prediction of malignancy of the nodule. In the recent years, publicly available databases provided different types of data to researchers, such as nodule characteristics, apart from the lung image and malignancy degree. In this paper, a study on the classification of pulmonary nodule characteristics using conventional features and deep features obtained from transfer learning method has been proposed. The results were assessed by sensitivity, specificity, and classification accuracy. The results of the study can be used to form multi-level classifiers in predicting malignancy by combining different types of features.","","Electronic:978-1-5090-6494-6; POD:978-1-5090-6495-3","10.1109/SIU.2017.7960357","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960357","malignancy prediction;nodule characteristics;pulmonary nodules;tansfer learning","Biomedical imaging;Computed tomography;Image databases;Lungs;Machine learning;Radio frequency;Support vector machines","cancer;image classification;learning (artificial intelligence);lung;medical image processing;radiology","conventional features;deep features;lung image;malignancy prediction;pulmonary nodule characteristics classification;transfer learning","","","","","","","","15-18 May 2017","","IEEE","IEEE Conference Publications"
"Multisource Transfer Learning With Convolutional Neural Networks for Lung Pattern Analysis","S. Christodoulidis; M. Anthimopoulos; L. Ebner; A. Christe; S. Mougiakakou","ARTORG Center for Biomedical Engineering Research, University of Bern, Bern, Switzerland","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","76","84","Early diagnosis of interstitial lung diseases is crucial for their treatment, but even experienced physicians find it difficult, as their clinical manifestations are similar. In order to assist with the diagnosis, computer-aided diagnosis systems have been developed. These commonly rely on a fixed scale classifier that scans CT images, recognizes textural lung patterns, and generates a map of pathologies. In a previous study, we proposed a method for classifying lung tissue patterns using a deep convolutional neural network (CNN), with an architecture designed for the specific problem. In this study, we present an improved method for training the proposed network by transferring knowledge from the similar domain of general texture classification. Six publicly available texture databases are used to pretrain networks with the proposed architecture, which are then fine-tuned on the lung tissue data. The resulting CNNs are combined in an ensemble and their fused knowledge is compressed back to a network with the original architecture. The proposed approach resulted in an absolute increase of about 2% in the performance of the proposed CNN. The results demonstrate the potential of transfer learning in the field of medical image analysis, indicate the textural nature of the problem and show that the method used for training a network can be as important as designing its architecture.","2168-2194;21682194","","10.1109/JBHI.2016.2636929","Bern University Hospital; 10.13039/501100001711 - Swiss National Science Foundation (SNSF); ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776792","Convolutional neural networks (CNNs);interstitial lung diseases (ILDs);knowledge distillation;model compression;model ensemble;texture classification;transfer learning","Biomedical imaging;Computed tomography;Databases;Knowledge engineering;Lungs;Machine learning;Training","biological tissues;computerised tomography;diseases;image classification;image texture;learning (artificial intelligence);lung;medical image processing;neural nets","CT images;computed tomography;computer-aided diagnosis;convolutional neural networks;fused knowledge compression;interstitial lung disease diagnosis;lung pattern analysis;lung tissue data;medical image analysis;multisource transfer learning;texture classification;texture databases","","","","","","","20161207","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Segmentation label propagation using deep convolutional neural networks and dense conditional random field","M. Gao; Z. Xu; L. Lu; A. Wu; I. Nogues; R. M. Summers; D. J. Mollura","Department of Radiology and Imaging Sciences, National Institutes of Health, Bethesda, MD 20892","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1265","1268","Availability and accessibility of large-scale annotated medical image datasets play an essential role in robust supervised learning of medical image analysis. Missed labeling of regions of interest is a common issue on existing medical image datasets due to the labor intensive nature of the annotation task which requires high levels of clinical proficiency. In this paper, we present a segmentation based label propagation method to a publicly available dataset on interstitial lung disease [3], to address the missing annotation challenge. Upon validation from an expert radiologist, the amount of available annotated training data is largely increased. Such a dataset expansion can can potentially increase the accuracy of Computer-aided Detection (CAD) systems. The proposed constrained segmentation propagation algorithm combines the cues from the initial annotations, deep convolutional neural networks and a dense fully-connected Conditional Random Field (CRF) that achieves high quantitative accuracy levels.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493497","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493497","Convolutional Neural Network;Dense Conditional Random Field;Interstitial Lung Disease;Multi-class Labeling;Segmentation Label Propagation","Biomedical imaging;Computed tomography;Image segmentation;Labeling;Lungs;Message passing;Neural networks","diseases;image segmentation;interstitials;learning (artificial intelligence);lung;medical image processing;neurophysiology","CAD systems;annotation task;computer-aided detection systems;dataset expansion;deep convolutional neural networks;dense conditional random field;dense fully-connected conditional Random field;high quantitative accuracy levels;interstitial lung disease;labor intensive nature;large-scale annotated medical image datasets;medical image analysis;medical image datasets;missing annotation challenge;regions of interest;robust supervised learning;segmentation based label propagation method;segmentation label propagation;segmentation propagation algorithm","","2","","11","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Deep random forest-based learning transfer to SVM for brain tumor segmentation","S. Amiri; I. Rekik; M. A. Mahjoub","SAGE laboratory, Higher Institute of Computer Science and Communication Techniques of Hammam sousse, Tunisia","2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)","20160728","2016","","","297","302","Using neuroimaging techniques to diagnose brain tumors and detect both visible and invisible cancer cells infiltration boundaries motivated the emergence of diverse tumor segmentation algorithms. Noting the large variability in both tumor appearance and shape, the task of automatic segmentation becomes more difficult. In this paper, we propose a random-forest (RF) based learning transfer to SVM classifier method for segmenting tumor lesions while capturing their complex characteristics. Our framework is composed of two cascaded stages. In the first stage, we train a random forest to learn the mapping from the image space to the tumor label space. In the testing stage, we use the predicted label output from the random forest and feed it along with the testing intensity image to an SVM classifier to get the refined segmentation. Then we make our RF-SVM cascaded classification steps deep through an iterative process. We tested our method on 20 patients with high-grade gliomas from the Brain Tumor Image Segmentation Challenge (BRATS) dataset. Our proposed framework significantly outperformed SVM-based segmentation and RF-based segmentation-when used solely.","","Electronic:978-1-4673-8526-8; POD:978-1-4673-8527-5","10.1109/ATSIP.2016.7523095","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523095","Brain tumor;MRI;Random Forest;SVM;Segmentation","Image segmentation;Lesions;Magnetic resonance imaging;Radio frequency;Support vector machines;Vegetation","brain;cancer;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neurophysiology;support vector machines;tumours","BRATS;RF-SVM cascaded classification;brain tumor diagnosis;brain tumor image segmentation;cancer cells infiltration boundaries;deep random forest-based learning transfer;iterative process;neuroimaging techniques","","","","","","","","21-23 March 2016","","IEEE","IEEE Conference Publications"
"Deep feature learning for pulmonary nodule classification in a lung CT","B. C. Kim; Y. S. Sung; H. I. Suk","Department of Brain and Cognitive Engineering, Korea University, Republic of Korea","2016 4th International Winter Conference on Brain-Computer Interface (BCI)","20160421","2016","","","1","3","In this paper, we propose a novel method of identifying pulmonary nodules in a lung CT. Specifically, we devise a deep neural network by which we extract abstract information inherent in raw hand-crafted imaging features. We then combine the deep learned representations with the original raw imaging features into a long feature vector. By taking the combined feature vectors, we train a classifier, preceded by a feature selection via t-test. To validate the effectiveness of the proposed method, we performed experiments on our in-house dataset of 20 subjects; 3,598 pulmonary nodules (malignant: 178, benign: 3,420), which were manually segmented by a radiologist. In our experiments, we achieved the maximal accuracy of 95.5%, sensitivity of 94.4%, and AUC of 0.987, outperforming the competing method.","","Electronic:978-1-4673-7842-0; POD:978-1-4673-7843-7","10.1109/IWW-BCI.2016.7457462","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457462","Deep learning;Lung cancer;Pulmonary nodule classification;Stacked denoising autoencoder","Cancer;Computed tomography;Feature extraction;Lungs;Noise reduction;Training","computerised tomography;feature extraction;feature selection;image classification;learning (artificial intelligence);lung;medical image processing;neural nets","abstract information extraction;classifier training;deep feature learning;deep learned representations;deep neural network;feature selection;feature vector;hand-crafted imaging features;lung CT;pulmonary nodule classification;pulmonary nodule identification;raw imaging features;t-test","","","","13","","","","22-24 Feb. 2016","","IEEE","IEEE Conference Publications"
"Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks","Q. Dou; H. Chen; L. Yu; L. Zhao; J. Qin; D. Wang; V. C. Mok; L. Shi; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong, HK, China","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1182","1195","Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.","0278-0062;02780062","","10.1109/TMI.2016.2528129","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403984","3D convolutional neural networks;biomarker detection;cerebral microbleeds;deep learning;susceptibility-weighted imaging","Biomarkers;Feature extraction;Kernel;MIMICs;Medical diagnostic imaging;Three-dimensional displays","biomedical MRI;blood;blood vessels;brain;cognition;diseases;feature extraction;haemodynamics;medical image processing;neurophysiology;probability","3D FCN strategy;3D convolutional neural networks;3D fully convolutional network strategy;CMB detection;MR volume extraction;MRI;automatic cerebral microbleed detection;blood vessels;cerebrovascular diseases;cognitive dysfunctions;current clinical routine;diagnostic biomarkers;haemorrhages;low-level hand-crafted descriptors;magnetic resonance images;massive redundant computations;probabilities;radiologists;representative high-level features;spatial contextual information;traditional sliding window strategy;well-trained 3D CNN discrimination","","15","","52","","","20160211","May 2016","","IEEE","IEEE Journals & Magazines"
"Brain tumor grading based on Neural Networks and Convolutional Neural Networks","Y. Pan; W. Huang; Z. Lin; W. Zhu; J. Zhou; J. Wong; Z. Ding","School of EEE, Nanyang Technological University, Singapore","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","699","702","This paper studies brain tumor grading using multiphase MRI images and compares the results with various configurations of deep learning structure and baseline Neural Networks. The MRI images are used directly into the learning machine, with some combination operations between multiphase MRIs. Compared to other researches, which involve additional effort to design and choose feature sets, the approach used in this paper leverages the learning capability of deep learning machine. We present the grading performance on the testing data measured by the sensitivity and specificity. The results show a maximum improvement of 18% on grading performance of Convolutional Neural Networks based on sensitivity and specificity compared to Neural Networks. We also visualize the kernels trained in different layers and display some self-learned features obtained from Convolutional Neural Networks.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318458","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318458","","Artificial neural networks;Biological neural networks;Image segmentation;Kernel;Sensitivity and specificity;Training;Tumors","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;tumours","baseline neural networks;brain tumor grading;convolutional neural networks;deep learning machine;deep learning structure;grading performance;multiphase MRI imaging;self-learned features;testing data","","","","8","","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"A new NMF-autoencoder based CAD system for early diagnosis of prostate cancer","I. Reda; A. Shalaby; M. A. El-Ghar; F. Khalifa; M. Elmogy; A. Aboulfotouh; E. Hosseini-Asl; A. El-Baz; R. Keynton","Faculty of Computers and Information, Mansoura University, Mansoura 35516, Egypt","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1237","1240","In this paper, we propose a novel non-invasive framework for the early diagnosis of prostate cancer from diffusion-weighted magnetic reasoning imaging (DW-MRI). The proposed approach consists of three main steps. In the first step, the prostate is localized and segmented based on a new level-set model. This model is guided by a stochastic speed function that is derived using nonnegative matrix factorization (NMF). The NMF attributes are calculated using information from the MRI intensity, a probabilistic shape model, and the spatial interactions between prostate voxels. In the second step, the apparent diffusion coefficient (ADC) of the segmented prostate volume is mathematically calculated for different b-values. To preserve continuity, the calculated ADC values are normalized and refined using a Generalized Gauss-Markov Random Field (GGMRF) image model. The cumulative distribution function (CDF) of refined ADC for the prostate tissues at different b-values are then constructed. These CDFs are considered as global features which can be used to distinguish between benign and malignant tumors. Finally, a deep learning auto-encoder network, trained by a non-negativity constraint algorithm (NCAE), is used to classify the prostate tumor as benign or malignant based on the CDFs extracted from the previous step. Preliminary experiments on 42 clinical DW-MRI data sets resulted in 97.6% correct classification (sensitivity = 100% and specificity = 95.24%), indicating the high accuracy of the proposed framework.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493490","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493490","CAD;MGRF;NMF;Prostate cancer","Design automation;Image segmentation;Magnetic resonance imaging;Prostate cancer;Shape;Solid modeling","Markov processes;biodiffusion;biomedical MRI;cancer;feature extraction;image classification;image coding;image segmentation;learning (artificial intelligence);matrix decomposition;medical image processing;probability;random processes;set theory;tumours","DW-MRI;MRI intensity;NMF-autoencoder based CAD system;apparent diffusion coefficient;benign tumor;cumulative distribution function;deep learning autoencoder network;diffusion-weighted magnetic reasoning imaging;generalized Gauss-Markov random field image model;level-set model;malignant tumor;noninvasive framework;nonnegative matrix factorization;nonnegativity constraint algorithm;probabilistic shape model;prostate cancer early diagnosis;prostate tissues;prostate tumor classification;prostate voxels;segmented prostate volume;spatial interactions;stochastic speed function","","2","","21","","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"DeepPap: Deep Convolutional Networks for Cervical Cell Classification","L. Zhang; L. Lu; I. Nogues; R. Summers; S. Liu; J. Yao","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory and also with the Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892 USA.(email:ling.zhang3@nih.gov)","IEEE Journal of Biomedical and Health Informatics","","2017","PP","99","1","1","Automation-assisted cervical screening via Pap smear or liquid-based cytology (LBC) is a highly effective cell imaging based cancer detection tool, where cells are partitioned into ”abnormal” and ”normal” categories. However, the success of most traditional classification methods relies on the presence of accurate cell segmentations. Despite sixty years of research in this field, accurate segmentation remains a challenge in the presence of cell clusters and pathologies. Moreover, previous classification methods are only built upon the extraction of hand-crafted features, such as morphology and texture. This paper addresses these limitations by proposing a method to directly classify cervical cells – without prior segmentation – based on deep features, using convolutional neural networks (ConvNets). First, the ConvNet is pre-trained on a natural image dataset. It is subsequently fine-tuned on a cervical cell dataset consisting of adaptively re-sampled image patches coarsely centered on the nuclei. In the testing phase, aggregation is used to average the prediction scores of a similar set of image patches. The proposed method is evaluated on both Pap smear and LBC datasets. Results show that our method outperforms previous algorithms in classification accuracy (98.3%), area under the curve (AUC) (0.99) values, and especially specificity (98.3%), when applied to the Herlev benchmark Pap smear dataset and evaluated using five-fold cross-validation. Similar superior performances are also achieved on the HEMLBC (H&E stained manual LBC) dataset. Our method is promising for the development of automation-assisted reading systems in primary cervical screening.","2168-2194;21682194","","10.1109/JBHI.2017.2705583","10.13039/501100001809 - National Natural Science Foundation of China; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932065","Cell classification;Cervical cytology;Deep learning;Neural networks;Pap smear","Feature extraction;Image segmentation;Imaging;Informatics;Neural networks;Testing;Training","","","","","","","","","20170519","","","IEEE","IEEE Early Access Articles"
"A Robust Deep Model for Improved Classification of AD/MCI Patients","F. Li; L. Tran; K. H. Thung; S. Ji; D. Shen; J. Li","Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2015","19","5","1610","1616","Accurate classification of Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), plays a critical role in possibly preventing progression of memory impairment and improving quality of life for AD patients. Among many research tasks, it is of a particular interest to identify noninvasive imaging biomarkers for AD diagnosis. In this paper, we present a robust deep learning system to identify different progression stages of AD patients based on MRI and PET scans. We utilized the dropout technique to improve classical deep learning by preventing its weight coadaptation, which is a typical cause of overfitting in deep learning. In addition, we incorporated stability selection, an adaptive learning factor, and a multitask learning strategy into the deep learning framework. We applied the proposed method to the ADNI dataset, and conducted experiments for AD and MCI conversion diagnosis. Experimental results showed that the dropout technique is very effective in AD diagnosis, improving the classification accuracies by 5.9% on average as compared to the classical deep learning methods.","2168-2194;21682194","","10.1109/JBHI.2015.2429556","10.13039/501100001677 - NIH grants; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101222","Alzheimer’s Disease;Alzheimer's disease (AD);Deep Learning;Early Diagnosis;MRI;PET;deep learning;early diagnosis;magnetic resonance imaging (MRI);positron emission tomography (PET)","Computational modeling;Feature extraction;Magnetic resonance imaging;Positron emission tomography;Principal component analysis;Support vector machines;Training","biomedical MRI;cognition;diseases;learning (artificial intelligence);medical computing;neurophysiology;positron emission tomography","AD conversion diagnosis;AD diagnosis;AD patients;ADNI dataset;Alzheimer's disease;MCI conversion diagnosis;MCI patients;MRI;PET scans;adaptive learning factor;classical deep learning method;classification accuracy;deep learning framework;dropout technique;improved classification;memory impairment;mild cognitive impairment;multitask learning strategy;noninvasive imaging biomarkers;prodromal stage;progression stages;quality of life;robust deep learning system;stability selection;weight coadaptation","0;Alzheimer Disease;Early Diagnosis;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Mild Cognitive Impairment;Models, Theoretical;Positron-Emission Tomography;Principal Component Analysis;Support Vector Machine","11","","29","","","20150504","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Detecting Anatomical Landmarks From Limited Medical Imaging Data Using Two-Stage Task-Oriented Deep Neural Networks","J. Zhang; M. Liu; D. Shen","Department of Radiology and the Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Image Processing","20170718","2017","26","10","4753","4764","One of the major challenges in anatomical landmark detection, based on deep neural networks, is the limited availability of medical imaging data for network learning. To address this problem, we present a two-stage task-oriented deep learning method to detect large-scale anatomical landmarks simultaneously in real time, using limited training data. Specifically, our method consists of two deep convolutional neural networks (CNN), with each focusing on one specific task. Specifically, to alleviate the problem of limited training data, in the first stage, we propose a CNN based regression model using millions of image patches as input, aiming to learn inherent associations between local image patches and target anatomical landmarks. To further model the correlations among image patches, in the second stage, we develop another CNN model, which includes a) a fully convolutional network that shares the same architecture and network weights as the CNN used in the first stage and also b) several extra layers to jointly predict coordinates of multiple anatomical landmarks. Importantly, our method can jointly detect large-scale (e.g., thousands of) landmarks in real time. We have conducted various experiments for detecting 1200 brain landmarks from the 3D T1-weighted magnetic resonance images of 700 subjects, and also 7 prostate landmarks from the 3D computed tomography images of 73 subjects. The experimental results show the effectiveness of our method regarding both accuracy and efficiency in the anatomical landmark detection.","1057-7149;10577149","","10.1109/TIP.2017.2721106","10.13039/100000002 - NIH; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961205","Anatomical landmark detection;deep convolutional neural networks;limited medical imaging data;real-time;task-oriented","Biological neural networks;Biomedical imaging;Machine learning;Testing;Three-dimensional displays;Training;Training data","biomedical MRI;computerised tomography;feedforward neural nets;learning (artificial intelligence);medical image processing;object detection;regression analysis","3D T1-weighted magnetic resonance images;3D computed tomography images;CNN based regression model;anatomical landmark coordinate prediction;anatomical landmark detection;brain landmarks;deep convolutional neural networks;fully convolutional network;local image patches;medical imaging data;network learning;prostate landmarks;training data;two-stage task-oriented deep learning method;two-stage task-oriented deep neural networks","","","","","","","20170628","Oct. 2017","","IEEE","IEEE Journals & Magazines"
"Convolutional neural networks for predicting molecular profiles of non-small cell lung cancer","D. Yu; M. Zhou; F. Yang; D. Dong; O. Gevaert; Z. Liu; J. Shi; J. Tian","The Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, China","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","20170619","2017","","","569","572","Quantitative imaging biomarkers identification has become a powerful tool for predictive diagnosis given increasingly available clinical imaging data. In parallel, molecular profiles have been well documented in non-small cell lung cancers (NSCLCs). However, there has been limited studies on leveraging the two major sources for improving lung cancer computer-aided diagnosis. In this paper, we investigate the problem of predicting molecular profiles with CT imaging arrays in NSCLC. In particular, we formulate a discriminative convolutional neural network to learn deep features for predicting epidermal growth factor receptor (EGFR) mutation states that are associated with cancer cell growth. We evaluated our approach on two independent datasets including a discovery set with 595 patients (Datset1) and a validation set with 89 patients (Dataset2). Extensive experimental results demonstrated that the learned CNN-based features are effective in predicting EGFR mutation states (AUC=0.828, ACC=76.16%) on Dataset1, and it further demonstrated generalized predictive performance (AUC=0.668, ACC=67.55%) on Dataset2.","","Electronic:978-1-5090-1172-8; POD:978-1-5090-1173-5","10.1109/ISBI.2017.7950585","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950585","Computed tomography;Computed-aided diagnosis;Convolutional neural networks;Non-Small Cell Lung Carcinoma","Cancer;Computed tomography;Convolution;Feature extraction;Lungs;Neural networks","cancer;cellular biophysics;computerised tomography;feature extraction;lung;medical image processing;molecular biophysics;neural nets;proteins","CT imaging arrays;EGFR mutation state prediction;biomarker identification;cancer cell growth;clinical imaging data;discriminative convolutional neural network;epidermal growth factor receptor;learned CNN-based feature;lung cancer computer-aided diagnosis;molecular profile;nonsmall cell lung cancer;quantitative imaging","","","","","","","","18-21 April 2017","","IEEE","IEEE Conference Publications"
"Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks","A. A. A. Setio; F. Ciompi; G. Litjens; P. Gerke; C. Jacobs; S. J. van Riel; M. M. W. Wille; M. Naqibullah; C. I. Sánchez; B. van Ginneken","Diagnostic Image Analysis Group at the Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1160","1169","We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.","0278-0062;02780062","","10.1109/TMI.2016.2536809","The Netherlands Organization for Scientific Research; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422783","Computed tomography;computer-aided detection;convolutional networks;deep learning;lung cancer;pulmonary nodule","Cancer;Computed tomography;Design automation;Feature extraction;Lesions;Lungs;Solids","cancer;computerised tomography;feature extraction;image classification;image fusion;medical image processing;tumours","2D ConvNets;2D patches;ANODE09 challenge;CAD system;CT images;computer-aided detection system;data augmentation;dedicated fusion method;differently oriented planes;discriminative features;false positive reduction;final classification;multiple streams;multiview ConvNets;multiview convolutional networks;nodule candidates;publicly available LIDC-IDRI dataset;pulmonary nodule detection;training data","","10","1","47","","","20160301","May 2016","","IEEE","IEEE Journals & Magazines"
"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel","Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA","IEEE Journal of Biomedical and Health Informatics","20170520","2017","21","1","48","55","Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","2168-2194;21682194","","10.1109/JBHI.2016.2631401","European Regions Development; FIS; Marie-Curie Actions Program of the European Union; 10.13039/100000002 - NIH; 10.13039/100007065 - NVIDIA; 10.13039/501100000783 - REA; 10.13039/501100003741 - ICREA; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798","Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound","Atherosclerosis;Feature extraction;Imaging;Lipidomics;Machine learning;Neural networks;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","calcified tissue;cardiovascular events;carotid plaque composition;carotid ultrasound;cerebrovascular events;convolutional neural network;deep learning framework;fibrous cap;fibrous tissue;image noise;imaging features;lipid core;plaque constituents;thresholding values","","","","","","","20161122","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"A Bottom-Up Approach for Pancreas Segmentation Using Cascaded Superpixels and (Deep) Image Patch Labeling","A. Farag; L. Lu; H. R. Roth; J. Liu; E. Turkbey; R. M. Summers","Department of Radiology and Imaging Sciences, Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Image Processing","20161124","2017","26","1","386","399","Robust organ segmentation is a prerequisite for computer-aided diagnosis, quantitative imaging analysis, pathology detection, and surgical assistance. For organs with high anatomical variability (e.g., the pancreas), previous segmentation approaches report low accuracies, compared with well-studied organs, such as the liver or heart. We present an automated bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans. The method generates a hierarchical cascade of information propagation by classifying image patches at different resolutions and cascading (segments) superpixels. The system contains four steps: 1) decomposition of CT slice images into a set of disjoint boundary-preserving superpixels; 2) computation of pancreas class probability maps via dense patch labeling; 3) superpixel classification by pooling both intensity and probability features to form empirical statistics in cascaded random forest frameworks; and 4) simple connectivity based post-processing. Dense image patch labeling is conducted using two methods: efficient random forest classification on image histogram, location and texture features; and more expensive (but more accurate) deep convolutional neural network classification, on larger image windows (i.e., with more spatial contexts). Over-segmented 2-D CT slices by the simple linear iterative clustering approach are adopted through model/parameter calibration and labeled at the superpixel level for positive (pancreas) or negative (non-pancreas or background) classes. The proposed method is evaluated on a data set of 80 manually segmented CT volumes, using six-fold cross-validation. Its performance equals or surpasses other state-of-the-art methods (evaluated by “leave-one-patient-out”), with a dice coefficient of 70.7% and Jaccard index of 57.9%. In addition, the computational efficiency has improved significantly, requiring a - ere 6 ~ 8 min per testing case, versus ≥ 10 h for other methods. The segmentation framework using deep patch labeling confidences is also more numerically stable, as reflected in the smaller performance metric standard deviations. Finally, we implement a multi-atlas label fusion (MALF) approach for pancreas segmentation using the same data set. Under six-fold cross-validation, our bottom-up segmentation method significantly outperforms its MALF counterpart: 70.7±13.0% versus 52.51±20.84% in dice coefficients.","1057-7149;10577149","","10.1109/TIP.2016.2624198","10.13039/100000098 - Intramural Research Program of the National Institutes of Health Clinical Center; ","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727966","Abdominal computed tomography (CT);cascaded random forest;deep convolutional neural networks;dense image patch labeling;pancreas segmentation","Computed tomography;Image segmentation;Labeling;Liver;Pancreas;Shape","biological organs;computerised tomography;image segmentation;iterative methods;learning (artificial intelligence);medical image processing;neural nets;probability","CT slice image decomposition;abdominal computed tomography scans;bottom-up approach;cascaded superpixels;computer-aided diagnosis;deep convolutional neural network classification;deep image patch labeling;dense patch labeling;disjoint boundary-preserving superpixels;image histogram;image location;linear iterative clustering approach;model-parameter calibration;multiatlas label fusion approach;pancreas class probability maps;pancreas segmentation;pathology detection;probability features;quantitative imaging analysis;random forest classification;superpixel classification;surgical assistance;texture features","","","","","","","20161101","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Fast Convolutional Neural Network Training Using Selective Data Sampling: Application to Hemorrhage Detection in Color Fundus Images","M. J. J. P. van Grinsven; B. van Ginneken; C. B. Hoyng; T. Theelen; C. I. Sánchez","Diagnostic Image Analysis Group, Department of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1273","1284","Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.","0278-0062;02780062","","10.1109/TMI.2016.2526689","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401052","Convolutional neural network;deep learning;hemorrhage;selective sampling","Biomedical imaging;Databases;Hemorrhaging;Image analysis;Image color analysis;Observers;Training","biomedical optical imaging;blood;computer vision;image classification;image colour analysis;image sampling;learning (artificial intelligence);medical image processing;neural nets;sensitivity analysis","CNN learning process;CNN training iteration;color fundus images;computer vision applications;deep learning network architectures;dynamically selecting misclassified negative samples;fast convolutional neural network training;hemorrhage detection;independent test set;medical image analysis tasks;receiver operating characteristics curve;selective data sampling;selective sampling method","","7","","48","","","20160208","May 2016","","IEEE","IEEE Journals & Magazines"
